# ICML Paper Experiments - Multi-Node Cluster Simulation
# =======================================================
#
# This docker-compose file creates a simulated multi-node cluster on a single A100 server
# for ICML paper experiments. Each "node" is a container with allocated GPU memory.
#
# Architecture:
#   - 1x sage-gateway (Control Plane + API Gateway)
#   - 4x vLLM backends (simulating 4 GPU nodes, using MPS or memory partitioning)
#   - 2x Embedding servers (CPU-based, simulating heterogeneous cluster)
#   - 1x benchmark client
#
# Usage:
#   # Start cluster with 4 vLLM backends
#   docker-compose -f experiments/icml_paper/docker-compose.cluster.yml up -d
#
#   # Scale to different number of backends
#   docker-compose -f experiments/icml_paper/docker-compose.cluster.yml up -d --scale vllm-backend=2
#
#   # Stop cluster
#   docker-compose -f experiments/icml_paper/docker-compose.cluster.yml down

version: '3.8'

# ==============================================================================
# Network Configuration
# ==============================================================================
networks:
  sage-cluster:
    driver: bridge
    ipam:
      config:
      - subnet: 172.28.0.0/16

# ==============================================================================
# Shared Volumes
# ==============================================================================
volumes:
  sage-models:
    name: sage-icml-models
  benchmark-results:
    name: sage-icml-results

# ==============================================================================
# Services
# ==============================================================================
services:

  # ============================================================================
  # SAGE Gateway (Control Plane + API)
  # ============================================================================
  sage-gateway:
    image: ghcr.io/intellistream/sage:latest
    build:
      context: ../..
      dockerfile: experiments/icml_paper/Dockerfile.sage
    container_name: sage-gateway
    hostname: gateway
    networks:
      sage-cluster:
        ipv4_address: 172.28.0.10
    ports:
    - "8888:8888"     # Gateway API (external access)
    environment:
      # Gateway configuration
    - SAGE_GATEWAY_HOST=0.0.0.0
    - SAGE_GATEWAY_PORT=8888
      # Control Plane configuration
    - SAGE_CP_SCHEDULING_POLICY=${SCHEDULING_POLICY:-hybrid}
    - SAGE_CP_LLM_BACKENDS=vllm-1:8001,vllm-2:8001,vllm-3:8001,vllm-4:8001
    - SAGE_CP_EMBEDDING_BACKENDS=embedding-1:8090,embedding-2:8090
      # Logging
    - SAGE_LOG_LEVEL=INFO
    volumes:
    - ./config:/app/config:ro
    - benchmark-results:/app/results
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8888/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    depends_on:
      vllm-1:
        condition: service_healthy
      embedding-1:
        condition: service_healthy
    restart: unless-stopped

  # ============================================================================
  # vLLM Backend Instances (GPU)
  # Using NVIDIA MPS or memory fraction to share single A100
  # ============================================================================
  vllm-1:
    image: vllm/vllm-openai:latest
    container_name: vllm-1
    hostname: vllm-1
    networks:
      sage-cluster:
        ipv4_address: 172.28.0.21
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']    # Use GPU 0
            capabilities: [gpu]
    environment:
    - CUDA_VISIBLE_DEVICES=0
    - VLLM_GPU_MEMORY_UTILIZATION=0.2    # 20% of GPU memory per instance
    command: >
      --model ${LLM_MODEL:-Qwen/Qwen2.5-7B-Instruct}
      --host 0.0.0.0
      --port 8001
      --gpu-memory-utilization 0.2
      --max-model-len 4096
      --dtype auto
      --trust-remote-code
    volumes:
    - sage-models:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s  # vLLM needs time to load model
    restart: unless-stopped

  vllm-2:
    image: vllm/vllm-openai:latest
    container_name: vllm-2
    hostname: vllm-2
    networks:
      sage-cluster:
        ipv4_address: 172.28.0.22
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
    environment:
    - CUDA_VISIBLE_DEVICES=0
    - VLLM_GPU_MEMORY_UTILIZATION=0.2
    command: >
      --model ${LLM_MODEL:-Qwen/Qwen2.5-7B-Instruct}
      --host 0.0.0.0
      --port 8001
      --gpu-memory-utilization 0.2
      --max-model-len 4096
      --dtype auto
      --trust-remote-code
    volumes:
    - sage-models:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s
    restart: unless-stopped

  vllm-3:
    image: vllm/vllm-openai:latest
    container_name: vllm-3
    hostname: vllm-3
    networks:
      sage-cluster:
        ipv4_address: 172.28.0.23
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
    environment:
    - CUDA_VISIBLE_DEVICES=0
    - VLLM_GPU_MEMORY_UTILIZATION=0.2
    command: >
      --model ${LLM_MODEL:-Qwen/Qwen2.5-7B-Instruct}
      --host 0.0.0.0
      --port 8001
      --gpu-memory-utilization 0.2
      --max-model-len 4096
      --dtype auto
      --trust-remote-code
    volumes:
    - sage-models:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s
    restart: unless-stopped

  vllm-4:
    image: vllm/vllm-openai:latest
    container_name: vllm-4
    hostname: vllm-4
    networks:
      sage-cluster:
        ipv4_address: 172.28.0.24
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
    environment:
    - CUDA_VISIBLE_DEVICES=0
    - VLLM_GPU_MEMORY_UTILIZATION=0.2
    command: >
      --model ${LLM_MODEL:-Qwen/Qwen2.5-7B-Instruct}
      --host 0.0.0.0
      --port 8001
      --gpu-memory-utilization 0.2
      --max-model-len 4096
      --dtype auto
      --trust-remote-code
    volumes:
    - sage-models:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s
    restart: unless-stopped

  # ============================================================================
  # Embedding Service Instances (CPU-based for heterogeneous cluster simulation)
  # ============================================================================
  embedding-1:
    image: ghcr.io/intellistream/sage:latest
    build:
      context: ../..
      dockerfile: experiments/icml_paper/Dockerfile.sage
    container_name: embedding-1
    hostname: embedding-1
    networks:
      sage-cluster:
        ipv4_address: 172.28.0.31
    environment:
    - EMBEDDING_MODEL=${EMBEDDING_MODEL:-BAAI/bge-m3}
    - EMBEDDING_PORT=8090
    - EMBEDDING_DEVICE=cpu    # CPU-based for heterogeneous simulation
    command: >
      python -m sage.common.components.sage_embedding.embedding_server
      --model ${EMBEDDING_MODEL:-BAAI/bge-m3}
      --host 0.0.0.0
      --port 8090
      --device cpu
    volumes:
    - sage-models:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8090/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  embedding-2:
    image: ghcr.io/intellistream/sage:latest
    build:
      context: ../..
      dockerfile: experiments/icml_paper/Dockerfile.sage
    container_name: embedding-2
    hostname: embedding-2
    networks:
      sage-cluster:
        ipv4_address: 172.28.0.32
    environment:
    - EMBEDDING_MODEL=${EMBEDDING_MODEL:-BAAI/bge-m3}
    - EMBEDDING_PORT=8090
    - EMBEDDING_DEVICE=cpu
    command: >
      python -m sage.common.components.sage_embedding.embedding_server
      --model ${EMBEDDING_MODEL:-BAAI/bge-m3}
      --host 0.0.0.0
      --port 8090
      --device cpu
    volumes:
    - sage-models:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8090/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # ============================================================================
  # Benchmark Client
  # ============================================================================
  benchmark-client:
    image: ghcr.io/intellistream/sage:latest
    build:
      context: ../..
      dockerfile: experiments/icml_paper/Dockerfile.sage
    container_name: benchmark-client
    hostname: benchmark
    networks:
      sage-cluster:
        ipv4_address: 172.28.0.100
    environment:
    - SAGE_GATEWAY_URL=http://sage-gateway:8888
    - BENCHMARK_OUTPUT_DIR=/app/results
    volumes:
    - ./scripts:/app/scripts:ro
    - benchmark-results:/app/results
    working_dir: /app
    # Keep container running for interactive experiments
    command: ["tail", "-f", "/dev/null"]
    depends_on:
      sage-gateway:
        condition: service_healthy
