import logging
import json
import time
from typing import Any, Dict, List

import yaml
from dotenv import load_dotenv
from sage.common.utils.config.loader import load_config
from sage.core.api.function.batch_function import BatchFunction
from sage.core.api.function.map_function import MapFunction
from sage.core.api.local_environment import LocalEnvironment
from sage.common.config.output_paths import get_output_file


def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    return config


class BatchFileSource(BatchFunction):
    """æ‰¹é‡æ–‡ä»¶æ•°æ®æºç®—å­ï¼Œè¯»å–æµ‹è¯•æ•°æ®å¹¶æ”¯æŒæ‰¹é‡å¤„ç†"""

    def __init__(self, config: dict):
        self.config = config
        self.data_path = config.get("data_path")
        self.max_samples = config.get("max_samples", None)
        data = self.load_test_data()
        if self.max_samples:
            data = data[: self.max_samples]
        # å…ˆåŠ è½½æ•°æ®ä»¥ç¡®å®šé»˜è®¤æ‰¹æ¬¡å¤§å°
        self.batch_size = config.get("batch_size", len(data))  # é»˜è®¤ä¸ºæ•´ä¸ªæ•°æ®é›†å¤§å°
        self.current_batch = 0  # å½“å‰æ‰¹æ¬¡ç´¢å¼•
        self.total_batches = (len(data) + self.batch_size - 1) // self.batch_size

    def load_test_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        data = []
        with open(self.data_path, "r", encoding="utf-8") as f:
            for line in f:
                data.append(json.loads(line))
        return data

    def execute(self):
        """å¤„ç†æ•°æ®æºï¼Œè¿”å›æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®"""
        data = self.load_test_data()

        # å¦‚æœå·²ç»å¤„ç†å®Œæ‰€æœ‰æ‰¹æ¬¡ï¼Œè¿”å› None è¡¨ç¤ºæ²¡æœ‰æ›´å¤šæ•°æ®
        if self.current_batch >= self.total_batches:
            return None

        # è·å–å½“å‰æ‰¹æ¬¡çš„æ•°æ®
        start_idx = self.current_batch * self.batch_size
        end_idx = min(start_idx + self.batch_size, len(data))
        batch = data[start_idx:end_idx]

        result = {
            "batch_data": batch,
            "batch_id": self.current_batch,
            "total_batches": self.total_batches,
        }

        # æ›´æ–°å½“å‰æ‰¹æ¬¡ç´¢å¼•
        self.current_batch += 1

        return result


class Generator(MapFunction):
    """å®éªŒç”Ÿæˆç®—å­ï¼Œç”Ÿæˆæ¨¡å‹å›ç­”"""

    def __init__(self, config: dict):
        self.config = config
        self.model_name = config.get("model_name", "Mistral-7B-Instruct-v0.1")
        self.use_context = config.get("use_context", True)  # æ§åˆ¶æ˜¯å¦ä½¿ç”¨æ£€ç´¢ä¸Šä¸‹æ–‡
        self.top_k = config.get("top_k", 3)

        from vllm import LLM, SamplingParams

        self.llm = LLM(model=self.model_name, gpu_memory_utilization=0.8)
        self.sampling_params = SamplingParams(temperature=0, max_tokens=100)

    def get_retrieved_docs_from_data(self, item: Dict[str, Any]) -> List[str]:
        """ä»æ•°æ®ä¸­æå–å·²æ£€ç´¢çš„æ–‡æ¡£"""
        retrieved_docs = []
        ctxs = item.get("ctxs", [])

        for ctx in ctxs[: self.top_k]:
            if "text" in ctx and ctx["text"].strip():
                retrieved_docs.append(ctx["text"])

        return retrieved_docs

    def build_prompt(self, question: str, context: str = None) -> str:
        """æ„å»ºæç¤ºè¯"""
        if self.use_context and context:
            if "llama" in self.model_name.lower():
                prompt = f"[INST]{context}\n{question}[/INST]"
            else:
                prompt = f"<s>[INST]{context}\n{question}[/INST]"
        else:
            if "llama" in self.model_name.lower():
                prompt = f"[INST]{question}[/INST]"
            else:
                prompt = f"### Instruction:\n{question}\n\n### Response:\n"

        return prompt

    def execute(self, batch_data: dict):
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
        batch = batch_data["batch_data"]
        prompts = []
        batch_with_context = []  # ä¿å­˜å¸¦æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ‰¹æ¬¡æ•°æ®

        # ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªé—®é¢˜æ„å»ºæç¤ºè¯
        for item in batch:
            question = item["question"]
            context = None
            retrieved_context = []

            if self.use_context:
                # ä½¿ç”¨æ•°æ®ä¸­å·²æœ‰çš„æ£€ç´¢æ–‡æ¡£
                retrieved_docs = self.get_retrieved_docs_from_data(item)
                if retrieved_docs:
                    # æ„å»ºevidence paragraph
                    evidences = []
                    ctxs = item.get("ctxs", [])[: self.top_k]
                    for i, ctx in enumerate(ctxs):
                        if "text" in ctx and ctx["text"].strip():
                            title = ctx.get("title", "")
                            evidence = f"[{i+1}] {title}\n{ctx['text']}"
                            evidences.append(evidence)
                            retrieved_context.append(ctx["text"])  # ä¿å­˜åŸå§‹ä¸Šä¸‹æ–‡æ–‡æœ¬
                    context = "\n".join(evidences)

            prompt = self.build_prompt(question, context)
            prompts.append(prompt)

            # åˆ›å»ºå¸¦æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„itemå‰¯æœ¬
            item_with_context = item.copy()
            if retrieved_context:
                item_with_context["retrieved_context"] = retrieved_context
            batch_with_context.append(item_with_context)

        # ä½¿ç”¨VLLM
        outputs = self.llm.generate(prompts, self.sampling_params)
        responses = [output.outputs[0].text for output in outputs]

        # è¿”å›å¸¦æœ‰åŸå§‹æ•°æ®ã€ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç”Ÿæˆå›ç­”çš„ç»“æœ
        return {
            "batch_data": batch_with_context,  # ä½¿ç”¨å¸¦æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ•°æ®
            "responses": responses,
            "batch_id": batch_data["batch_id"],
            "total_batches": batch_data["total_batches"],
        }


class PostProcessor(MapFunction):
    """åå¤„ç†ç®—å­ï¼Œå¤„ç†ç­”æ¡ˆå¹¶è¿›è¡Œé¢„æµ‹"""

    def __init__(self, config: dict):
        self.config = config
        self.extract_prediction = config.get("extract_prediction", True)

    def postprocess_model_output(self, generated_text: str) -> str:
        """ç»Ÿä¸€çš„æ¨¡å‹è¾“å‡ºåå¤„ç†å‡½æ•°"""
        # å–ç¬¬ä¸€æ®µå†…å®¹ï¼ˆé‡åˆ°åŒæ¢è¡Œç¬¦åœæ­¢ï¼‰
        processed_text = generated_text.split("\n\n")[0]

        # ç§»é™¤ç»“æŸæ ‡è®°
        processed_text = processed_text.replace("</s>", "")

        # ç§»é™¤å¼€å¤´çš„ç©ºæ ¼
        if len(processed_text) > 0 and processed_text[0] == " ":
            processed_text = processed_text[1:]

        return processed_text

    def execute(self, batch_result: dict):
        """å¤„ç†æ‰¹æ¬¡ç»“æœ"""
        batch_data = batch_result["batch_data"]
        responses = batch_result["responses"]

        processed_results = []

        for i, (item, response) in enumerate(zip(batch_data, responses)):
            # åå¤„ç†å›ç­”
            processed_response = self.postprocess_model_output(response)

            # æŒ‰ç…§æŒ‡å®šæ ¼å¼åˆ›å»ºç»“æœé¡¹
            result_item = {
                "id": item.get("id", i),
                "question": item["question"],
                "ground_truth": item.get("answers", []),
                "model_output": processed_response,
            }

            # å¦‚æœåœ¨Generatorä¸­å·²ç»ä¿å­˜äº†æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œç›´æ¥ä½¿ç”¨
            if "retrieved_context" in item:
                result_item["retrieved_context"] = item["retrieved_context"]

            processed_results.append(result_item)

        return {
            "results": processed_results,
            "batch_id": batch_result["batch_id"],
            "total_batches": batch_result["total_batches"],
        }


class Sink(MapFunction):
    """å®éªŒç»“æœä¿å­˜ç®—å­"""

    def __init__(self, config: dict):
        self.config = config
        default_output_path = get_output_file("experiment_results.json", "experiments")
        self.output_path = config.get("output_path", str(default_output_path))
        self.save_mode = config.get(
            "save_mode", "incremental"
        )  # 'incremental' æˆ– 'final'
        self.all_results = []

    def execute(self, processed_batch: dict):
        """å¤„ç†å¹¶ä¿å­˜å®éªŒç»“æœ"""
        batch_results = processed_batch["results"]
        batch_id = processed_batch["batch_id"]
        total_batches = processed_batch["total_batches"]

        # æ·»åŠ åˆ°æ€»ç»“æœä¸­
        self.all_results.extend(batch_results)

        logging.info(
            f"âœ… å·²å¤„ç†æ‰¹æ¬¡ {batch_id + 1}/{total_batches}ï¼Œå½“å‰æ€»ç»“æœæ•°: {len(self.all_results)}"
        )

        # æ ¹æ®ä¿å­˜æ¨¡å¼å†³å®šä½•æ—¶ä¿å­˜
        if self.save_mode == "incremental":
            self._save_results(batch_id + 1, total_batches)
        elif self.save_mode == "final" and batch_id + 1 == total_batches:
            self._save_results(batch_id + 1, total_batches)

    def _save_results(self, current_batch: int, total_batches: int):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        from datetime import datetime

        results = {
            "experiment_config": {
                "model_name": self.config.get("model_name", "unknown"),
                "use_context": self.config.get("use_context", True),
                "top_k": self.config.get("top_k", 5),
                "batch_size": self.config.get("batch_size", 10),
                "timestamp": datetime.now().isoformat(),
                "total_samples": len(self.all_results),
                "completed_batches": f"{current_batch}/{total_batches}",
            },
            "results": self.all_results,
        }

        with open(self.output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logging.info(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {self.output_path}")


def pipeline_run(config: dict) -> None:
    """
    åˆ›å»ºå¹¶è¿è¡Œå®éªŒæ•°æ®å¤„ç†ç®¡é“

    Args:
        config (dict): åŒ…å«å„æ¨¡å—é…ç½®çš„é…ç½®å­—å…¸ã€‚
    """
    env = LocalEnvironment()

    # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
    (
        env.from_source(BatchFileSource, config["source"])
        .map(Generator, config["generator"])
        .map(PostProcessor, config["post_processor"])
        .sink(Sink, config["sink"])
    )

    env.submit()
    # env.run()
    # time.sleep(10)  # ç­‰å¾…ç®¡é“è¿è¡Œ5ç§’
    env.close()

    # # åˆ›å»ºç®—å­å®ä¾‹
    # source = BatchFileSource(config["source"])
    # generator = Generator(config["generator"])
    # post_processor = PostProcessor(config["post_processor"])
    # sink = Sink(config["sink"])

    # # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
    # for batch_data in source.execute():
    #     batch_result = generator.execute(batch_data)
    #     processed_batch = post_processor.execute(batch_result)
    #     sink.execute(processed_batch)


if __name__ == "__main__":
    from sage.common.utils.logging.custom_logger import CustomLogger

    CustomLogger.disable_global_console_debug()
    load_dotenv(override=False)
    # åŠ è½½é…ç½®æ–‡ä»¶
    config_path = "./experiments/config/experiment_config.yaml"
    config = load_config(config_path)

    logging.info("ğŸš€ å¼€å§‹è¿è¡Œå®éªŒç®¡é“...")
    logging.info(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {config['source']['data_path']}")
    logging.info(f"ğŸ”„ ä½¿ç”¨ä¸Šä¸‹æ–‡: {config['generator']['use_context']}")
    logging.info(f"ğŸ’¾ è¾“å‡ºè·¯å¾„: {config['sink']['output_path']}")

    pipeline_run(config)
