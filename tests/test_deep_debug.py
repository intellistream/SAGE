"""
æ·±åº¦è°ƒè¯•ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯Pythonå¯¹è±¡å¤ç”¨æˆ–å…¶ä»–ç¼“å­˜æœºåˆ¶å¯¼è‡´çš„é—®é¢˜
"""

import threading
import time
import sys
import os
import gc
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sage_core.function.base_function import BaseFunction


class MockRuntimeContext:
    """æ¨¡æ‹Ÿè¿è¡Œæ—¶ä¸Šä¸‹æ–‡"""
    def __init__(self, name="test_context"):
        import logging
        self.logger = logging.getLogger(name)
        self.name = name
        self.env_name = "test_env"
        self._service_manager = None
    
    @property
    def service_manager(self):
        if self._service_manager is None:
            from sage_runtime.service.service_caller import ServiceManager
            self._service_manager = ServiceManager(self)
        return self._service_manager


class TestFunction(BaseFunction):
    """æµ‹è¯•ç”¨çš„Functionç±»"""
    
    def execute(self, data):
        return data


def test_object_lifecycle():
    """æµ‹è¯•å¯¹è±¡ç”Ÿå‘½å‘¨æœŸå’Œå¤ç”¨"""
    print("Testing object lifecycle and reuse...")
    
    func = TestFunction()
    func.ctx = MockRuntimeContext("lifecycle_test")
    
    # åˆ›å»ºä¸€æ‰¹ä»£ç†å¯¹è±¡
    proxies_batch_1 = []
    for i in range(5):
        proxy = func.call_service["test_service"]
        proxies_batch_1.append((id(proxy), proxy))
        print(f"Batch 1, proxy {i}: ID = {id(proxy)}")
    
    print(f"Batch 1 unique IDs: {len(set(pid for pid, _ in proxies_batch_1))}")
    
    # åˆ é™¤å¼•ç”¨ï¼Œå¼ºåˆ¶åƒåœ¾å›æ”¶
    del proxies_batch_1
    gc.collect()
    
    print("\nAfter garbage collection...")
    
    # åˆ›å»ºç¬¬äºŒæ‰¹ä»£ç†å¯¹è±¡
    proxies_batch_2 = []
    for i in range(5):
        proxy = func.call_service["test_service"]
        proxies_batch_2.append((id(proxy), proxy))
        print(f"Batch 2, proxy {i}: ID = {id(proxy)}")
    
    print(f"Batch 2 unique IDs: {len(set(pid for pid, _ in proxies_batch_2))}")


def test_concurrent_with_detailed_tracking():
    """å¸¦è¯¦ç»†è¿½è¸ªçš„å¹¶å‘æµ‹è¯•"""
    print("\nTesting concurrent with detailed tracking...")
    
    results = []
    results_lock = threading.Lock()
    
    # è·Ÿè¸ªæ¯ä¸ªä»£ç†å¯¹è±¡çš„åˆ›å»ºæ—¶é—´å’Œçº¿ç¨‹
    proxy_creation_info = {}
    creation_lock = threading.Lock()
    
    def track_proxy_creation(proxy_id, thread_id, worker_id, call_id, proxy_type):
        with creation_lock:
            if proxy_id in proxy_creation_info:
                print(f"âš ï¸  REUSE DETECTED: Proxy ID {proxy_id} already exists!")
                print(f"   Original: {proxy_creation_info[proxy_id]}")
                print(f"   New: thread={thread_id}, worker={worker_id}, call={call_id}, type={proxy_type}")
            else:
                proxy_creation_info[proxy_id] = {
                    'thread_id': thread_id,
                    'worker_id': worker_id,
                    'call_id': call_id,
                    'proxy_type': proxy_type,
                    'timestamp': time.time()
                }
    
    def detailed_worker(worker_id):
        """è¯¦ç»†è·Ÿè¸ªçš„å·¥ä½œçº¿ç¨‹"""
        try:
            print(f"Worker {worker_id} starting in thread {threading.current_thread().ident}")
            
            # æ¯ä¸ªworkeråˆ›å»ºç‹¬ç«‹çš„functionå®ä¾‹
            func = TestFunction()
            func.ctx = MockRuntimeContext(f"detailed_context_{worker_id}")
            
            for call_id in range(3):
                print(f"Worker {worker_id}, call {call_id}")
                
                # è·å–åŒæ­¥ä»£ç†
                sync_proxy = func.call_service["test_service"]
                sync_id = id(sync_proxy)
                track_proxy_creation(sync_id, threading.current_thread().ident, worker_id, call_id, 'sync')
                
                # è·å–å¼‚æ­¥ä»£ç†
                async_proxy = func.call_service_async["test_service"]
                async_id = id(async_proxy)
                track_proxy_creation(async_id, threading.current_thread().ident, worker_id, call_id, 'async')
                
                # å†æ¬¡è·å–åŒæ ·æœåŠ¡çš„ä»£ç†ï¼Œåº”è¯¥æ˜¯ä¸åŒçš„å¯¹è±¡
                sync_proxy_2 = func.call_service["test_service"]
                sync_id_2 = id(sync_proxy_2)
                track_proxy_creation(sync_id_2, threading.current_thread().ident, worker_id, f"{call_id}_b", 'sync')
                
                result = {
                    'worker_id': worker_id,
                    'call_id': call_id,
                    'thread_id': threading.current_thread().ident,
                    'sync_id_1': sync_id,
                    'async_id': async_id,
                    'sync_id_2': sync_id_2,
                }
                
                with results_lock:
                    results.append(result)
                
                # éªŒè¯åŒä¸€æ¬¡è°ƒç”¨ä¸­çš„ä¸¤ä¸ªåŒæ­¥ä»£ç†æ˜¯ä¸åŒçš„
                if sync_id == sync_id_2:
                    print(f"âš ï¸  Worker {worker_id}, call {call_id}: Same sync proxy ID used twice!")
                
                time.sleep(0.005)  # çŸ­æš‚å»¶è¿Ÿ
                
        except Exception as e:
            print(f"Worker {worker_id} failed: {e}")
            import traceback
            traceback.print_exc()
    
    # å¯åŠ¨3ä¸ªå·¥ä½œçº¿ç¨‹
    threads = []
    for i in range(3):
        thread = threading.Thread(target=detailed_worker, args=(i,))
        threads.append(thread)
        thread.start()
    
    # ç­‰å¾…å®Œæˆ
    for thread in threads:
        thread.join()
    
    print(f"\n=== Detailed Analysis ===")
    print(f"Total results: {len(results)}")
    print(f"Total proxy creations tracked: {len(proxy_creation_info)}")
    
    # åˆ†ææ¯ä¸ªworkerçš„ç»“æœ
    for worker_id in range(3):
        worker_results = [r for r in results if r['worker_id'] == worker_id]
        print(f"\nWorker {worker_id}:")
        print(f"  Results: {len(worker_results)}")
        
        all_ids = []
        for result in worker_results:
            all_ids.extend([result['sync_id_1'], result['async_id'], result['sync_id_2']])
        
        unique_ids = set(all_ids)
        print(f"  Total proxy IDs: {len(all_ids)}")
        print(f"  Unique proxy IDs: {len(unique_ids)}")
        
        if len(all_ids) != len(unique_ids):
            print(f"  âš ï¸  Worker {worker_id} has reused proxy IDs!")
    
    # å…¨å±€åˆ†æ
    all_sync_1_ids = set(r['sync_id_1'] for r in results)
    all_async_ids = set(r['async_id'] for r in results)
    all_sync_2_ids = set(r['sync_id_2'] for r in results)
    
    print(f"\n=== Global Analysis ===")
    print(f"Unique sync_1 IDs: {len(all_sync_1_ids)}")
    print(f"Unique async IDs: {len(all_async_ids)}")
    print(f"Unique sync_2 IDs: {len(all_sync_2_ids)}")
    print(f"Expected unique IDs for each type: {len(results)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è·¨ç±»å‹çš„IDé‡å¤
    all_ids_combined = all_sync_1_ids | all_async_ids | all_sync_2_ids
    total_ids_expected = len(results) * 3
    
    print(f"All unique IDs combined: {len(all_ids_combined)}")
    print(f"Total IDs expected: {total_ids_expected}")
    
    if len(all_ids_combined) == total_ids_expected:
        print("âœ… All proxy objects are truly unique across all types and calls")
        return True
    else:
        print("âŒ Some proxy IDs are being reused")
        return False


if __name__ == "__main__":
    print("Running deep debug tests...\n")
    
    # æµ‹è¯•å¯¹è±¡ç”Ÿå‘½å‘¨æœŸ
    test_object_lifecycle()
    
    # æµ‹è¯•è¯¦ç»†å¹¶å‘è·Ÿè¸ª
    success = test_concurrent_with_detailed_tracking()
    
    if success:
        print("\nğŸ‰ Deep debug tests passed!")
    else:
        print("\nâŒ Deep debug tests revealed issues.")
        sys.exit(1)
