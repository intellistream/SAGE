diff --git a/README.md b/README.md
index 307c678..51e2d85 100644
--- a/README.md
+++ b/README.md
@@ -255,7 +255,7 @@ env = RemoteEnvironment()
 #### ğŸ“˜ About Long Running
 If your pipeline is meant to run as a long-lived service, use:
 ```python
- # deprecated
+env.run_streaming() 
 ```
 
 See more examples under [sage_examples](sage_examples)
@@ -298,7 +298,7 @@ Sage Engine is the core execution component that orchestrates the compilation an
 The Engine operates in four main phases:
 
 1. **Pipeline Collection**: Gathers user-defined logical pipelines built through DataStream API and validates pipeline integrity
-2. **Compilation & Optimization**: Uses ExecutionGraph to transform logical pipelines into optimized physical execution graphs with parallelism expansion
+2. **Compilation & Optimization**: Uses Compiler to transform logical pipelines into optimized physical execution graphs with parallelism expansion
 3. **Runtime Scheduling**: Selects appropriate Runtime (local/distributed) and converts execution graphs into concrete DAG nodes
 4. **Execution Monitoring**: Monitors pipeline execution status, collects performance metrics, and handles fault recovery
 
diff --git a/config/config_ray.yaml b/config/config_ray.yaml
index b5f8457..71eb3e2 100644
--- a/config/config_ray.yaml
+++ b/config/config_ray.yaml
@@ -43,4 +43,4 @@ promptor:
 
 sink:
   platform: "remote"
-  file_path: "/home/tjy/SAGE/output/response.txt"
+
diff --git a/deployment/README.md b/deployment/README.md
deleted file mode 100644
index 1375397..0000000
--- a/deployment/README.md
+++ /dev/null
@@ -1 +0,0 @@
-# å¦‚ä½•æ¶è®¾ SAGE é›†ç¾¤ä»¥åŠå¯åŠ¨çš„æµç¨‹
\ No newline at end of file
diff --git a/deployment/jobmanager_controller.py b/deployment/jobmanager_controller.py
deleted file mode 100755
index 4a9c34f..0000000
--- a/deployment/jobmanager_controller.py
+++ /dev/null
@@ -1,873 +0,0 @@
-#!/usr/bin/env python3
-"""
-SAGE JobManager Controller
-å‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºç®¡ç†å’Œç›‘æ§ JobManager ä½œä¸š
-"""
-
-import argparse
-import json
-import sys
-import time
-import os
-from pathlib import Path
-from typing import Dict, List, Any, Optional
-from datetime import datetime
-import signal
-
-# æ·»åŠ é¡¹ç›®è·¯å¾„
-project_root = Path(__file__).parent.parent
-sys.path.append(str(project_root))
-
-try:
-    from sage_core.jobmanager_client import JobManagerClient
-    from sage_utils.actor_wrapper import ActorWrapper
-    import yaml
-    from tabulate import tabulate
-    from colorama import Fore, Back, Style, init
-    import click
-except ImportError as e:
-    print(f"Missing required dependencies: {e}")
-    print("Please install: pip install pyyaml tabulate colorama click")
-    sys.exit(1)
-
-# åˆå§‹åŒ–colorama
-init(autoreset=True)
-
-class ControllerError(Exception):
-    """ControlleråŸºç¡€å¼‚å¸¸"""
-    pass
-
-class ConnectionError(ControllerError):
-    """è¿æ¥å¼‚å¸¸"""
-    pass
-
-class JobNotFoundError(ControllerError):
-    """ä½œä¸šæœªæ‰¾åˆ°å¼‚å¸¸"""
-    pass
-
-class JobManagerController:
-    """JobManageræ§åˆ¶å™¨"""
-    
-    def __init__(self, daemon_host: str = "127.0.0.1", daemon_port: int = 19001):
-        self.daemon_host = daemon_host
-        self.daemon_port = daemon_port
-        self.client: Optional[JobManagerClient] = None
-        self.jobmanager: Optional[ActorWrapper] = None
-        self.connected = False
-        
-        # åŠ è½½é…ç½®
-        self.config = self._load_config()
-        
-    def _load_config(self) -> Dict[str, Any]:
-        """åŠ è½½é…ç½®æ–‡ä»¶"""
-        config_path = Path.home() / ".sage" / "config.yaml"
-        default_config = {
-            "daemon": {
-                "host": "127.0.0.1",
-                "port": 19001
-            },
-            "output": {
-                "format": "table",
-                "colors": True
-            },
-            "monitor": {
-                "refresh_interval": 5
-            }
-        }
-        
-        if config_path.exists():
-            try:
-                with open(config_path) as f:
-                    user_config = yaml.safe_load(f) or {}
-                    # åˆå¹¶é…ç½®
-                    default_config.update(user_config)
-            except Exception as e:
-                self._print_warning(f"Failed to load config: {e}")
-        
-        return default_config
-    
-    def connect(self) -> bool:
-        """è¿æ¥åˆ°JobManager"""
-        try:
-            self._print_info(f"Connecting to JobManager daemon at {self.daemon_host}:{self.daemon_port}...")
-            
-            self.client = JobManagerClient(self.daemon_host, self.daemon_port)
-            
-            # å¥åº·æ£€æŸ¥
-            health = self.client.health_check()
-            if health.get("status") != "success":
-                raise ConnectionError(f"Daemon health check failed: {health.get('message')}")
-            
-            # è·å–JobManagerå¥æŸ„
-            self.jobmanager = self.client.get_actor_handle()
-            self.connected = True
-            
-            self._print_success("Connected to JobManager successfully")
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to connect: {e}")
-            self.connected = False
-            return False
-    
-    def disconnect(self):
-        """æ–­å¼€è¿æ¥"""
-        self.connected = False
-        self.client = None
-        self.jobmanager = None
-    
-    def ensure_connected(self):
-        """ç¡®ä¿å·²è¿æ¥"""
-        if not self.connected:
-            if not self.connect():
-                raise ConnectionError("Not connected to JobManager")
-    
-    def _resolve_job_identifier(self, identifier: str) -> Optional[str]:
-        """è§£æä½œä¸šæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯ä½œä¸šç¼–å·æˆ–UUIDï¼‰"""
-        try:
-            self.ensure_connected()
-            
-            # è·å–ä½œä¸šåˆ—è¡¨
-            jobs = self.jobmanager.list_jobs()
-            
-            # å¦‚æœæ˜¯æ•°å­—ï¼Œå½“ä½œä½œä¸šç¼–å·å¤„ç†
-            if identifier.isdigit():
-                job_index = int(identifier) - 1  # è½¬æ¢ä¸º0åŸºç´¢å¼•
-                if 0 <= job_index < len(jobs):
-                    return jobs[job_index].get('uuid')
-                else:
-                    self._print_error(f"Job number {identifier} is out of range (1-{len(jobs)})")
-                    return None
-            
-            # å¦‚æœæ˜¯UUIDï¼ˆå®Œæ•´æˆ–éƒ¨åˆ†ï¼‰
-            # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
-            for job in jobs:
-                if job.get('uuid') == identifier:
-                    return identifier
-            
-            # ç„¶åå°è¯•å‰ç¼€åŒ¹é…
-            matching_jobs = [job for job in jobs if job.get('uuid', '').startswith(identifier)]
-            
-            if len(matching_jobs) == 1:
-                return matching_jobs[0].get('uuid')
-            elif len(matching_jobs) > 1:
-                self._print_error(f"Ambiguous job identifier '{identifier}'. Matches:")
-                for i, job in enumerate(matching_jobs, 1):
-                    print(f"  {i}. {job.get('uuid')} ({job.get('name', 'unknown')})")
-                return None
-            else:
-                self._print_error(f"No job found matching '{identifier}'")
-                return None
-                
-        except Exception as e:
-            self._print_error(f"Failed to resolve job identifier: {e}")
-            return None
-    
-    # ==================== å‘½ä»¤å®ç° ====================
-    
-    def cmd_list(self, status_filter: Optional[str] = None, format_type: str = "table", full_uuid: bool = False) -> bool:
-        """åˆ—å‡ºæ‰€æœ‰ä½œä¸š"""
-        try:
-            self.ensure_connected()
-            
-            # è·å–ä½œä¸šåˆ—è¡¨
-            jobs = self.jobmanager.list_jobs()
-            
-            # çŠ¶æ€è¿‡æ»¤
-            if status_filter:
-                jobs = [job for job in jobs if job.get("status") == status_filter]
-            
-            # æ ¼å¼åŒ–è¾“å‡º
-            if format_type == "json":
-                print(json.dumps({"jobs": jobs}, indent=2))
-            else:
-                self._format_job_table(jobs, short_uuid=not full_uuid)
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to list jobs: {e}")
-            return False
-    
-    def cmd_show(self, job_identifier: str, verbose: bool = False) -> bool:
-        """æ˜¾ç¤ºä½œä¸šè¯¦æƒ…"""
-        try:
-            # è§£æä½œä¸šæ ‡è¯†ç¬¦
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-            
-            self.ensure_connected()
-            
-            # è·å–ä½œä¸šçŠ¶æ€
-            job_info = self.jobmanager.get_job_status(job_uuid)
-            
-            if not job_info:
-                raise JobNotFoundError(f"Job {job_uuid} not found")
-            
-            self._format_job_details(job_info, verbose)
-            return True
-            
-        except JobNotFoundError as e:
-            self._print_error(str(e))
-            return False
-        except Exception as e:
-            self._print_error(f"Failed to show job: {e}")
-            return False
-    
-    def cmd_stop(self, job_identifier: str, force: bool = False) -> bool:
-        """åœæ­¢ä½œä¸š"""
-        try:
-            # è§£æä½œä¸šæ ‡è¯†ç¬¦
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            # ç¡®è®¤æ“ä½œ
-            if not force:
-                # æ˜¾ç¤ºä½œä¸šä¿¡æ¯ç”¨äºç¡®è®¤
-                job_info = self.jobmanager.get_job_status(job_uuid)
-                if job_info:
-                    job_name = job_info.get('name', 'unknown')
-                    print(f"Job to stop: {job_name} ({job_uuid})")
-                
-                if not click.confirm(f"Are you sure you want to stop this job?"):
-                    self._print_info("Operation cancelled")
-                    return True
-            
-            # åœæ­¢ä½œä¸š
-            result = self.jobmanager.pause_job(job_uuid)
-            
-            if result.get("status") == "stopped":
-                self._print_success(f"Job {job_uuid[:8]}... stopped successfully")
-            else:
-                self._print_error(f"Failed to stop job: {result.get('message')}")
-                return False
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to stop job: {e}")
-            return False
-    
-    def cmd_status(self, job_identifier: str) -> bool:
-        """è·å–ä½œä¸šçŠ¶æ€"""
-        try:
-            # è§£æä½œä¸šæ ‡è¯†ç¬¦
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            job_info = self.jobmanager.get_job_status(job_uuid)
-            
-            if not job_info:
-                raise JobNotFoundError(f"Job {job_uuid} not found")
-            
-            status = job_info.get("status", "unknown")
-            job_name = job_info.get("name", "unknown")
-            self._print_status_colored(f"Job '{job_name}' ({job_uuid[:8]}...) status: {status}")
-            
-            return True
-            
-        except JobNotFoundError as e:
-            self._print_error(str(e))
-            return False
-        except Exception as e:
-            self._print_error(f"Failed to get job status: {e}")
-            return False
-    
-    def cmd_info(self) -> bool:
-        """æ˜¾ç¤ºJobManagerç³»ç»Ÿä¿¡æ¯"""
-        try:
-            self.ensure_connected()
-            
-            # è·å–ç³»ç»Ÿä¿¡æ¯
-            info = self.jobmanager.get_server_info()
-            
-            print(f"\n{Fore.CYAN}=== JobManager System Information ==={Style.RESET_ALL}")
-            print(f"Session ID: {info.get('session_id')}")
-            print(f"Log Directory: {info.get('log_base_dir')}")
-            print(f"Total Jobs: {info.get('environments_count', 0)}")
-            
-            # ç»Ÿè®¡ä½œä¸šçŠ¶æ€
-            jobs = info.get('jobs', [])
-            status_counts = {}
-            for job in jobs:
-                status = job.get('status', 'unknown')
-                status_counts[status] = status_counts.get(status, 0) + 1
-            
-            if status_counts:
-                print(f"\nJob Status Summary:")
-                for status, count in status_counts.items():
-                    print(f"  {status}: {count}")
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to get system info: {e}")
-            return False
-    
-    def cmd_health(self) -> bool:
-        """å¥åº·æ£€æŸ¥"""
-        try:
-            if not self.client:
-                self.client = JobManagerClient(self.daemon_host, self.daemon_port)
-            
-            health = self.client.health_check()
-            
-            if health.get("status") == "success":
-                self._print_success("JobManager is healthy")
-                
-                daemon_status = health.get("daemon_status", {})
-                print(f"Daemon: {daemon_status.get('socket_service')}")
-                print(f"Actor: {daemon_status.get('actor_name')}@{daemon_status.get('namespace')}")
-                
-                return True
-            else:
-                self._print_warning(f"Health check warning: {health.get('message')}")
-                return False
-                
-        except Exception as e:
-            self._print_error(f"Health check failed: {e}")
-            return False
-    
-    def cmd_continue(self, job_identifier: str, force: bool = False) -> bool:
-        """ç»§ç»­ä½œä¸š"""
-        try:
-            # è§£æä½œä¸šæ ‡è¯†ç¬¦
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            # ç¡®è®¤æ“ä½œ
-            if not force:
-                job_info = self.jobmanager.get_job_status(job_uuid)
-                if job_info:
-                    job_name = job_info.get('name', 'unknown')
-                    print(f"Job to continue: {job_name} ({job_uuid})")
-                
-                if not click.confirm(f"Are you sure you want to continue this job?"):
-                    self._print_info("Operation cancelled")
-                    return True
-            
-            # é‡å¯ä½œä¸š
-            result = self.jobmanager.continue_job(job_uuid)
-            
-            if result.get("status") == "running":
-                self._print_success(f"Job {job_uuid[:8]}... continued successfully")
-            else:
-                self._print_error(f"Failed to continue job: {result.get('message')}")
-                return False
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to continue job: {e}")
-            return False
-
-    def cmd_delete(self, job_identifier: str, force: bool = False) -> bool:
-        """åˆ é™¤ä½œä¸š"""
-        try:
-            # è§£æä½œä¸šæ ‡è¯†ç¬¦
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            # ç¡®è®¤æ“ä½œ
-            if not force:
-                job_info = self.jobmanager.get_job_status(job_uuid)
-                if job_info:
-                    job_name = job_info.get('name', 'unknown')
-                    job_status = job_info.get('status', 'unknown')
-                    print(f"Job to delete: {job_name} ({job_uuid})")
-                    print(f"Current status: {job_status}")
-                
-                if not click.confirm(f"Are you sure you want to delete this job? This action cannot be undone."):
-                    self._print_info("Operation cancelled")
-                    return True
-            
-            # åˆ é™¤ä½œä¸š
-            result = self.jobmanager.delete_job(job_uuid, force=force)
-            
-            if result.get("status") == "deleted":
-                self._print_success(f"Job {job_uuid[:8]}... deleted successfully")
-            else:
-                self._print_error(f"Failed to delete job: {result.get('message')}")
-                return False
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to delete job: {e}")
-            return False
-
-
-    def cmd_cleanup(self, force: bool = False) -> bool:
-        """æ¸…ç†æ‰€æœ‰ä½œä¸š"""
-        try:
-            self.ensure_connected()
-            
-            # ç¡®è®¤æ“ä½œ
-            if not force:
-                jobs = self.jobmanager.list_jobs()
-                if not jobs:
-                    self._print_info("No jobs to cleanup")
-                    return True
-                
-                print(f"Found {len(jobs)} jobs to cleanup:")
-                for job in jobs:
-                    print(f"  - {job.get('name')} ({job.get('uuid')[:8]}...) [{job.get('status')}]")
-                
-                if not click.confirm(f"Are you sure you want to cleanup all {len(jobs)} jobs?"):
-                    self._print_info("Operation cancelled")
-                    return True
-            
-            # æ¸…ç†æ‰€æœ‰ä½œä¸š
-            result = self.jobmanager.cleanup_all_jobs()
-            
-            if result.get("status") == "success":
-                self._print_success(result.get("message"))
-            else:
-                self._print_error(f"Failed to cleanup jobs: {result.get('message')}")
-                return False
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to cleanup jobs: {e}")
-            return False
-
-    def cmd_monitor(self, refresh_interval: int = 5) -> bool:
-        """å®æ—¶ç›‘æ§æ‰€æœ‰ä½œä¸š"""
-        try:
-            self.ensure_connected()
-            
-            self._print_info(f"Monitoring jobs (refresh every {refresh_interval}s, press Ctrl+C to stop)")
-            
-            # è®¾ç½®ä¿¡å·å¤„ç†
-            def signal_handler(signum, frame):
-                print("\nMonitoring stopped")
-                sys.exit(0)
-            
-            signal.signal(signal.SIGINT, signal_handler)
-            
-            while True:
-                # æ¸…å±
-                os.system('clear' if os.name == 'posix' else 'cls')
-                
-                # æ˜¾ç¤ºæ ‡é¢˜
-                print(f"{Fore.CYAN}=== SAGE JobManager Monitor ==={Style.RESET_ALL}")
-                print(f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
-                print()
-                
-                # è·å–å¹¶æ˜¾ç¤ºä½œä¸šåˆ—è¡¨
-                jobs = self.jobmanager.list_jobs()
-                self._format_job_table(jobs)
-                
-                # ç­‰å¾…
-                time.sleep(refresh_interval)
-                
-        except KeyboardInterrupt:
-            print("\nMonitoring stopped")
-            return True
-        except Exception as e:
-            self._print_error(f"Monitor failed: {e}")
-            return False
-    
-    def cmd_watch(self, job_identifier: str, refresh_interval: int = 2) -> bool:
-        """ç›‘æ§ç‰¹å®šä½œä¸š"""
-        try:
-            # è§£æä½œä¸šæ ‡è¯†ç¬¦
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            self._print_info(f"Watching job {job_uuid[:8]}... (refresh every {refresh_interval}s)")
-            
-            def signal_handler(signum, frame):
-                print("\nWatching stopped")
-                sys.exit(0)
-            
-            signal.signal(signal.SIGINT, signal_handler)
-            
-            while True:
-                # æ¸…å±
-                os.system('clear' if os.name == 'posix' else 'cls')
-                
-                # æ˜¾ç¤ºä½œä¸šè¯¦æƒ…
-                job_info = self.jobmanager.get_job_status(job_uuid)
-                
-                if not job_info:
-                    self._print_error(f"Job {job_uuid} not found")
-                    return False
-                
-                print(f"{Fore.CYAN}=== Watching Job {job_uuid[:8]}... ==={Style.RESET_ALL}")
-                print(f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
-                print()
-                
-                self._format_job_details(job_info, verbose=True)
-                
-                time.sleep(refresh_interval)
-                
-        except KeyboardInterrupt:
-            print("\nWatching stopped")
-            return True
-        except Exception as e:
-            self._print_error(f"Watch failed: {e}")
-            return False
-    
-    # ==================== äº¤äº’å¼Shell ====================
-    
-    def cmd_shell(self) -> bool:
-        """è¿›å…¥äº¤äº’å¼shell"""
-        try:
-            self.ensure_connected()
-            
-            print(f"{Fore.GREEN}SAGE JobManager Interactive Shell{Style.RESET_ALL}")
-            print("Type 'help' for available commands, 'exit' to quit")
-            
-            while True:
-                try:
-                    cmd_input = input(f"{Fore.BLUE}sage-jm> {Style.RESET_ALL}").strip()
-                    
-                    if not cmd_input:
-                        continue
-                    
-                    if cmd_input in ['exit', 'quit']:
-                        break
-                    
-                    if cmd_input == 'help':
-                        self._show_shell_help()
-                        continue
-                    
-                    # è§£æå‘½ä»¤
-                    parts = cmd_input.split()
-                    cmd = parts[0]
-                    args = parts[1:]
-                    
-                    # æ‰§è¡Œå‘½ä»¤
-                    if cmd == 'list':
-                        status_filter = None
-                        full_uuid = False
-                        if '--status' in args:
-                            idx = args.index('--status')
-                            if idx + 1 < len(args):
-                                status_filter = args[idx + 1]
-                        if '--full-uuid' in args:
-                            full_uuid = True
-                        self.cmd_list(status_filter, format_type="table", full_uuid=full_uuid)
-                        
-                    elif cmd == 'show':
-                        if args:
-                            verbose = '--verbose' in args or '-v' in args
-                            job_identifier = args[0]
-                            self.cmd_show(job_identifier, verbose)
-                        else:
-                            self._print_error("Usage: show <job_number_or_uuid> [--verbose]")
-                            
-                    elif cmd == 'stop':
-                        if args:
-                            self.cmd_stop(args[0])
-                        else:
-                            self._print_error("Usage: stop <job_number_or_uuid>")
-                            
-                    elif cmd == 'status':
-                        if args:
-                            self.cmd_status(args[0])
-                        else:
-                            self._print_error("Usage: status <job_number_or_uuid>")
-                            
-                    elif cmd == 'info':
-                        self.cmd_info()
-                        
-                    elif cmd == 'health':
-                        self.cmd_health()
-                        
-                    else:
-                        self._print_error(f"Unknown command: {cmd}")
-                        
-                except KeyboardInterrupt:
-                    print()
-                    continue
-                except EOFError:
-                    break
-            
-            print("Goodbye!")
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Shell failed: {e}")
-            return False
-    
-    def _show_shell_help(self):
-        """æ˜¾ç¤ºshellå¸®åŠ©"""
-        help_text = """
-Available Commands:
-  list [--status STATUS] [--full-uuid]    List all jobs
-  show <job_number_or_uuid> [--verbose]   Show job details  
-  stop <job_number_or_uuid>               Stop a job
-  status <job_number_or_uuid>             Get job status
-  info                                    Show system information
-  health                                  Health check
-  help                                    Show this help
-  exit/quit                               Exit shell
-
-Job Identifiers:
-  You can use either job numbers (1, 2, 3...) or UUIDs
-  Examples: show 1, show abc123, stop 2, status def456
-"""
-        print(help_text)
-    
-    # ==================== è¾“å‡ºæ ¼å¼åŒ– ====================
-    
-    def _format_job_table(self, jobs: List[Dict[str, Any]], short_uuid: bool = False):
-        """æ ¼å¼åŒ–ä½œä¸šè¡¨æ ¼"""
-        if not jobs:
-            self._print_info("No jobs found")
-            return
-        
-        # æ ¹æ®ç»ˆç«¯å®½åº¦å†³å®šæ˜¯å¦æ˜¾ç¤ºå®Œæ•´UUID
-        import shutil
-        terminal_width = shutil.get_terminal_size().columns
-        
-        if short_uuid or terminal_width < 120:
-            headers = ['#', 'UUID (Short)', 'Name', 'Status', 'Started', 'Runtime']
-        else:
-            headers = ['#', 'UUID', 'Name', 'Status', 'Started', 'Runtime']
-        
-        rows = []
-        
-        for i, job in enumerate(jobs, 1):
-            full_uuid = job.get('uuid', 'unknown')
-            
-            if short_uuid or terminal_width < 120:
-                uuid_display = full_uuid[:8] + '...' if len(full_uuid) > 8 else full_uuid
-            else:
-                uuid_display = full_uuid
-                
-            name = job.get('name', 'unknown')
-            status = job.get('status', 'unknown')
-            start_time = job.get('start_time', 'unknown')
-            runtime = job.get('runtime', 'unknown')
-            
-            # çŠ¶æ€ç€è‰²
-            if status == 'running':
-                status = f"{Fore.GREEN}{status}{Style.RESET_ALL}"
-            elif status == 'stopped':
-                status = f"{Fore.YELLOW}{status}{Style.RESET_ALL}"
-            elif status == 'failed':
-                status = f"{Fore.RED}{status}{Style.RESET_ALL}"
-            
-            rows.append([i, uuid_display, name, status, start_time, runtime])
-        
-        print(tabulate(rows, headers=headers, tablefmt='grid'))
-        
-        # å¦‚æœä½¿ç”¨çŸ­UUIDï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
-        if short_uuid or terminal_width < 120:
-            print(f"\n{Fore.BLUE}ğŸ’¡ Tip:{Style.RESET_ALL} Use job number (#) or full UUID for commands")
-            print(f"   Example: sage-jm show 1  or  sage-jm show {jobs[0].get('uuid', '')}")
-            print(f"   Use --full-uuid to see complete UUIDs")
-    
-    def _format_job_details(self, job_info: Dict[str, Any], verbose: bool = False):
-        """æ ¼å¼åŒ–ä½œä¸šè¯¦æƒ…"""
-        print(f"{Fore.CYAN}=== Job Details ==={Style.RESET_ALL}")
-        
-        uuid = job_info.get('uuid', 'unknown')
-        name = job_info.get('name', 'unknown')
-        status = job_info.get('status', 'unknown')
-        
-        print(f"UUID: {uuid}")
-        print(f"Name: {name}")
-        
-        # çŠ¶æ€ç€è‰²
-        if status == 'running':
-            status_colored = f"{Fore.GREEN}{status}{Style.RESET_ALL}"
-        elif status == 'stopped':
-            status_colored = f"{Fore.YELLOW}{status}{Style.RESET_ALL}"
-        elif status == 'failed':
-            status_colored = f"{Fore.RED}{status}{Style.RESET_ALL}"
-        else:
-            status_colored = status
-        
-        print(f"Status: {status_colored}")
-        print(f"Start Time: {job_info.get('start_time', 'unknown')}")
-        print(f"Runtime: {job_info.get('runtime', 'unknown')}")
-        
-        if verbose:
-            if 'error' in job_info:
-                print(f"Error: {job_info['error']}")
-            
-            # æ˜¾ç¤ºæ›´å¤šè¯¦ç»†ä¿¡æ¯
-            print(f"\nEnvironment Details:")
-            env_info = job_info.get('environment', {})
-            for key, value in env_info.items():
-                print(f"  {key}: {value}")
-    
-    def _print_success(self, message: str):
-        """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
-        print(f"{Fore.GREEN}âœ“{Style.RESET_ALL} {message}")
-    
-    def _print_error(self, message: str):
-        """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
-        print(f"{Fore.RED}âœ—{Style.RESET_ALL} {message}")
-    
-    def _print_warning(self, message: str):
-        """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
-        print(f"{Fore.YELLOW}âš {Style.RESET_ALL} {message}")
-    
-    def _print_info(self, message: str):
-        """æ‰“å°ä¿¡æ¯æ¶ˆæ¯"""
-        print(f"{Fore.BLUE}â„¹{Style.RESET_ALL} {message}")
-    
-    def _print_status_colored(self, message: str):
-        """æ‰“å°å¸¦é¢œè‰²çš„çŠ¶æ€æ¶ˆæ¯"""
-        if 'running' in message:
-            print(message.replace('running', f"{Fore.GREEN}running{Style.RESET_ALL}"))
-        elif 'stopped' in message:
-            print(message.replace('stopped', f"{Fore.YELLOW}stopped{Style.RESET_ALL}"))
-        elif 'failed' in message:
-            print(message.replace('failed', f"{Fore.RED}failed{Style.RESET_ALL}"))
-        else:
-            print(message)
-
-def create_parser():
-    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
-    parser = argparse.ArgumentParser(
-        prog='sage-jm',
-        description='SAGE JobManager Controller'
-    )
-    
-    # å…¨å±€å‚æ•°
-    parser.add_argument('--host', default='127.0.0.1', help='Daemon host')
-    parser.add_argument('--port', type=int, default=19001, help='Daemon port')
-    parser.add_argument('--no-color', action='store_true', help='Disable colored output')
-    
-    # å­å‘½ä»¤
-    subparsers = parser.add_subparsers(dest='command', help='Available commands')
-    
-    # list å‘½ä»¤
-    list_parser = subparsers.add_parser('list', help='List all jobs')
-    list_parser.add_argument('--status', choices=['running', 'stopped', 'failed'], help='Filter by status')
-    list_parser.add_argument('--format', choices=['table', 'json'], default='table', help='Output format')
-    list_parser.add_argument('--full-uuid', action='store_true', help='Show full UUIDs instead of short versions')
-    
-    # show å‘½ä»¤
-    show_parser = subparsers.add_parser('show', help='Show job details')
-    show_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    show_parser.add_argument('--verbose', '-v', action='store_true', help='Show verbose details')
-    
-    # stop å‘½ä»¤
-    stop_parser = subparsers.add_parser('stop', help='Stop a job')
-    stop_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    stop_parser.add_argument('--force', '-f', action='store_true', help='Force stop without confirmation')
-    
-    # status å‘½ä»¤
-    status_parser = subparsers.add_parser('status', help='Get job status')
-    status_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    
-    # info å‘½ä»¤
-    subparsers.add_parser('info', help='Show system information')
-    
-    # health å‘½ä»¤
-    subparsers.add_parser('health', help='Health check')
-    
-    # monitor å‘½ä»¤
-    monitor_parser = subparsers.add_parser('monitor', help='Monitor all jobs')
-    monitor_parser.add_argument('--refresh', type=int, default=5, help='Refresh interval in seconds')
-    
-    # watch å‘½ä»¤
-    watch_parser = subparsers.add_parser('watch', help='Watch specific job')
-    watch_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    watch_parser.add_argument('--refresh', type=int, default=2, help='Refresh interval in seconds')
-
-    # continue å‘½ä»¤
-    continue_parser = subparsers.add_parser('continue', help='continue a job')
-    continue_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    continue_parser.add_argument('--force', '-f', action='store_true', help='Force continue without confirmation')
-    
-    # delete å‘½ä»¤
-    delete_parser = subparsers.add_parser('delete', help='Delete a job')
-    delete_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    delete_parser.add_argument('--force', '-f', action='store_true', help='Force delete without confirmation')
-    
-    # cleanup å‘½ä»¤
-    cleanup_parser = subparsers.add_parser('cleanup', help='Cleanup all jobs')
-    cleanup_parser.add_argument('--force', '-f', action='store_true', help='Force cleanup without confirmation')
-
-    # shell å‘½ä»¤
-    subparsers.add_parser('shell', help='Enter interactive shell')
-    
-    return parser
-
-def main():
-    """ä¸»å‡½æ•°"""
-    parser = create_parser()
-    args = parser.parse_args()
-    
-    # ç¦ç”¨é¢œè‰²è¾“å‡º
-    if args.no_color:
-        import colorama
-        colorama.deinit()
-    
-    # åˆ›å»ºæ§åˆ¶å™¨
-    controller = JobManagerController(args.host, args.port)
-    
-    # æ‰§è¡Œå‘½ä»¤
-    if not args.command:
-        parser.print_help()
-        return
-    
-    success = True
-    
-    try:
-        if args.command == 'list':
-            success = controller.cmd_list(args.status, args.format, args.full_uuid)
-        elif args.command == 'show':
-            success = controller.cmd_show(args.job_identifier, args.verbose)
-        elif args.command == 'stop':
-            success = controller.cmd_stop(args.job_identifier, args.force)
-        elif args.command == 'continue':
-            success = controller.cmd_continue(args.job_identifier, args.force)
-        elif args.command == 'delete':
-            success = controller.cmd_delete(args.job_identifier, args.force)
-        elif args.command == 'status':
-            success = controller.cmd_status(args.job_identifier)
-        elif args.command == 'info':
-            success = controller.cmd_info()
-        elif args.command == 'health':
-            success = controller.cmd_health()
-        elif args.command == 'monitor':
-            success = controller.cmd_monitor(args.refresh)
-        elif args.command == 'watch':
-            success = controller.cmd_watch(args.job_identifier, args.refresh)
-        elif args.command == 'shell':
-            success = controller.cmd_shell()
-        elif args.command == 'cleanup':
-            success = controller.cmd_cleanup(args.force)
-        else:
-            print(f"Unknown command: {args.command}")
-            success = False
-            
-    except KeyboardInterrupt:
-        print("\nOperation cancelled")
-        success = True
-    except Exception as e:
-        print(f"Unexpected error: {e}")
-        success = False
-    finally:
-        controller.disconnect()
-    
-    sys.exit(0 if success else 1)
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/deployment/jobmanager_daemon.py b/deployment/jobmanager_daemon.py
deleted file mode 100644
index f3af9dc..0000000
--- a/deployment/jobmanager_daemon.py
+++ /dev/null
@@ -1,497 +0,0 @@
-
-import socket, ray
-import threading
-import time
-import json
-import pickle
-import signal
-import sys
-import os
-from pathlib import Path
-from typing import Optional, Dict, Any, TYPE_CHECKING
-from sage_utils.custom_logger import CustomLogger
-from sage_jobmanager.remote_job_manager import RemoteJobManager
-from ray.actor import ActorHandle
-    
-
-
-
-class RayJobManagerDaemon:
-    """
-    Ray JobManagerå®ˆæŠ¤æœåŠ¡
-    è´Ÿè´£å¯åŠ¨detached JobManager Actorå¹¶é€šè¿‡socketæš´éœ²å¥æŸ„
-    """
-    
-    def __init__(self, 
-                 host: str = "127.0.0.1", 
-                 port: int = 19001,
-                 actor_name: str = "sage_global_jobmanager",
-                 namespace: str = "sage_system"):
-        """
-        åˆå§‹åŒ–å®ˆæŠ¤æœåŠ¡
-        
-        Args:
-            host: SocketæœåŠ¡ç›‘å¬åœ°å€
-            port: SocketæœåŠ¡ç«¯å£  
-            actor_name: JobManager Actoråç§°
-            namespace: Rayå‘½åç©ºé—´
-        """
-        self.host = host
-        self.port = port
-        self.actor_name = actor_name
-        self.namespace = namespace
-        
-        # Actorå¥æŸ„
-        self._actor_handle: Optional[ActorHandle] = None
-        
-        # SocketæœåŠ¡
-        self._server_socket: Optional[socket.socket] = None
-        self._server_thread: Optional[threading.Thread] = None
-        self._running = False
-        
-        # æ—¥å¿—
-        self._setup_logging()
-        
-        # ä¿¡å·å¤„ç†
-        self._setup_signal_handlers()
-        
-    def _setup_logging(self):
-        """è®¾ç½®æ—¥å¿—"""
-        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼šå½“å‰æ–‡ä»¶åœ¨deploymentç›®å½•ä¸‹ï¼Œéœ€è¦å›åˆ°é¡¹ç›®æ ¹ç›®å½•
-        project_root = Path(__file__).parent.parent
-        log_dir = project_root / "logs" / "daemon"
-        log_dir.mkdir(parents=True, exist_ok=True)
-        
-        timestamp = time.strftime("%Y%m%d_%H%M%S")
-        self.logger = CustomLogger([
-            ("console", "INFO"),
-            (log_dir / f"jobmanager_daemon_{timestamp}.log", "DEBUG"),
-            (log_dir / "daemon_error.log", "ERROR")
-        ], name="JobManagerDaemon")
-        
-    def _setup_signal_handlers(self):
-        """è®¾ç½®ä¿¡å·å¤„ç†"""
-        def signal_handler(signum, frame):
-            self.logger.info(f"Received signal {signum}, shutting down daemon...")
-            self.shutdown()
-            sys.exit(0)
-            
-        signal.signal(signal.SIGINT, signal_handler)
-        signal.signal(signal.SIGTERM, signal_handler)
-    
-    def start_daemon(self):
-        """å¯åŠ¨å®ˆæŠ¤æœåŠ¡"""
-        try:
-            self.logger.info("Starting Ray JobManager Daemon...")
-            
-            # 1. ç¡®ä¿è¿›ç¨‹å†…Rayå®¢æˆ·ç«¯å·²åˆå§‹åŒ–
-            if not ray.is_initialized():
-                ray.init(address="auto", _temp_dir="/var/lib/ray_shared")
-                self.logger.info("Ray initialized")
-            
-            # 2. å¯åŠ¨JobManager Actor
-            self._start_jobmanager_actor()
-            
-            # 3. å¯åŠ¨SocketæœåŠ¡
-            self._start_socket_service()
-            
-            self.logger.info(f"Daemon started successfully on {self.host}:{self.port}")
-            return True
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start daemon: {e}")
-            self.shutdown()
-            return False
-    
-    def _start_jobmanager_actor(self):
-        """å¯åŠ¨JobManager Actor"""
-        try:
-            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨Actor
-            try:
-                existing_actor = ray.get_actor(self.actor_name, namespace=self.namespace)
-                # æµ‹è¯•Actoræ˜¯å¦å¥åº·
-                ray.get(existing_actor.health_check.remote(), timeout=5)
-                self.logger.info(f"Found existing healthy JobManager Actor: {self.actor_name}")
-                self._actor_handle = existing_actor
-                return
-            except (ValueError, ray.exceptions.RayActorError):
-                self.logger.info("No existing healthy Actor found, creating new one")
-            
-            # åˆ›å»ºæ–°çš„detached Actor
-            self.logger.info(f"Creating detached JobManager Actor: {self.actor_name}")
-            
-            self._actor_handle = RemoteJobManager.options(
-                name=self.actor_name,
-                namespace=self.namespace,
-                lifetime="detached",  # å…³é”®ï¼šä½¿ç”¨detachedç”Ÿå‘½å‘¨æœŸ
-                max_restarts=3,
-                max_task_retries=2,
-                resources={"jobmanager": 1.0}  # èµ„æºæ ‡è®°é˜²æ­¢å›æ”¶
-            ).remote()
-            
-            # éªŒè¯Actoråˆ›å»ºæˆåŠŸ - ä½¿ç”¨Rayå†…ç½®çš„readyæ£€æŸ¥
-            self.logger.info("Waiting for Actor to be ready...")
-            ray.get(self._actor_handle.get_actor_info.remote(), timeout=30)
-            
-            # è·å–ActoråŸºæœ¬ä¿¡æ¯
-            try:
-                actor_id = self._actor_handle._actor_id.hex()
-                self.logger.info(f"JobManager Actor created successfully: {actor_id}")
-            except Exception as e:
-                self.logger.info("JobManager Actor created (could not get ID)")
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start JobManager Actor: {e}")
-            raise
-    
-    def _start_socket_service(self):
-        """å¯åŠ¨SocketæœåŠ¡"""
-        try:
-            self._server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-            self._server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
-            self._server_socket.bind((self.host, self.port))
-            self._server_socket.listen(10)
-            
-            self._running = True
-            self._server_thread = threading.Thread(
-                target=self._socket_server_loop,
-                name="SocketServer",
-                daemon=True
-            )
-            self._server_thread.start()
-            
-            self.logger.info(f"Socket service started on {self.host}:{self.port}")
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start socket service: {e}")
-            raise
-    
-    def _socket_server_loop(self):
-        """SocketæœåŠ¡ä¸»å¾ªç¯"""
-        self.logger.debug("Socket server loop started")
-        
-        while self._running:
-            try:
-                if not self._server_socket:
-                    break
-                    
-                self._server_socket.settimeout(1.0)  # 1ç§’è¶…æ—¶ï¼Œå…è®¸æ£€æŸ¥_runningçŠ¶æ€
-                
-                try:
-                    client_socket, address = self._server_socket.accept()
-                    self.logger.debug(f"New client connected: {address}")
-                    
-                    # åœ¨æ–°çº¿ç¨‹ä¸­å¤„ç†å®¢æˆ·ç«¯
-                    client_thread = threading.Thread(
-                        target=self._handle_client,
-                        args=(client_socket, address),
-                        name=f"Client-{address[0]}:{address[1]}",
-                        daemon=True
-                    )
-                    client_thread.start()
-                    
-                except socket.timeout:
-                    continue
-                    
-            except Exception as e:
-                if self._running:
-                    self.logger.error(f"Error in socket server loop: {e}")
-                break
-        
-        self.logger.debug("Socket server loop stopped")
-    
-    def _handle_client(self, client_socket: socket.socket, address: tuple):
-        """å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚"""
-        try:
-            with client_socket:
-                # æ¥æ”¶è¯·æ±‚
-                request_data = self._receive_message(client_socket)
-                if not request_data:
-                    return
-                
-                request = json.loads(request_data.decode('utf-8'))
-                self.logger.debug(f"Received request from {address}: {request}")
-                
-                # å¤„ç†è¯·æ±‚
-                response = self._process_request(request)
-                
-                # å‘é€å“åº”
-                self._send_message(client_socket, json.dumps(response).encode('utf-8'))
-                
-        except Exception as e:
-            self.logger.error(f"Error handling client {address}: {e}")
-    
-    def _receive_message(self, client_socket: socket.socket) -> Optional[bytes]:
-        """æ¥æ”¶æ¶ˆæ¯"""
-        try:
-            # æ¥æ”¶æ¶ˆæ¯é•¿åº¦
-            length_data = client_socket.recv(4)
-            if len(length_data) != 4:
-                return None
-            
-            message_length = int.from_bytes(length_data, byteorder='big')
-            if message_length <= 0 or message_length > 10 * 1024 * 1024:  # 10MBé™åˆ¶
-                return None
-            
-            # æ¥æ”¶æ¶ˆæ¯å†…å®¹
-            message_data = b''
-            while len(message_data) < message_length:
-                chunk = client_socket.recv(min(message_length - len(message_data), 8192))
-                if not chunk:
-                    return None
-                message_data += chunk
-            
-            return message_data
-            
-        except Exception as e:
-            self.logger.debug(f"Error receiving message: {e}")
-            return None
-    
-    def _send_message(self, client_socket: socket.socket, message: bytes):
-        """å‘é€æ¶ˆæ¯"""
-        try:
-            # å‘é€æ¶ˆæ¯é•¿åº¦
-            length_data = len(message).to_bytes(4, byteorder='big')
-            client_socket.sendall(length_data)
-            
-            # å‘é€æ¶ˆæ¯å†…å®¹
-            client_socket.sendall(message)
-            
-        except Exception as e:
-            self.logger.debug(f"Error sending message: {e}")
-    
-    def _process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚"""
-        try:
-            action = request.get("action", "")
-            
-            if action == "get_actor_handle":
-                return self._handle_get_actor_handle(request)
-            elif action == "get_actor_info":
-                return self._handle_get_actor_info(request)
-            elif action == "health_check":
-                return self._handle_health_check(request)
-            elif action == "restart_actor":
-                return self._handle_restart_actor(request)
-            else:
-                return {
-                    "status": "error",
-                    "message": f"Unknown action: {action}",
-                    "request_id": request.get("request_id")
-                }
-                
-        except Exception as e:
-            self.logger.error(f"Error processing request: {e}")
-            return {
-                "status": "error", 
-                "message": str(e),
-                "request_id": request.get("request_id")
-            }
-    
-    def _handle_get_actor_handle(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """å¤„ç†è·å–Actorå¥æŸ„è¯·æ±‚"""
-        if self._actor_handle is None:
-            return {
-                "status": "error",
-                "message": "No JobManager Actor available",
-                "request_id": request.get("request_id")
-            }
-        
-        try:
-            # åºåˆ—åŒ–Actorå¥æŸ„
-            # æ³¨æ„ï¼šRay ActorHandleéœ€è¦ç‰¹æ®Šå¤„ç†
-            actor_serialized = pickle.dumps(self._actor_handle)
-            
-            return {
-                "status": "success",
-                "actor_handle": actor_serialized.hex(),  # è½¬æ¢ä¸ºhexå­—ç¬¦ä¸²ä¼ è¾“
-                "actor_name": self.actor_name,
-                "namespace": self.namespace,
-                "request_id": request.get("request_id")
-            }
-            
-        except Exception as e:
-            self.logger.error(f"Failed to serialize actor handle: {e}")
-            return {
-                "status": "error",
-                "message": f"Failed to serialize actor handle: {e}",
-                "request_id": request.get("request_id")
-            }
-    
-    def _handle_get_actor_info(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """å¤„ç†è·å–Actorä¿¡æ¯è¯·æ±‚"""
-        if self._actor_handle is None:
-            return {
-                "status": "error",
-                "message": "No JobManager Actor available",
-                "request_id": request.get("request_id")
-            }
-        
-        try:
-            # æ„å»ºåŸºæœ¬çš„Actorä¿¡æ¯
-            actor_info = {
-                "actor_name": self.actor_name,
-                "namespace": self.namespace,
-            }
-            
-            # å°è¯•è·å–Actor ID
-            try:
-                actor_info["actor_id"] = self._actor_handle._actor_id.hex()
-            except:
-                actor_info["actor_id"] = "unknown"
-            
-            # æ£€æŸ¥Actoræ˜¯å¦ready
-            try:
-                ray.get(self._actor_handle.__ray_ready__.remote(), timeout=3)
-                actor_info["status"] = "ready"
-            except:
-                actor_info["status"] = "not_ready"
-            
-            return {
-                "status": "success",
-                "actor_info": actor_info,
-                "request_id": request.get("request_id")
-            }
-        except Exception as e:
-            return {
-                "status": "error",
-                "message": f"Failed to get actor info: {e}",
-                "request_id": request.get("request_id")
-            }
-    
-    def _handle_health_check(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """å¤„ç†å¥åº·æ£€æŸ¥è¯·æ±‚"""
-        daemon_status = {
-            "daemon_running": True,
-            "socket_service": f"{self.host}:{self.port}",
-            "actor_name": self.actor_name,
-            "namespace": self.namespace
-        }
-        
-        if self._actor_handle is None:
-            daemon_status["actor_available"] = False
-            return {
-                "status": "warning",
-                "message": "Daemon running but no Actor available",
-                "daemon_status": daemon_status,
-                "request_id": request.get("request_id")
-            }
-        
-        try:
-            # ä½¿ç”¨Rayå†…ç½®çš„å¥åº·æ£€æŸ¥
-            ray.get(self._actor_handle.__ray_ready__.remote(), timeout=5)
-            daemon_status["actor_available"] = True
-            daemon_status["actor_ready"] = True
-            
-            return {
-                "status": "success",
-                "message": "Daemon and Actor are healthy",
-                "daemon_status": daemon_status,
-                "request_id": request.get("request_id")
-            }
-        except Exception as e:
-            daemon_status["actor_available"] = False
-            daemon_status["actor_error"] = str(e)
-            
-            return {
-                "status": "error",
-                "message": f"Actor health check failed: {e}",
-                "daemon_status": daemon_status,
-                "request_id": request.get("request_id")
-            }
-    
-    def _handle_restart_actor(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """å¤„ç†é‡å¯Actorè¯·æ±‚"""
-        try:
-            # åœæ­¢ç°æœ‰Actor
-            if self._actor_handle:
-                try:
-                    ray.kill(self._actor_handle)
-                    self.logger.info("Old Actor killed")
-                except:
-                    pass
-            
-            # å¯åŠ¨æ–°Actor
-            self._start_jobmanager_actor()
-            
-            return {
-                "status": "success",
-                "message": "Actor restarted successfully",
-                "request_id": request.get("request_id")
-            }
-        except Exception as e:
-            return {
-                "status": "error",
-                "message": f"Failed to restart Actor: {e}",
-                "request_id": request.get("request_id")
-            }
-    
-    def shutdown(self):
-        """å…³é—­å®ˆæŠ¤æœåŠ¡"""
-        self.logger.info("Shutting down daemon...")
-        
-        # åœæ­¢socketæœåŠ¡
-        self._running = False
-        if self._server_socket:
-            try:
-                self._server_socket.close()
-            except:
-                pass
-        
-        # ç­‰å¾…æœåŠ¡çº¿ç¨‹ç»“æŸ
-        if self._server_thread and self._server_thread.is_alive():
-            self._server_thread.join(timeout=5)
-        
-        # æ³¨æ„ï¼šä¸è¦æ€æ­»detached Actorï¼Œå®ƒåº”è¯¥ç»§ç»­è¿è¡Œ
-        # åªæ˜¯é‡Šæ”¾å¥æŸ„
-        self._actor_handle = None
-        
-        self.logger.info("Daemon shutdown complete")
-    
-    def run_forever(self):
-        """è¿è¡Œå®ˆæŠ¤æœåŠ¡ç›´åˆ°æ”¶åˆ°åœæ­¢ä¿¡å·"""
-        if not self.start_daemon():
-            return False
-        
-        try:
-            while self._running:
-                time.sleep(1)
-        except KeyboardInterrupt:
-            pass
-        finally:
-            self.shutdown()
-        
-        return True
-
-
-
-# ==================== å‘½ä»¤è¡Œå·¥å…· ====================
-
-def main():
-    """å‘½ä»¤è¡Œå…¥å£"""
-    import argparse
-    
-    parser = argparse.ArgumentParser(description="Ray JobManager Daemon")
-    parser.add_argument("--host", default="127.0.0.1", help="Daemon host")
-    parser.add_argument("--port", type=int, default=19001, help="Daemon port")
-    parser.add_argument("--actor-name", default="sage_global_jobmanager", help="Actor name")
-    parser.add_argument("--namespace", default="sage_system", help="Ray namespace")
-    
-    args = parser.parse_args()
-    
-    daemon = RayJobManagerDaemon(
-        host=args.host,
-        port=args.port,
-        actor_name=args.actor_name,
-        namespace=args.namespace
-    )
-    
-    print(f"Starting JobManager Daemon on {args.host}:{args.port}")
-    print(f"Actor: {args.actor_name}@{args.namespace}")
-    print("Press Ctrl+C to stop...")
-    
-    daemon.run_forever()
-
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/deployment/sage_deployment.sh b/deployment/sage_deployment.sh
deleted file mode 100755
index 683285a..0000000
--- a/deployment/sage_deployment.sh
+++ /dev/null
@@ -1,606 +0,0 @@
-#!/bin/bash
-
-# SAGE System Startup Script
-# åŠŸèƒ½ï¼š
-# 1. æ£€æŸ¥å¹¶å¯åŠ¨ Ray head èŠ‚ç‚¹
-# 2. è®¾ç½® Ray session æƒé™
-# 3. æ£€æŸ¥å¹¶å¯åŠ¨ JobManager Daemon
-# 4. éªŒè¯ç³»ç»ŸçŠ¶æ€
-# 5. è®¾ç½®å‘½ä»¤è¡Œå·¥å…·
-
-set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º
-
-# é…ç½®å‚æ•°
-RAY_HEAD_PORT=${RAY_HEAD_PORT:-10001}           # GCS server port
-RAY_CLIENT_PORT=${RAY_CLIENT_PORT:-10002}       # Client server port (æ–°å¢)
-RAY_DASHBOARD_PORT=${RAY_DASHBOARD_PORT:-8265}
-RAY_TEMP_DIR=${RAY_TEMP_DIR:-/var/lib/ray_shared}
-DAEMON_HOST=${DAEMON_HOST:-127.0.0.1}
-DAEMON_PORT=${DAEMON_PORT:-19001}
-ACTOR_NAME=${ACTOR_NAME:-sage_global_jobmanager}
-NAMESPACE=${NAMESPACE:-sage_system}
-
-# é¢œè‰²è¾“å‡º
-RED='\033[0;31m'
-GREEN='\033[0;32m'
-YELLOW='\033[1;33m'
-BLUE='\033[0;34m'
-NC='\033[0m' # No Color
-
-# æ—¥å¿—å‡½æ•°
-log_info() {
-    echo -e "${BLUE}[INFO]${NC} $1"
-}
-
-log_success() {
-    echo -e "${GREEN}[SUCCESS]${NC} $1"
-}
-
-log_warning() {
-    echo -e "${YELLOW}[WARNING]${NC} $1"
-}
-
-log_error() {
-    echo -e "${RED}[ERROR]${NC} $1"
-}
-
-# æ£€æŸ¥å‘½ä»¤æ˜¯å¦å­˜åœ¨
-check_command() {
-    if ! command -v "$1" &> /dev/null; then
-        log_error "Command '$1' not found. Please install it first."
-        exit 1
-    fi
-}
-
-# æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
-check_port() {
-    local port=$1
-    if lsof -Pi :$port -sTCP:LISTEN -t >/dev/null 2>&1; then
-        return 0  # ç«¯å£è¢«å ç”¨
-    else
-        return 1  # ç«¯å£æœªè¢«å ç”¨
-    fi
-}
-
-# æ£€æŸ¥ç«¯å£èŒƒå›´æ˜¯å¦å¯ç”¨
-check_port_range() {
-    local start_port=$1
-    local end_port=$2
-    local conflicts=()
-    
-    for port in $(seq $start_port $end_port); do
-        if check_port $port; then
-            conflicts+=($port)
-        fi
-    done
-    
-    if [ ${#conflicts[@]} -gt 0 ]; then
-        log_warning "Ports in use: ${conflicts[*]}"
-        return 1
-    else
-        return 0
-    fi
-}
-
-# ç­‰å¾…ç«¯å£å¯åŠ¨
-wait_for_port() {
-    local port=$1
-    local timeout=${2:-30}
-    local count=0
-    
-    log_info "Waiting for port $port to be ready..."
-    
-    while [ $count -lt $timeout ]; do
-        if check_port $port; then
-            log_success "Port $port is ready"
-            return 0
-        fi
-        sleep 1
-        count=$((count + 1))
-        echo -n "."
-    done
-    
-    echo
-    log_error "Timeout waiting for port $port"
-    return 1
-}
-
-# è®¾ç½® Ray temp ç›®å½•æƒé™
-setup_ray_temp_dir() {
-    log_info "Setting up Ray temp directory: $RAY_TEMP_DIR"
-    
-    if [ ! -d "$RAY_TEMP_DIR" ]; then
-        sudo mkdir -p "$RAY_TEMP_DIR"
-        log_info "Created Ray temp directory: $RAY_TEMP_DIR"
-    fi
-    
-    sudo chmod 1777 "$RAY_TEMP_DIR"
-    log_success "Ray temp directory permissions set"
-}
-
-# è®¾ç½® Ray session æƒé™
-setup_ray_session_permissions() {
-    log_info "Setting Ray session permissions..."
-    
-    # æŸ¥æ‰¾æœ€æ–°çš„ Ray session
-    local ray_session_dir="/tmp/ray"
-    if [ -d "$ray_session_dir" ]; then
-        # ç»™ session_latest ç¬¦å·é“¾æ¥è®¾ç½®æƒé™
-        local session_latest="$ray_session_dir/session_latest"
-        if [ -L "$session_latest" ]; then
-            local actual_session=$(readlink -f "$session_latest")
-            if [ -d "$actual_session" ]; then
-                log_info "Setting permissions for Ray session: $actual_session"
-                sudo chmod -R 755 "$actual_session" 2>/dev/null || true
-                
-                # ç‰¹åˆ«è®¾ç½® session.json æƒé™
-                local session_json="$actual_session/session.json"
-                if [ -f "$session_json" ]; then
-                    sudo chmod 644 "$session_json"
-                    log_success "Ray session permissions updated"
-                else
-                    log_warning "session.json not found at $session_json"
-                fi
-            fi
-        fi
-    else
-        log_warning "Ray session directory not found at $ray_session_dir"
-    fi
-}
-
-# è®¾ç½®å‘½ä»¤è¡Œå·¥å…·
-setup_cli_tools() {
-    log_info "Setting up SAGE command line tools..."
-    
-    local script_dir=$(dirname $(realpath $0))
-    local controller_script="$script_dir/jobmanager_controller.py"
-    local symlink_path="/usr/local/bin/sage-jm"
-    
-    # æ£€æŸ¥ controller è„šæœ¬æ˜¯å¦å­˜åœ¨
-    if [ ! -f "$controller_script" ]; then
-        log_warning "JobManager controller script not found at $controller_script"
-        return 1
-    fi
-    
-    # ä½¿è„šæœ¬å¯æ‰§è¡Œ
-    chmod +x "$controller_script" 2>/dev/null || {
-        log_warning "Failed to make controller script executable (permission denied)"
-        log_info "You may need to run: chmod +x $controller_script"
-    }
-    
-    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç¬¦å·é“¾æ¥
-    if [ -L "$symlink_path" ]; then
-        local current_target=$(readlink "$symlink_path")
-        if [ "$current_target" = "$controller_script" ]; then
-            log_success "Command line tool 'sage-jm' is already set up"
-            return 0
-        else
-            log_info "Updating existing sage-jm symlink"
-            sudo rm -f "$symlink_path"
-        fi
-    elif [ -f "$symlink_path" ]; then
-        log_warning "File exists at $symlink_path (not a symlink)"
-        log_info "Please remove it manually to install sage-jm command"
-        return 1
-    fi
-    
-    # åˆ›å»ºç¬¦å·é“¾æ¥
-    if sudo ln -s "$controller_script" "$symlink_path" 2>/dev/null; then
-        log_success "Command line tool 'sage-jm' installed successfully"
-        
-        # éªŒè¯å®‰è£…
-        if command -v sage-jm >/dev/null 2>&1; then
-            log_success "sage-jm command is ready to use"
-        else
-            log_warning "sage-jm command not found in PATH"
-            log_info "You may need to restart your terminal or add /usr/local/bin to PATH"
-        fi
-    else
-        log_warning "Failed to create sage-jm symlink (sudo required)"
-        log_info "To manually install the command line tool, run:"
-        log_info "  sudo ln -s $controller_script /usr/local/bin/sage-jm"
-        return 1
-    fi
-}
-
-# æ£€æŸ¥ Ray çŠ¶æ€
-check_ray_status() {
-    if ray status >/dev/null 2>&1; then
-        log_success "Ray cluster is running"
-        ray status
-        return 0
-    else
-        log_info "Ray cluster is not running"
-        return 1
-    fi
-}
-
-# å¯åŠ¨ Ray head èŠ‚ç‚¹
-start_ray_head() {
-    log_info "Starting Ray head node..."
-    
-    # æ£€æŸ¥ç«¯å£å†²çª
-    log_info "Checking port availability..."
-    local ports_to_check=($RAY_HEAD_PORT $RAY_CLIENT_PORT $RAY_DASHBOARD_PORT)
-    local conflicts=()
-    
-    for port in "${ports_to_check[@]}"; do
-        if check_port $port; then
-            conflicts+=($port)
-        fi
-    done
-    
-    if [ ${#conflicts[@]} -gt 0 ]; then
-        log_error "Port conflicts detected: ${conflicts[*]}"
-        log_error "Please stop conflicting services or use different ports"
-        exit 1
-    fi
-    
-    # è®¾ç½® Ray temp ç›®å½•
-    setup_ray_temp_dir
-    
-    # å¯åŠ¨ Ray headï¼Œæ˜ç¡®æŒ‡å®šç«¯å£é¿å…å†²çª
-    log_info "Starting Ray with ports: GCS=$RAY_HEAD_PORT, Client=$RAY_CLIENT_PORT, Dashboard=$RAY_DASHBOARD_PORT"
-    
-    ray start --head \
-        --temp-dir="$RAY_TEMP_DIR" \
-        --disable-usage-stats \
-        --verbose \
-        --resources='{"jobmanager": 1.0}'  # æ·»åŠ è‡ªå®šä¹‰èµ„æº
-        # --port=$RAY_HEAD_PORT \
-        # --dashboard-port=$RAY_DASHBOARD_PORT \
-        #--ray-client-server-port=$RAY_CLIENT_PORT \
-
-    
-    # ç­‰å¾… Ray å¯åŠ¨å®Œæˆ
-    sleep 5
-    
-    # è®¾ç½® session æƒé™
-    setup_ray_session_permissions
-    
-    # éªŒè¯ Ray çŠ¶æ€
-    if check_ray_status; then
-        log_success "Ray head node started successfully"
-        log_info "Ray ports: GCS=$RAY_HEAD_PORT, Client=$RAY_CLIENT_PORT, Dashboard=$RAY_DASHBOARD_PORT"
-    else
-        log_error "Failed to start Ray head node"
-        exit 1
-    fi
-}
-
-# æ£€æŸ¥ JobManager Daemon çŠ¶æ€
-check_daemon_status() {
-    if check_port $DAEMON_PORT; then
-        # å°è¯•å¥åº·æ£€æŸ¥
-        local health_check=$(python3 -c "
-import sys
-sys.path.append('$(dirname $(realpath $0))/..')
-from sage_core.jobmanager_client import JobManagerClient
-try:
-    client = JobManagerClient('$DAEMON_HOST', $DAEMON_PORT)
-    response = client.health_check()
-    if response.get('status') == 'success':
-        print('healthy')
-    else:
-        print('unhealthy')
-except:
-    print('error')
-" 2>/dev/null)
-        
-        if [ "$health_check" = "healthy" ]; then
-            log_success "JobManager Daemon is running and healthy"
-            return 0
-        else
-            log_warning "JobManager Daemon is running but not healthy"
-            return 1
-        fi
-    else
-        log_info "JobManager Daemon is not running"
-        return 1
-    fi
-}
-
-# å¯åŠ¨ JobManager Daemon
-start_daemon() {
-    log_info "Starting JobManager Daemon..."
-    
-    # æ£€æŸ¥å®ˆæŠ¤è¿›ç¨‹ç«¯å£
-    if check_port $DAEMON_PORT; then
-        log_error "Port $DAEMON_PORT is already in use"
-        log_error "Please stop the conflicting service or use a different port"
-        return 1
-    fi
-    
-    # è·å–è„šæœ¬ç›®å½•
-    local script_dir=$(dirname $(realpath $0))
-    local project_root=$(dirname "$script_dir")
-    
-    # è®¾ç½® Python è·¯å¾„
-    export PYTHONPATH="$project_root:$PYTHONPATH"
-    
-    # åå°å¯åŠ¨å®ˆæŠ¤è¿›ç¨‹
-    nohup python3 "$script_dir/jobmanager_daemon.py" \
-        --host "$DAEMON_HOST" \
-        --port "$DAEMON_PORT" \
-        --actor-name "$ACTOR_NAME" \
-        --namespace "$NAMESPACE" \
-        > /tmp/sage_daemon.log 2>&1 &
-    
-    local daemon_pid=$!
-    log_info "JobManager Daemon started with PID: $daemon_pid"
-    
-    # ç­‰å¾…å®ˆæŠ¤è¿›ç¨‹å¯åŠ¨
-    if wait_for_port $DAEMON_PORT 30; then
-        # å†æ¬¡æ£€æŸ¥å¥åº·çŠ¶æ€
-        sleep 2
-        if check_daemon_status; then
-            log_success "JobManager Daemon started successfully"
-            echo $daemon_pid > /tmp/sage_daemon.pid
-        else
-            log_error "JobManager Daemon started but health check failed"
-            # æ˜¾ç¤ºæ—¥å¿—ä»¥å¸®åŠ©è°ƒè¯•
-            log_info "Daemon log output:"
-            tail -20 /tmp/sage_daemon.log
-            return 1
-        fi
-    else
-        log_error "JobManager Daemon failed to start"
-        # æ˜¾ç¤ºæ—¥å¿—ä»¥å¸®åŠ©è°ƒè¯•
-        log_info "Daemon log output:"
-        tail -20 /tmp/sage_daemon.log
-        return 1
-    fi
-}
-
-# æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
-show_status() {
-    echo
-    log_info "=== SAGE System Status ==="
-    
-    # Ray çŠ¶æ€
-    echo -e "\n${BLUE}Ray Cluster:${NC}"
-    if check_ray_status >/dev/null 2>&1; then
-        echo "  âœ“ Running"
-        echo "  GCS Port: $RAY_HEAD_PORT"
-        echo "  Client Port: $RAY_CLIENT_PORT" 
-        echo "  Dashboard Port: $RAY_DASHBOARD_PORT"
-    else
-        echo "  âœ— Not running"
-    fi
-    
-    # JobManager Daemon çŠ¶æ€
-    echo -e "\n${BLUE}JobManager Daemon:${NC}"
-    if check_daemon_status; then
-        echo "  âœ“ Running and healthy at $DAEMON_HOST:$DAEMON_PORT"
-        
-        # æ˜¾ç¤º Actor ä¿¡æ¯
-        local actor_info=$(python3 -c "
-import sys
-sys.path.append('$(dirname $(realpath $0))/..')
-from sage_core.jobmanager_client import JobManagerClient
-try:
-    client = JobManagerClient('$DAEMON_HOST', $DAEMON_PORT)
-    response = client.get_actor_info()
-    if response.get('status') == 'success':
-        info = response.get('actor_info', {})
-        print(f\"  Actor: {info.get('actor_name', 'N/A')}@{info.get('namespace', 'N/A')}\")
-        print(f\"  Actor ID: {info.get('actor_id', 'N/A')}\")
-    else:
-        print('  Actor info unavailable')
-except:
-    print('  Actor info error')
-" 2>/dev/null)
-        echo "$actor_info"
-    else
-        echo "  âœ— Not running or unhealthy"
-    fi
-    
-    # å‘½ä»¤è¡Œå·¥å…·çŠ¶æ€
-    echo -e "\n${BLUE}Command Line Tools:${NC}"
-    if command -v sage-jm >/dev/null 2>&1; then
-        echo "  âœ“ sage-jm command available"
-    else
-        echo "  âœ— sage-jm command not available"
-    fi
-    
-    # è®¿é—®ä¿¡æ¯
-    echo -e "\n${BLUE}Access Information:${NC}"
-    echo "  Ray Dashboard: http://localhost:$RAY_DASHBOARD_PORT"
-    echo "  JobManager API: tcp://$DAEMON_HOST:$DAEMON_PORT"
-    echo "  Logs: /tmp/sage/logs/"
-    echo "  Daemon Log: /tmp/sage_daemon.log"
-}
-
-# æ˜¾ç¤ºä½¿ç”¨æŒ‡å—
-show_usage_guide() {
-    echo
-    log_info "=== SAGE System Ready ==="
-    echo
-    echo -e "${GREEN}ğŸ‰ SAGE system started successfully!${NC}"
-    echo
-    echo -e "${BLUE}Quick Start Guide:${NC}"
-    echo
-    
-    # æ£€æŸ¥å‘½ä»¤è¡Œå·¥å…·æ˜¯å¦å¯ç”¨
-    if command -v sage-jm >/dev/null 2>&1; then
-        echo -e "${GREEN}ğŸ“‹ Job Management Commands:${NC}"
-        echo "  sage-jm list                    # List all jobs"
-        echo "  sage-jm show <job_uuid>         # Show job details"
-        echo "  sage-jm stop <job_uuid>         # Stop a job"
-        echo "  sage-jm health                  # Check system health"
-        echo "  sage-jm monitor                 # Real-time monitoring"
-        echo "  sage-jm shell                   # Interactive shell"
-        echo
-        echo -e "${GREEN}ğŸ” Monitoring:${NC}"
-        echo "  sage-jm monitor --refresh 3     # Monitor with 3s refresh"
-        echo "  sage-jm watch <job_uuid>        # Watch specific job"
-        echo
-        echo -e "${GREEN}â„¹ï¸  Getting Help:${NC}"
-        echo "  sage-jm --help                  # Show all commands"
-        echo "  sage-jm <command> --help        # Command-specific help"
-        echo
-    else
-        echo -e "${YELLOW}âš ï¸  Command line tool setup:${NC}"
-        echo "  The 'sage-jm' command is not available in your PATH."
-        echo "  To set it up manually, run:"
-        echo "    sudo ln -s $(dirname $(realpath $0))/jobmanager_controller.py /usr/local/bin/sage-jm"
-        echo
-    fi
-    
-    echo -e "${GREEN}ğŸŒ Web Interfaces:${NC}"
-    echo "  Ray Dashboard: http://localhost:$RAY_DASHBOARD_PORT"
-    echo
-    echo -e "${GREEN}ğŸ“ Important Paths:${NC}"
-    echo "  Logs: {project folder}/logs/jobmanager_{timestamp}.log"
-    echo "  Daemon Log: {project folder}/logs/daemon"
-    echo "  Ray Temp: $RAY_TEMP_DIR"
-    echo
-    echo -e "${GREEN}ğŸ”„ System Management:${NC}"
-    echo "  ./sage_deployment.sh status      # Check system status"
-    echo "  ./sage_deployment.sh restart     # Restart system"
-    echo "  ./sage_deployment.sh stop        # Stop system"
-    echo
-    echo -e "${BLUE}Happy coding with SAGE! ğŸš€${NC}"
-}
-
-# åœæ­¢ç³»ç»Ÿ
-stop_system() {
-    log_info "Stopping SAGE system..."
-    
-    # åœæ­¢ JobManager Daemon
-    if [ -f /tmp/sage_daemon.pid ]; then
-        local daemon_pid=$(cat /tmp/sage_daemon.pid)
-        if kill -0 $daemon_pid 2>/dev/null; then
-            log_info "Stopping JobManager Daemon (PID: $daemon_pid)..."
-            kill $daemon_pid
-            sleep 2
-            # å¼ºåˆ¶æ€æ­»
-            kill -9 $daemon_pid 2>/dev/null || true
-        fi
-        rm -f /tmp/sage_daemon.pid
-    fi
-    
-    # åœæ­¢ Ray
-    if ray status >/dev/null 2>&1; then
-        log_info "Stopping Ray cluster..."
-        ray stop
-    fi
-    
-    log_success "SAGE system stopped"
-}
-
-# å®‰è£…å‘½ä»¤è¡Œå·¥å…·
-install_cli() {
-    log_info "Installing SAGE command line tools..."
-    
-    # è®¾ç½®å‘½ä»¤è¡Œå·¥å…·
-    if setup_cli_tools; then
-        echo
-        log_success "âœ… Command line tools installed successfully!"
-        echo
-        echo -e "${GREEN}You can now use:${NC}"
-        echo "  sage-jm --help          # Show help"
-        echo "  sage-jm list             # List jobs"
-        echo "  sage-jm health           # Check health"
-        echo "  sage-jm monitor          # Monitor jobs"
-        echo
-    else
-        echo
-        log_error "âŒ Failed to install command line tools"
-        echo
-        echo -e "${YELLOW}Manual installation:${NC}"
-        echo "  chmod +x $(dirname $(realpath $0))/jobmanager_controller.py"
-        echo "  sudo ln -s $(dirname $(realpath $0))/jobmanager_controller.py /usr/local/bin/sage-jm"
-    fi
-}
-
-# ä¸»å‡½æ•°
-main() {
-    case "${1:-start}" in
-        start)
-            log_info "Starting SAGE System..."
-            log_info "Configuration:"
-            log_info "  Ray GCS Port: $RAY_HEAD_PORT"
-            log_info "  Ray Client Port: $RAY_CLIENT_PORT"
-            log_info "  Ray Dashboard Port: $RAY_DASHBOARD_PORT"
-            log_info "  Daemon Port: $DAEMON_PORT"
-            
-            # æ£€æŸ¥ä¾èµ–
-            check_command "ray"
-            check_command "python3"
-            check_command "lsof"
-            
-            # 1. æ£€æŸ¥å¹¶å¯åŠ¨ Ray
-            if ! check_ray_status >/dev/null 2>&1; then
-                start_ray_head
-            else
-                log_success "Ray cluster is already running"
-                # ä»ç„¶éœ€è¦è®¾ç½®æƒé™
-                setup_ray_session_permissions
-            fi
-            
-            # 2. æ£€æŸ¥å¹¶å¯åŠ¨ JobManager Daemon
-            if ! check_daemon_status; then
-                start_daemon
-            else
-                log_success "JobManager Daemon is already running"
-            fi
-            
-            # 3. è®¾ç½®å‘½ä»¤è¡Œå·¥å…·
-            setup_cli_tools
-            
-            # 4. æ˜¾ç¤ºçŠ¶æ€å’Œä½¿ç”¨æŒ‡å—
-            show_status
-            show_usage_guide
-            ;;
-            
-        stop)
-            stop_system
-            ;;
-            
-        restart)
-            stop_system
-            sleep 2
-            main start
-            ;;
-            
-        status)
-            show_status
-            ;;
-            
-        install-cli)
-            install_cli
-            ;;
-            
-        *)
-            echo "Usage: $0 {start|stop|restart|status|install-cli}"
-            echo
-            echo "Commands:"
-            echo "  start       - Start Ray cluster and JobManager Daemon"
-            echo "  stop        - Stop the entire SAGE system"
-            echo "  restart     - Restart the system"
-            echo "  status      - Show system status"
-            echo "  install-cli - Install command line tools only"
-            echo
-            echo "Environment Variables:"
-            echo "  RAY_HEAD_PORT=$RAY_HEAD_PORT         (GCS server port)"
-            echo "  RAY_CLIENT_PORT=$RAY_CLIENT_PORT       (Client server port)"
-            echo "  RAY_DASHBOARD_PORT=$RAY_DASHBOARD_PORT"
-            echo "  DAEMON_HOST=$DAEMON_HOST"
-            echo "  DAEMON_PORT=$DAEMON_PORT"
-            echo
-            echo "After starting, use 'sage-jm' command to manage jobs:"
-            echo "  sage-jm list           # List all jobs"
-            echo "  sage-jm health         # Check system health"
-            echo "  sage-jm monitor        # Real-time monitoring"
-            echo "  sage-jm --help         # Show all commands"
-            exit 1
-            ;;
-    esac
-}
-
-# æ•è·ä¿¡å·
-trap 'log_info "Interrupted, stopping system..."; stop_system; exit 1' INT TERM
-
-# æ‰§è¡Œä¸»å‡½æ•°
-main "$@"
\ No newline at end of file
diff --git a/deployment/start_jobmanager.sh b/deployment/start_jobmanager.sh
deleted file mode 100644
index e590c08..0000000
--- a/deployment/start_jobmanager.sh
+++ /dev/null
@@ -1,52 +0,0 @@
-#!/bin/bash
-# filepath: /home/tjy/SAGE/scripts/start_jobmanager.sh
-
-SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
-DAEMON_SCRIPT="$SCRIPT_DIR/jobmanager_daemon.py"
-
-# æ£€æŸ¥å¹¶å¯åŠ¨JobManager
-echo "ğŸš€ Checking JobManager service..."
-
-python3 "$DAEMON_SCRIPT" ensure
-
-if [ $? -eq 0 ]; then
-    echo "âœ… JobManager is running on 127.0.0.1:19000"
-else
-    echo "âŒ Failed to start JobManager"
-    exit 1
-fi
-
-# usage:
-# # ç»™è„šæœ¬æ‰§è¡Œæƒé™
-# chmod +x scripts/jobmanager_daemon.py
-# chmod +x scripts/start_jobmanager.sh
-
-# # 1. æ£€æŸ¥å¹¶å¯åŠ¨JobManagerï¼ˆæ¨èï¼‰
-# python3 scripts/jobmanager_daemon.py ensure
-
-# # 2. ä½¿ç”¨bashè„šæœ¬
-# ./scripts/start_jobmanager.sh
-
-# # 3. å…¶ä»–å‘½ä»¤
-# python3 scripts/jobmanager_daemon.py status    # æŸ¥çœ‹çŠ¶æ€
-# python3 scripts/jobmanager_daemon.py stop      # åœæ­¢æœåŠ¡
-# python3 scripts/jobmanager_daemon.py restart   # é‡å¯æœåŠ¡
-# python3 scripts/jobmanager_daemon.py start     # å¯åŠ¨æœåŠ¡
-
-# systemd service:
-
-# [Unit]
-# Description=SAGE JobManager Service
-# After=network.target
-
-# [Service]
-# Type=simple
-# User=tjy
-# WorkingDirectory=/home/tjy/SAGE
-# ExecStart=/usr/bin/python3 /home/tjy/SAGE/scripts/jobmanager_daemon.py start
-# ExecStop=/usr/bin/python3 /home/tjy/SAGE/scripts/jobmanager_daemon.py stop
-# Restart=always
-# RestartSec=10
-
-# [Install]
-# WantedBy=multi-user.target
\ No newline at end of file
diff --git a/deployment/stop_ray.sh b/deployment/stop_ray.sh
deleted file mode 100755
index d52941a..0000000
--- a/deployment/stop_ray.sh
+++ /dev/null
@@ -1,5 +0,0 @@
-# ç¬¬ä¸€æ­¥ï¼šåœæ­¢è€çš„ Ray å®ä¾‹
-ray stop --force
-
-# ç¬¬äºŒæ­¥ï¼šæ¸…ç† tmp ä¸‹æ®‹ç•™çš„ Ray æ–‡ä»¶
-rm -rf /tmp/ray
\ No newline at end of file
diff --git a/playground/AbstractFileSource.java b/playground/AbstractFileSource.java
deleted file mode 100644
index 8b0e197..0000000
--- a/playground/AbstractFileSource.java
+++ /dev/null
@@ -1,349 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.file.src;
-
-import org.apache.flink.annotation.PublicEvolving;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.connector.source.Boundedness;
-import org.apache.flink.api.connector.source.Source;
-import org.apache.flink.api.connector.source.SourceReader;
-import org.apache.flink.api.connector.source.SourceReaderContext;
-import org.apache.flink.api.connector.source.SplitEnumerator;
-import org.apache.flink.api.connector.source.SplitEnumeratorContext;
-import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
-import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;
-import org.apache.flink.connector.file.src.enumerate.FileEnumerator;
-import org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumerator;
-import org.apache.flink.connector.file.src.impl.FileSourceReader;
-import org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator;
-import org.apache.flink.connector.file.src.reader.BulkFormat;
-import org.apache.flink.connector.file.src.reader.StreamFormat;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-import org.apache.flink.util.FlinkRuntimeException;
-
-import javax.annotation.Nullable;
-
-import java.io.IOException;
-import java.time.Duration;
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.stream.Collectors;
-
-import static org.apache.flink.util.Preconditions.checkArgument;
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * The base class for File Sources. The main implementation to use is the {@link FileSource}, which
- * also has the majority of the documentation.
- *
- * <p>To read new formats, one commonly does NOT need to extend this class, but should implement a
- * new Format Reader (like {@link StreamFormat}, {@link BulkFormat} and use it with the {@code
- * FileSource}.
- *
- * <p>The only reason to extend this class is when a source needs a different type of <i>split</i>,
- * meaning an extension of the {@link FileSourceSplit} to carry additional information.
- *
- * @param <T> The type of the events/records produced by this source.
- * @param <SplitT> The subclass type of the FileSourceSplit used by the source implementation.
- */
-@PublicEvolving
-public abstract class AbstractFileSource<T, SplitT extends FileSourceSplit>
-        implements Source<T, SplitT, PendingSplitsCheckpoint<SplitT>>, ResultTypeQueryable<T> {
-
-    private static final long serialVersionUID = 1L;
-
-    final Path[] inputPaths;
-
-    private final FileEnumerator.Provider enumeratorFactory;
-
-    private final FileSplitAssigner.Provider assignerFactory;
-
-    private final BulkFormat<T, SplitT> readerFormat;
-
-    @Nullable private final ContinuousEnumerationSettings continuousEnumerationSettings;
-
-    // ------------------------------------------------------------------------
-
-    protected AbstractFileSource(
-            final Path[] inputPaths,
-            final FileEnumerator.Provider fileEnumerator,
-            final FileSplitAssigner.Provider splitAssigner,
-            final BulkFormat<T, SplitT> readerFormat,
-            @Nullable final ContinuousEnumerationSettings continuousEnumerationSettings) {
-
-        checkArgument(inputPaths.length > 0);
-        this.inputPaths = inputPaths;
-        this.enumeratorFactory = checkNotNull(fileEnumerator);
-        this.assignerFactory = checkNotNull(splitAssigner);
-        this.readerFormat = checkNotNull(readerFormat);
-        this.continuousEnumerationSettings = continuousEnumerationSettings;
-    }
-
-    // ------------------------------------------------------------------------
-    //  Getters
-    // ------------------------------------------------------------------------
-
-    protected FileEnumerator.Provider getEnumeratorFactory() {
-        return enumeratorFactory;
-    }
-
-    public FileSplitAssigner.Provider getAssignerFactory() {
-        return assignerFactory;
-    }
-
-    @Nullable
-    public ContinuousEnumerationSettings getContinuousEnumerationSettings() {
-        return continuousEnumerationSettings;
-    }
-
-    // ------------------------------------------------------------------------
-    //  Source API Methods
-    // ------------------------------------------------------------------------
-
-    @Override
-    public Boundedness getBoundedness() {
-        return continuousEnumerationSettings == null
-                ? Boundedness.BOUNDED
-                : Boundedness.CONTINUOUS_UNBOUNDED;
-    }
-
-    @Override
-    public SourceReader<T, SplitT> createReader(SourceReaderContext readerContext) {
-        return new FileSourceReader<>(
-                readerContext, readerFormat, readerContext.getConfiguration());
-    }
-
-    @Override
-    public SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>> createEnumerator(
-            SplitEnumeratorContext<SplitT> enumContext) {
-
-        final FileEnumerator enumerator = enumeratorFactory.create();
-
-        // read the initial set of splits (which is also the total set of splits for bounded
-        // sources)
-        final Collection<FileSourceSplit> splits;
-        try {
-            // TODO - in the next cleanup pass, we should try to remove the need to "wrap unchecked"
-            // here
-            splits = enumerator.enumerateSplits(inputPaths, enumContext.currentParallelism());
-        } catch (IOException e) {
-            throw new FlinkRuntimeException("Could not enumerate file splits", e);
-        }
-
-        return createSplitEnumerator(enumContext, enumerator, splits, null);
-    }
-
-    @Override
-    public SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>> restoreEnumerator(
-            SplitEnumeratorContext<SplitT> enumContext,
-            PendingSplitsCheckpoint<SplitT> checkpoint) {
-
-        final FileEnumerator enumerator = enumeratorFactory.create();
-
-        // cast this to a collection of FileSourceSplit because the enumerator code work
-        // non-generically just on that base split type
-        @SuppressWarnings("unchecked")
-        final Collection<FileSourceSplit> splits =
-                (Collection<FileSourceSplit>) checkpoint.getSplits();
-
-        return createSplitEnumerator(
-                enumContext, enumerator, splits, checkpoint.getAlreadyProcessedPaths());
-    }
-
-    @Override
-    public abstract SimpleVersionedSerializer<SplitT> getSplitSerializer();
-
-    @Override
-    public SimpleVersionedSerializer<PendingSplitsCheckpoint<SplitT>>
-            getEnumeratorCheckpointSerializer() {
-        return new PendingSplitsCheckpointSerializer<>(getSplitSerializer());
-    }
-
-    @Override
-    public TypeInformation<T> getProducedType() {
-        return readerFormat.getProducedType();
-    }
-
-    // ------------------------------------------------------------------------
-    //  helpers
-    // ------------------------------------------------------------------------
-
-    private SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>> createSplitEnumerator(
-            SplitEnumeratorContext<SplitT> context,
-            FileEnumerator enumerator,
-            Collection<FileSourceSplit> splits,
-            @Nullable Collection<Path> alreadyProcessedPaths) {
-
-        // cast this to a collection of FileSourceSplit because the enumerator code work
-        // non-generically just on that base split type
-        @SuppressWarnings("unchecked")
-        final SplitEnumeratorContext<FileSourceSplit> fileSplitContext =
-                (SplitEnumeratorContext<FileSourceSplit>) context;
-
-        final FileSplitAssigner splitAssigner = assignerFactory.create(splits);
-
-        if (continuousEnumerationSettings == null) {
-            // bounded case
-            return castGeneric(new StaticFileSplitEnumerator(fileSplitContext, splitAssigner));
-        } else {
-            // unbounded case
-            if (alreadyProcessedPaths == null) {
-                alreadyProcessedPaths = splitsToPaths(splits);
-            }
-
-            return castGeneric(
-                    new ContinuousFileSplitEnumerator(
-                            fileSplitContext,
-                            enumerator,
-                            splitAssigner,
-                            inputPaths,
-                            alreadyProcessedPaths,
-                            continuousEnumerationSettings.getDiscoveryInterval().toMillis()));
-        }
-    }
-
-    @SuppressWarnings("unchecked")
-    private SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>> castGeneric(
-            final SplitEnumerator<FileSourceSplit, PendingSplitsCheckpoint<FileSourceSplit>>
-                    enumerator) {
-
-        // cast arguments away then cast them back. Java Generics Hell :-/
-        return (SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>>)
-                (SplitEnumerator<?, ?>) enumerator;
-    }
-
-    private static Collection<Path> splitsToPaths(Collection<FileSourceSplit> splits) {
-        return splits.stream()
-                .map(FileSourceSplit::path)
-                .collect(Collectors.toCollection(HashSet::new));
-    }
-
-    // ------------------------------------------------------------------------
-    //  Builder
-    // ------------------------------------------------------------------------
-
-    /**
-     * The generic base builder. This builder carries a <i>SELF</i> type to make it convenient to
-     * extend this for subclasses, using the following pattern.
-     *
-     * <pre>{@code
-     * public class SubBuilder<T> extends AbstractFileSourceBuilder<T, SubBuilder<T>> {
-     *     ...
-     * }
-     * }</pre>
-     *
-     * <p>That way, all return values from builder method defined here are typed to the sub-class
-     * type and support fluent chaining.
-     *
-     * <p>We don't make the publicly visible builder generic with a SELF type, because it leads to
-     * generic signatures that can look complicated and confusing.
-     */
-    protected abstract static class AbstractFileSourceBuilder<
-            T,
-            SplitT extends FileSourceSplit,
-            SELF extends AbstractFileSourceBuilder<T, SplitT, SELF>> {
-
-        // mandatory - have no defaults
-        protected final Path[] inputPaths;
-        protected final BulkFormat<T, SplitT> readerFormat;
-
-        // optional - have defaults
-        protected FileEnumerator.Provider fileEnumerator;
-        protected FileSplitAssigner.Provider splitAssigner;
-        @Nullable protected ContinuousEnumerationSettings continuousSourceSettings;
-
-        protected AbstractFileSourceBuilder(
-                final Path[] inputPaths,
-                final BulkFormat<T, SplitT> readerFormat,
-                final FileEnumerator.Provider defaultFileEnumerator,
-                final FileSplitAssigner.Provider defaultSplitAssigner) {
-
-            this.inputPaths = checkNotNull(inputPaths);
-            this.readerFormat = checkNotNull(readerFormat);
-            this.fileEnumerator = defaultFileEnumerator;
-            this.splitAssigner = defaultSplitAssigner;
-        }
-
-        /** Creates the file source with the settings applied to this builder. */
-        public abstract AbstractFileSource<T, SplitT> build();
-
-        /**
-         * Sets this source to streaming ("continuous monitoring") mode.
-         *
-         * <p>This makes the source a "continuous streaming" source that keeps running, monitoring
-         * for new files, and reads these files when they appear and are discovered by the
-         * monitoring.
-         *
-         * <p>The interval in which the source checks for new files is the {@code
-         * discoveryInterval}. Shorter intervals mean that files are discovered more quickly, but
-         * also imply more frequent listing or directory traversal of the file system / object
-         * store.
-         */
-        public SELF monitorContinuously(Duration discoveryInterval) {
-            checkNotNull(discoveryInterval, "discoveryInterval");
-            checkArgument(
-                    !(discoveryInterval.isNegative() || discoveryInterval.isZero()),
-                    "discoveryInterval must be > 0");
-
-            this.continuousSourceSettings = new ContinuousEnumerationSettings(discoveryInterval);
-            return self();
-        }
-
-        /**
-         * Sets this source to bounded (batch) mode.
-         *
-         * <p>In this mode, the source processes the files that are under the given paths when the
-         * application is started. Once all files are processed, the source will finish.
-         *
-         * <p>This setting is also the default behavior. This method is mainly here to "switch back"
-         * to bounded (batch) mode, or to make it explicit in the source construction.
-         */
-        public SELF processStaticFileSet() {
-            this.continuousSourceSettings = null;
-            return self();
-        }
-
-        /**
-         * Configures the {@link FileEnumerator} for the source. The File Enumerator is responsible
-         * for selecting from the input path the set of files that should be processed (and which to
-         * filter out). Furthermore, the File Enumerator may split the files further into
-         * sub-regions, to enable parallelization beyond the number of files.
-         */
-        public SELF setFileEnumerator(FileEnumerator.Provider fileEnumerator) {
-            this.fileEnumerator = checkNotNull(fileEnumerator);
-            return self();
-        }
-
-        /**
-         * Configures the {@link FileSplitAssigner} for the source. The File Split Assigner
-         * determines which parallel reader instance gets which {@link FileSourceSplit}, and in
-         * which order these splits are assigned.
-         */
-        public SELF setSplitAssigner(FileSplitAssigner.Provider splitAssigner) {
-            this.splitAssigner = checkNotNull(splitAssigner);
-            return self();
-        }
-
-        @SuppressWarnings("unchecked")
-        private SELF self() {
-            return (SELF) this;
-        }
-    }
-}
\ No newline at end of file
diff --git a/playground/BatchWordCount.java b/playground/BatchWordCount.java
deleted file mode 100644
index 968cd80..0000000
--- a/playground/BatchWordCount.java
+++ /dev/null
@@ -1,29 +0,0 @@
-ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
-
-DataSet<String> text = env.readTextFile("/path/to/file");
-
-DataSet<Tuple2<String, Integer>> counts =
-        // split up the lines in pairs (2-tuples) containing: (word,1)
-        text.flatMap(new Tokenizer())
-        // group by the tuple field "0" and sum up tuple field "1"
-        .groupBy(0)
-        .sum(1);
-
-counts.writeAsCsv(outputPath, "\n", " ");
-
-// User-defined functions
-public static class Tokenizer implements FlatMapFunction<String, Tuple2<String, Integer>> {
-
-    @Override
-    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
-        // normalize and split the line
-        String[] tokens = value.toLowerCase().split("\\W+");
-
-        // emit the pairs
-        for (String token : tokens) {
-            if (token.length() > 0) {
-                out.collect(new Tuple2<String, Integer>(token, 1));
-            }
-        }
-    }
-}
\ No newline at end of file
diff --git a/playground/ExecutionEnvironmentImpl.java b/playground/ExecutionEnvironmentImpl.java
deleted file mode 100644
index 81c984b..0000000
--- a/playground/ExecutionEnvironmentImpl.java
+++ /dev/null
@@ -1,363 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.datastream.impl;
-
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.RuntimeExecutionMode;
-import org.apache.flink.api.common.eventtime.WatermarkStrategy;
-import org.apache.flink.api.common.functions.InvalidTypesException;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.connector.dsv2.FromDataSource;
-import org.apache.flink.api.connector.dsv2.Source;
-import org.apache.flink.api.connector.dsv2.WrappedSource;
-import org.apache.flink.api.dag.Transformation;
-import org.apache.flink.api.java.typeutils.MissingTypeInfo;
-import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
-import org.apache.flink.api.java.typeutils.TypeExtractor;
-import org.apache.flink.configuration.Configuration;
-import org.apache.flink.configuration.DeploymentOptions;
-import org.apache.flink.configuration.ExecutionOptions;
-import org.apache.flink.configuration.ReadableConfig;
-import org.apache.flink.connector.datagen.functions.FromElementsGeneratorFunction;
-import org.apache.flink.connector.datagen.source.DataGeneratorSource;
-import org.apache.flink.core.execution.DefaultExecutorServiceLoader;
-import org.apache.flink.core.execution.JobClient;
-import org.apache.flink.core.execution.PipelineExecutor;
-import org.apache.flink.core.execution.PipelineExecutorFactory;
-import org.apache.flink.core.execution.PipelineExecutorServiceLoader;
-import org.apache.flink.datastream.api.ExecutionEnvironment;
-import org.apache.flink.datastream.api.stream.NonKeyedPartitionStream.ProcessConfigurableAndNonKeyedPartitionStream;
-import org.apache.flink.datastream.impl.stream.NonKeyedPartitionStreamImpl;
-import org.apache.flink.datastream.impl.utils.StreamUtils;
-import org.apache.flink.streaming.api.environment.CheckpointConfig;
-import org.apache.flink.streaming.api.graph.StreamGraph;
-import org.apache.flink.streaming.api.graph.StreamGraphGenerator;
-import org.apache.flink.streaming.api.transformations.SourceTransformation;
-import org.apache.flink.streaming.runtime.translators.DataStreamV2SinkTransformationTranslator;
-import org.apache.flink.util.ExceptionUtils;
-import org.apache.flink.util.FlinkException;
-import org.apache.flink.util.Preconditions;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutionException;
-
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * The implementation of {@link ExecutionEnvironment}.
- *
- * <p>IMPORTANT: Even though this is not part of public API, {@link ExecutionEnvironment} will get
- * this class instance through reflection, so we must ensure that the package path, class name and
- * the signature of {@link #newInstance()} does not change.
- */
-public class ExecutionEnvironmentImpl implements ExecutionEnvironment {
-    private final List<Transformation<?>> transformations = new ArrayList<>();
-
-    private final ExecutionConfig executionConfig;
-
-    /** Settings that control the checkpointing behavior. */
-    private final CheckpointConfig checkpointCfg;
-
-    private final Configuration configuration;
-
-    private final ClassLoader userClassloader;
-
-    private final PipelineExecutorServiceLoader executorServiceLoader;
-
-    /**
-     * The environment of the context (local by default, cluster if invoked through command line).
-     */
-    private static ExecutionEnvironmentFactory contextEnvironmentFactory = null;
-
-    static {
-        try {
-            // All transformation translator must be put to a map in StreamGraphGenerator, but
-            // streaming-java is not depend on process-function module, using reflect to handle
-            // this.
-            DataStreamV2SinkTransformationTranslator.registerSinkTransformationTranslator();
-        } catch (Exception e) {
-            throw new RuntimeException(
-                    "Can not register process function transformation translator.", e);
-        }
-    }
-
-    /**
-     * Create and return an instance of {@link ExecutionEnvironment}.
-     *
-     * <p>IMPORTANT: The method is only expected to be called by {@link ExecutionEnvironment} via
-     * reflection, so we must ensure that the package path, class name and the signature of this
-     * method does not change.
-     */
-    public static ExecutionEnvironment newInstance() {
-        if (contextEnvironmentFactory != null) {
-            return contextEnvironmentFactory.createExecutionEnvironment(new Configuration());
-        } else {
-            final Configuration configuration = new Configuration();
-            configuration.set(DeploymentOptions.TARGET, "local");
-            configuration.set(DeploymentOptions.ATTACHED, true);
-            return new ExecutionEnvironmentImpl(
-                    new DefaultExecutorServiceLoader(), configuration, null);
-        }
-    }
-
-    ExecutionEnvironmentImpl(
-            PipelineExecutorServiceLoader executorServiceLoader,
-            Configuration configuration,
-            ClassLoader classLoader) {
-        this.executorServiceLoader = checkNotNull(executorServiceLoader);
-        this.configuration = configuration;
-        this.executionConfig = new ExecutionConfig(this.configuration);
-        this.checkpointCfg = new CheckpointConfig(this.configuration);
-        this.userClassloader = classLoader == null ? getClass().getClassLoader() : classLoader;
-        configure(configuration, userClassloader);
-    }
-
-    @Override
-    public void execute(String jobName) throws Exception {
-        StreamGraph streamGraph = getStreamGraph();
-        if (jobName != null) {
-            streamGraph.setJobName(jobName);
-        }
-
-        execute(streamGraph);
-    }
-
-    @Override
-    public RuntimeExecutionMode getExecutionMode() {
-        return configuration.get(ExecutionOptions.RUNTIME_MODE);
-    }
-
-    @Override
-    public ExecutionEnvironment setExecutionMode(RuntimeExecutionMode runtimeMode) {
-        checkNotNull(runtimeMode);
-        configuration.set(ExecutionOptions.RUNTIME_MODE, runtimeMode);
-        return this;
-    }
-
-    protected static void initializeContextEnvironment(ExecutionEnvironmentFactory ctx) {
-        contextEnvironmentFactory = ctx;
-    }
-
-    protected static void resetContextEnvironment() {
-        contextEnvironmentFactory = null;
-    }
-    /******
-     * // <OUT>: æ–¹æ³•æ³›å‹
-     * 
-     * 
-
-     */
-    
-    @Override
-    public <OUT> ProcessConfigurableAndNonKeyedPartitionStream<OUT> fromSource(
-            Source<OUT> source, String sourceName) {
-        if (source instanceof WrappedSource) {
-            org.apache.flink.api.connector.source.Source<OUT, ?, ?> innerSource =
-                    ((WrappedSource<OUT>) source).getWrappedSource();
-            final TypeInformation<OUT> resolvedTypeInfo =
-                    getSourceTypeInfo(innerSource, sourceName);
-
-            SourceTransformation<OUT, ?, ?> sourceTransformation =
-                    new SourceTransformation<>(
-                            sourceName,
-                            innerSource,
-                            WatermarkStrategy.noWatermarks(),
-                            resolvedTypeInfo,
-                            getParallelism(),
-                            false);
-            return StreamUtils.wrapWithConfigureHandle(
-                    new NonKeyedPartitionStreamImpl<>(this, sourceTransformation));
-        } else if (source instanceof FromDataSource) {
-            Collection<OUT> data = ((FromDataSource<OUT>) source).getData();
-            TypeInformation<OUT> outType = extractTypeInfoFromCollection(data);
-
-            FromElementsGeneratorFunction<OUT> generatorFunction =
-                    new FromElementsGeneratorFunction<>(outType, executionConfig, data);
-
-            DataGeneratorSource<OUT> generatorSource =
-                    new DataGeneratorSource<>(generatorFunction, data.size(), outType);
-
-            return fromSource(new WrappedSource<>(generatorSource), "Collection Source");
-        } else {
-            throw new UnsupportedOperationException(
-                    "Unsupported type of source, you could use DataStreamV2SourceUtils to wrap a FLIP-27 based source.");
-        }
-    }
-
-    public Configuration getConfiguration() {
-        return this.configuration;
-    }
-
-    public ExecutionConfig getExecutionConfig() {
-        return executionConfig;
-    }
-
-    public int getParallelism() {
-        return executionConfig.getParallelism();
-    }
-
-    public List<Transformation<?>> getTransformations() {
-        return transformations;
-    }
-
-    public void setParallelism(int parallelism) {
-        executionConfig.setParallelism(parallelism);
-    }
-
-    public CheckpointConfig getCheckpointCfg() {
-        return checkpointCfg;
-    }
-
-    // -----------------------------------------------
-    //              Internal Methods
-    // -----------------------------------------------
-
-    private static <OUT> TypeInformation<OUT> extractTypeInfoFromCollection(Collection<OUT> data) {
-        Preconditions.checkNotNull(data, "Collection must not be null");
-        if (data.isEmpty()) {
-            throw new IllegalArgumentException("Collection must not be empty");
-        }
-
-        OUT first = data.iterator().next();
-        if (first == null) {
-            throw new IllegalArgumentException("Collection must not contain null elements");
-        }
-
-        TypeInformation<OUT> typeInfo;
-        try {
-            typeInfo = TypeExtractor.getForObject(first);
-        } catch (Exception e) {
-            throw new RuntimeException(
-                    "Could not create TypeInformation for type "
-                            + first.getClass()
-                            + "; please specify the TypeInformation manually via the version of the "
-                            + "method that explicitly accepts it as an argument.",
-                    e);
-        }
-        return typeInfo;
-    }
-
-    @SuppressWarnings("unchecked")
-    private static <OUT, T extends TypeInformation<OUT>> T getSourceTypeInfo(
-            org.apache.flink.api.connector.source.Source<OUT, ?, ?> source, String sourceName) {
-        TypeInformation<OUT> resolvedTypeInfo = null;
-        if (source instanceof ResultTypeQueryable) {
-            resolvedTypeInfo = ((ResultTypeQueryable<OUT>) source).getProducedType();
-        }
-        if (resolvedTypeInfo == null) {
-            try {
-                resolvedTypeInfo =
-                        TypeExtractor.createTypeInfo(
-                                org.apache.flink.api.connector.source.Source.class,
-                                source.getClass(),
-                                0,
-                                null,
-                                null);
-            } catch (final InvalidTypesException e) {
-                resolvedTypeInfo = (TypeInformation<OUT>) new MissingTypeInfo(sourceName, e);
-            }
-        }
-        return (T) resolvedTypeInfo;
-    }
-
-    public void addOperator(Transformation<?> transformation) {
-        checkNotNull(transformation, "transformation must not be null.");
-        this.transformations.add(transformation);
-    }
-
-    private void execute(StreamGraph streamGraph) throws Exception {
-        final JobClient jobClient = executeAsync(streamGraph);
-
-        try {
-            if (configuration.get(DeploymentOptions.ATTACHED)) {
-                jobClient.getJobExecutionResult().get();
-            }
-            // TODO Supports accumulator.
-        } catch (Throwable t) {
-            // get() on the JobExecutionResult Future will throw an ExecutionException. This
-            // behaviour was largely not there in Flink versions before the PipelineExecutor
-            // refactoring so we should strip that exception.
-            Throwable strippedException = ExceptionUtils.stripExecutionException(t);
-            ExceptionUtils.rethrowException(strippedException);
-        }
-    }
-
-    private JobClient executeAsync(StreamGraph streamGraph) throws Exception {
-        checkNotNull(streamGraph, "StreamGraph cannot be null.");
-        final PipelineExecutor executor = getPipelineExecutor();
-
-        CompletableFuture<JobClient> jobClientFuture =
-                executor.execute(streamGraph, configuration, getClass().getClassLoader());
-
-        try {
-            // TODO Supports job listeners.
-            return jobClientFuture.get();
-        } catch (ExecutionException executionException) {
-            final Throwable strippedException =
-                    ExceptionUtils.stripExecutionException(executionException);
-            throw new FlinkException(
-                    String.format("Failed to execute job '%s'.", streamGraph.getJobName()),
-                    strippedException);
-        }
-    }
-
-    /** Get {@link StreamGraph} and clear all transformations. */
-    public StreamGraph getStreamGraph() {
-        final StreamGraph streamGraph = getStreamGraphGenerator(transformations).generate();
-        transformations.clear();
-        return streamGraph;
-    }
-
-    private StreamGraphGenerator getStreamGraphGenerator(List<Transformation<?>> transformations) {
-        if (transformations.size() <= 0) {
-            throw new IllegalStateException(
-                    "No operators defined in streaming topology. Cannot execute.");
-        }
-
-        // We copy the transformation so that newly added transformations cannot intervene with the
-        // stream graph generation.
-        return new StreamGraphGenerator(
-                new ArrayList<>(transformations), executionConfig, checkpointCfg, configuration);
-    }
-
-    private PipelineExecutor getPipelineExecutor() throws Exception {
-        checkNotNull(
-                configuration.get(DeploymentOptions.TARGET),
-                "No execution.target specified in your configuration file.");
-
-        final PipelineExecutorFactory executorFactory =
-                executorServiceLoader.getExecutorFactory(configuration);
-
-        checkNotNull(
-                executorFactory,
-                "Cannot find compatible factory for specified execution.target (=%s)",
-                configuration.get(DeploymentOptions.TARGET));
-
-        return executorFactory.getExecutor(configuration);
-    }
-
-    private void configure(ReadableConfig configuration, ClassLoader classLoader) {
-        this.configuration.addAll(Configuration.fromMap(configuration.toMap()));
-        executionConfig.configure(configuration, classLoader);
-        checkpointCfg.configure(configuration);
-    }
-}
\ No newline at end of file
diff --git a/playground/FileSource.java b/playground/FileSource.java
deleted file mode 100644
index d06799b..0000000
--- a/playground/FileSource.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.file.src;
-
-import org.apache.flink.annotation.PublicEvolving;
-import org.apache.flink.api.connector.source.DynamicParallelismInference;
-import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;
-import org.apache.flink.connector.file.src.assigners.LocalityAwareSplitAssigner;
-import org.apache.flink.connector.file.src.enumerate.BlockSplittingRecursiveEnumerator;
-import org.apache.flink.connector.file.src.enumerate.FileEnumerator;
-import org.apache.flink.connector.file.src.enumerate.NonSplittingRecursiveEnumerator;
-import org.apache.flink.connector.file.src.impl.StreamFormatAdapter;
-import org.apache.flink.connector.file.src.reader.BulkFormat;
-import org.apache.flink.connector.file.src.reader.StreamFormat;
-import org.apache.flink.core.fs.FileSystem;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-import org.apache.flink.util.FlinkRuntimeException;
-
-import javax.annotation.Nullable;
-
-import java.io.IOException;
-import java.time.Duration;
-import java.util.Collection;
-
-import static org.apache.flink.util.Preconditions.checkArgument;
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * A unified data source that reads files - both in batch and in streaming mode.
- *
- * <p>This source supports all (distributed) file systems and object stores that can be accessed via
- * the Flink's {@link FileSystem} class.
- *
- * <p>Start building a file source via one of the following calls:
- *
- * <ul>
- *   <li>{@link FileSource#forRecordStreamFormat(StreamFormat, Path...)}
- *   <li>{@link FileSource#forBulkFileFormat(BulkFormat, Path...)}
- * </ul>
- *
- * <p>This creates a {@link FileSource.FileSourceBuilder} on which you can configure all the
- * properties of the file source.
- *
- * <h2>Batch and Streaming</h2>
- *
- * <p>This source supports both bounded/batch and continuous/streaming data inputs. For the
- * bounded/batch case, the file source processes all files under the given path(s). In the
- * continuous/streaming case, the source periodically checks the paths for new files and will start
- * reading those.
- *
- * <p>When you start creating a file source (via the {@link FileSource.FileSourceBuilder} created
- * through one of the above-mentioned methods) the source is by default in bounded/batch mode. Call
- * {@link FileSource.FileSourceBuilder#monitorContinuously(Duration)} to put the source into
- * continuous streaming mode.
- *
- * <h2>Format Types</h2>
- *
- * <p>The reading of each file happens through file readers defined by <i>file formats</i>. These
- * define the parsing logic for the contents of the file. There are multiple classes that the source
- * supports. Their interfaces trade of simplicity of implementation and flexibility/efficiency.
- *
- * <ul>
- *   <li>A {@link StreamFormat} reads the contents of a file from a file stream. It is the simplest
- *       format to implement, and provides many features out-of-the-box (like checkpointing logic)
- *       but is limited in the optimizations it can apply (such as object reuse, batching, etc.).
- *   <li>A {@link BulkFormat} reads batches of records from a file at a time. It is the most "low
- *       level" format to implement, but offers the greatest flexibility to optimize the
- *       implementation.
- * </ul>
- *
- * <h2>Discovering / Enumerating Files</h2>
- *
- * <p>The way that the source lists the files to be processes is defined by the {@link
- * FileEnumerator}. The {@code FileEnumerator} is responsible to select the relevant files (for
- * example filter out hidden files) and to optionally splits files into multiple regions (= file
- * source splits) that can be read in parallel).
- *
- * @param <T> The type of the events/records produced by this source.
- */
-@PublicEvolving
-public final class FileSource<T> extends AbstractFileSource<T, FileSourceSplit>
-        implements DynamicParallelismInference {
-
-    private static final long serialVersionUID = 1L;
-
-    /** The default split assigner, a lazy locality-aware assigner. */
-    public static final FileSplitAssigner.Provider DEFAULT_SPLIT_ASSIGNER =
-            LocalityAwareSplitAssigner::new;
-
-    /**
-     * The default file enumerator used for splittable formats. The enumerator recursively
-     * enumerates files, split files that consist of multiple distributed storage blocks into
-     * multiple splits, and filters hidden files (files starting with '.' or '_'). Files with
-     * suffixes of common compression formats (for example '.gzip', '.bz2', '.xy', '.zip', ...) will
-     * not be split.
-     */
-    public static final FileEnumerator.Provider DEFAULT_SPLITTABLE_FILE_ENUMERATOR =
-            BlockSplittingRecursiveEnumerator::new;
-
-    /**
-     * The default file enumerator used for non-splittable formats. The enumerator recursively
-     * enumerates files, creates one split for the file, and filters hidden files (files starting
-     * with '.' or '_').
-     */
-    public static final FileEnumerator.Provider DEFAULT_NON_SPLITTABLE_FILE_ENUMERATOR =
-            NonSplittingRecursiveEnumerator::new;
-
-    // ------------------------------------------------------------------------
-
-    private FileSource(
-            final Path[] inputPaths,
-            final FileEnumerator.Provider fileEnumerator,
-            final FileSplitAssigner.Provider splitAssigner,
-            final BulkFormat<T, FileSourceSplit> readerFormat,
-            @Nullable final ContinuousEnumerationSettings continuousEnumerationSettings) {
-
-        super(
-                inputPaths,
-                fileEnumerator,
-                splitAssigner,
-                readerFormat,
-                continuousEnumerationSettings);
-    }
-
-    @Override
-    public SimpleVersionedSerializer<FileSourceSplit> getSplitSerializer() {
-        return FileSourceSplitSerializer.INSTANCE;
-    }
-
-    @Override
-    public int inferParallelism(Context dynamicParallelismContext) {
-        FileEnumerator fileEnumerator = getEnumeratorFactory().create();
-
-        Collection<FileSourceSplit> splits;
-        try {
-            splits =
-                    fileEnumerator.enumerateSplits(
-                            inputPaths,
-                            dynamicParallelismContext.getParallelismInferenceUpperBound());
-        } catch (IOException e) {
-            throw new FlinkRuntimeException("Could not enumerate file splits", e);
-        }
-
-        return Math.min(
-                splits.size(), dynamicParallelismContext.getParallelismInferenceUpperBound());
-    }
-
-    // ------------------------------------------------------------------------
-    //  Entry-point Factory Methods
-    // ------------------------------------------------------------------------
-
-    /**
-     * Builds a new {@code FileSource} using a {@link StreamFormat} to read record-by-record from a
-     * file stream.
-     *
-     * <p>When possible, stream-based formats are generally easier (preferable) to file-based
-     * formats, because they support better default behavior around I/O batching or progress
-     * tracking (checkpoints).
-     *
-     * <p>Stream formats also automatically de-compress files based on the file extension. This
-     * supports files ending in ".deflate" (Deflate), ".xz" (XZ), ".bz2" (BZip2), ".gz", ".gzip"
-     * (GZip).
-     */
-    public static <T> FileSourceBuilder<T> forRecordStreamFormat(
-            final StreamFormat<T> streamFormat, final Path... paths) {
-        return forBulkFileFormat(new StreamFormatAdapter<>(streamFormat), paths);
-    }
-
-    /**
-     * Builds a new {@code FileSource} using a {@link BulkFormat} to read batches of records from
-     * files.
-     *
-     * <p>Examples for bulk readers are compressed and vectorized formats such as ORC or Parquet.
-     */
-    public static <T> FileSourceBuilder<T> forBulkFileFormat(
-            final BulkFormat<T, FileSourceSplit> bulkFormat, final Path... paths) {
-        checkNotNull(bulkFormat, "reader");
-        checkNotNull(paths, "paths");
-        checkArgument(paths.length > 0, "paths must not be empty");
-
-        return new FileSourceBuilder<>(paths, bulkFormat);
-    }
-
-    // ------------------------------------------------------------------------
-    //  Builder
-    // ------------------------------------------------------------------------
-
-    /**
-     * The builder for the {@code FileSource}, to configure the various behaviors.
-     *
-     * <p>Start building the source via one of the following methods:
-     *
-     * <ul>
-     *   <li>{@link FileSource#forRecordStreamFormat(StreamFormat, Path...)}
-     *   <li>{@link FileSource#forBulkFileFormat(BulkFormat, Path...)}
-     * </ul>
-     */
-    public static final class FileSourceBuilder<T>
-            extends AbstractFileSourceBuilder<T, FileSourceSplit, FileSourceBuilder<T>> {
-
-        FileSourceBuilder(Path[] inputPaths, BulkFormat<T, FileSourceSplit> readerFormat) {
-            super(
-                    inputPaths,
-                    readerFormat,
-                    readerFormat.isSplittable()
-                            ? DEFAULT_SPLITTABLE_FILE_ENUMERATOR
-                            : DEFAULT_NON_SPLITTABLE_FILE_ENUMERATOR,
-                    DEFAULT_SPLIT_ASSIGNER);
-        }
-
-        @Override
-        public FileSource<T> build() {
-            return new FileSource<>(
-                    inputPaths,
-                    fileEnumerator,
-                    splitAssigner,
-                    readerFormat,
-                    continuousSourceSettings);
-        }
-    }
-}
\ No newline at end of file
diff --git a/playground/Source.java b/playground/Source.java
deleted file mode 100644
index e7f2f77..0000000
--- a/playground/Source.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.api.connector.source;
-
-import org.apache.flink.annotation.Public;
-import org.apache.flink.api.common.watermark.WatermarkDeclaration;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-
-import java.util.Collections;
-import java.util.Set;
-
-/**
- * The interface for Source. It acts like a factory class that helps construct the {@link
- * SplitEnumerator} and {@link SourceReader} and corresponding serializers.
- *
- * @param <T> The type of records produced by the source.
- * @param <SplitT> The type of splits handled by the source.
- * @param <EnumChkT> The type of the enumerator checkpoints.
- */
-@Public
-public interface Source<T, SplitT extends SourceSplit, EnumChkT>
-        extends SourceReaderFactory<T, SplitT> {
-
-    /**
-     * Get the boundedness of this source.
-     *
-     * @return the boundedness of this source.
-     */
-    Boundedness getBoundedness();
-
-    /**
-     * Creates a new SplitEnumerator for this source, starting a new input.
-     *
-     * @param enumContext The {@link SplitEnumeratorContext context} for the split enumerator.
-     * @return A new SplitEnumerator.
-     * @throws Exception The implementor is free to forward all exceptions directly. Exceptions
-     *     thrown from this method cause JobManager failure/recovery.
-     */
-    SplitEnumerator<SplitT, EnumChkT> createEnumerator(SplitEnumeratorContext<SplitT> enumContext)
-            throws Exception;
-
-    /**
-     * Restores an enumerator from a checkpoint.
-     *
-     * @param enumContext The {@link SplitEnumeratorContext context} for the restored split
-     *     enumerator.
-     * @param checkpoint The checkpoint to restore the SplitEnumerator from.
-     * @return A SplitEnumerator restored from the given checkpoint.
-     * @throws Exception The implementor is free to forward all exceptions directly. Exceptions
-     *     thrown from this method cause JobManager failure/recovery.
-     */
-    SplitEnumerator<SplitT, EnumChkT> restoreEnumerator(
-            SplitEnumeratorContext<SplitT> enumContext, EnumChkT checkpoint) throws Exception;
-
-    // ------------------------------------------------------------------------
-    //  serializers for the metadata
-    // ------------------------------------------------------------------------
-
-    /**
-     * Creates a serializer for the source splits. Splits are serialized when sending them from
-     * enumerator to reader, and when checkpointing the reader's current state.
-     *
-     * @return The serializer for the split type.
-     */
-    SimpleVersionedSerializer<SplitT> getSplitSerializer();
-
-    /**
-     * Creates the serializer for the {@link SplitEnumerator} checkpoint. The serializer is used for
-     * the result of the {@link SplitEnumerator#snapshotState(long)} method.
-     *
-     * @return The serializer for the SplitEnumerator checkpoint.
-     */
-    SimpleVersionedSerializer<EnumChkT> getEnumeratorCheckpointSerializer();
-
-    /**
-     * Explicitly declare watermarks upfront. Each specific watermark must be declared in this
-     * method before it can be used.
-     *
-     * @return all watermark declarations used by this application.
-     */
-    default Set<? extends WatermarkDeclaration> declareWatermarks() {
-        return Collections.emptySet();
-    }
-}
\ No newline at end of file
diff --git a/playground/SourceOperatorFactory.java b/playground/SourceOperatorFactory.java
deleted file mode 100644
index 4624854..0000000
--- a/playground/SourceOperatorFactory.java
+++ /dev/null
@@ -1,236 +0,0 @@
-/*
-Licensed to the Apache Software Foundation (ASF) under one
-or more contributor license agreements.  See the NOTICE file
-distributed with this work for additional information
-regarding copyright ownership.  The ASF licenses this file
-to you under the Apache License, Version 2.0 (the
-"License"); you may not use this file except in compliance
-with the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package org.apache.flink.streaming.api.operators;
-
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.eventtime.WatermarkStrategy;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.watermark.WatermarkDeclaration;
-import org.apache.flink.api.connector.source.Boundedness;
-import org.apache.flink.api.connector.source.Source;
-import org.apache.flink.api.connector.source.SourceReader;
-import org.apache.flink.api.connector.source.SourceReaderContext;
-import org.apache.flink.api.connector.source.SourceSplit;
-import org.apache.flink.configuration.Configuration;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-import org.apache.flink.runtime.jobgraph.OperatorID;
-import org.apache.flink.runtime.operators.coordination.OperatorCoordinator;
-import org.apache.flink.runtime.operators.coordination.OperatorEventGateway;
-import org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider;
-import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;
-import org.apache.flink.streaming.runtime.tasks.ProcessingTimeServiceAware;
-import org.apache.flink.streaming.runtime.tasks.StreamTask.CanEmitBatchOfRecordsChecker;
-import org.apache.flink.streaming.runtime.watermark.AbstractInternalWatermarkDeclaration;
-import org.apache.flink.streaming.util.watermark.WatermarkUtils;
-import org.apache.flink.util.function.FunctionWithException;
-
-import javax.annotation.Nullable;
-
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
-import java.util.stream.Collectors;
-
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/** The Factory class for {@link SourceOperator}. */
-public class SourceOperatorFactory<OUT> extends AbstractStreamOperatorFactory<OUT>
-        implements CoordinatedOperatorFactory<OUT>, ProcessingTimeServiceAware {
-
-    private static final long serialVersionUID = 1L;
-
-    /** The {@link Source} to create the {@link SourceOperator}. */
-    private final Source<OUT, ?, ?> source;
-
-    /** The event time setup (timestamp assigners, watermark generators, etc.). */
-    private final WatermarkStrategy<OUT> watermarkStrategy;
-
-    /** Whether to emit intermediate watermarks or only one final watermark at the end of input. */
-    private final boolean emitProgressiveWatermarks;
-
-    /** The number of worker thread for the source coordinator. */
-    private final int numCoordinatorWorkerThread;
-
-    private @Nullable String coordinatorListeningID;
-
-    public SourceOperatorFactory(
-            Source<OUT, ?, ?> source, WatermarkStrategy<OUT> watermarkStrategy) {
-        this(source, watermarkStrategy, true /* emit progressive watermarks */, 1);
-    }
-
-    public SourceOperatorFactory(
-            Source<OUT, ?, ?> source,
-            WatermarkStrategy<OUT> watermarkStrategy,
-            boolean emitProgressiveWatermarks) {
-        this(source, watermarkStrategy, emitProgressiveWatermarks, 1);
-    }
-
-    public SourceOperatorFactory(
-            Source<OUT, ?, ?> source,
-            WatermarkStrategy<OUT> watermarkStrategy,
-            boolean emitProgressiveWatermarks,
-            int numCoordinatorWorkerThread) {
-        this.source = checkNotNull(source);
-        this.watermarkStrategy = checkNotNull(watermarkStrategy);
-        this.emitProgressiveWatermarks = emitProgressiveWatermarks;
-        this.numCoordinatorWorkerThread = numCoordinatorWorkerThread;
-    }
-
-    public Boundedness getBoundedness() {
-        return source.getBoundedness();
-    }
-
-    public void setCoordinatorListeningID(@Nullable String coordinatorListeningID) {
-        this.coordinatorListeningID = coordinatorListeningID;
-    }
-
-    @Override
-    public <T extends StreamOperator<OUT>> T createStreamOperator(
-            StreamOperatorParameters<OUT> parameters) {
-        final OperatorID operatorId = parameters.getStreamConfig().getOperatorID();
-        final OperatorEventGateway gateway =
-                parameters.getOperatorEventDispatcher().getOperatorEventGateway(operatorId);
-
-        final SourceOperator<OUT, ?> sourceOperator =
-                instantiateSourceOperator(
-                        parameters,
-                        source::createReader,
-                        gateway,
-                        source.getSplitSerializer(),
-                        watermarkStrategy,
-                        parameters.getProcessingTimeService(),
-                        parameters
-                                .getContainingTask()
-                                .getEnvironment()
-                                .getTaskManagerInfo()
-                                .getConfiguration(),
-                        parameters
-                                .getContainingTask()
-                                .getEnvironment()
-                                .getTaskManagerInfo()
-                                .getTaskManagerExternalAddress(),
-                        emitProgressiveWatermarks,
-                        parameters.getContainingTask().getCanEmitBatchOfRecords(),
-                        getSourceWatermarkDeclarations());
-
-        parameters.getOperatorEventDispatcher().registerEventHandler(operatorId, sourceOperator);
-
-        // today's lunch is generics spaghetti
-        @SuppressWarnings("unchecked")
-        final T castedOperator = (T) sourceOperator;
-
-        return castedOperator;
-    }
-
-    @Override
-    public OperatorCoordinator.Provider getCoordinatorProvider(
-            String operatorName, OperatorID operatorID) {
-        return new SourceCoordinatorProvider<>(
-                operatorName,
-                operatorID,
-                source,
-                numCoordinatorWorkerThread,
-                watermarkStrategy.getAlignmentParameters(),
-                coordinatorListeningID);
-    }
-
-    @SuppressWarnings("rawtypes")
-    @Override
-    public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {
-        return SourceOperator.class;
-    }
-
-    @Override
-    public boolean isStreamSource() {
-        return true;
-    }
-
-    @Override
-    public boolean isOutputTypeConfigurable() {
-        return source instanceof OutputTypeConfigurable;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public void setOutputType(TypeInformation<OUT> type, ExecutionConfig executionConfig) {
-        if (source instanceof OutputTypeConfigurable) {
-            ((OutputTypeConfigurable<OUT>) source).setOutputType(type, executionConfig);
-        }
-    }
-
-    public Set<? extends WatermarkDeclaration> getSourceWatermarkDeclarations() {
-        return source.declareWatermarks();
-    }
-
-    /**
-     * This is a utility method to conjure up a "SplitT" generics variable binding so that we can
-     * construct the SourceOperator without resorting to "all raw types". That way, this methods
-     * puts all "type non-safety" in one place and allows to maintain as much generics safety in the
-     * main code as possible.
-     */
-    @SuppressWarnings("unchecked")
-    private static <T, SplitT extends SourceSplit>
-            SourceOperator<T, SplitT> instantiateSourceOperator(
-                    StreamOperatorParameters<T> parameters,
-                    FunctionWithException<SourceReaderContext, SourceReader<T, ?>, Exception>
-                            readerFactory,
-                    OperatorEventGateway eventGateway,
-                    SimpleVersionedSerializer<?> splitSerializer,
-                    WatermarkStrategy<T> watermarkStrategy,
-                    ProcessingTimeService timeService,
-                    Configuration config,
-                    String localHostName,
-                    boolean emitProgressiveWatermarks,
-                    CanEmitBatchOfRecordsChecker canEmitBatchOfRecords,
-                    Collection<? extends WatermarkDeclaration> watermarkDeclarations) {
-
-        // jumping through generics hoops: cast the generics away to then cast them back more
-        // strictly typed
-        final FunctionWithException<SourceReaderContext, SourceReader<T, SplitT>, Exception>
-                typedReaderFactory =
-                        (FunctionWithException<
-                                        SourceReaderContext, SourceReader<T, SplitT>, Exception>)
-                                (FunctionWithException<?, ?, ?>) readerFactory;
-
-        final SimpleVersionedSerializer<SplitT> typedSplitSerializer =
-                (SimpleVersionedSerializer<SplitT>) splitSerializer;
-
-        Map<String, Boolean> watermarkIsAlignedMap =
-                WatermarkUtils.convertToInternalWatermarkDeclarations(
-                                new HashSet<>(watermarkDeclarations))
-                        .stream()
-                        .collect(
-                                Collectors.toMap(
-                                        AbstractInternalWatermarkDeclaration::getIdentifier,
-                                        AbstractInternalWatermarkDeclaration::isAligned));
-        return new SourceOperator<>(
-                parameters,
-                typedReaderFactory,
-                eventGateway,
-                typedSplitSerializer,
-                watermarkStrategy,
-                timeService,
-                config,
-                localHostName,
-                emitProgressiveWatermarks,
-                canEmitBatchOfRecords,
-                watermarkIsAlignedMap);
-    }
-}
\ No newline at end of file
diff --git a/playground/StreamingWordCount b/playground/StreamingWordCount
deleted file mode 100644
index 62b86e4..0000000
--- a/playground/StreamingWordCount
+++ /dev/null
@@ -1,127 +0,0 @@
-package com.my.examples.redpandaFlink;
-
-import org.apache.flink.api.common.functions.FlatMapFunction;
-import org.apache.flink.streaming.api.datastream.DataStream;
-import org.apache.flink.api.java.tuple.Tuple2;
-import org.apache.flink.util.Collector;
-import org.apache.flink.connector.kafka.source.KafkaSource;
-import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
-import org.apache.flink.connector.kafka.sink.KafkaSink;
-import org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchema;
-import org.apache.flink.api.common.serialization.SimpleStringSchema;
-import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-import org.apache.flink.api.common.eventtime.WatermarkStrategy;
-
-// this code was mostly cobbled together from an original demo found at:
-// https://github.com/redpanda-data/flink-kafka-examples/blob/main/src/main/java/io/redpanda/examples/WordCount.java
-
-public class StreamingJob {
-    final static String inputTopic = "words";
-    final static String outputTopic = "words-count";
-    final static String jobTitle = "WordCount";
-
-    public static void main(String[] args) throws Exception {
-        // Redpanda is listening on localhost. Remember to use the container name for the address
-        final String bootstrapServers = args.length > 0 ? args[0] : "redpanda-1:9092";
-
-        // set up the streaming execution environment
-        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
-
-        KafkaSource<String> source = KafkaSource.<String>builder()
-                .setBootstrapServers(bootstrapServers)
-                .setTopics(inputTopic)
-                .setStartingOffsets(OffsetsInitializer.earliest())
-                .setValueOnlyDeserializer(new SimpleStringSchema())
-                .build();
-
-        KafkaRecordSerializationSchema<String> serializer = KafkaRecordSerializationSchema.builder()
-                .setValueSerializationSchema(new SimpleStringSchema())
-                .setTopic(outputTopic)
-                .build();
-
-        KafkaSink<String> sink = KafkaSink.<String>builder()
-                .setBootstrapServers(bootstrapServers)
-                .setRecordSerializer(serializer)
-                .build();
-
-
-        /**
-         * Adds a data {@link Source} to the environment to get a {@link DataStream}.
-         *
-         * <p>The result will be either a bounded data stream (that can be processed in a batch way) or
-         * an unbounded data stream (that must be processed in a streaming way), based on the
-         * boundedness property of the source, as defined by {@link Source#getBoundedness()}.
-         *
-         * <p>The result type (that is used to create serializers for the produced data events) will be
-         * automatically extracted. This is useful for sources that describe the produced types already
-         * in their configuration, to avoid having to declare the type multiple times. For example the
-         * file sources and Kafka sources already define the produced byte their
-         * parsers/serializers/formats, and can forward that information.
-         *
-         * @param source the user defined source
-         * @param sourceName Name of the data source
-         * @param <OUT> type of the returned stream
-         * @return the data stream constructed
-         */
-        @PublicEvolving
-        public <OUT> DataStreamSource<OUT> fromSource(
-                Source<OUT, ?, ?> source,
-                WatermarkStrategy<OUT> timestampsAndWatermarks,
-                String sourceName) {
-            return fromSource(source, timestampsAndWatermarks, sourceName, null);
-        }
-
-        DataStream<String> text = env.fromSource(source, WatermarkStrategy.noWatermarks(), "Redpanda Source");
-
-        // Split up the lines in pairs (2-tuples) containing: (word,1)
-        DataStream<String> counts = text.flatMap(new Tokenizer())
-        // Group by the tuple field "0" and sum up tuple field "1"
-        .keyBy(value -> value.f0)
-        .sum(1)
-        .flatMap(new Reducer());
-
-        // Add the sinkTo so results
-        // are written to the outputTopic
-        counts.sinkTo(sink);
-
-        // Execute program
-        env.execute(jobTitle);
-    }
-
-    /**
-     * Implements the string tokenizer that splits sentences into words as a user-defined
-     * FlatMapFunction. The function takes a line (String) and splits it into multiple pairs in the
-     * form of "(word,1)" ({@code Tuple2<String, Integer>}).
-     */
-    public static final class Tokenizer
-            implements FlatMapFunction<String, Tuple2<String, Integer>> {
-
-        @Override
-        public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
-            // Normalize and split the line
-            String[] tokens = value.toLowerCase().split("\\W+");
-
-            // Emit the pairs
-            for (String token : tokens) {
-                if (token.length() > 0) {
-                    out.collect(new Tuple2<>(token, 1));
-                }
-            }
-        }
-    }
-
-    // Implements a simple reducer using FlatMap to
-    // reduce the Tuple2 into a single string for
-    // writing to kafka topics
-    public static final class Reducer
-            implements FlatMapFunction<Tuple2<String, Integer>, String> {
-
-        @Override
-        public void flatMap(Tuple2<String, Integer> value, Collector<String> out) {
-            // Convert the pairs to a string
-            // for easy writing to Kafka Topic
-            String count = value.f0 + " " + value.f1;
-            out.collect(count);
-        }
-    }
-}
\ No newline at end of file
diff --git a/playground/WrappedSource.java b/playground/WrappedSource.java
deleted file mode 100644
index f766e81..0000000
--- a/playground/WrappedSource.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.api.connector.dsv2;
-
-import org.apache.flink.annotation.Internal;
-
-/** A simple {@link Source} implementation that wrap a FLIP-27 source. */
-@Internal
-public class WrappedSource<T> implements Source<T> {
-    org.apache.flink.api.connector.source.Source<T, ?, ?> wrappedSource;
-
-    public WrappedSource(org.apache.flink.api.connector.source.Source<T, ?, ?> wrappedSource) {
-        this.wrappedSource = wrappedSource;
-    }
-
-    public org.apache.flink.api.connector.source.Source<T, ?, ?> getWrappedSource() {
-        return wrappedSource;
-    }
-}
\ No newline at end of file
diff --git a/pytest.ini b/pytest.ini
deleted file mode 100644
index aa7f5b7..0000000
--- a/pytest.ini
+++ /dev/null
@@ -1,2 +0,0 @@
-[pytest]
-norecursedirs = build dist .eggs
diff --git a/requirements.txt b/requirements.txt
index 4bb9099..614e19a 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -43,6 +43,3 @@ ray[default]==2.37.0
 python-multipart>=0.0.6
 huggingface_hub>=0.22.2
 kafka-python>=2.2.0
-dill>=0.3.8
-tabulate>=0.9.0
-colorama>=0.4.6
\ No newline at end of file
diff --git a/sage_core/environment/__init__.py b/sage_common_funs/__init__.py
similarity index 100%
rename from sage_core/environment/__init__.py
rename to sage_common_funs/__init__.py
diff --git a/sage_jobmanager/utils/__init__.py b/sage_common_funs/agent/__init__.py
similarity index 100%
rename from sage_jobmanager/utils/__init__.py
rename to sage_common_funs/agent/__init__.py
diff --git a/sage_common_funs/agent/agent.py b/sage_common_funs/agent/agent.py
new file mode 100644
index 0000000..2e1c611
--- /dev/null
+++ b/sage_common_funs/agent/agent.py
@@ -0,0 +1,165 @@
+from calendar import c
+from sage_common_funs.utils.generator_model import apply_generator_model
+from sage_core.function.map_function import MapFunction
+from jinja2 import Template
+
+from sage_utils.custom_logger import CustomLogger
+from typing import Any,Tuple
+import requests
+import json
+import re, time
+
+
+class Tool:
+    def __init__(self, name, func, description):
+        self.name = name
+        self.func = func
+        self.description = description
+
+    def run(self, *args, **kwargs):
+        return self.func(*args, **kwargs)
+    
+class BochaSearch:
+    def __init__(self,api_key):
+        self.url = "https://api.bochaai.com/v1/web-search"
+        self.api_key = api_key
+        self.headers = {
+            'Authorization': api_key,
+            'Content-Type': 'application/json'
+        }
+
+    def run(self, query):
+        payload = json.dumps({
+            "query": query,
+            "summary": True,
+            "count": 10,
+            "page": 1
+        })
+        response = requests.request("POST", self.url, headers=self.headers, data=payload)
+        return response.json()
+
+PREFIX = """Answer the following questions as best you can. You have access to the following tools:{tool_names}"""
+FORMAT_INSTRUCTIONS = """Always respond in the following JSON format:
+
+```json
+{{
+  "thought": "your thought process",
+  "action": "the action to take, should be one of [{tool_names}]",
+  "action_input": "the input to the action",
+  "observation": "Result from tool after execution",
+  "final_answer": "Final answer to the original question"
+}}
+```
+Notes:
+If you are taking an action, set 'final_answer' to "" and 'observation' to "".
+If you have enough information to answer, set 'action' to "", and fill in 'final_answer' directly. 
+"""
+
+SUFFIX = """Begin!
+Question: {input}
+Thought:{agent_scratchpad}
+"""
+
+
+class BaseAgent(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.logger.set_console_level("DEBUG")
+        self.config = config
+        search = BochaSearch(api_key=self.config["search_api_key"])
+
+        self.tools = [
+            Tool(
+                name = "Search",
+                func=search.run,
+                description="useful for when you need to search to answer questions about current events"
+            )
+        ]
+        self.tools = {tool.name: tool for tool in self.tools}
+        self.tool_names = ", ".join(self.tools.keys())  # ä¿®å¤ç‚¹
+        self.format_instructions = FORMAT_INSTRUCTIONS.format(tool_names=self.tool_names)
+        self.prefix= PREFIX.format(tool_names=self.tool_names)
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"],
+            base_url=self.config["base_url"],
+            api_key=self.config["api_key"],
+            seed=42 
+        )
+        self.max_steps=self.config.get("max_steps", 5)
+
+    def get_prompt(self, input, agent_scratchpad):
+        return self.prefix + self.format_instructions + SUFFIX.format(
+            input=input, agent_scratchpad=agent_scratchpad
+        )
+    
+    def parse_json_output(self, output: str) -> dict:
+        # å°è¯•ç›´æ¥åŠ è½½
+        try:
+            return json.loads(output)
+        except json.JSONDecodeError:
+            pass
+
+        # å¦‚æœä¸æ˜¯çº¯ JSONï¼Œå†è¯•å›¾ä» Markdown ä¸­æå–
+        match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", output, re.DOTALL)
+        if match:
+            json_str = match.group(1).strip()
+            try:
+                return json.loads(json_str)
+            except json.JSONDecodeError as e:
+                raise ValueError(f"Malformed JSON inside Markdown: {str(e)}") from e
+
+        # å…œåº•æŠ¥é”™
+        raise ValueError("Invalid JSON format: No valid JSON found (either plain or wrapped in Markdown)")
+        
+    def execute(self, data: str) -> Tuple[str, str]:
+        query = data
+        agent_scratchpad = ""
+        count = 0
+        while True:
+            count += 1
+            self.logger.debug(f"Step {count}: Processing query: {query}")
+            if count > self.max_steps:
+                # raise ValueError("Max steps exceeded.")
+                return (query,"")
+            
+            prompt = self.get_prompt(query, agent_scratchpad)
+            self.logger.debug(f"Prompt: {prompt}")
+            prompt=[{"role":"user","content":prompt}]
+            output = self.model.generate(prompt)
+            self.logger.debug(output)
+            output=self.parse_json_output(output)
+            # self.logger.debug(output)
+            if output.get("final_answer") is not "":
+                final_answer = output["final_answer"]
+                
+                self.logger.debug(f"Final Answer: {final_answer}")
+                return (query,final_answer)
+
+            action, action_input = output.get("action"), output.get("action_input")
+
+            if action is None:
+                # raise ValueError("Could not parse action.")
+                return (query,"")
+
+            if action not in self.tools:
+                # raise ValueError(f"Unknown tool requested: {action}")
+                return (query,"")
+
+            tool = self.tools[action]
+            tool_result = tool.run(action_input)
+            self.logger.debug(f"Tool {action} result: {tool_result}")
+            snippets =[item["snippet"] for item in tool_result["data"]["webPages"]["value"]]
+            observation = "\n".join(snippets)
+            self.logger.debug(f"Observation: {observation}")
+            agent_scratchpad += str(output) + f"\nObservation: {observation}\nThought: "
+            time.sleep(5)
+
+# import yaml
+# def load_config(path: str) -> dict:
+#     with open(path, 'r') as f:
+#         return yaml.safe_load(f)
+
+# config=load_config("/home/zsl/workspace/sage/api/operator/operator_impl/config.yaml")
+# agent=BaseAgent(config)
+# agent.run("ä½ æ˜¯è°")
\ No newline at end of file
diff --git a/sage_libs/io/__init__.py b/sage_common_funs/io/__init__.py
similarity index 100%
rename from sage_libs/io/__init__.py
rename to sage_common_funs/io/__init__.py
diff --git a/sage_libs/io/sink.py b/sage_common_funs/io/sink.py
similarity index 90%
rename from sage_libs/io/sink.py
rename to sage_common_funs/io/sink.py
index f7f9e24..cad3c4c 100644
--- a/sage_libs/io/sink.py
+++ b/sage_common_funs/io/sink.py
@@ -1,4 +1,5 @@
 from sage_core.function.sink_function import SinkFunction
+from sage_utils.custom_logger import CustomLogger
 from typing import Tuple, List, Union, Type, Any
 import os
 
@@ -33,22 +34,11 @@ class RetriveSink(SinkFunction):
 
 
 class FileSink(SinkFunction):
-    def __init__(self, config: dict = None, **kwargs):
+    def __init__(self, config: dict = None,  **kwargs):
         super().__init__(**kwargs)
-        
-        file_path = config.get("file_path", "qa_output.txt") if config else "qa_output.txt"
+        os.makedirs("output", exist_ok=True)
+        self.file_path = os.path.join("output", config.get("file_path", "qa_output.txt"))
         self.config = config
-        
-        # åˆ¤æ–­è·¯å¾„ç±»å‹å¹¶å¤„ç†
-        if os.path.isabs(file_path):
-            # ç»å¯¹è·¯å¾„ï¼šç›´æ¥ä½¿ç”¨
-            self.file_path = file_path
-            # ç¡®ä¿ç›®å½•å­˜åœ¨
-            os.makedirs(os.path.dirname(file_path), exist_ok=True)
-        else:
-            # ç›¸å¯¹è·¯å¾„ï¼šæ·»åŠ outputå‰ç¼€
-            os.makedirs("output", exist_ok=True)
-            self.file_path = os.path.join("output", file_path)
 
         # åˆ›å»ºæˆ–æ¸…ç©ºæ–‡ä»¶
         with open(self.file_path, "w", encoding="utf-8") as f:
diff --git a/sage_libs/io/source.py b/sage_common_funs/io/source.py
similarity index 90%
rename from sage_libs/io/source.py
rename to sage_common_funs/io/source.py
index 9528ec9..5aef831 100644
--- a/sage_libs/io/source.py
+++ b/sage_common_funs/io/source.py
@@ -1,6 +1,8 @@
 
 from sage_core.function.source_function import SourceFunction
-from sage_libs.io.utils.data_loader import resolve_data_path
+from sage_utils.custom_logger import CustomLogger
+from sage_utils.data_loader import resolve_data_path
+from typing import List
 
 
 class FileSource(SourceFunction):
@@ -44,8 +46,9 @@ class FileSource(SourceFunction):
                         self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Read query: {line.strip()}\033[0m ")
                         return line.strip()  # Return non-empty lines
                     else:
-                        self.logger.info(f"\033[33m[ {self.__class__.__name__}]: Reached end of file, maintaining position.\033[0m ")
+                        self.logger.info(f"\033[33m[ {self.__class__.__name__}]: Reached end of file, resetting position.\033[0m ")
                         # Reset position if end of file is reached (optional)
+                        self.file_pos = 0
                         continue
         except FileNotFoundError:
             self.logger.error(f"File not found: {self.data_path}")
diff --git a/sage_libs/io/utils/__init__.py b/sage_common_funs/rag/__init__.py
similarity index 100%
rename from sage_libs/io/utils/__init__.py
rename to sage_common_funs/rag/__init__.py
diff --git a/sage_common_funs/rag/arxiv.py b/sage_common_funs/rag/arxiv.py
new file mode 100644
index 0000000..507586f
--- /dev/null
+++ b/sage_common_funs/rag/arxiv.py
@@ -0,0 +1,276 @@
+from typing import Any, List, Literal, Optional, Union
+from typing import Any, List, Optional
+from urllib.parse import quote
+import feedparser
+import os
+import requests
+import time
+import json
+import fitz
+from PIL import Image
+from collections import Counter
+import json
+import re
+
+from sage_core.function.map_function import MapFunction
+
+
+
+class Paper:
+    def __init__(self, path, title='', url='', abs='', authors=[],**kwargs):
+        super().__init__(**kwargs)
+        # åˆå§‹åŒ–å‡½æ•°ï¼Œæ ¹æ®pdfè·¯å¾„åˆå§‹åŒ–Paperå¯¹è±¡                
+        self.url = url  # æ–‡ç« é“¾æ¥
+        self.path = path  # pdfè·¯å¾„
+        self.section_names = []  # æ®µè½æ ‡é¢˜
+        self.section_texts = {}  # æ®µè½å†…å®¹
+        self.abs = abs
+        self.title_page = 0
+        if title == '':
+            self.pdf = fitz.open(self.path)  # pdfæ–‡æ¡£
+            self.title = self.get_title()
+            self.parse_pdf()
+        else:
+            self.title = title
+        self.authors = authors
+        self.roman_num = ["I", "II", 'III', "IV", "V", "VI", "VII", "VIII", "IIX", "IX", "X"]
+        self.digit_num = [str(d + 1) for d in range(10)]
+        self.first_image = ''
+
+    def parse_pdf(self):
+        self.pdf = fitz.open(self.path)  # pdfæ–‡æ¡£
+        self.text_list = [page.get_text() for page in self.pdf]
+        self.all_text = ' '.join(self.text_list)
+        self.extract_section_infomation()
+        self.section_texts.update({"title": self.title})
+        self.pdf.close()
+
+    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®å­—ä½“çš„å¤§å°ï¼Œè¯†åˆ«æ¯ä¸ªç« èŠ‚åç§°ï¼Œå¹¶è¿”å›ä¸€ä¸ªåˆ—è¡¨
+    def get_chapter_names(self, ):
+        # # æ‰“å¼€ä¸€ä¸ªpdfæ–‡ä»¶
+        doc = fitz.open(self.path)  # pdfæ–‡æ¡£
+        text_list = [page.get_text() for page in doc]
+        all_text = ''
+        for text in text_list:
+            all_text += text
+        # # åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨ç« èŠ‚åç§°
+        chapter_names = []
+        for line in all_text.split('\n'):
+            line_list = line.split(' ')
+            if '.' in line:
+                point_split_list = line.split('.')
+                space_split_list = line.split(' ')
+                if 1 < len(space_split_list) < 5:
+                    if 1 < len(point_split_list) < 5 and (
+                            point_split_list[0] in self.roman_num or point_split_list[0] in self.digit_num):
+                        # print("line:", line)
+                        chapter_names.append(line)
+
+        return chapter_names
+
+    def get_title(self):
+        doc = self.pdf  # æ‰“å¼€pdfæ–‡ä»¶
+        max_font_size = 0  # åˆå§‹åŒ–æœ€å¤§å­—ä½“å¤§å°ä¸º0
+        max_string = ""  # åˆå§‹åŒ–æœ€å¤§å­—ä½“å¤§å°å¯¹åº”çš„å­—ç¬¦ä¸²ä¸ºç©º
+        max_font_sizes = [0]
+        for page_index, page in enumerate(doc):  # éå†æ¯ä¸€é¡µ
+            text = page.get_text("dict")  # è·å–é¡µé¢ä¸Šçš„æ–‡æœ¬ä¿¡æ¯
+            blocks = text["blocks"]  # è·å–æ–‡æœ¬å—åˆ—è¡¨
+            for block in blocks:  # éå†æ¯ä¸ªæ–‡æœ¬å—
+                if block["type"] == 0 and len(block['lines']):  # å¦‚æœæ˜¯æ–‡å­—ç±»å‹
+                    if len(block["lines"][0]["spans"]):
+                        font_size = block["lines"][0]["spans"][0]["size"]  # è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€æ®µæ–‡å­—çš„å­—ä½“å¤§å°
+                        max_font_sizes.append(font_size)
+                        if font_size > max_font_size:  # å¦‚æœå­—ä½“å¤§å°å¤§äºå½“å‰æœ€å¤§å€¼
+                            max_font_size = font_size  # æ›´æ–°æœ€å¤§å€¼
+                            max_string = block["lines"][0]["spans"][0]["text"]  # æ›´æ–°æœ€å¤§å€¼å¯¹åº”çš„å­—ç¬¦ä¸²
+        max_font_sizes.sort()
+        # print("max_font_sizes", max_font_sizes[-10:])
+        cur_title = ''
+        for page_index, page in enumerate(doc):  # éå†æ¯ä¸€é¡µ
+            text = page.get_text("dict")  # è·å–é¡µé¢ä¸Šçš„æ–‡æœ¬ä¿¡æ¯
+            blocks = text["blocks"]  # è·å–æ–‡æœ¬å—åˆ—è¡¨
+            for block in blocks:  # éå†æ¯ä¸ªæ–‡æœ¬å—
+                if block["type"] == 0 and len(block['lines']):  # å¦‚æœæ˜¯æ–‡å­—ç±»å‹
+                    if len(block["lines"][0]["spans"]):
+                        cur_string = block["lines"][0]["spans"][0]["text"]  # æ›´æ–°æœ€å¤§å€¼å¯¹åº”çš„å­—ç¬¦ä¸²
+                        font_flags = block["lines"][0]["spans"][0]["flags"]  # è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€æ®µæ–‡å­—çš„å­—ä½“ç‰¹å¾
+                        font_size = block["lines"][0]["spans"][0]["size"]  # è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€æ®µæ–‡å­—çš„å­—ä½“å¤§å°
+                        # print(font_size)
+                        if abs(font_size - max_font_sizes[-1]) < 0.3 or abs(font_size - max_font_sizes[-2]) < 0.3:
+                            # print("The string is bold.", max_string, "font_size:", font_size, "font_flags:", font_flags)                            
+                            if len(cur_string) > 4 and "arXiv" not in cur_string:
+                                # print("The string is bold.", max_string, "font_size:", font_size, "font_flags:", font_flags) 
+                                if cur_title == '':
+                                    cur_title += cur_string
+                                else:
+                                    cur_title += ' ' + cur_string
+                                self.title_page = page_index
+                                # break
+        title = cur_title.replace('\n', ' ')
+        return title
+
+    def extract_section_infomation(self):
+        doc = fitz.open(self.path)
+
+        # è·å–æ–‡æ¡£ä¸­æ‰€æœ‰å­—ä½“å¤§å°
+        font_sizes = []
+        for page in doc:
+            blocks = page.get_text("dict")["blocks"]
+            for block in blocks:
+                if 'lines' not in block:
+                    continue
+                lines = block["lines"]
+                for line in lines:
+                    for span in line["spans"]:
+                        font_sizes.append(span["size"])
+        most_common_size, _ = Counter(font_sizes).most_common(1)[0]
+
+        # æŒ‰ç…§æœ€é¢‘ç¹çš„å­—ä½“å¤§å°ç¡®å®šæ ‡é¢˜å­—ä½“å¤§å°çš„é˜ˆå€¼
+        threshold = most_common_size * 1
+        section_dict = {}
+        section_dict["Abstract"] = ""
+        last_heading = None
+        subheadings = []
+        heading_font = -1
+        # éå†æ¯ä¸€é¡µå¹¶æŸ¥æ‰¾å­æ ‡é¢˜
+        found_abstract = False
+        upper_heading = False
+        font_heading = False
+        for page in doc:
+            blocks = page.get_text("dict")["blocks"]
+            for block in blocks:
+                if not found_abstract:
+                    try:
+                        text = json.dumps(block)
+                    except:
+                        continue
+                    if re.search(r"\bAbstract\b", text, re.IGNORECASE):
+                        found_abstract = True
+                        last_heading = "Abstract"
+                if found_abstract:
+                    if 'lines' not in block:
+                        continue
+                    lines = block["lines"]
+                    for line in lines:
+                        for span in line["spans"]:
+                            # å¦‚æœå½“å‰æ–‡æœ¬æ˜¯å­æ ‡é¢˜
+                            if not font_heading and span["text"].isupper() and sum(1 for c in span["text"] if c.isupper() and ('A' <= c <='Z')) > 4:  # é’ˆå¯¹ä¸€äº›æ ‡é¢˜å¤§å°ä¸€æ ·,ä½†æ˜¯å…¨å¤§å†™çš„è®ºæ–‡
+                                upper_heading = True
+                                heading = span["text"].strip()
+                                if "References" in heading:  # reference ä»¥åçš„å†…å®¹ä¸è€ƒè™‘
+                                    self.section_names = subheadings
+                                    self.section_texts = section_dict
+                                    return
+                                subheadings.append(heading)
+                                if last_heading is not None:
+                                    section_dict[last_heading] = section_dict[last_heading].strip()
+                                section_dict[heading] = ""
+                                last_heading = heading
+                            if not upper_heading and span["size"] > threshold and re.match(  # æ­£å¸¸æƒ…å†µä¸‹,é€šè¿‡å­—ä½“å¤§å°åˆ¤æ–­
+                                    r"[A-Z][a-z]+(?:\s[A-Z][a-z]+)*",
+                                    span["text"].strip()):
+                                font_heading = True
+                                if heading_font == -1:
+                                    heading_font = span["size"]
+                                elif heading_font != span["size"]:
+                                    continue
+                                heading = span["text"].strip()
+                                if "References" in heading:  # reference ä»¥åçš„å†…å®¹ä¸è€ƒè™‘
+                                    self.section_names = subheadings
+                                    self.section_texts = section_dict
+                                    return
+                                subheadings.append(heading)
+                                if last_heading is not None:
+                                    section_dict[last_heading] = section_dict[last_heading].strip()
+                                section_dict[heading] = ""
+                                last_heading = heading
+                            # å¦åˆ™å°†å½“å‰æ–‡æœ¬æ·»åŠ åˆ°ä¸Šä¸€ä¸ªå­æ ‡é¢˜çš„æ–‡æœ¬ä¸­
+                            elif last_heading is not None:
+                                section_dict[last_heading] += " " + span["text"].strip()
+        self.section_names = subheadings
+        self.section_texts = section_dict
+
+class ArxivPDFDownloader(MapFunction):
+    def __init__(self, config):
+        super().__init__()
+        config = config["ArxivPDFDownloader"]
+        self.max_results = config.get("max_results", 5)
+        self.save_dir = config.get("save_dir", "arxiv_pdfs")
+        os.makedirs(self.save_dir, exist_ok=True)
+
+    def execute(self, data: str) -> List[str]:
+        self.query = data
+        base_url = 'http://export.arxiv.org/api/query?'
+        encoded_query = quote(self.query)
+        query = f'search_query={encoded_query}&start=0&max_results={self.max_results}&sortBy=submittedDate&sortOrder=descending'
+        url = base_url + query
+        feed = feedparser.parse(url)
+
+        pdf_paths = []
+
+        print(feed)
+        for entry in feed.entries:
+            arxiv_id = entry.id.split('/abs/')[-1]
+            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
+            pdf_path = os.path.join(self.save_dir, f"{arxiv_id}.pdf")
+
+            if not os.path.exists(pdf_path):
+                try:
+                    resp = requests.get(pdf_url, timeout=15)
+                    if resp.status_code == 200:
+                        with open(pdf_path, 'wb') as f:
+                            f.write(resp.content)
+                        pdf_paths.append(pdf_path)
+                        self.logger.info(f"Downloaded: {pdf_path}")
+                    else:
+                        self.logger.error(f"HTTP {resp.status_code} for {pdf_url}")
+                except Exception as e:
+                    self.logger.error(f"Failed to download {pdf_url}: {e}")
+            else:
+                self.logger.info(f"File already exists: {pdf_path}")
+                pdf_paths.append(pdf_path)
+
+            time.sleep(1)  # é˜²æ­¢è¯·æ±‚è¿‡å¿«
+
+        return pdf_paths
+
+
+class ArxivPDFParser(MapFunction):
+    def __init__(self, config):
+        super().__init__()
+        config = config["ArxivPDFParser"]
+        print(config)
+        self.output_dir = config.get("output_dir", "arxiv_structured_json")
+        os.makedirs(self.output_dir, exist_ok=True)
+
+    def execute(self, data: str) -> List[str]:
+
+        pdf_paths = data
+        output_paths = []
+
+        for pdf_path in pdf_paths:
+
+            filename = os.path.basename(pdf_path).replace('.pdf', '.json')
+            json_path = os.path.join(self.output_dir, filename)
+
+            if not os.path.exists(json_path):
+                try:
+                    paper = Paper(pdf_path)
+                    paper.parse_pdf()
+                    with open(json_path, 'w', encoding='utf-8') as f:
+                        json.dump({
+                            "title": paper.title,
+                            "authors": paper.authors,
+                            "abs": paper.abs,
+                            "sections": paper.section_texts
+                        }, f, ensure_ascii=False, indent=4)
+                    output_paths.append(json_path)
+                    self.logger.info(f"Parsed and saved: {json_path}")
+                except Exception as e:
+                    self.logger.error(f"Failed to parse {pdf_path}: {e}")
+            else:
+                self.logger.info(f"JSON already exists: {json_path}")
+                output_paths.append(json_path)
+
+        return output_paths
\ No newline at end of file
diff --git a/sage_common_funs/rag/chunk.py b/sage_common_funs/rag/chunk.py
new file mode 100644
index 0000000..76b2c4c
--- /dev/null
+++ b/sage_common_funs/rag/chunk.py
@@ -0,0 +1,157 @@
+from typing import Any, List, Literal, Optional, Union
+from sage_core.function.map_function import MapFunction
+
+from typing import Any, List, Optional
+from sentence_transformers import SentenceTransformer
+from transformers import AutoTokenizer
+
+class CharacterSplitter(MapFunction):
+    """
+    A source rag that reads a file and splits its contents into overlapping chunks.
+
+    Input: None (reads directly from a file at the configured path).
+    Output: A Data object containing a list of text chunks.
+
+    Config:
+        - data_path: Path to the input text file.
+        - chunk_size: Number of tokens per chunk (default: 512).
+        - overlap: Number of overlapping tokens (default: 128).
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.config = config
+        self.chunk_size = self.config.get("chunk_size", 512)
+        self.overlap = self.config.get("overlap", 128)
+
+    def _split_text(self, text: str) -> List[str]:
+        """
+        Splits text into chunks of length `chunk_size` with `overlap` between chunks.
+
+        :param text: The full text string to be split.
+        :return: A list of chunked text strings.
+        """
+        tokens = list(text)  # character-level split
+        chunks = []
+        start = 0
+        while start < len(tokens):
+            end = start + self.chunk_size
+            chunk = tokens[start:end]
+            chunks.append("".join(chunk))
+            start += self.chunk_size - self.overlap  # move forward with overlap
+        return chunks
+
+    def execute(self,data:str) -> List[str]:
+        """
+        Reads and splits the file into overlapping text chunks.
+
+        :return: A Data object containing a list of text chunks.
+        """
+        content=data
+        try:
+            chunks = self._split_text(content)
+            return chunks
+        except Exception as e:
+            self.logger.error(f"CharacterSplitter error: {e}", exc_info=True)
+
+
+class SentenceTransformersTokenTextSplitter(MapFunction):
+    """
+    A source rag that splits text into tokens using SentenceTransformer's tokenizer.
+
+    Input: A Data object containing the text to be split.
+    Output: A Data object containing a list of token-based text chunks.
+
+    Config:
+        - chunk_overlap: Number of overlapping tokens between chunks (default: 50).
+        - model_name: The model name for SentenceTransformer (default: "sentence-transformers/all-mpnet-base-v2").
+        - chunk_size: Optional number of tokens per chunk.
+    """
+
+    def __init__(self, config: dict) -> None:
+        super().__init__()
+        self.config = config.get("chunk", {})
+        self.model_name = self.config.get("model_name", "sentence-transformers/all-mpnet-base-v2")
+        self.chunk_size = self.config.get("chunk_size", 512)
+        self.chunk_overlap = self.config.get("chunk_overlap", 50)
+
+        try:
+            # Load the SentenceTransformer model
+            self._model = SentenceTransformer(self.model_name)
+            # Use AutoTokenizer for transformer-based tokenization
+            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
+        except ImportError:
+            raise ImportError(
+                "Could not import sentence_transformers or transformers python packages. "
+                "Please install them with `pip install sentence-transformers transformers`."
+            )
+        except Exception as e:
+            self.logger.error(f"Error while loading model or tokenizer: {e}")
+            raise e
+        
+        if self.chunk_overlap >= self.chunk_size:
+            raise ValueError("Chunk overlap must be less than chunk size.")
+        if self.chunk_size <= 0:
+            raise ValueError("Chunk size must be greater than 0.")
+
+    def split_text_on_tokens(self, text: str) -> List[str]:
+        """
+        Splits incoming text into smaller chunks using the tokenizer.
+
+        :param text: The full text string to be split.
+        :return: A list of token-based text chunks.
+        """
+        print(text)
+        _max_length_equal_32_bit_integer: int = 2**32
+        splits: List[str] = []
+        input_ids = self.tokenizer.encode(text, truncation=True, padding=False)
+        start_idx = 0
+
+        print(f"Input IDs: {input_ids}")
+
+        # Iterate through the text and split it into chunks
+        while start_idx < len(input_ids):
+            print(f"Start Index: {start_idx}")
+            # Define the end of the current chunk
+            cur_idx = min(start_idx + self.chunk_size, len(input_ids))
+            chunk_ids = input_ids[start_idx:cur_idx]
+
+            # Decode the chunk and add it to the list of splits
+            splits.append(self.tokenizer.decode(chunk_ids, skip_special_tokens=True))
+            
+            # Move the starting index forward with the overlap
+            start_idx = cur_idx - self.chunk_overlap
+
+            # Break the loop when we've processed all tokens
+            if cur_idx == len(input_ids):
+                break
+
+        return splits
+
+    def execute(self, data: str) -> List[str]:
+        """
+        Splits the input text data into smaller token-based chunks.
+
+        :param data: The input Data object containing the text to be split.
+        :return: A Data object containing a list of token-based text chunks.
+        """
+        content = data
+        # print(f"Content: {content}")
+        try:
+            chunks = self.split_text_on_tokens(content)
+            return chunks
+        except Exception as e:
+            self.logger.error(f"SentenceTransformersTokenTextSplitter error: {e}", exc_info=True)
+
+# config={
+#     "chunk": {
+#         "chunk_size": 8,
+#         "chunk_overlap": 2,
+#         "model_name": "sentence-transformers/all-mpnet-base-v2",
+#     }
+# }
+
+# split=SentenceTransformersTokenTextSplitter(config)
+# print(split.execute(Data("This is a operator_test sentence to be split into smaller chunks.This is a operator_test sentence to be split into smaller chunks.This is a operator_test sentence to be split into smaller chunks.This is a operator_test sentence to be split into smaller chunks.")))
+
+
diff --git a/sage_common_funs/rag/evaluate.py b/sage_common_funs/rag/evaluate.py
new file mode 100644
index 0000000..9794479
--- /dev/null
+++ b/sage_common_funs/rag/evaluate.py
@@ -0,0 +1,138 @@
+from collections import Counter
+import numpy as np
+import torch
+from transformers import AutoTokenizer, AutoModel
+from sklearn.metrics.pairwise import cosine_similarity
+from rouge import Rouge
+
+from sage_core.function.map_function import MapFunction
+
+
+
+class F1Evaluate(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+    
+    def _get_tokens(self, text):
+        return text.lower().split()
+    
+    def _f1_score(self, prediction, reference):
+        reference_tokens = self._get_tokens(reference)
+        prediction_tokens = self._get_tokens(prediction)
+
+        common_tokens = Counter(reference_tokens) & Counter(prediction_tokens)
+        num_common = sum(common_tokens.values())
+
+        if len(reference_tokens) == 0 or len(prediction_tokens) == 0:
+            return int(reference_tokens == prediction_tokens)
+
+        if num_common == 0:
+            return 0
+
+        precision = num_common / len(prediction_tokens)
+        recall = num_common / len(reference_tokens)
+        f1 = (2 * precision * recall) / (precision + recall)
+        return f1
+
+    def execute(self, data: tuple[str, str]):
+        reference, prediction = data
+        score = self._f1_score(prediction, reference)
+
+        print(f"\033[93m[F1 Score] : {score:.4f}\033[0m")
+
+
+class BertRecallEvaluate(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.model = AutoModel.from_pretrained("bert-base-uncased")
+        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
+        
+    def bert_recall(self, reference, generated):
+        if not generated.strip():
+            return 0.0
+        inputs_ref = self.tokenizer(reference, return_tensors="pt", padding=True, truncation=True)
+        inputs_gen = self.tokenizer(generated, return_tensors="pt", padding=True, truncation=True)
+
+        with torch.no_grad():
+            outputs_ref = self.model(**inputs_ref).last_hidden_state.mean(dim=1)
+            outputs_gen = self.model(**inputs_gen).last_hidden_state.mean(dim=1)
+
+        similarity = cosine_similarity(outputs_ref.numpy(), outputs_gen.numpy())
+        return similarity[0][0]
+
+    def execute(self, data: tuple[str, str]):
+        reference, generated = data
+        score = self.bert_recall(reference, generated)
+
+        print(f"\033[95m[BERT Recall] : {score:.4f}\033[0m")
+
+
+class RougeLEvaluate(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.rouge = Rouge()
+
+    def rouge_l(self, reference, generated):
+        if not generated.strip():
+            return 0.0
+        scores = self.rouge.get_scores(generated, reference)
+        return scores[0]['rouge-l']['f']
+
+    def execute(self, data: tuple[str, str]):
+        reference, generated = data
+        score = self.rouge_l(reference, generated)
+
+        print(f"\033[94m[ROUGE-L] : {score:.4f}\033[0m")
+
+
+class BRSEvaluate(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.bert_recall_evaluate = BertRecallEvaluate(config)
+        self.rouge_l_evaluate = RougeLEvaluate(config)
+
+    def BRS(self, reference, generated):
+        bert_rec = self.bert_recall_evaluate.bert_recall(reference, generated)
+        rouge_l_score = self.rouge_l_evaluate.rouge_l(reference, generated)
+
+        if bert_rec == 0 or rouge_l_score == 0:
+            return 0
+        return (2 * bert_rec * rouge_l_score) / (bert_rec + rouge_l_score)
+
+    def execute(self, data: tuple[str, str]):
+        reference, generated = data
+        score = self.BRS(reference, generated)
+
+        print(f"\033[92m[BRS Score] : {score:.4f}\033[0m")
+
+
+
+# def test_evaluate_functions():
+#     # æ¨¡æ‹Ÿä¸€æ¡æ•°æ®ï¼šreference å’Œ generated
+#     reference = "The cat sits on the mat."
+#     generated = "A cat is sitting on a mat."
+
+#     data = Data((reference, generated))
+
+#     config = {}  # æµ‹è¯•æ—¶configå¯ä»¥æ˜¯ç©ºçš„
+
+#     # åˆå§‹åŒ–æ‰€æœ‰è¯„ä¼°å™¨
+#     f1_eval = F1Evaluate(config)
+#     bert_recall_eval = BertRecallEvaluate(config)
+#     rouge_l_eval = RougeLEvaluate(config)
+#     brs_eval = BRSEvaluate(config)
+
+#     # åˆ†åˆ«æ‰§è¡Œ
+#     print("\n=== F1 Evaluate ===")
+#     f1_eval.execute(data)
+
+#     print("\n=== BERT Recall Evaluate ===")
+#     bert_recall_eval.execute(data)
+
+#     print("\n=== ROUGE-L Evaluate ===")
+#     rouge_l_eval.execute(data)
+
+#     print("\n=== BRS Evaluate ===")
+#     brs_eval.execute(data)
+
+# test_evaluate_functions()
\ No newline at end of file
diff --git a/sage_common_funs/rag/generator.py b/sage_common_funs/rag/generator.py
new file mode 100644
index 0000000..b14dcce
--- /dev/null
+++ b/sage_common_funs/rag/generator.py
@@ -0,0 +1,153 @@
+import os
+from typing import Tuple,List
+from sage_common_funs.utils.generator_model import apply_generator_model
+from sage_core.function.map_function import MapFunction 
+from sage_core.function.base_function import StatefulFunction
+from sage_utils.custom_logger import CustomLogger
+
+class OpenAIGenerator(MapFunction):
+    """
+    OpenAIGenerator is a generator rag that interfaces with a specified OpenAI model
+    to generate responses based on input data.
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+
+        """
+        Initializes the OpenAIGenerator instance with configuration parameters.
+
+        :param config: Dictionary containing configuration for the generator, including 
+                       the method, model name, base URL, API key, etc.
+        """
+        self.config = config
+        # Apply the generator model with the provided configuration
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"],
+            base_url=self.config["base_url"],
+            api_key=self.config["api_key"] or os.getenv("ALIBABA_API_KEY"),
+            seed=42  # Hardcoded seed for reproducibility
+        )
+        self.num = 1
+
+
+
+    # å…¶ä¸­åŸæœ‰çš„**kwargsåº”è¯¥ç”±å‡½æ•°å†…éƒ¨æˆ–è€…dataå†…éƒ¨æä¾›
+    def execute(self, data: list) -> Tuple[str, str]:
+        """
+        Executes the response generation using the configured model based on the input data.
+
+        :param data: Data object containing a list of input data.
+                     The second item in the list is expected to be a dictionary with a "content" key that contains the user's query.
+        :param kwargs: Additional parameters for the model generation (e.g., temperature, max_tokens, etc.).
+
+        :return: A Data object containing a tuple (user_query, response), where user_query is the original input query,
+                and response is the generated response from the model.
+        """
+        # Extract the user query from the input data
+        user_query = data[0] if len(data) > 1  else None
+ 
+        prompt = data[1] if len(data) > 1 else data
+
+        response = self.model.generate(prompt)
+
+        self.num += 1
+
+        self.logger.info(f"[ {self.__class__.__name__}]: Response: {response}")
+
+        # Return the generated response along with the original user query as a tuple
+        return (user_query, response)
+
+class OpenAIGeneratorWithHistory(StatefulFunction):
+    """
+    OpenAIGenerator with global dialogue memory.
+    Maintains a rolling history of past user and assistant turns (stateful).
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.config = config
+
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"],
+            base_url=self.config["base_url"],
+            api_key=self.config["api_key"] or os.getenv("ALIBABA_API_KEY"),
+            seed=42
+        )
+
+        # å…¨å±€å†å²çŠ¶æ€ï¼ŒæŒ‰å¯¹è¯è½®æ¬¡è®°å½•å­—ç¬¦ä¸²
+        self.dialogue_history: List[str] = []
+        self.history_turns = config.get("max_history_turns", 5)
+        self.num = 1
+
+    def execute(self, data: List, **kwargs) -> Tuple[str, str]:
+        """
+        Expects input data: [user_query, prompt_dict]
+        Where prompt_dict includes {"content": ...}
+        """
+        user_query = data[0] if len(data) > 1 else None
+        prompt_info = data[1] if len(data) > 1 else data
+
+        new_turns = [entry for entry in prompt_info if entry["role"] in ("user", "system")]
+
+        history_to_use = self.dialogue_history[-2 * self.history_turns:]
+        full_prompt = history_to_use + new_turns
+
+        self.logger.debug(f"[Prompt with history]:\n{full_prompt}")
+
+        response = self.model.generate(full_prompt, **kwargs)
+
+        for entry in new_turns:
+            if entry["role"] == "user":
+                self.dialogue_history.append(entry)
+        self.dialogue_history.append({"role": "assistant", "content": response})
+        self.dialogue_history = self.dialogue_history[-2 * self.history_turns:]  # ä¿ç•™æœ€è¿‘ N è½®
+
+        self.logger.info(f"\033[32m[{self.__class__.__name__}] Response: {response}\033[0m")
+
+        return (user_query, response)
+
+class HFGenerator(MapFunction):
+    """
+    HFGenerator is a generator rag that interfaces with a Hugging Face model
+    to generate responses based on input data.
+    """
+
+    def __init__(self, config, **kwargs):
+        """
+        Initializes the HFGenerator instance with configuration parameters.
+
+        :param config: Dictionary containing configuration for the generator, including
+                       the method and model name.
+        """
+        super().__init__(**kwargs)
+        self.config = config
+        # Apply the generator model with the provided configuration
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"]
+        )
+
+    def execute(self, data: list, **kwargs) -> Tuple[str, str]:
+        """
+        Executes the response generation using the configured Hugging Face model based on the input data.
+
+        :param data: Data object containing a list of input data.
+                     The expected format and the content of the data depend on the model's requirements.
+        :param kwargs: Additional parameters for the model generation (e.g., temperature, max_tokens, etc.).
+
+        :return: A Data object containing the generated response as a string.
+        """
+        # Generate the response from the Hugging Face model using the provided data and additional arguments
+        user_query = data[0] if len(data) > 1  else None
+ 
+        prompt = data[1] if len(data) > 1 else data
+        
+        response = self.model.generate(prompt, **kwargs)
+
+        # Return the generated response as a Data object
+        self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Response: {response}\033[0m ")
+
+        return (user_query, response)
diff --git a/sage_common_funs/rag/promptor.py b/sage_common_funs/rag/promptor.py
new file mode 100644
index 0000000..b1f35c1
--- /dev/null
+++ b/sage_common_funs/rag/promptor.py
@@ -0,0 +1,181 @@
+from jinja2 import Template
+from sage_core.function.base_function import StatefulFunction, MemoryFunction
+
+from sage_core.function.map_function import MapFunction
+
+from sage_utils.custom_logger import CustomLogger
+
+QA_prompt_template='''Instruction:
+You are an intelligent assistant with access to a knowledge base. Answer the question below with reference to the provided context.
+Only give me the answer and do not output any other words.
+{%- if external_corpus %}
+Relevant corpus for the current question:
+{{ external_corpus }}
+{%- endif %}
+'''
+
+summarization_prompt_template = '''Instruction:
+You are an intelligent assistant. Summarize the content provided below in a concise and clear manner.
+Only provide the summary and do not include any additional information.
+{%- if external_corpus %}
+Content to summarize:
+{{ external_corpus }}
+{%- endif %}
+'''
+QA_prompt_template = Template(QA_prompt_template)
+summarization_prompt_template = Template(summarization_prompt_template)
+
+
+class QAPromptor(MapFunction):
+    """
+    QAPromptor is a prompt rag that generates a QA-style prompt using
+    an external corpus and a user query. This class is designed to prepare 
+    the necessary prompt structure for a question-answering model.
+
+    Attributes:
+        config: Configuration data for initializing the prompt rag (e.g., model details, etc.).
+        prompt_template: A template used for generating the system prompt, typically includes context or instructions.
+    """
+    
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+
+        """
+        Initializes the QAPromptor instance with configuration and prompt template.
+
+        :param config: Dictionary containing configuration for the prompt rag.
+        """
+        self.config = config  # Store the configuration for later use
+        self.prompt_template = QA_prompt_template  # Load the QA prompt template
+
+    # sage_lib/functions/rag/qapromptor.py
+    def execute(self, data) -> list:
+        """
+        ç”Ÿæˆ ChatGPT é£æ ¼çš„ promptï¼ˆsystem+user ä¸¤æ¡æ¶ˆæ¯ï¼‰ã€‚
+
+        æ”¯æŒä¸¤ç§è¾“å…¥ï¼š
+        1. (query, external_corpus_list_or_str))
+        2. query_str)
+        """
+        try:
+            # -------- è§£æè¾“å…¥ --------
+            raw = data
+            if isinstance(raw, tuple) and len(raw) == 2:
+                query, external_corpus = raw
+                if isinstance(external_corpus, list):
+                    external_corpus = "\n".join(external_corpus)
+            else:
+                query = raw
+                external_corpus = ""
+
+            external_corpus = external_corpus or ""
+
+            # -------- system prompt --------
+            if external_corpus:
+                system_prompt = {
+                    "role": "system",
+                    "content": self.prompt_template.render(external_corpus=external_corpus),
+                }
+            else:
+                system_prompt = {
+                    "role": "system",
+                    "content": (
+                        "You are a helpful AI assistant. "
+                        "Answer the user's questions accurately."
+                    ),
+                }
+
+            # -------- user prompt --------
+            user_prompt = {
+                "role": "user",
+                "content": f"Question: {query}",
+            }
+
+            prompt = [system_prompt, user_prompt]
+            return [query,prompt]
+
+        except Exception as e:
+            self.logger.error(
+                "QAPromptor error: %s | input=%s", e, getattr(data, "data", "")
+            )
+            fallback = [
+                {"role": "system", "content": "System encountered an error."},
+                {
+                    "role": "user",
+                    "content": (
+                        "Question: Error occurred. Please try again."
+                        f" (Original: {getattr(data, 'data', '')})"
+                    ),
+                },
+            ]
+            return fallback
+
+
+class SummarizationPromptor(MapFunction):
+    """
+    QAPromptor is a prompt rag that generates a QA-style prompt using
+    an external corpus and a user query. This class is designed to prepare
+    the necessary prompt structure for a question-answering model.
+
+    Attributes:
+        config: Configuration data for initializing the prompt rag (e.g., model details, etc.).
+        prompt_template: A template used for generating the system prompt, typically includes context or instructions.
+    """
+
+    def __init__(self, config):
+        """
+        Initializes the QAPromptor instance with configuration and prompt template.
+
+        :param config: Dictionary containing configuration for the prompt rag.
+        """
+        super().__init__()
+        self.config = config  # Store the configuration for later use
+        self.prompt_template = summarization_prompt_template  # Load the summarization prompt template
+
+    def execute(self, data) -> list:
+        """
+        Generates a QA-style prompt for the input question and external corpus.
+
+        This method takes the query and external corpus, processes the corpus
+        into a single string, and creates a system prompt and user prompt based
+        on a predefined template.
+
+        :param data: A Data object containing a tuple. The first element is the query (a string),
+                     and the second is a list of external corpus (contextual information for the model).
+
+        :return: A Data object containing a list with two prompts:
+                 1. system_prompt: A system prompt based on the template with external corpus data.
+                 2. user_prompt: A user prompt containing the question to be answered.
+        """
+        # Unpack the input data into query and external_corpus
+        query, external_corpus = data
+
+        # Combine the external corpus list into a single string (in case it's split into multiple parts)
+        external_corpus = "".join(external_corpus)
+
+        # Prepare the base data for the system prompt, which includes the external corpus
+        base_system_prompt_data = {
+            "external_corpus": external_corpus
+        }
+
+        # query = data
+        # Create the system prompt using the template and the external corpus data
+        system_prompt = {
+            "role": "system",
+            "content": self.prompt_template.render(**base_system_prompt_data)
+        }
+        # system_prompt = {
+        #     "role": "system",
+        #     "content": ""
+        # }
+        # Create the user prompt using the query
+        user_prompt = {
+            "role": "user",
+            "content": f"Question: {query}"
+        }
+
+        # Combine the system and user prompts into one list
+        prompt = [system_prompt, user_prompt]
+
+        # Return the prompt list wrapped in a Data object
+        return prompt
diff --git a/sage_common_funs/rag/refiner.py b/sage_common_funs/rag/refiner.py
new file mode 100644
index 0000000..b8ad16c
--- /dev/null
+++ b/sage_common_funs/rag/refiner.py
@@ -0,0 +1,117 @@
+from sage_core.function.map_function import MapFunction
+
+from sage_common_funs.utils.generator_model import apply_generator_model
+from typing import Tuple,List
+import logging
+
+
+class AbstractiveRecompRefiner(MapFunction):
+    """
+    AbstractiveRecompRefiner is an abstractive refiner using the RECOMP approach. 
+    This class is responsible for refining retrieved documents by generating concise summaries
+    that directly answer a given question. The summary is generated using an external model.
+
+    Attributes:
+        logger: Logger for logging errors and information.
+        config: Configuration dictionary that holds settings for the refiner (e.g., model parameters).
+        model: A model instance used for generating summaries based on the provided input.
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        """
+        Initializes the AbstractiveRecompRefiner instance with configuration and model.
+
+        :param config: Dictionary containing configuration for the refiner, including model details 
+                       (method, model_name, base_url, api_key, etc.).
+        """
+        self.config = config  # Store the refiner configuration
+        # Apply the generator model based on provided configuration
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"],
+            base_url=self.config["base_url"],
+            api_key=self.api_key,
+            seed=42  # Set a seed for reproducibility of results
+        )
+
+    def execute(self, data: Tuple[str, List[str]]) -> Tuple[str, List[str]]:
+        """
+        Executes the refining process by generating concise summaries for retrieved documents.
+
+        :param data: A Data object containing a tuple of (query, doc_set), where:
+                     - query: The user question that needs to be answered.
+                     - doc_set: A list of retrieved documents relevant to the query.
+
+        :return: A Data object containing a tuple (query, emit_docs), where:
+                 - query: The original user question.
+                 - emit_docs: A list of generated summaries corresponding to the documents.
+        """
+        try:
+            # Unpack the input data into the query and document set
+            query, doc_set = data
+            emit_docs = []  # List to hold the summaries of the documents
+            
+            # Process each retrieved document
+            for retrieval_docs in doc_set:
+                # Format the input for the model (question + document content)
+                query, processed_docs = self._format_input(query, retrieval_docs)
+                # Build the prompt for the model
+                input_prompt = self.build_prompt(query, processed_docs)
+                # Generate a summary for the document
+                summary = self.model.generate(input_prompt)
+                summary = [summary]  # Wrap the summary in a list
+                emit_docs.append(summary)  # Add the summary to the list of emitted documents
+
+        except Exception as e:
+            # Log any errors that occur during the refining process
+            self.logger.error(f"{str(e)} when RefinerFuction")
+            raise RuntimeError(f"Refining error: {str(e)}")
+        
+        # Return the refined results as a Data object
+        return (query, emit_docs[0])
+
+    def _format_input(self, question, retrieval_result):
+        """
+        Formats the input for the model by processing the retrieved documents.
+
+        :param question: The user query that needs to be answered.
+        :param retrieval_result: A list of documents retrieved for the question.
+
+        :return: A tuple containing the question and the processed documents, where:
+                 - The question remains unchanged.
+                 - The documents are formatted into a single string.
+        """
+        # Process the documents by joining all the lines in each document
+        processed_docs = [
+            "\n".join(doc.split("\n")[:])  # Here we are just splitting and joining to simulate processing
+            for doc in retrieval_result
+        ]
+        processed_docs = '\n'.join(processed_docs)  # Join all processed documents into one string
+        return question, processed_docs
+
+    def build_prompt(self, question, docs):
+        """
+        Builds the prompt for the model to generate a summary from the provided documents.
+
+        :param question: The user query that needs to be answered.
+        :param docs: The processed documents that will be used to generate the summary.
+
+        :return: A list containing a single dictionary, which represents the user prompt in the required format.
+        """
+        user_content = f"""
+        Generate a concise, factual summary from the document below that specifically answers the question. Follow these requirements:
+        1. Extract ONLY information directly related to the question
+        2. Present results using bullet points or short structured paragraphs
+        3. Include critical elements: 
+        - Numerical data/percentages 
+        - Time periods/dates 
+        - Definitive conclusions
+        - Named entities (people/organizations/locations)
+        4. Exclude speculative statements and irrelevant details
+
+        Question:
+        {question}
+
+        Document:
+        """
diff --git a/sage_common_funs/rag/reranker.py b/sage_common_funs/rag/reranker.py
new file mode 100644
index 0000000..84d8204
--- /dev/null
+++ b/sage_common_funs/rag/reranker.py
@@ -0,0 +1,300 @@
+import torch
+from typing import List, Tuple
+from transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoModelForCausalLM
+import logging
+
+from sage_core.function.map_function import MapFunction
+
+
+
+class BGEReranker(MapFunction):
+    """
+    A reranker that uses the BAAI/bge-reranker-v2-m3 model to reorder a list of retrieved documents.
+    The model assigns relevance scores to the documents and ranks them accordingly.
+
+    Input: A tuple of (query, List[retrieved_documents])
+    Output: A tuple of (query, List[reranked_documents_with_scores])
+
+    Attributes:
+        logger: Logger for logging error and information messages.
+        config: Configuration dictionary containing reranker settings (model name, top_k, etc.).
+        device: Device ('cuda' or 'cpu') where the model will be loaded.
+        tokenizer: Tokenizer used to preprocess input queries and documents.
+        model: The pre-trained reranking model.
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        """
+        Initializes the BGEReranker with configuration settings and loads the model.
+
+        :param config: Dictionary containing configuration options, including model name and device settings.
+        """
+        self.config = config
+        self.device = "cuda" if torch.cuda.is_available() else "cpu"  # Set device to GPU if available, otherwise CPU
+        
+        # Load tokenizer and model using the provided model name
+        self.tokenizer, self.model = self._load_model(self.config["model_name"])
+        self.model = self.model.to(self.device)
+        self.model.eval()  # Set the model to evaluation mode
+
+    def _load_model(self, model_name: str):
+        """
+        Loads the tokenizer and model for the reranker.
+
+        :param model_name: Name of the pre-trained model to load.
+        :return: Tuple containing the tokenizer and the model.
+        """
+        try:
+            self.logger.info(f"Loading reranker: {model_name}")
+            tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load the tokenizer
+            model = AutoModelForSequenceClassification.from_pretrained(model_name)  # Load the model
+            return tokenizer, model
+        except Exception as e:
+            self.logger.error(f"Failed to load model {model_name}: {str(e)}")
+            raise RuntimeError(f"Model loading failed: {str(e)}")
+        
+    def execute(self, data: Tuple[str, List[str]]):
+        """
+        Executes the reranking process:
+        1. Unpacks the input data (query and list of documents).
+        2. Generates query-document pairs.
+        3. Calculates relevance scores using the model.
+        4. Sorts documents based on their relevance scores.
+
+        :param data: A Data object containing a tuple of (query, doc_set), where:
+                     - query: The user query.
+                     - doc_set: List of retrieved documents.
+        :return: A Data object containing a tuple (query, reranked_documents_with_scores).
+        """
+        try:
+            query, doc_set = data  # Unpack the input data
+            top_k = self.config["topk"]  # Get the top-k parameter for reranking
+
+            # Generate query-document pairs for scoring
+            pairs = [(query, doc) for doc in doc_set]
+
+            # Tokenize the pairs and move inputs to the appropriate device
+            raw_inputs = self.tokenizer(
+                            pairs,
+                            padding=True,
+                            truncation=True,
+                            max_length=512,
+                            return_tensors="pt"
+                        )
+            inputs = {
+                k: v.to(self.device) if isinstance(v, torch.Tensor) else v
+                for k, v in raw_inputs.items()
+            }
+
+
+            
+            # Perform inference and calculate scores
+            scores = self.model(**inputs).logits.view(-1).float()
+
+            # Create a list of scored documents
+            scored_docs = [
+                {"retrieved_docs": doc, "relevance_score": score}
+                for doc, score in zip(doc_set, scores)
+            ]
+            
+            # Sort the documents by relevance score in descending order
+            reranked_docs = sorted(scored_docs, key=lambda x: x["relevance_score"], reverse=True)[:top_k]
+            reranked_docs_list = [doc["retrieved_docs"] for doc in reranked_docs]
+            self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Rerank Results: {reranked_docs_list }\033[0m ")
+            self.logger.debug(f"Top score: {reranked_docs[0]['relevance_score'] if reranked_docs else 'N/A'}")
+
+        except Exception as e:
+            raise RuntimeError(f"BGEReranker error: {str(e)}")
+        
+        return [query, reranked_docs_list]  # Return the reranked documents along with the original query
+
+
+class LLMbased_Reranker(MapFunction):
+    """
+    A reranker that uses the BAAI/bge-reranker-v2-gemma model to determine if a retrieved document contains an answer to a given query.
+    It scores the documents with 'Yes' or 'No' predictions based on whether the document answers the query.
+
+    Input: A tuple of (query, List[retrieved_documents])
+    Output: A tuple of (query, List[reranked_documents_with_scores])
+
+    Attributes:
+        logger: Logger for logging error and information messages.
+        config: Configuration dictionary containing reranker settings (model name, top_k, etc.).
+        device: Device ('cuda' or 'cpu') where the model will be loaded.
+        tokenizer: Tokenizer used to preprocess input queries and documents.
+        model: The pre-trained reranking model.
+        yes_loc: Token ID representing 'Yes' (used for scoring).
+    """
+
+    def __init__(self, config, model_name: str = "BAAI/bge-reranker-v2-gemma"):
+        """
+        Initializes the LLMbased_Reranker with configuration settings and loads the model.
+
+        :param config: Dictionary containing configuration options, including model name and device settings.
+        :param model_name: Name of the pre-trained model to load (default is "BAAI/bge-reranker-v2-gemma").
+        """
+        super().__init__()
+        self.config = config
+        self.logger = logging.getLogger(self.__class__.__name__)
+        self.device = "cuda" if torch.cuda.is_available() else "cpu"  # Set device to GPU if available, otherwise CPU
+
+        # Load tokenizer and model using the provided model name
+        self.tokenizer, self.model = self._load_model(model_name)
+        self.model = self.model.to(self.device)
+        
+        # Get the token ID for the 'Yes' token (used for classification)
+        self.yes_loc = self.tokenizer('Yes', add_special_tokens=False)['input_ids'][0]
+
+    def _load_model(self, model_name: str):
+        """
+        Loads the tokenizer and model for the reranker.
+
+        :param model_name: Name of the pre-trained model to load.
+        :return: Tuple containing the tokenizer and the model.
+        """
+        try:
+            self.logger.info(f"Loading reranker: {model_name}")
+            tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load the tokenizer
+            model = AutoModelForCausalLM.from_pretrained(model_name)  # Load the model
+            return tokenizer, model
+        except Exception as e:
+            self.logger.error(f"Failed to load model {model_name}: {str(e)}")
+            raise RuntimeError(f"Model loading failed: {str(e)}")
+
+    def get_inputs(self, pairs, tokenizer, prompt=None, max_length=1024):
+        """
+        Prepares the input for the model, including the prompt and the query-document pairs.
+
+        :param pairs: List of query-document pairs.
+        :param tokenizer: The tokenizer used to process the input data.
+        :param prompt: Optional prompt to guide the model (defaults to a generic query-passage prompt).
+        :param max_length: Maximum length of the tokenized input sequences.
+        :return: A tensor of tokenized inputs, ready for model inference.
+        """
+        if prompt is None:
+            prompt = "Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'."
+        
+        sep = "\n"
+        prompt_inputs = tokenizer(prompt, return_tensors=None, add_special_tokens=False)['input_ids']
+        sep_inputs = tokenizer(sep, return_tensors=None, add_special_tokens=False)['input_ids']
+        
+        inputs = []
+        for query, passage in pairs:
+            query_inputs = tokenizer(f'A: {query}', return_tensors=None, add_special_tokens=False, max_length=max_length * 3 // 4, truncation=True)
+            passage_inputs = tokenizer(f'B: {passage}', return_tensors=None, add_special_tokens=False, max_length=max_length, truncation=True)
+            
+            item = tokenizer.prepare_for_model(
+                [tokenizer.bos_token_id] + query_inputs['input_ids'],
+                sep_inputs + passage_inputs['input_ids'],
+                truncation='only_second',
+                max_length=max_length,
+                padding=False,
+                return_attention_mask=False,
+                return_token_type_ids=False,
+                add_special_tokens=False
+            )
+            item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs
+            item['attention_mask'] = [1] * len(item['input_ids'])
+            inputs.append(item)
+        
+        return tokenizer.pad(
+            inputs,
+            padding=True,
+            max_length=max_length + len(sep_inputs) + len(prompt_inputs),
+            pad_to_multiple_of=8,
+            return_tensors='pt',
+        )
+
+    # @torch.inference_mode()
+    def execute(self, data: Tuple[str, List[str]]) -> Tuple[str, List[str]]:
+        """
+        Executes the reranking process:
+        1. Unpacks the input data (query and list of documents).
+        2. Generates query-document pairs for classification.
+        3. Calculates relevance scores based on 'Yes'/'No' predictions.
+        4. Sorts documents based on their relevance scores.
+
+        :param data: A Data object containing a tuple of (query, doc_set), where:
+                     - query: The user query.
+                     - doc_set: List of retrieved documents.
+        :return: A Data object containing a tuple (query, reranked_documents_with_scores).
+        """
+        try:
+            query, doc_set = data  # Unpack the input data
+            doc_set = [doc_set]  # Wrap doc_set in a list for processing
+            top_k = self.config["topk"]  # Get the top-k parameter for reranking
+            emit_docs = []  # Initialize the list to store reranked documents
+
+            for retrieved_docs in doc_set:
+                # Generate query-document pairs for classification
+                pairs = [[query, doc] for doc in retrieved_docs]
+                
+                # Tokenize the pairs and move inputs to the appropriate device
+                with torch.no_grad():
+                    raw_inputs = self.get_inputs(pairs, self.tokenizer)
+                    inputs = {k: v.to(self.device) for k, v in raw_inputs.items()}
+
+                    scores = self.model(**inputs, return_dict=True).logits[:, -1, self.yes_loc].view(-1).float()
+
+                # Create a list of scored documents
+                scored_docs = [
+                    {"retrieved_docs": doc, "relevance_score": score}
+                    for doc, score in zip(retrieved_docs, scores)
+                ]
+                
+                # Sort the documents by relevance score in descending order
+                reranked_docs = sorted(scored_docs, key=lambda x: x["relevance_score"], reverse=True)[:top_k]
+                reranked_docs_list = [doc["retrieved_docs"] for doc in reranked_docs]
+                emit_docs.append(reranked_docs_list)
+                self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Rerank Results: {reranked_docs_list }\033[0m ")
+                self.logger.debug(f"Top score: {reranked_docs[0]['relevance_score'] if reranked_docs else 'N/A'}")
+
+        except Exception as e:
+            self.logger.error(f"{str(e)} when RerankerFuncton")
+            raise RuntimeError(f"Reranker error: {str(e)}")
+        
+        emit_docs = emit_docs[0]  # Only return the first set of reranked documents
+        return (query, emit_docs)  # Return the reranked documents along with the original query
+
+
+if __name__ == '__main__':
+
+   # è®¾ç½®é…ç½®
+    config1 = {
+        "reranker": {
+            "model_name":"BAAI/bge-reranker-v2-m3",
+            "top_k": 3
+        }
+    }
+
+    config2 = {
+        "reranker": {
+            "model_name":"BAAI/bge-reranker-v2-gemma",
+            "top_k": 3
+        }
+    }
+
+    # åˆ›å»ºå®ä¾‹
+    # reranker = BGEReranker(config)
+    reranker = LLMbased_Reranker(config2)
+    # æµ‹è¯•æ•°æ®
+    query = "What is the capital of France?"
+    docs = [
+        "Paris is the capital of France.",
+        "Berlin is a city in Germany.",
+        "The Eiffel Tower is located in Paris.",
+        "France is a country in Western Europe.",
+        "Madrid is the capital of Spain."
+    ]
+
+    # æ‰§è¡Œé‡æ’
+    input_data = (query, docs)
+    output = reranker.execute(input_data)
+
+    # è¾“å‡ºç»“æœ
+    result_query, result_docs = output
+    print("Query:", result_query)
+    print("Top-k Re-ranked Documents:")
+    for i, doc in enumerate(result_docs, 1):
+        print(f"{i}. {doc}")
\ No newline at end of file
diff --git a/sage_common_funs/rag/retriever.py b/sage_common_funs/rag/retriever.py
new file mode 100644
index 0000000..b97f84d
--- /dev/null
+++ b/sage_common_funs/rag/retriever.py
@@ -0,0 +1,76 @@
+from typing import Tuple, List
+import time  # æ›¿æ¢ asyncio ä¸º time ç”¨äºåŒæ­¥å»¶è¿Ÿ
+
+from sage_core.function.map_function import MapFunction
+from sage_core.function.base_function import MemoryFunction, StatefulFunction
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.runtime_context import RuntimeContext
+
+# æ›´æ–°åçš„ SimpleRetriever
+class DenseRetriever(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+
+        self.config = config
+
+        
+        if self.config.get("ltm", False):
+            self.ltm_config = self.config.get("ltm", {})
+        else:
+            self.ltm = None
+
+    def execute(self, data: str) -> Tuple[str, List[str]]:
+
+        input_query = data[0] if isinstance(data, tuple) and len(data) > 0 else data
+        chunks = []
+        self.logger.info(f"[ {self.__class__.__name__}]: Retrieving from LTM")
+        self.logger.info(f"Starting retrieval for query: {input_query}")
+        # LTM æ£€ç´¢
+        if self.config.get("ltm", False):
+            self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Retrieving from LTM \033[0m ")
+            try:
+                # ä½¿ç”¨LTMé…ç½®å’Œè¾“å…¥æŸ¥è¯¢è°ƒç”¨æ£€ç´¢
+                ltm_results = self.runtime_context.retrieve(
+                    query=input_query,
+                    collection_config=self.ltm_config
+                )
+                self.logger.info(f"Retrieved {len(ltm_results)} from LTM")
+                self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Retrieval Results: {ltm_results}\033[0m ")
+                chunks.extend(ltm_results)
+
+            except Exception as e:
+                self.logger.error(f"LTM retrieval failed: {str(e)}")
+
+        return (input_query, chunks)
+    
+class BM25sRetriever(MapFunction): # ç›®å‰runtime contextè¿˜åªæ”¯æŒltm
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.config = config
+        self.bm25s_collection = self.config.get("bm25s_collection")
+        self.bm25s_config = self.config.get("bm25s_config", {})
+
+
+    def execute(self, data: str) -> Tuple[str, List[str]]:
+        input_query = data
+        chunks = []
+        self.logger.debug(f"Starting BM25s retrieval for query: {input_query}")
+
+        if not self.bm25s_collection:
+            raise ValueError("BM25s collection is not configured.")
+
+        try:
+            # ä½¿ç”¨BM25sé…ç½®å’Œè¾“å…¥æŸ¥è¯¢è°ƒç”¨æ£€ç´¢
+            bm25s_results = self.runtime_context.retrieve(
+                # self.bm25s_collection,
+                query=input_query,
+                collection_config=self.bm25s_config
+            )
+            chunks.extend(bm25s_results)
+            self.logger.info(f"\033[32m[ {self.__class__.__name__}]:Query: {input_query} Retrieved {len(bm25s_results)} from BM25s\033[0m ")
+            print(input_query)
+            print(bm25s_results)
+        except Exception as e:
+            self.logger.error(f"BM25s retrieval failed: {str(e)}")
+
+        return (input_query, chunks)
\ No newline at end of file
diff --git a/sage_common_funs/rag/searcher.py b/sage_common_funs/rag/searcher.py
new file mode 100644
index 0000000..50d449c
--- /dev/null
+++ b/sage_common_funs/rag/searcher.py
@@ -0,0 +1,40 @@
+from sage_core.function.map_function import MapFunction
+
+from typing import Dict, Any
+import requests
+import json
+import logging
+
+class BochaWebSearch(MapFunction):
+
+    def __init__(self, config: Dict[str, Any], **kwargs):
+        super().__init__(**kwargs)
+        self.api_key = config.get("api_key")
+        self.count = config.get("count", 10)
+        self.page = config.get("page", 1)
+        self.summary = config.get("summary", True)
+        self.url = "https://api.bochaai.com/v1/web-search"
+
+        if not self.api_key:
+            raise ValueError("BochaWebSearch requires an 'api_key' in config.")
+
+    def execute(self, data: str) -> Dict[str, Any]:
+        query = data
+        headers = {
+            'Authorization': self.api_key,
+            'Content-Type': 'application/json'
+        }
+        payload = {
+            "query": query,
+            "summary": self.summary,
+            "count": self.count,
+            "page": self.page
+        }
+
+        try:
+            response = requests.post(self.url, headers=headers, json=payload)
+            response.raise_for_status()
+            result = response.json()
+            return result
+        except Exception as e:
+            self.logger.error(f"BochaWebSearch error: {e}", exc_info=True)
diff --git a/sage_common_funs/rag/trigger.py b/sage_common_funs/rag/trigger.py
new file mode 100644
index 0000000..55886ae
--- /dev/null
+++ b/sage_common_funs/rag/trigger.py
@@ -0,0 +1,166 @@
+import asyncio
+import queue
+import threading
+import time
+from typing import Any, Optional
+
+from sage_core.function.map_function import MapFunction
+
+
+
+class TriggerableSource(MapFunction):
+    """
+    å¯è§¦å‘çš„æ•°æ®æºï¼Œæ”¯æŒå¤–éƒ¨è¾“å…¥è§¦å‘å¤„ç†
+    """
+    
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        """
+        åˆå§‹åŒ–å¯è§¦å‘æ•°æ®æº
+        
+        Args:
+            config: é…ç½®å­—å…¸
+        """
+        self.config = config.get("source", {})
+        
+        # è¾“å…¥é˜Ÿåˆ—ï¼Œç”¨äºæ¥æ”¶å¤–éƒ¨è§¦å‘çš„æ•°æ®
+        self.input_queue = queue.Queue()
+        
+        # æ§åˆ¶æ ‡å¿—
+        self._running = False
+        self._stop_requested = False
+        
+        # ç”¨äºå¤–éƒ¨è°ƒç”¨çš„é”
+        self._trigger_lock = threading.Lock()
+        
+        # ç­‰å¾…æ¨¡å¼é…ç½®
+        self.wait_timeout = self.config.get("wait_timeout", 1.0)  # ç­‰å¾…æ–°è¾“å…¥çš„è¶…æ—¶æ—¶é—´
+        self.enable_polling = self.config.get("enable_polling", True)  # æ˜¯å¦å¯ç”¨è½®è¯¢æ¨¡å¼
+        
+        self.logger.info(f"TriggerableSource initialized with config: {self.config}")
+    
+    def execute(self) -> Optional[Any]:
+        """
+        æ‰§è¡Œæ–¹æ³•ï¼Œä»è¾“å…¥é˜Ÿåˆ—è·å–æ•°æ®
+        
+        Returns:
+            Dataå¯¹è±¡æˆ–None
+        """
+        try:
+            if self.enable_polling:
+                # è½®è¯¢æ¨¡å¼ï¼šç­‰å¾…ä¸€æ®µæ—¶é—´åè¶…æ—¶è¿”å›None
+                try:
+                    data = self.input_queue.get(timeout=self.wait_timeout)
+                    if data is None:  # åœæ­¢ä¿¡å·
+                        return None
+                    self.logger.debug(f"Got triggered data: {data}")
+                    return data
+                except queue.Empty:
+                    # è¶…æ—¶ï¼Œè¿”å›Noneï¼Œè®©è°ƒç”¨è€…å†³å®šæ˜¯å¦ç»§ç»­è½®è¯¢
+                    return None
+            else:
+                # é˜»å¡æ¨¡å¼ï¼šä¸€ç›´ç­‰å¾…ç›´åˆ°æœ‰æ•°æ®
+                data = self.input_queue.get()
+                if data is None:  # åœæ­¢ä¿¡å·
+                    return None
+                self.logger.debug(f"Got triggered data: {data}")
+                return data
+                
+        except Exception as e:
+            self.logger.error(f"Error in TriggerableSource.execute(): {e}")
+            return None
+    
+    def trigger(self, data: Any) -> bool:
+        """
+        å¤–éƒ¨è§¦å‘æ¥å£ï¼Œå°†æ•°æ®æ”¾å…¥é˜Ÿåˆ—
+        
+        Args:
+            data: è¦å¤„ç†çš„æ•°æ®
+            
+        Returns:
+            bool: æ˜¯å¦æˆåŠŸè§¦å‘
+        """
+        try:
+            with self._trigger_lock:
+                if self._stop_requested:
+                    self.logger.warning("Source has been stopped, ignoring trigger")
+                    return False
+                
+                self.input_queue.put(data)
+                self.logger.debug(f"Triggered with data: {data}")
+                return True
+                
+        except Exception as e:
+            self.logger.error(f"Error triggering source: {e}")
+            return False
+    
+    def stop(self):
+        """åœæ­¢æ•°æ®æº"""
+        self._stop_requested = True
+        # å‘é€åœæ­¢ä¿¡å·
+        try:
+            self.input_queue.put(None)
+        except Exception as e:
+            self.logger.error(f"Error stopping source: {e}")
+    
+    def is_empty(self) -> bool:
+        """æ£€æŸ¥è¾“å…¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º"""
+        return self.input_queue.empty()
+    
+    def queue_size(self) -> int:
+        """è·å–é˜Ÿåˆ—å¤§å°"""
+        return self.input_queue.qsize()
+
+
+class RESTApiSource(TriggerableSource):
+    """
+    ä¸“é—¨ç”¨äºREST APIè¯·æ±‚çš„æ•°æ®æº
+    """
+    def __init__(self, config: dict, **kwargs):
+        super().__init__(config, **kwargs)
+        
+        # APIç‰¹å®šé…ç½®
+        self.request_timeout = self.config.get("request_timeout", 30.0)
+        self.max_queue_size = self.config.get("max_queue_size", 100)
+        
+        # è¯·æ±‚IDè¿½è¸ª
+        self._request_counter = 0
+        self._pending_requests = {}
+        
+    def trigger_request(self, request_data: dict, request_id: str = None) -> str:
+        """
+        è§¦å‘APIè¯·æ±‚å¤„ç†
+        
+        Args:
+            request_data: è¯·æ±‚æ•°æ®
+            request_id: å¯é€‰çš„è¯·æ±‚ID
+            
+        Returns:
+            str: è¯·æ±‚ID
+        """
+        if request_id is None:
+            self._request_counter += 1
+            request_id = f"req_{self._request_counter}"
+        
+        # æ£€æŸ¥é˜Ÿåˆ—å¤§å°
+        if self.queue_size() >= self.max_queue_size:
+            raise ValueError(f"Request queue is full (max: {self.max_queue_size})")
+        
+        # åŒ…è£…è¯·æ±‚æ•°æ®
+        wrapped_request = {
+            "request_id": request_id,
+            "data": request_data,
+            "timestamp": time.time()
+        }
+        
+        success = self.trigger(wrapped_request)
+        if success:
+            self._pending_requests[request_id] = wrapped_request
+            self.logger.info(f"API request {request_id} queued")
+            return request_id
+        else:
+            raise RuntimeError(f"Failed to queue API request {request_id}")
+    
+    def get_pending_requests(self) -> list:
+        """è·å–å¾…å¤„ç†è¯·æ±‚åˆ—è¡¨"""
+        return list(self._pending_requests.keys())
\ No newline at end of file
diff --git a/sage_common_funs/rag/writer.py b/sage_common_funs/rag/writer.py
new file mode 100644
index 0000000..40da4bd
--- /dev/null
+++ b/sage_common_funs/rag/writer.py
@@ -0,0 +1,76 @@
+from typing import Union, List, Tuple, Optional, Dict
+from sage_core.function.map_function import MapFunction
+
+
+
+class MemoryWriter(MapFunction):
+
+    def __init__(self, config: dict, **kwargs):
+        super().__init__(config, **kwargs)
+        self.state = None
+        self.config = config
+        # åˆå§‹åŒ–å„ç±»å‹é›†åˆ
+        self.collections = {}
+
+        # é…ç½®STM
+        if self.config.get("stm", False):
+            stm_config = self.config.get("stm_config", {})
+            self.collections["stm"] = {
+                "collection": self.config.get("stm_collection"),
+                "config": stm_config
+            }
+
+        # é…ç½®LTM
+        if self.config.get("ltm", False):
+            ltm_config = self.config.get("ltm_config", {})
+            self.collections["ltm"] = {
+                "collection": self.config.get("ltm_collection"),
+                "config": ltm_config
+            }
+
+        # é…ç½®DCM
+        if self.config.get("dcm", False):
+            dcm_config = self.config.get("dcm_config", {})
+            self.collections["dcm"] = {
+                "collection": self.config.get("dcm_collection"),
+                "config": dcm_config
+            }
+        # TODO: åœ¨runtime_contextä¸­å¢åŠ çŠ¶æ€ç®¡ç†
+        # Issue URL: https://github.com/intellistream/SAGE/issues/219
+        # state = getRuntimeContext().getState(xxx)
+
+
+    def execute(self, data: Union[str, List[str], Tuple[str, str]]):
+        input_data = data
+
+        # ç»Ÿä¸€æ•°æ®ç±»å‹å¤„ç†
+        processed_data = []
+        if isinstance(input_data, list):
+            processed_data = input_data
+        elif isinstance(input_data, tuple) and len(input_data) == 2:
+            processed_data = [f"{input_data[0]}{input_data[1]}"]  # æ‹¼æ¥å…ƒç»„
+        elif isinstance(input_data, str):
+            processed_data = [input_data]
+        else:
+            self.logger.error(f"Unsupported data type: {type(input_data)}")
+            return data
+
+        # å†™å…¥æ‰€æœ‰å¯ç”¨çš„é›†åˆ
+        for mem_type, settings in self.collections.items():
+            collection = settings["collection"]
+            config = settings["config"]
+            if not collection:
+                self.logger.warning(f"{mem_type.upper()} collection not initialized")
+                continue
+
+            try:
+                self.state.store(
+                    collection=collection,
+                    documents=processed_data,
+                    collection_config=config
+                )
+                self.logger.debug(f"Stored {len(processed_data)} chunks to {mem_type.upper()}")
+            except Exception as e:
+                self.logger.error(f"Failed to store to {mem_type.upper()}: {str(e)}")
+
+        return data  # è¿”å›åŸå§‹æ•°æ®
\ No newline at end of file
diff --git a/sage_common_funs/utils/__init__.py b/sage_common_funs/utils/__init__.py
new file mode 100644
index 0000000..f4caa39
--- /dev/null
+++ b/sage_common_funs/utils/__init__.py
@@ -0,0 +1,6 @@
+from .generator_model import OpenAIClient,HFGenerator
+
+__all__ =[ 
+    "OpenAIClient",
+    "HFGenerator"
+]
\ No newline at end of file
diff --git a/sage_utils/clients/generator_model.py b/sage_common_funs/utils/generator_model.py
similarity index 72%
rename from sage_utils/clients/generator_model.py
rename to sage_common_funs/utils/generator_model.py
index df27dc3..acb0ec4 100644
--- a/sage_utils/clients/generator_model.py
+++ b/sage_common_funs/utils/generator_model.py
@@ -1,5 +1,22 @@
-from sage_utils.clients.hf import HFGenerator
-from sage_utils.clients.openaiclient import OpenAIClient
+from .openaiclient import OpenAIClient
+from .hf import HFGenerator
+
+# class GeneratorModel:
+#     def __init__(self, method: str, model_name: str, **kwargs):
+#         if method=="openai":
+#             self.model=OpenAIClient(model_name,**kwargs)
+#         elif method=="hf":
+#             self.model=HFGenerator(model_name,**kwargs)
+#         else:
+#             raise ValueError("this method isn't supported")
+        
+#     def generate(self, prompt: str, **kwargs) :
+
+#         response=self.model.generate(prompt, **kwargs)
+
+#         return response
+
+
 class GeneratorFactory:
     @staticmethod
     def create_generator(method: str, model_name: str, **kwargs):
diff --git a/sage_utils/clients/hf.py b/sage_common_funs/utils/hf.py
similarity index 100%
rename from sage_utils/clients/hf.py
rename to sage_common_funs/utils/hf.py
diff --git a/sage_utils/clients/openaiclient.py b/sage_common_funs/utils/openaiclient.py
similarity index 100%
rename from sage_utils/clients/openaiclient.py
rename to sage_common_funs/utils/openaiclient.py
diff --git a/sage_core/README.md b/sage_core/README.md
index 8a72981..1e4515d 100644
--- a/sage_core/README.md
+++ b/sage_core/README.md
@@ -45,7 +45,7 @@ Sage Core æ˜¯ Sage æ¡†æ¶çš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£æ•°æ®æµç®¡é“çš„å®šä¹‰ã€ç¼–
 
 #### ç¼–è¯‘å™¨ç³»ç»Ÿ (`sage_core/core/compiler/`)
 - **`compiler.py`**: ç®¡é“ç¼–è¯‘å™¨
-  - `ExecutionGraph`: å°†é€»è¾‘ç®¡é“ç¼–è¯‘ä¸ºç‰©ç†æ‰§è¡Œå›¾
+  - `Compiler`: å°†é€»è¾‘ç®¡é“ç¼–è¯‘ä¸ºç‰©ç†æ‰§è¡Œå›¾
   - `GraphNode`: è¡¨ç¤ºæ‰§è¡Œå›¾ä¸­çš„èŠ‚ç‚¹
   - `GraphEdge`: è¡¨ç¤ºèŠ‚ç‚¹é—´çš„æ•°æ®è¿æ¥
   - æ”¯æŒå¹¶è¡Œåº¦å±•å¼€å’Œè¿æ¥ä¼˜åŒ–
diff --git a/sage_core/__init__.py b/sage_core/__init__.py
index e69de29..016ef8e 100644
--- a/sage_core/__init__.py
+++ b/sage_core/__init__.py
@@ -0,0 +1,10 @@
+# from .api import env, model, memory, operator, query
+
+# # åªæš´éœ²å››ä¸ªå­æ¨¡å—ï¼Œä¿æŒæ¸…æ™°çš„æ¨¡å—è¾¹ç•Œ
+# memory = api.memory
+# model = api.model
+# operator = api.operator
+# env = api.env
+# query = api.query
+
+# __all__ = ["memory", "model", "operator", "env", "query"]
\ No newline at end of file
diff --git a/sage_core/api/connected_streams.py b/sage_core/api/connected_streams.py
index 9cbaa6e..201102c 100644
--- a/sage_core/api/connected_streams.py
+++ b/sage_core/api/connected_streams.py
@@ -10,7 +10,7 @@ from sage_core.function.comap_function import BaseCoMapFunction
 from sage_core.function.join_function import BaseJoinFunction
 if TYPE_CHECKING:
     from .datastream import DataStream
-    from sage_core.environment.base_environment import BaseEnvironment
+    from .env import BaseEnvironment
 
 class ConnectedStreams:
     """è¡¨ç¤ºå¤šä¸ªtransformationè¿æ¥åçš„æµç»“æœ"""
@@ -140,6 +140,21 @@ class ConnectedStreams:
                 f"CoMap operations require CoMap function with mapN methods."
             )
         
+        # Check if function has is_comap property (should be True for CoMap functions)
+        try:
+            # Create a temporary instance to check is_comap property
+            temp_instance = function()
+            if not hasattr(temp_instance, 'is_comap') or not temp_instance.is_comap:
+                raise TypeError(
+                    f"Function {function.__name__} must have is_comap=True property. "
+                    f"Ensure your function properly inherits from BaseCoMapFunction."
+                )
+        except Exception as e:
+            raise TypeError(
+                f"Failed to validate CoMap function {function.__name__}: {e}. "
+                f"Ensure the function can be instantiated and has is_comap=True."
+            )
+        
         # Validate that function supports the required number of input streams
         required_methods = [f'map{i}' for i in range(input_stream_count)]
         missing_methods = []
@@ -514,5 +529,5 @@ class ConnectedStreams:
         for input_index, upstream_trans in enumerate(self.transformations):
             tr.add_upstream(upstream_trans, input_index=input_index)
 
-        self._environment.pipeline.append(tr)
+        self._environment._pipeline.append(tr)
         return DataStream(self._environment, tr)
\ No newline at end of file
diff --git a/sage_core/api/datastream.py b/sage_core/api/datastream.py
index ffcd19c..3d2946c 100644
--- a/sage_core/api/datastream.py
+++ b/sage_core/api/datastream.py
@@ -12,7 +12,7 @@ from sage_core.function.lambda_function import wrap_lambda, detect_lambda_type
 from .connected_streams import ConnectedStreams
 from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment
+    from .env import BaseEnvironment
     from .datastream import DataStream
 
 T = TypeVar("T")
@@ -20,8 +20,14 @@ T = TypeVar("T")
 
 class DataStream(Generic[T]):
     """è¡¨ç¤ºå•ä¸ªtransformationç”Ÿæˆçš„æµç»“æœ"""
-    def __init__(self, env:'BaseEnvironment', transformation: 'BaseTransformation'):
-        self.logger = CustomLogger()
+    def __init__(self, env:BaseEnvironment, transformation: BaseTransformation):
+        self.logger = CustomLogger(
+            filename=f"Datastream_{transformation.function_class.__name__}",
+            env_name = env.name,
+            console_output="WARNING",
+            file_output=True,
+            global_output = "DEBUG",
+        )
         self._environment = env
         self.transformation = transformation
         self._type_param = self._resolve_type_param()
@@ -191,7 +197,7 @@ class DataStream(Generic[T]):
         # è¿æ¥åˆ°è¾“å…¥ç´¢å¼•0ï¼ˆå•è¾“å…¥æƒ…å†µï¼‰
         tr.add_upstream(self.transformation, input_index=0)
         
-        self._environment.pipeline.append(tr)
+        self._environment._pipeline.append(tr)
         return DataStream(self._environment, tr)
     
     def _resolve_type_param(self):
diff --git a/sage_core/api/env.py b/sage_core/api/env.py
new file mode 100644
index 0000000..f4ac4ef
--- /dev/null
+++ b/sage_core/api/env.py
@@ -0,0 +1,305 @@
+from __future__ import annotations
+
+import time
+from typing import Type, Union, Any, List
+from enum import Enum
+import sage_memory.api
+from sage_core.function.base_function import BaseFunction
+from sage_core.api.datastream import DataStream
+from sage_core.transformation.base_transformation import BaseTransformation
+from sage_core.transformation.source_transformation import SourceTransformation
+from sage_core.transformation.future_transformation import FutureTransformation
+from sage_utils.custom_logger import CustomLogger
+from sage_utils.name_server import get_name
+from sage_core.function.lambda_function import wrap_lambda
+
+
+
+class BaseEnvironment:
+
+    def __init__(self, name: str, config: dict | None, *, platform: str = "local"):
+        self.name = get_name(name)
+        self.logger = CustomLogger(
+            filename=f"Environment_{name}",
+            env_name = name,
+            console_output="WARNING",
+            file_output=True,
+            global_output = "DEBUG",
+        )
+        
+
+        self.config: dict = dict(config or {})
+        self.platform:str = platform
+        # ç”¨äºæ”¶é›†æ‰€æœ‰ BaseTransformationï¼Œä¾› Compiler æ„å»º DAG
+        self._pipeline: List[BaseTransformation] = []
+        self._filled_futures: dict = {}  # è®°å½•å·²å¡«å……çš„future streamä¿¡æ¯ï¼šname -> {future_transformation, actual_transformation, filled_at}
+        self.runtime_context = dict  # éœ€è¦åœ¨compileré‡Œé¢å®ä¾‹åŒ–ã€‚
+        self.memory_collection = None  # ç”¨äºå­˜å‚¨å†…å­˜é›†åˆ
+        self.is_running = False
+
+    def from_kafka_source(self, 
+                         bootstrap_servers: str,
+                         topic: str,
+                         group_id: str,
+                         auto_offset_reset: str = 'latest',
+                         value_deserializer: str = 'json',
+                         buffer_size: int = 10000,
+                         max_poll_records: int = 500,
+                         **kafka_config) -> DataStream:
+        """
+        åˆ›å»ºKafkaæ•°æ®æºï¼Œé‡‡ç”¨Flinkå…¼å®¹çš„æ¶æ„è®¾è®¡
+        
+        Args:
+            bootstrap_servers: Kafkaé›†ç¾¤åœ°å€ (ä¾‹: "localhost:9092")
+            topic: Kafkaä¸»é¢˜åç§°
+            group_id: æ¶ˆè´¹è€…ç»„IDï¼Œç”¨äºoffsetç®¡ç†
+            auto_offset_reset: offseté‡ç½®ç­–ç•¥ ('latest'/'earliest'/'none')
+            value_deserializer: ååºåˆ—åŒ–æ–¹å¼ ('json'/'string'/'bytes'æˆ–è‡ªå®šä¹‰å‡½æ•°)
+            buffer_size: æœ¬åœ°ç¼“å†²åŒºå¤§å°ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±
+            max_poll_records: æ¯æ¬¡pollçš„æœ€å¤§è®°å½•æ•°ï¼Œæ§åˆ¶æ‰¹å¤„ç†å¤§å°
+            **kafka_config: å…¶ä»–Kafka Consumeré…ç½®å‚æ•°
+            
+        Returns:
+            DataStream: å¯ç”¨äºæ„å»ºå¤„ç†pipelineçš„æ•°æ®æµ
+            
+        Example:
+            # åŸºæœ¬ä½¿ç”¨
+            kafka_stream = env.from_kafka_source(
+                bootstrap_servers="localhost:9092",
+                topic="user_events", 
+                group_id="sage_consumer"
+            )
+            
+            # é«˜çº§é…ç½®
+            kafka_stream = env.from_kafka_source(
+                bootstrap_servers="kafka1:9092,kafka2:9092",
+                topic="events",
+                group_id="sage_app",
+                auto_offset_reset="earliest",
+                buffer_size=20000,
+                max_poll_records=1000,
+                session_timeout_ms=30000,
+                security_protocol="SSL"
+            )
+            
+            # æ„å»ºå¤„ç†pipeline
+            result = (kafka_stream
+                     .map(ProcessEventFunction)
+                     .filter(FilterFunction)
+                     .sink(OutputSinkFunction))
+        """
+        from sage_core.function.kafka_source import KafkaSourceFunction
+        
+        # åˆ›å»ºKafka Source Function
+        transformation = SourceTransformation(
+            self,
+            KafkaSourceFunction,
+            bootstrap_servers=bootstrap_servers,
+            topic=topic,
+            group_id=group_id,
+            auto_offset_reset=auto_offset_reset,
+            value_deserializer=value_deserializer,
+            buffer_size=buffer_size,
+            max_poll_records=max_poll_records,
+            **kafka_config
+        )
+        
+        self._pipeline.append(transformation)
+        self.logger.info(f"Kafka source created for topic: {topic}, group: {group_id}")
+        
+        return DataStream(self, transformation)
+
+
+
+    def from_source(self, function: Union[Type[BaseFunction], callable], *args, **kwargs) -> DataStream:
+        if callable(function) and not isinstance(function, type):
+            # è¿™æ˜¯ä¸€ä¸ª lambda å‡½æ•°æˆ–æ™®é€šå‡½æ•°
+            function = wrap_lambda(function, 'flatmap')
+        transformation = SourceTransformation(self, function, *args,**kwargs)
+        
+        self._pipeline.append(transformation)
+        return DataStream(self, transformation)
+
+    def from_future(self, name: str) -> DataStream:
+        """
+        åˆ›å»ºä¸€ä¸ªfuture streamå ä½ç¬¦ï¼Œç”¨äºå»ºç«‹åé¦ˆè¾¹ã€‚
+        
+        Args:
+            name: future streamçš„åç§°ï¼Œç”¨äºæ ‡è¯†å’Œè°ƒè¯•
+            
+        Returns:
+            DataStream: åŒ…å«FutureTransformationçš„æ•°æ®æµ
+            
+        Example:
+            future_stream = env.from_future("feedback_loop")
+            # ä½¿ç”¨future_streamå‚ä¸pipelineæ„å»º
+            result = source.connect(future_stream).comap(CombineFunction)
+            # æœ€åå¡«å……future
+            result.fill_future(future_stream)
+        """
+        transformation = FutureTransformation(self, name)
+        self._pipeline.append(transformation)
+        return DataStream(self, transformation)
+
+    # TODO: add a new type of source with handler returned.
+    def create_source(self):
+        pass
+
+    def submit(self, name="example_pipeline"):
+        # self.debug_print_pipeline()
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        engine.submit_env(self)
+        # time.sleep(10) # ç­‰å¾…ç®¡é“å¯åŠ¨
+        while (self.initialized() is False):
+            time.sleep(1)
+
+    def run_once(self, node:str = None):
+        """
+        è¿è¡Œä¸€æ¬¡ç®¡é“ï¼Œé€‚ç”¨äºæµ‹è¯•æˆ–è°ƒè¯•ã€‚
+        """
+        if(self.is_running):
+            self.logger.warning("Pipeline is already running. ")
+            return
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        engine.run_once(self)
+        # time.sleep(10) # ç­‰å¾…ç®¡é“å¯åŠ¨
+
+    def run_streaming(self, node: str = None):
+        """
+        è¿è¡Œç®¡é“ï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚
+        """
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        engine.run_streaming(self)
+        # time.sleep(10) # ç­‰å¾…ç®¡é“å¯åŠ¨
+
+    def stop(self):
+        """
+        åœæ­¢ç®¡é“è¿è¡Œã€‚
+        """
+        self.logger.info("Stopping pipeline...")
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        engine.stop_pipeline(self)
+        # self.close()
+
+    def close(self):
+        """
+        å…³é—­ç®¡é“è¿è¡Œã€‚
+        """
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        # 1) åœæ­¢æœ¬ç¯å¢ƒå¯¹åº”çš„ DAG æ‰§è¡Œ
+        engine.stop_pipeline(self)
+        # 2) å…³é—­è¯¥ç¯å¢ƒåœ¨ Engine ä¸­çš„å¼•ç”¨ï¼Œå¹¶åœ¨æ— å‰©ä½™ç¯å¢ƒæ—¶å½»åº• shutdown Engine
+        engine.close_pipeline(self)
+        # 3) æ¸…ç†è‡ªèº«å¼•ç”¨ï¼Œä»¥æ‰“ç ´å¾ªç¯é“¾
+        self._pipeline.clear()
+
+
+
+    def set_memory(self, config = None):
+        self.memory_collection = sage_memory.api.get_memory(config=config, remote=(self.platform != "local"), env_name=self.name)
+
+    def set_memory_collection(self, collection):
+
+        self.memory_collection = collection 
+        
+    # TODO: å†™ä¸€ä¸ªåˆ¤æ–­Env æ˜¯å¦å·²ç»å®Œå…¨åˆå§‹åŒ–å¹¶å¼€å§‹æ‰§è¡Œçš„å‡½æ•°
+    def initialized(self):
+        pass
+
+    def get_filled_futures(self) -> dict:
+        """
+        è·å–æ‰€æœ‰å·²å¡«å……çš„future streamä¿¡æ¯
+        
+        Returns:
+            dict: å·²å¡«å……çš„future streamä¿¡æ¯ï¼Œæ ¼å¼ä¸º:
+                {
+                    'future_name': {
+                        'future_transformation': FutureTransformation,
+                        'actual_transformation': BaseTransformation,
+                        'filled_at': timestamp
+                    }
+                }
+        """
+        return self._filled_futures.copy()
+    
+    def has_unfilled_futures(self) -> bool:
+        """
+        æ£€æŸ¥pipelineä¸­æ˜¯å¦è¿˜æœ‰æœªå¡«å……çš„future streams
+        
+        Returns:
+            bool: å¦‚æœå­˜åœ¨æœªå¡«å……çš„future streamsè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
+        """
+        from sage_core.transformation.future_transformation import FutureTransformation
+        for transformation in self._pipeline:
+            if isinstance(transformation, FutureTransformation) and not transformation.filled:
+                return True
+        return False
+    
+    def validate_pipeline_for_compilation(self) -> None:
+        """
+        éªŒè¯pipelineæ˜¯å¦å¯ä»¥è¿›è¡Œç¼–è¯‘
+        æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœªå¡«å……çš„future streams
+        
+        Raises:
+            RuntimeError: å¦‚æœå­˜åœ¨æœªå¡«å……çš„future streams
+        """
+        from sage_core.transformation.future_transformation import FutureTransformation
+        unfilled_futures = []
+        
+        for transformation in self._pipeline:
+            if isinstance(transformation, FutureTransformation) and not transformation.filled:
+                unfilled_futures.append(transformation.future_name)
+        
+        if unfilled_futures:
+            raise RuntimeError(
+                f"Cannot compile pipeline with unfilled future streams: {unfilled_futures}. "
+                f"Please fill all future streams using fill_future() before compilation."
+            )
+
+    ########################################################
+    #                auxiliary methods                     #
+    ########################################################
+
+    def _append(self, transformation: BaseTransformation):
+        """å°† BaseTransformation æ·»åŠ åˆ°ç®¡é“ä¸­ï¼ˆCompiler ä¼šä½¿ç”¨ï¼‰ã€‚"""
+        self.pipeline.append(transformation)
+        return DataStream(self, transformation)
+    
+    @property
+    def pipeline(self) -> List[BaseTransformation]:  # noqa: D401
+        """è¿”å› BaseTransformation åˆ—è¡¨ï¼ˆCompiler ä¼šä½¿ç”¨ï¼‰ã€‚"""
+        return self._pipeline
+
+
+class LocalEnvironment(BaseEnvironment):
+    """
+    æœ¬åœ°æ‰§è¡Œç¯å¢ƒï¼ˆä¸ä½¿ç”¨ Rayï¼‰ï¼Œç”¨äºå¼€å‘è°ƒè¯•æˆ–å°è§„æ¨¡æµ‹è¯•ã€‚
+    """
+
+    def __init__(self, name: str = "local_environment", config: dict | None = None):
+        super().__init__(name, config, platform="local")
+
+
+class RemoteEnvironment(BaseEnvironment):
+    """
+    åˆ†å¸ƒå¼æ‰§è¡Œç¯å¢ƒï¼ˆRayï¼‰ï¼Œç”¨äºç”Ÿäº§æˆ–å¤§è§„æ¨¡éƒ¨ç½²ã€‚
+    """
+
+    def __init__(self, name: str = "remote_environment", config: dict | None = None):
+        super().__init__(name, config, platform="remote")
+
+
+class DevEnvironment(BaseEnvironment):
+    """
+    æ··åˆæ‰§è¡Œç¯å¢ƒï¼Œå¯æ ¹æ®é…ç½®åŠ¨æ€é€‰æ‹©æœ¬åœ°æˆ– Rayã€‚
+    config ä¸­å¯åŒ…å« 'use_ray': bool æ¥åˆ‡æ¢è¿è¡Œæ—¶ã€‚
+    """
+
+    def __init__(self, name: str = "dev_environment", config: dict | None = None):
+        cfg = dict(config or {})
+        super().__init__(name, cfg, platform="hybrid")
diff --git a/sage_core/api/local_environment.py b/sage_core/api/local_environment.py
deleted file mode 100644
index 30afbf9..0000000
--- a/sage_core/api/local_environment.py
+++ /dev/null
@@ -1,38 +0,0 @@
-from __future__ import annotations
-
-from typing import Optional, TYPE_CHECKING
-from sage_core.environment.base_environment import BaseEnvironment
-from sage_utils.actor_wrapper import ActorWrapper
-if TYPE_CHECKING:
-    from sage_jobmanager.job_manager import JobManager
-
-class LocalEnvironment(BaseEnvironment):
-    """æœ¬åœ°ç¯å¢ƒï¼Œç›´æ¥ä½¿ç”¨æœ¬åœ°JobManagerå®ä¾‹"""
-
-    def __init__(self, name: str, config: dict | None = None):
-        super().__init__(name, config, platform="local")
-        
-        # æœ¬åœ°ç¯å¢ƒä¸éœ€è¦å®¢æˆ·ç«¯
-        self._engine_client = None
-
-    def submit(self):
-        # åºåˆ—åŒ–ç¯å¢ƒ
-        env_uuid = self.jobmanager.submit_job(self)
-        
-        if env_uuid:
-            self.env_uuid = env_uuid
-            self.logger.info(f"Environment submitted with UUID: {self.env_uuid}")
-        else:
-            raise RuntimeError("Failed to submit environment: no UUID returned")
-
-    @property
-    def jobmanager(self) -> 'JobManager':
-        """ç›´æ¥è¿”å›JobManagerçš„å•ä¾‹å®ä¾‹"""
-        if self._jobmanager is None:
-            from sage_jobmanager.job_manager import JobManager
-            # è·å–JobManagerå•ä¾‹å®ä¾‹
-            jobmanager_instance = JobManager()
-            # æœ¬åœ°ç¯å¢ƒç›´æ¥è¿”å›JobManagerå®ä¾‹ï¼Œä¸ä½¿ç”¨ActorWrapper
-            self._jobmanager = jobmanager_instance
-            
-        return self._jobmanager
\ No newline at end of file
diff --git a/sage_core/api/remote_environment.py b/sage_core/api/remote_environment.py
deleted file mode 100644
index 63e8549..0000000
--- a/sage_core/api/remote_environment.py
+++ /dev/null
@@ -1,119 +0,0 @@
-from __future__ import annotations
-
-from typing import Optional, TYPE_CHECKING
-from sage_core.environment.base_environment import BaseEnvironment
-from sage_core.jobmanager_client import JobManagerClient
-from sage_utils.actor_wrapper import ActorWrapper
-if TYPE_CHECKING:
-    from sage_jobmanager.job_manager import JobManager
-
-class RemoteEnvironment(BaseEnvironment):
-    """è¿œç¨‹ç¯å¢ƒï¼Œé€šè¿‡å®¢æˆ·ç«¯è¿æ¥è¿œç¨‹JobManager"""
-
-    def __init__(self, name: str, config: dict | None = None, host: str = "127.0.0.1", port: int = 19001):
-        super().__init__(name, config, platform="remote")
-        
-        # è®¾ç½®è¿œç¨‹è¿æ¥é…ç½®
-        self.daemon_host = host
-        self.daemon_port = port
-        
-        # æ›´æ–°é…ç½®
-        self.config.update({
-            "engine_host": self.daemon_host,
-            "engine_port": self.daemon_port
-        })
-
-    @property
-    def client(self) -> JobManagerClient:
-        """è·å–è¿œç¨‹JobManagerå®¢æˆ·ç«¯"""
-        if self._engine_client is None:
-            self._engine_client = JobManagerClient(
-                host=self.daemon_host, 
-                port=self.daemon_port
-            )
-        return self._engine_client
-
-    @property
-    def jobmanager(self) -> 'JobManager': # æ˜¯actorwrapperæ— æ„ŸåŒ…ç€çš„
-        """é€šè¿‡å®¢æˆ·ç«¯è·å–è¿œç¨‹JobManagerå¥æŸ„"""
-        if self._jobmanager is None:
-            self._jobmanager = self.client.get_actor_handle()
-        return self._jobmanager
-
-    def submit(self):
-        # åºåˆ—åŒ–ç¯å¢ƒ
-        from sage_utils.serialization.dill_serializer import trim_object_for_ray
-        serialized_env = trim_object_for_ray(self)
-        
-        # é€šè¿‡jobmanagerå±æ€§æäº¤job
-        env_uuid = self.jobmanager.submit_job(serialized_env)
-        
-        if env_uuid:
-            self.env_uuid = env_uuid
-            self.logger.info(f"Environment submitted with UUID: {self.env_uuid}")
-        else:
-            raise RuntimeError("Failed to submit environment: no UUID returned")
-
-    def stop(self):
-        """åœæ­¢è¿œç¨‹ç¯å¢ƒ"""
-        if not self.env_uuid:
-            self.logger.warning("Remote environment not submitted, nothing to stop")
-            return
-        
-        self.logger.info("Stopping remote pipeline...")
-        
-        try:
-            response = self.jobmanager.pause_job(self.env_uuid)
-            
-            if response.get("status") == "stopped":
-                self.is_running = False
-                self.logger.info("Remote pipeline stopped successfully")
-            else:
-                self.logger.warning(f"Failed to stop remote pipeline: {response.get('message')}")
-                
-        except Exception as e:
-            self.logger.error(f"Error stopping remote pipeline: {e}")
-
-    def close(self):
-        """å…³é—­è¿œç¨‹ç¯å¢ƒ"""
-        if not self.env_uuid:
-            self.logger.warning("Remote environment not submitted, nothing to close")
-            return
-        
-        self.logger.info("Closing remote environment...")
-        
-        try:
-            response = self.jobmanager.pause_job(self.env_uuid)
-            
-            if response.get("status") == "stopped":
-                self.logger.info("Remote environment closed successfully")
-            else:
-                self.logger.warning(f"Failed to close remote environment: {response.get('message')}")
-                
-        except Exception as e:
-            self.logger.error(f"Error closing remote environment: {e}")
-        finally:
-            # æ¸…ç†æœ¬åœ°èµ„æº
-            self.is_running = False
-            self.env_uuid = None
-            
-            # æ¸…ç†ç®¡é“
-            self.pipeline.clear()
-
-    def health_check(self):
-        """æ£€æŸ¥è¿œç¨‹JobManagerå¥åº·çŠ¶æ€"""
-        try:
-            response = self.client.health_check()
-            return response
-        except Exception as e:
-            self.logger.error(f"Health check failed: {e}")
-            return {"status": "error", "message": str(e)}
-
-    def get_remote_info(self):
-        """è·å–è¿œç¨‹JobManagerä¿¡æ¯"""
-        try:
-            response = self.client.get_actor_info()
-            return response
-        except Exception as e:
-            self.logger.error(f"Failed to get remote info: {e}")
-            return {"status": "error", "message": str(e)}
\ No newline at end of file
diff --git a/sage_core/engine.py b/sage_core/engine.py
new file mode 100644
index 0000000..03ceebd
--- /dev/null
+++ b/sage_core/engine.py
@@ -0,0 +1,138 @@
+from typing import TYPE_CHECKING
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.local_thread_pool import LocalThreadPool
+from sage_runtime.mixed_dag import MixedDAG
+import threading
+if TYPE_CHECKING:
+    from sage_runtime.compiler import Compiler
+    from sage_core.api.env import BaseEnvironment
+
+
+class Engine:
+    _instance = None
+    _lock = threading.Lock()
+
+    def __init__(self):
+
+        # ç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡
+        if hasattr(self, "_initialized"):
+            return
+        self._initialized = True
+        self.graphs: dict[str, 'Compiler'] = {}  # å­˜å‚¨ pipeline åç§°åˆ° SageGraph çš„æ˜ å°„
+        self.env_to_dag: dict[str, 'MixedDAG'] = {}  # å­˜å‚¨nameåˆ°dagçš„æ˜ å°„ï¼Œå…¶ä¸­dagçš„ç±»å‹ä¸ºDAGæˆ–RayDAG
+        # print("Engine initialized")
+        self.logger = CustomLogger(
+            filename=f"SageEngine",
+            console_output="WARNING",
+            file_output=True,
+            global_output="WARNING",
+            name="SageEngine"
+        )
+
+    def __new__(cls):
+        # ç¦æ­¢ç›´æ¥å®ä¾‹åŒ–
+        raise RuntimeError("è¯·é€šè¿‡ get_instance() æ–¹æ³•è·å–å®ä¾‹")
+
+    # ç”¨æ¥è·å–ç±»çš„å”¯ä¸€å®ä¾‹
+    # åŒä¸€ä¸ªè¿›ç¨‹ä¸­åªå­˜åœ¨å”¯ä¸€çš„å®ä¾‹
+    @classmethod
+    def get_instance(cls):
+        # åŒé‡æ£€æŸ¥é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # ç»•è¿‡ __new__ çš„å¼‚å¸¸ï¼Œç›´æ¥åˆ›å»ºå®ä¾‹
+                    instance = super().__new__(cls)
+                    instance.__init__()
+                    cls._instance = instance
+        return cls._instance
+
+    def submit_env(self, env: 'BaseEnvironment'):
+        from sage_runtime.compiler import Compiler
+        # env, graphå’Œdagç”¨çš„éƒ½æ˜¯åŒä¸€ä¸ªåå­—
+        graph = Compiler(env)
+        # graph.debug_print_graph()
+        self.graphs[graph.name] = graph
+        try:
+            self.logger.info(f"Received mixed graph '{graph.name}' with {len(graph.nodes)} nodes")
+            # ç¼–è¯‘å›¾
+            mixed_dag = MixedDAG(graph, env)
+            self.env_to_dag[env.name] = mixed_dag  # å­˜å‚¨ DAG åˆ°å­—å…¸ä¸­
+            mixed_dag.submit()
+            self.logger.info(f"Mixed graph '{graph.name}' submitted to runtime manager.")
+        except Exception as e:
+            self.logger.info(f"Failed to submit graph '{graph.name}': {e}")
+            raise
+
+    def run_once(self, env: 'BaseEnvironment', node: str = None):
+        """
+        æ‰§è¡Œä¸€æ¬¡ç¯å¢ƒçš„ DAG
+        """
+        self.logger.info(f"Executing DAG for environment '{env.name}'")
+        dag = self.env_to_dag.get(env.name)
+        self.logger.debug(f"Found DAG for environment '{env.name}': {dag}")
+        dag.execute_once()
+        self.logger.info(f"DAG for environment '{env.name}' have completed execution.")
+
+    def run_streaming(self, env: 'BaseEnvironment', node: str = None):
+        """
+        æ‰§è¡Œä¸€æ¬¡ç¯å¢ƒçš„ DAG
+        """
+        self.logger.info(f"Executing streaming DAG for environment '{env.name}'")
+        dag = self.env_to_dag.get(env.name)
+        dag.execute_streaming()
+        self.logger.info(f"Streaming DAG for environment '{env.name}' have started.")
+
+    def stop_pipeline(self, env: 'BaseEnvironment'):
+        """
+        åœæ­¢æŒ‡å®šç¯å¢ƒçš„ DAG
+        """
+        self.logger.info(f"Stopping DAG for environment '{env.name}'")
+        dag = self.env_to_dag.get(env.name)
+        if dag:
+            dag.stop()
+            self.logger.info(f"DAG for environment '{env.name}' has been stopped.")
+        else:
+            self.logger.warning(f"No DAG found for environment '{env.name}'")
+
+    def close_pipeline(self, env: 'BaseEnvironment'):
+        """
+        åœæ­¢æŒ‡å®šç¯å¢ƒçš„ DAG
+        """
+        self.logger.info(f"Stopping DAG for environment '{env.name}'")
+        graph = self.graphs.pop(env.name, None)
+        if graph:
+            # graph.destroy()
+            self.logger.info(f"Graph for environment '{env.name}' has been destroyed.")
+        
+        dag = self.env_to_dag.pop(env.name, None)
+        if dag:
+            dag.close()
+            self.logger.info(f"DAG for environment '{env.name}' has been closed.")
+        else:
+            self.logger.warning(f"No DAG found for environment '{env.name}'")
+        # å¦‚æœæ²¡æœ‰å‰©ä½™ç¯å¢ƒï¼Œæ‰§è¡Œå®Œæ•´ shutdown
+        if not self.env_to_dag:
+            self.shutdown()
+
+    def shutdown(self):
+        """
+        å®Œæ•´é‡Šæ”¾ Engine æŒæœ‰çš„æ‰€æœ‰èµ„æºï¼š
+        - åœæ‰ RuntimeManagerï¼ˆçº¿ç¨‹ã€Ray actor ç­‰ï¼‰
+        - åœæ‰å¯èƒ½çš„ TCP/HTTP server
+        - æ¸…ç©º DAG æ˜ å°„ä¸ç¼“å­˜
+        - é‡ç½® Engine å•ä¾‹
+        """
+        self.logger.info("Shutting down Engine and releasing resources")
+        try:
+            local_runtime = LocalThreadPool.get_instance()
+            local_runtime.shutdown()
+        except Exception:
+            self.logger.exception("Error shutting down RuntimeManager:{e}")
+            raise
+
+        self.env_to_dag.clear()
+        self.graphs.clear()
+
+        Engine._instance = None
+        self.logger.info("Engine shutdown complete")
diff --git a/sage_core/environment/README.md b/sage_core/environment/README.md
deleted file mode 100644
index b4cc13b..0000000
--- a/sage_core/environment/README.md
+++ /dev/null
@@ -1,3 +0,0 @@
-Base_environment -> stream/batch -> localstream/localbatch/remotestream/remotebatch
-
-Base_environment -> local/remote -> localstream/localbatch/remotestream/remotebatch
\ No newline at end of file
diff --git a/sage_core/environment/base_environment.py b/sage_core/environment/base_environment.py
deleted file mode 100644
index a20f402..0000000
--- a/sage_core/environment/base_environment.py
+++ /dev/null
@@ -1,291 +0,0 @@
-from __future__ import annotations
-from abc import ABC, abstractmethod
-from datetime import datetime
-import os
-from pathlib import Path
-from typing import List, Optional, TYPE_CHECKING, Type, Union
-from sage_core.function.base_function import BaseFunction
-from sage_core.function.lambda_function import wrap_lambda
-import sage_memory.api
-from sage_core.api.datastream import DataStream
-from sage_core.transformation.base_transformation import BaseTransformation
-from sage_core.transformation.source_transformation import SourceTransformation
-from sage_core.transformation.future_transformation import FutureTransformation
-from sage_utils.custom_logger import CustomLogger
-from sage_jobmanager.utils.name_server import get_name
-from sage_core.jobmanager_client import JobManagerClient
-from sage_utils.actor_wrapper import ActorWrapper
-if TYPE_CHECKING:
-    from sage_jobmanager.job_manager import JobManager
-
-class BaseEnvironment(ABC):
-
-    __state_exclude__ = ["_engine_client", "client", "jobmanager"]
-    # ä¼šè¢«ç»§æ‰¿ï¼Œä½†æ˜¯ä¸ä¼šè¢«è‡ªåŠ¨åˆå¹¶
-
-    def __init__(self, name: str, config: dict | None, *, platform: str = "local"):
-
-        self.name = get_name(name)
-        self.uuid: Optional[str] # ç”±jobmanagerç”Ÿæˆ
-
-        self.config: dict = dict(config or {})
-        self.platform:str = platform
-        # ç”¨äºæ”¶é›†æ‰€æœ‰ BaseTransformationï¼Œä¾› ExecutionGraph æ„å»º DAG
-        self.pipeline: List[BaseTransformation] = []
-        self._filled_futures: dict = {}  # è®°å½•å·²å¡«å……çš„future streamä¿¡æ¯ï¼šname -> {future_transformation, actual_transformation, filled_at}
-        self.runtime_context = dict  # éœ€è¦åœ¨compileré‡Œé¢å®ä¾‹åŒ–ã€‚
-        self.memory_collection = None  # ç”¨äºå­˜å‚¨å†…å­˜é›†åˆ
-        self.is_running = False
-        self.env_base_dir: Optional[str] = None  # ç¯å¢ƒåŸºç¡€ç›®å½•ï¼Œç”¨äºå­˜å‚¨æ—¥å¿—å’Œå…¶ä»–æ–‡ä»¶
-        # JobManager ç›¸å…³
-        self._jobmanager: Optional[ActorWrapper] = None
-        
-        # Engine å®¢æˆ·ç«¯ç›¸å…³
-        self._engine_client: Optional[JobManagerClient] = None
-        self.env_uuid: Optional[str] = None
-
-    ########################################################
-    #                  user interface                      #
-    ########################################################
-
-    def set_memory(self, config = None):
-        self.memory_collection = sage_memory.api.get_memory(config=config, remote=(self.platform != "local"), env_name=self.name)
-
-
-
-    def from_kafka_source(self, 
-                         bootstrap_servers: str,
-                         topic: str,
-                         group_id: str,
-                         auto_offset_reset: str = 'latest',
-                         value_deserializer: str = 'json',
-                         buffer_size: int = 10000,
-                         max_poll_records: int = 500,
-                         **kafka_config) -> DataStream:
-        """
-        åˆ›å»ºKafkaæ•°æ®æºï¼Œé‡‡ç”¨Flinkå…¼å®¹çš„æ¶æ„è®¾è®¡
-        
-        Args:
-            bootstrap_servers: Kafkaé›†ç¾¤åœ°å€ (ä¾‹: "localhost:9092")
-            topic: Kafkaä¸»é¢˜åç§°
-            group_id: æ¶ˆè´¹è€…ç»„IDï¼Œç”¨äºoffsetç®¡ç†
-            auto_offset_reset: offseté‡ç½®ç­–ç•¥ ('latest'/'earliest'/'none')
-            value_deserializer: ååºåˆ—åŒ–æ–¹å¼ ('json'/'string'/'bytes'æˆ–è‡ªå®šä¹‰å‡½æ•°)
-            buffer_size: æœ¬åœ°ç¼“å†²åŒºå¤§å°ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±
-            max_poll_records: æ¯æ¬¡pollçš„æœ€å¤§è®°å½•æ•°ï¼Œæ§åˆ¶æ‰¹å¤„ç†å¤§å°
-            **kafka_config: å…¶ä»–Kafka Consumeré…ç½®å‚æ•°
-            
-        Returns:
-            DataStream: å¯ç”¨äºæ„å»ºå¤„ç†pipelineçš„æ•°æ®æµ
-            
-        Example:
-            # åŸºæœ¬ä½¿ç”¨
-            kafka_stream = env.from_kafka_source(
-                bootstrap_servers="localhost:9092",
-                topic="user_events", 
-                group_id="sage_consumer"
-            )
-            
-            # é«˜çº§é…ç½®
-            kafka_stream = env.from_kafka_source(
-                bootstrap_servers="kafka1:9092,kafka2:9092",
-                topic="events",
-                group_id="sage_app",
-                auto_offset_reset="earliest",
-                buffer_size=20000,
-                max_poll_records=1000,
-                session_timeout_ms=30000,
-                security_protocol="SSL"
-            )
-            
-            # æ„å»ºå¤„ç†pipeline
-            result = (kafka_stream
-                     .map(ProcessEventFunction)
-                     .filter(FilterFunction)
-                     .sink(OutputSinkFunction))
-        """
-        from sage_core.function.kafka_source import KafkaSourceFunction
-        
-        # åˆ›å»ºKafka Source Function
-        transformation = SourceTransformation(
-            self,
-            KafkaSourceFunction,
-            bootstrap_servers=bootstrap_servers,
-            topic=topic,
-            group_id=group_id,
-            auto_offset_reset=auto_offset_reset,
-            value_deserializer=value_deserializer,
-            buffer_size=buffer_size,
-            max_poll_records=max_poll_records,
-            **kafka_config
-        )
-        
-        self.pipeline.append(transformation)
-        self.logger.info(f"Kafka source created for topic: {topic}, group: {group_id}")
-        
-        return DataStream(self, transformation)
-
-    def from_source(self, function: Union[Type[BaseFunction], callable], *args, **kwargs) -> DataStream:
-        if callable(function) and not isinstance(function, type):
-            # è¿™æ˜¯ä¸€ä¸ª lambda å‡½æ•°æˆ–æ™®é€šå‡½æ•°
-            function = wrap_lambda(function, 'flatmap')
-        transformation = SourceTransformation(self, function, *args, **kwargs)
-
-        self.pipeline.append(transformation)
-        return DataStream(self, transformation)
-
-
-
-    def from_collection(self, function: Union[Type[BaseFunction], callable], *args, **kwargs) -> DataStream:
-        if callable(function) and not isinstance(function, type):
-            # è¿™æ˜¯ä¸€ä¸ª lambda å‡½æ•°æˆ–æ™®é€šå‡½æ•°
-            function = wrap_lambda(function, 'flatmap')
-        transformation = SourceTransformation(self, function, *args,
-                                              **kwargs)  # TODO: add a new transformation å»å‘Šè¯‰engineè¿™ä¸ªinput sourceæ˜¯æœ‰ç•Œçš„ï¼Œå½“æ‰§è¡Œå®Œæ¯•ä¹‹åï¼Œä¼šå‘é€ä¸€ä¸ªendofinputä¿¡å·æ¥åœæ­¢æ‰€æœ‰è¿›ç¨‹ã€‚
-
-        self.pipeline.append(transformation)
-        return DataStream(self, transformation)
-
-
-    def from_future(self, name: str) -> DataStream:
-        """
-        åˆ›å»ºä¸€ä¸ªfuture streamå ä½ç¬¦ï¼Œç”¨äºå»ºç«‹åé¦ˆè¾¹ã€‚
-        
-        Args:
-            name: future streamçš„åç§°ï¼Œç”¨äºæ ‡è¯†å’Œè°ƒè¯•
-            
-        Returns:
-            DataStream: åŒ…å«FutureTransformationçš„æ•°æ®æµ
-            
-        Example:
-            future_stream = env.from_future("feedback_loop")
-            # ä½¿ç”¨future_streamå‚ä¸pipelineæ„å»º
-            result = source.connect(future_stream).comap(CombineFunction)
-            # æœ€åå¡«å……future
-            result.fill_future(future_stream)
-        """
-        transformation = FutureTransformation(self, name)
-        self.pipeline.append(transformation)
-        return DataStream(self, transformation)
-
-    ########################################################
-    #                jobmanager interface                  #
-    ########################################################
-    @abstractmethod
-    def submit(self):
-        pass
-        # åºåˆ—åŒ–ç¯å¢ƒ
-        from sage_utils.serialization.dill_serializer import serialize_object
-        serialized_env = serialize_object(self)
-        
-        # é€šè¿‡jobmanagerå±æ€§æäº¤job
-        env_uuid = self.jobmanager.submit_job(serialized_env)
-        
-        if env_uuid:
-            self.env_uuid = env_uuid
-            self.logger.info(f"Environment submitted with UUID: {self.env_uuid}")
-        else:
-            raise RuntimeError("Failed to submit environment: no UUID returned")
-
-    def stop(self):
-        """åœæ­¢ç®¡é“è¿è¡Œ"""
-        if not self.env_uuid:
-            self.logger.warning("Environment not submitted, nothing to stop")
-            return
-        
-        self.logger.info("Stopping pipeline...")
-        
-        try:
-            response = self.jobmanager.pause_job(self.env_uuid)
-            
-            if response.get("status") == "success":
-                self.is_running = False
-                self.logger.info("Pipeline stopped successfully")
-            else:
-                self.logger.warning(f"Failed to stop pipeline: {response.get('message')}")
-        except Exception as e:
-            self.logger.error(f"Error stopping pipeline: {e}")
-
-    def close(self):
-        """å…³é—­ç®¡é“è¿è¡Œ"""
-        if not self.env_uuid:
-            self.logger.warning("Environment not submitted, nothing to close")
-            return
-        
-        self.logger.info("Closing environment...")
-        
-        try:
-            response = self.jobmanager.pause_job(self.env_uuid)
-            
-            if response.get("status") == "success":
-                self.logger.info("Environment closed successfully")
-            else:
-                self.logger.warning(f"Failed to close environment: {response.get('message')}")
-                
-        except Exception as e:
-            self.logger.error(f"Error closing environment: {e}")
-        finally:
-            # æ¸…ç†æœ¬åœ°èµ„æº
-            self.is_running = False
-            self.env_uuid = None
-            
-            # æ¸…ç†ç®¡é“
-            self.pipeline.clear()
-
-    ########################################################
-    #                properties                            #
-    ########################################################
-
-    @property
-    def logger(self):
-        if not hasattr(self, "_logger"):
-            self._logger = CustomLogger()
-        return self._logger
-
-    @property
-    def client(self)-> JobManagerClient:
-        if self._engine_client is None:
-            # ä»é…ç½®ä¸­è·å– Engine åœ°å€ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
-            daemon_host = self.config.get("engine_host", "127.0.0.1")
-            daemon_port = self.config.get("engine_port", 19000)
-            
-            self._engine_client = JobManagerClient(host=daemon_host, port=daemon_port)
-            
-        return self._engine_client
-
-
-    @property
-    @abstractmethod
-    def jobmanager(self) -> 'JobManager':
-        return
-        # """è·å–JobManagerå¥æŸ„ï¼Œé€šè¿‡ActorWrapperå°è£…ä»¥æä¾›é€æ˜è°ƒç”¨"""
-        # if self._jobmanager is None:
-        #     self._jobmanager = self.client.get_actor_handle()
-        # return self._jobmanager
-
-
-
-    ########################################################
-    #                auxiliary methods                     #
-    ########################################################
-
-    def _append(self, transformation: BaseTransformation):
-        """å°† BaseTransformation æ·»åŠ åˆ°ç®¡é“ä¸­ï¼ˆCompiler ä¼šä½¿ç”¨ï¼‰ã€‚"""
-        self.pipeline.append(transformation)
-        return DataStream(self, transformation)
-
-    def setup_logging_system(self, log_base_dir: str): 
-        # this method is called by jobmanager when receiving the job, not the user
-        self.session_timestamp = datetime.now()
-        self.session_id = self.session_timestamp.strftime("%Y%m%d_%H%M%S")
-        # self.log_base_dir = log_base_dir
-        self.env_base_dir = os.path.join(log_base_dir, f"env_{self.name}_{self.session_id}")
-        Path(self.env_base_dir).mkdir(parents=True, exist_ok=True)
-
-        self._logger = CustomLogger([
-                ("console", "INFO"),  # æ§åˆ¶å°æ˜¾ç¤ºé‡è¦ä¿¡æ¯
-                (os.path.join(self.env_base_dir, "Environment.log"), "DEBUG"),  # è¯¦ç»†æ—¥å¿—
-                (os.path.join(self.env_base_dir, "Error.log"), "ERROR")  # é”™è¯¯æ—¥å¿—
-            ],
-            name = f"Environment_{self.name}",
-        )
\ No newline at end of file
diff --git a/sage_core/function/__init__.py b/sage_core/function/__init__.py
index ecdf37e..d152b7f 100644
--- a/sage_core/function/__init__.py
+++ b/sage_core/function/__init__.py
@@ -1,19 +1,19 @@
 
 
-# from .base_function import BaseFunction
-# from .map_function import MapFunction
-# from .filter_function import FilterFunction
-# from .flatmap_function import FlatMapFunction
-# # from .lambda_function import LambdaFunction
-# from .sink_function import SinkFunction 
-# from .source_function import SourceFunction
+from .base_function import BaseFunction
+from .map_function import MapFunction
+from .filter_function import FilterFunction
+from .flatmap_function import FlatMapFunction
+# from .lambda_function import LambdaFunction
+from .sink_function import SinkFunction 
+from .source_function import SourceFunction
 
-# __all__ = [
-#     "BaseFunction",
-#     "MapFunction",
-#     "FilterFunction",
-#     "FlatMapFunction",
-#     "LambdaFunction",
-#     "SinkFunction",
-#     "SourceFunction",
-# ]
+__all__ = [
+    "BaseFunction",
+    "MapFunction",
+    "FilterFunction",
+    "FlatMapFunction",
+    "LambdaFunction",
+    "SinkFunction",
+    "SourceFunction",
+]
diff --git a/sage_core/function/base_function.py b/sage_core/function/base_function.py
index 7ffb85f..03bf32e 100644
--- a/sage_core/function/base_function.py
+++ b/sage_core/function/base_function.py
@@ -1,10 +1,14 @@
 import os
 from abc import ABC, abstractmethod
 from typing import Type, List, Tuple, Any, TYPE_CHECKING, Union
+
+from dotenv import load_dotenv
+
+from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
     from sage_runtime.runtime_context import RuntimeContext
 
-from sage_utils.state_persistence import load_function_state, save_function_state
+from sage_runtime.state_persistence import load_function_state, save_function_state
 
 
 # æ„é€ æ¥æºäºsage_runtime/operator/factory.py
@@ -15,17 +19,28 @@ class BaseFunction(ABC):
     """
 
     def __init__(self, ctx:'RuntimeContext' = None, **kwargs):
-        self.ctx = ctx
+        self.runtime_context = ctx
+        self.name = ctx.name if ctx else self.__class__.__name__
+        self.env_name = ctx.env_name if ctx else None
+        self._logger = CustomLogger(
+            filename=f"Function_{self.name}",
+            env_name= self.env_name,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output = "WARNING",
+            name = f"{self.name}_{self.__class__.__name__}",
+            session_folder=ctx.session_folder if ctx else None
+        )
         # self.runtime_context.create_logger()
         self.logger.info(f"Function {self.name} initialized")
         
     @property
     def logger(self):
-        return self.ctx.logger
-    
-    @property
-    def name(self):
-        return self.ctx.name
+        if not hasattr(self, "_logger"):
+            import logging
+            self._logger = logging.getLogger(f"{self.__class__.__name__}")
+        return self._logger
+
 
     # @abstractmethod
     # def close(self, *args, **kwargs):
@@ -56,6 +71,10 @@ class BaseFunction(ABC):
         """
         pass
 
+
+
+
+
 class MemoryFunction(BaseFunction):
     def __init__(self):
         self.runtime_context = None  # éœ€è¦åœ¨compileré‡Œé¢å®ä¾‹åŒ–ã€‚
@@ -76,17 +95,17 @@ class StatefulFunction(BaseFunction):
         super().__init__(**kwargs)
         # æ³¨å…¥ä¸Šä¸‹æ–‡
         # æ¢å¤ä¸Šæ¬¡ checkpoint
-        chkpt_dir = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
-        chkpt_path = os.path.join(chkpt_dir, f"{self.ctx.name}.chkpt")
+        chkpt_dir = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
+        chkpt_path = os.path.join(chkpt_dir, f"{self.runtime_context.name}.chkpt")
         load_function_state(self, chkpt_path)
 
     def save_state(self):
         """
         å°†å½“å‰å¯¹è±¡çŠ¶æ€æŒä¹…åŒ–åˆ° diskï¼Œ
         """
-        base = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
+        base = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
         os.makedirs(base, exist_ok=True)
-        path = os.path.join(base, f"{self.ctx.name}.chkpt")
+        path = os.path.join(base, f"{self.runtime_context.name}.chkpt")
         save_function_state(self, path)
 
 # class MemoryFunction(BaseFunction):
diff --git a/sage_core/function/flatmap_collector.py b/sage_core/function/flatmap_collector.py
index 756b027..0fa3914 100644
--- a/sage_core/function/flatmap_collector.py
+++ b/sage_core/function/flatmap_collector.py
@@ -1,19 +1,30 @@
 from typing import Optional, List, Any, Dict, Tuple
-from sage_runtime.runtime_context import RuntimeContext
-from sage_runtime.router.base_router import BaseRouter
+from sage_utils.custom_logger import CustomLogger
+
+
 class Collector:
     """
     Enhanced Collector class for collecting data from a function.
     Supports both immediate emission and batched collection.
     """
 
-    def __init__(self, ctx:RuntimeContext, router:BaseRouter):
+    def __init__(self, operator=None, session_folder: str = None, name: str = None):
+        self.operator = operator
+        self.logger = CustomLogger(
+            filename=f"Node_{name}",
+            session_folder=session_folder,
+            console_output=False,
+            file_output=True,
+            global_output=True,
+            name=f"{name}_Collector"
+        ) if name else None
+        
         # æ•°æ®æ”¶é›†ç¼“å­˜
-        self.ctx = ctx
         self._collected_data: List[Any] = []
+        self._batch_mode = True  # é»˜è®¤å¯ç”¨æ‰¹å¤„ç†æ¨¡å¼
         
         self.logger.debug(f"Collector initialized with batch_mode={self._batch_mode}") if self.logger else None
-        
+
     def collect(self, data: Any):
         """
         Collect data. Behavior depends on batch_mode setting.
@@ -22,9 +33,36 @@ class Collector:
             data: The data to collect
             tag: Optional output tag
         """
-        # æ‰¹å¤„ç†æ¨¡å¼ï¼šå…ˆæ”¶é›†ï¼Œåè¾“å‡º
-        self._collected_data.append(data)
-        self.logger.debug(f"Data collected in batch mode: {data} data")
+        if self._batch_mode:
+            # æ‰¹å¤„ç†æ¨¡å¼ï¼šå…ˆæ”¶é›†ï¼Œåè¾“å‡º
+            self._collected_data.append(data)
+            if self.logger:
+                self.logger.debug(f"Data collected in batch mode: {data} data")
+        else:
+            # å³æ—¶æ¨¡å¼ï¼šç«‹å³è¾“å‡º
+            if self.operator:
+                self.operator.emit(data)
+                if self.logger:
+                    self.logger.debug(f"Data emitted immediately: {data} data")
+            else:
+                # æ²¡æœ‰operatorï¼Œåªèƒ½æ”¶é›†
+                self._collected_data.append(data)
+                if self.logger:
+                    self.logger.warning(f"No operator set, data collected: {data} data")
+
+    def collect_multiple(self, data_list: List[Any], tag: Optional[str] = None):
+        """
+        Collect multiple data items at once.
+        
+        Args:
+            data_list: List of data items to collect
+            tag: Optional output tag for all items
+        """
+        for data in data_list:
+            self.collect(data)
+        
+        if self.logger:
+            self.logger.debug(f"Collected {len(data_list)} items via collect_multiple")
 
     def get_collected_data(self) -> List[Any]:
         """
@@ -53,7 +91,77 @@ class Collector:
         if self.logger and count > 0:
             self.logger.debug(f"Cleared {count} collected items")
 
+    def flush(self):
+        """
+        Flush all collected data to the operator (emit all collected items).
+        Only works when operator is set.
+        """
+        if not self.operator:
+            if self.logger:
+                self.logger.warning("Cannot flush: no operator set")
+            return
+        
+        count = 0
+        for data, tag in self._collected_data:
+            try:
+                self.operator.emitdata
+                count += 1
+            except Exception as e:
+                if self.logger:
+                    self.logger.error(f"Error emitting data during flush: {e}", exc_info=True)
+        
+        if self.logger:
+            self.logger.debug(f"Flushed {count} items to operator")
+        
+        self.clear()
+
+    def set_batch_mode(self, batch_mode: bool):
+        """
+        Set the batch mode for the collector.
+        
+        Args:
+            batch_mode: True for batch mode, False for immediate mode
+        """
+        old_mode = self._batch_mode
+        self._batch_mode = batch_mode
+        if self.logger:
+            self.logger.debug(f"Batch mode changed from {old_mode} to {batch_mode}")
+
+    def is_batch_mode(self) -> bool:
+        """
+        Check if collector is in batch mode.
+        
+        Returns:
+            bool: True if in batch mode, False otherwise
+        """
+        return self._batch_mode
+
+    def set_operator(self, operator):
+        """
+        Set the operator for this collector.
+        
+        Args:
+            operator: The operator instance
+        """
+        self.operator = operator
+        if self.logger:
+            self.logger.debug(f"Operator set: {operator.__class__.__name__}")
 
-    @property
-    def logger(self):
-        return self.ctx.logger
\ No newline at end of file
+    def get_statistics(self) -> Dict[str, Any]:
+        """
+        Get collector statistics.
+        
+        Returns:
+            Dict[str, Any]: Statistics dictionary
+        """
+        tag_counts = {}
+        for _, tag in self._collected_data:
+            tag_key = tag if tag is not None else "default"
+            tag_counts[tag_key] = tag_counts.get(tag_key, 0) + 1
+        
+        return {
+            "total_collected": len(self._collected_data),
+            "batch_mode": self._batch_mode,
+            "has_operator": self.operator is not None,
+            "tag_distribution": tag_counts
+        }
diff --git a/sage_core/function/flatmap_function.py b/sage_core/function/flatmap_function.py
index 802d96d..bd3fa30 100644
--- a/sage_core/function/flatmap_function.py
+++ b/sage_core/function/flatmap_function.py
@@ -58,6 +58,24 @@ class FlatMapFunction(BaseFunction):
         self.out.collect(data)
         self.logger.debug(f"Data collected: {data}")
 
+    def collect_multiple(self, data_list: Iterable[Any]):
+        """
+        Convenience method to collect multiple data items at once.
+        
+        Args:
+            data_list: Iterable of data items to collect
+            tag: Optional output tag
+        """
+        if self.out is None:
+            raise RuntimeError("Collector not initialized. This should be set by the operator.")
+        
+        count = 0
+        for item in data_list:
+            self.out.collect(item)
+            count += 1
+        
+        self.logger.debug(f"Collected {count} items via collect_multiple")
+
     @abstractmethod
     def execute(self, data: Any) -> Optional[Iterable[Any]]:
         """
diff --git a/sage_core/function/kafka_source.py b/sage_core/function/kafka_source.py
index ff81fa2..be5f0d4 100644
--- a/sage_core/function/kafka_source.py
+++ b/sage_core/function/kafka_source.py
@@ -1,5 +1,7 @@
 from sage_core.function.source_function import SourceFunction
-from typing import Callable, Dict, Any, TYPE_CHECKING
+from sage_utils.custom_logger import CustomLogger
+from sage_utils.data_loader import resolve_data_path
+from typing import List, Callable, Dict, Any, TYPE_CHECKING
 import threading, json, queue
 if TYPE_CHECKING:
     from sage_runtime.runtime_context import RuntimeContext
diff --git a/sage_core/function/map_function.py b/sage_core/function/map_function.py
index 8227f92..25f55b3 100644
--- a/sage_core/function/map_function.py
+++ b/sage_core/function/map_function.py
@@ -3,7 +3,7 @@ from typing import Type, List, Tuple, Any, TYPE_CHECKING, Union
 from sage_core.function.base_function import BaseFunction
 
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 if TYPE_CHECKING:
     from sage_runtime.runtime_context import RuntimeContext
 
diff --git a/sage_core/function/source_function.py b/sage_core/function/source_function.py
index 6b5be37..db88d7b 100644
--- a/sage_core/function/source_function.py
+++ b/sage_core/function/source_function.py
@@ -6,15 +6,6 @@ from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
     from sage_runtime.runtime_context import RuntimeContext
 
-class StopSignal:
-    """
-    åœæ­¢ä¿¡å·ç±»ï¼Œç”¨äºæ ‡è¯†ä»»åŠ¡åœæ­¢
-    """
-    def __init__(self, name: str):
-        self.name = name
-
-    def __repr__(self) -> str:
-        return f"<StopSignal {self.name}>"
 
 class SourceFunction(BaseFunction):
     """
@@ -23,7 +14,7 @@ class SourceFunction(BaseFunction):
     æºå‡½æ•°ä¸æ¥æ”¶è¾“å…¥æ•°æ®ï¼Œåªäº§ç”Ÿè¾“å‡ºæ•°æ®
     é€šå¸¸ç”¨äºè¯»å–æ–‡ä»¶ã€æ•°æ®åº“ã€APIç­‰å¤–éƒ¨æ•°æ®æº
     """
-
+    
     @abstractmethod
     def execute(self) -> Any:
         """
diff --git a/sage_core/jobmanager_client.py b/sage_core/jobmanager_client.py
deleted file mode 100644
index efc8ce2..0000000
--- a/sage_core/jobmanager_client.py
+++ /dev/null
@@ -1,102 +0,0 @@
-import ray
-import socket
-import json
-import pickle
-from ray.actor import ActorHandle
-from pathlib import Path
-from typing import Optional, Dict, Any
-from sage_jobmanager.remote_job_manager import RemoteJobManager
-from sage_utils.custom_logger import CustomLogger
-from sage_utils.actor_wrapper import ActorWrapper
-
-# ==================== å®¢æˆ·ç«¯å·¥å…·ç±» ====================
-
-class JobManagerClient:
-    """JobManagerå®¢æˆ·ç«¯ï¼Œç”¨äºè¿æ¥å®ˆæŠ¤æœåŠ¡è·å–Actorå¥æŸ„"""
-    
-    def __init__(self, host: str = "127.0.0.1", port: int = 19001):
-        self.host = host
-        self.port = port
-    
-    def _send_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """å‘é€è¯·æ±‚åˆ°å®ˆæŠ¤æœåŠ¡"""
-        try:
-            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
-                sock.settimeout(10)
-                sock.connect((self.host, self.port))
-                
-                # å‘é€è¯·æ±‚
-                request_data = json.dumps(request).encode('utf-8')
-                length_data = len(request_data).to_bytes(4, byteorder='big')
-                sock.sendall(length_data + request_data)
-                
-                # æ¥æ”¶å“åº”
-                response_length_data = sock.recv(4)
-                response_length = int.from_bytes(response_length_data, byteorder='big')
-                
-                response_data = b''
-                while len(response_data) < response_length:
-                    chunk = sock.recv(min(response_length - len(response_data), 8192))
-                    response_data += chunk
-                
-                return json.loads(response_data.decode('utf-8'))
-                
-        except Exception as e:
-            return {"status": "error", "message": f"Connection error: {e}"}
-    
-    def get_actor_handle(self) -> 'RemoteJobManager':
-        """è·å–JobManager Actorå¥æŸ„"""
-        import uuid
-        request = {
-            "action": "get_actor_handle",
-            "request_id": str(uuid.uuid4())
-        }
-        if not ray.is_initialized():
-            ray.init(address="auto", _temp_dir="/var/lib/ray_shared")
-
-        response = self._send_request(request)
-        
-        if response.get("status") != "success":
-            raise Exception(f"Failed to get actor handle: {response.get('message')}")
-        
-        # ååºåˆ—åŒ–Actorå¥æŸ„
-        actor_handle_hex = response.get("actor_handle")
-        actor_handle_bytes = bytes.fromhex(actor_handle_hex)
-        actor_handle = pickle.loads(actor_handle_bytes)
-        jobmanager = ActorWrapper(actor_handle)
-        return jobmanager
-    
-    def get_actor_info(self) -> Dict[str, Any]:
-        """è·å–Actorä¿¡æ¯"""
-        import uuid
-        request = {
-            "action": "get_actor_info",
-            "request_id": str(uuid.uuid4())
-        }
-        
-        response = self._send_request(request)
-        
-        if response.get("status") != "success":
-            raise Exception(f"Failed to get actor info: {response.get('message')}")
-        
-        return response.get("actor_info", {})
-    
-    def health_check(self) -> Dict[str, Any]:
-        """å¥åº·æ£€æŸ¥"""
-        import uuid
-        request = {
-            "action": "health_check",
-            "request_id": str(uuid.uuid4())
-        }
-        
-        return self._send_request(request)
-    
-    def restart_actor(self) -> Dict[str, Any]:
-        """é‡å¯Actor"""
-        import uuid
-        request = {
-            "action": "restart_actor", 
-            "request_id": str(uuid.uuid4())
-        }
-        
-        return self._send_request(request)
\ No newline at end of file
diff --git a/sage_core/operator/base_operator.py b/sage_core/operator/base_operator.py
index 1ebcac3..76f8eea 100644
--- a/sage_core/operator/base_operator.py
+++ b/sage_core/operator/base_operator.py
@@ -1,78 +1,200 @@
 
 from abc import ABC, abstractmethod
 from typing import Any, List, Dict, Optional, Set, TYPE_CHECKING, Type, Tuple
-from sage_core.function.source_function import StopSignal
-from sage_runtime.task.base_task import BaseTask
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.unified_emit_context import UnifiedEmitContext
+from sage_runtime.io.packet import Packet
 
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_runtime.router.connection import Connection
+    from sage_runtime.io.connection import Connection
     from sage_runtime.runtime_context import RuntimeContext
-    from sage_jobmanager.factory.function_factory import FunctionFactory
-    from sage_runtime.router.base_router import BaseRouter
+    from sage_runtime.function.factory import FunctionFactory
 
 class BaseOperator(ABC):
     def __init__(self, 
                  function_factory: 'FunctionFactory', ctx: 'RuntimeContext', *args,
                  **kwargs):
         
-        self.received_stop_signals: Set[str] = set()
-        self.ctx: 'RuntimeContext' = ctx
+        self.name:str
         self.function:'BaseFunction'
-        self.router:'BaseRouter'     # ç”±taskä¼ ä¸‹æ¥çš„
-        self.task: Optional['BaseTask'] = None
+        self._emit_context: 'UnifiedEmitContext'
+
+        self.function_factory = function_factory
+        self.downstream_groups:Dict[int, Dict[int, 'Connection']] = {}
+        self.downstream_group_roundrobin: Dict[int, int] = {}
+
         try:
-            self.function = function_factory.create_function(self.name, ctx)
-            self.logger.debug(f"Created function instance with {function_factory}")
+            self.name = ctx.name
+            self.logger = CustomLogger(
+                filename=f"Node_{ctx.name}",
+                env_name=ctx.env_name,
+                console_output="WARNING",
+                file_output="DEBUG",
+                global_output = "WARNING",
+                name = f"{ctx.name}_{self.__class__.__name__}",
+                session_folder=ctx.session_folder
+            )
+            self.runtime_context = ctx
+            self.function = self.function_factory.create_function(self.name, ctx)
+
+            self._emit_context = UnifiedEmitContext(name = ctx.name, session_folder=ctx.session_folder, env_name = ctx.env_name) 
+            self.logger.debug(f"Created function instance with {self.function_factory}")
 
         except Exception as e:
             self.logger.error(f"Failed to create function instance: {e}", exc_info=True)
 
-    # TODO: å»æ‰stateful functionçš„æ¦‚å¿µï¼Œç”¨æŸäº›ç­–ç•¥å¯¹äºfunctionå†…éƒ¨çš„å¯åºåˆ—åŒ–å­—æ®µåšé™æ€ä¿å­˜å’Œcheckpoint
-    def save_state(self):
+    def receive_packet(self, packet: 'Packet' = None):
+        self.process_packet(packet)
         from sage_core.function.base_function import StatefulFunction
         if isinstance(self.function, StatefulFunction):
             self.function.save_state()
 
-    def receive_packet(self, packet: 'Packet'):
-        """l
-        æ¥æ”¶æ•°æ®åŒ…å¹¶å¤„ç†
-        """
-        if packet is None:
-            self.logger.warning(f"Received None packet in {self.name}")
-            return
-        if isinstance(packet, StopSignal):
-            self.handle_stop_signal(packet) 
-            return
-        self.logger.debug(f"Operator {self.name} received packet: {packet}")
-        # å¤„ç†æ•°æ®åŒ…
-        self.process_packet(packet)
+    @abstractmethod
+    def process_packet(self, packet: 'Packet' = None):
+        return
 
-    def handle_stop_signal(self, stop_signal: StopSignal):
+
+    def emit_packet(self, packet: 'Packet'):
         """
-        å¤„ç†åœæ­¢ä¿¡å·
+        æ–°å¢ï¼šç›´æ¥å‘é€packetï¼Œæ ¹æ®å…¶åˆ†åŒºä¿¡æ¯é€‰æ‹©è·¯ç”±ç­–ç•¥
+        
+        Args:
+            packet: è¦å‘é€çš„packetï¼Œå¯èƒ½åŒ…å«åˆ†åŒºä¿¡æ¯
         """
-        if stop_signal.name in self.received_stop_signals:
-            self.logger.debug(f"Already received stop signal from {stop_signal.name}")
-            return
+        if self._emit_context is None:
+            raise RuntimeError(f"Emit context not set for operator {self.name}")
+        self.logger.debug(f"Emitting packet: {packet}")
+        # æ ¹æ®packetçš„åˆ†åŒºä¿¡æ¯é€‰æ‹©è·¯ç”±ç­–ç•¥
+        if packet.is_keyed():
+            self._emit_keyed_packet(packet)
+        else:
+            self._emit_round_robin_packet(packet)
+
+    def _emit_keyed_packet(self, packet: 'Packet'):
+        """ä½¿ç”¨åˆ†åŒºä¿¡æ¯è¿›è¡Œè·¯ç”±"""
+        strategy = packet.partition_strategy
+        partition_key = packet.partition_key
         
-        self.received_stop_signals.add(stop_signal.name)
-        self.logger.info(f"Handling stop signal from {stop_signal.name}")
-        # å‘é€åœæ­¢ä¿¡å·åˆ°è·¯ç”±å™¨
-        self.router.send_stop_signal(stop_signal)
+        for broadcast_index, parallel_targets in self.downstream_groups.items():
+            if strategy == "hash":
+                target_index = hash(partition_key) % len(parallel_targets)
+                connection = parallel_targets[target_index]
+                self._send_packet_to_connection(connection, packet)
+                
+            elif strategy == "broadcast":
+                # å¹¿æ’­åˆ°æ‰€æœ‰å®ä¾‹
+                for connection in parallel_targets.values():
+                    self._send_packet_to_connection(connection, packet)
+                    
+            elif strategy == "round_robin":
+                # å¿½ç•¥é”®ï¼Œä½¿ç”¨è½®è¯¢
+                current_round_robin = self.downstream_group_roundrobin[broadcast_index]
+                connection = parallel_targets[current_round_robin % len(parallel_targets)]
+                self.downstream_group_roundrobin[broadcast_index] += 1
+                self._send_packet_to_connection(connection, packet)
+                
+            else:
+                self.logger.warning(f"Unknown partition strategy: {strategy}, using round-robin")
+                self._emit_round_robin_packet(packet)
+
+    def _emit_round_robin_packet(self, packet: 'Packet'):
+        """ä½¿ç”¨round-robinç­–ç•¥è·¯ç”±"""
+        for broadcast_index, parallel_targets in self.downstream_groups.items():
+            current_round_robin = self.downstream_group_roundrobin[broadcast_index]
+            connection = parallel_targets[current_round_robin % len(parallel_targets)]
+            self.downstream_group_roundrobin[broadcast_index] += 1
+            self._send_packet_to_connection(connection, packet)
+
+    def _send_packet_to_connection(self, connection: 'Connection', packet: 'Packet'):
+        """å‘é€packetåˆ°æŒ‡å®šè¿æ¥"""
+        try:
+            # æ›´æ–°packetçš„ç›®æ ‡è¾“å…¥ç´¢å¼•
+            routed_packet = Packet(
+                payload=packet.payload,
+                input_index=connection.target_input_index,
+                partition_key=packet.partition_key,
+                partition_strategy=packet.partition_strategy,
+            )
+            
+            self._emit_context.send_packet_direct(connection, routed_packet)
+            self.logger.debug(f"Sent {'keyed' if packet.is_keyed() else 'unkeyed'} packet to {connection.target_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Failed to send packet to {connection.target_name}: {e}", exc_info=True)
 
-    @abstractmethod
-    def process_packet(self, packet: 'Packet' = None):
-        return
 
-    @property
-    def name(self) -> str:
-        """è·å–ä»»åŠ¡åç§°"""
-        return self.ctx.name
 
-    @property
-    def logger(self) -> CustomLogger:
-        """è·å–å½“å‰ä»»åŠ¡çš„æ—¥å¿—è®°å½•å™¨"""
-        return self.ctx.logger
\ No newline at end of file
+    def add_connection(self, connection: 'Connection') -> None:
+        """
+        æ·»åŠ ä¸‹æ¸¸è¿æ¥ï¼Œä½¿ç”¨Connectionå¯¹è±¡çš„å±æ€§ä½œä¸ºç´¢å¼•ï¼Œä¿å­˜å®Œæ•´çš„Connectionå¯¹è±¡
+        
+        Args:
+            connection: Connectionå¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¿æ¥ä¿¡æ¯
+        """
+        broadcast_index = connection.broadcast_index
+        parallel_index = connection.parallel_index
+        
+        # Debug log
+        self.logger.debug(
+            f"broadcast_index={broadcast_index}, parallel_index={parallel_index}, "
+            f"target={connection.target_name}"
+            f"connection_type={connection.connection_type.value}"
+        )
+        
+        # åˆå§‹åŒ–å¹¿æ’­ç»„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
+        if broadcast_index not in self.downstream_groups:
+            self.downstream_groups[broadcast_index] = {}
+            self.downstream_group_roundrobin[broadcast_index] = 0
+        
+        # ä¿å­˜å®Œæ•´çš„Connectionå¯¹è±¡
+        self.downstream_groups[broadcast_index][parallel_index] = connection
+        
+        # æ‰“å°è¿æ¥çš„è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
+        # if self.logger.isEnabledFor(10):  # DEBUG level
+        # print(connection.debug_info())
+        
+        self.logger.debug(
+            f"Added downstream connection: -> "
+            f"{connection.target_name} "
+            f"(type: {connection.connection_type.value})"
+        )
+
+    def get_downstream_connections(self, output_tag: str = None) -> List['Connection']:
+        """
+        è·å–æ‰€æœ‰ä¸‹æ¸¸è¿æ¥ï¼Œå¯é€‰æ‹©æ€§åœ°æŒ‰è¾“å‡ºæ ‡ç­¾è¿‡æ»¤
+        
+        Args:
+            output_tag: å¯é€‰çš„è¾“å‡ºæ ‡ç­¾è¿‡æ»¤å™¨
+            
+        Returns:
+            List['Connection']: è¿æ¥åˆ—è¡¨
+        """
+        connections = []
+        
+        for broadcast_groups in self.downstream_groups.values():
+            for connection in broadcast_groups.values():
+                connections.append(connection)
+        
+        return connections
+    
+    def get_wrapped_operator(self):
+        """
+            è¿™ä¸ªæ–¹æ³•æ˜¯ç”¨æ¥è®©ideæ»¡æ„çš„ï¼Œç”¨æ¥ä»£è¡¨OperatorWrapperæä¾›çš„è¿™ä¸ªæ–¹æ³•
+        """
+        pass
+
+    def _send_to_connection(self, connection:'Connection', data):
+        """
+        å‘é€æ•°æ®åˆ°æŒ‡å®šè¿æ¥çš„è¾…åŠ©æ–¹æ³•
+        
+        Args:
+            connection: ç›®æ ‡è¿æ¥
+            data: è¦å‘é€çš„æ•°æ®ï¼ˆå·²è§£åŒ…çš„åŸå§‹æ•°æ®ï¼‰
+        """
+        try:
+            packet = Packet(data, connection.target_input_index)
+            self._emit_context.send_packet_direct(connection, packet)
+            self.logger.debug(f"Sent data to {connection.target_name}")
+        except Exception as e:
+            self.logger.error(f"Failed to send data to {connection.target_name}: {e}", exc_info=True)
\ No newline at end of file
diff --git a/sage_core/operator/comap_operator.py b/sage_core/operator/comap_operator.py
index f1b35a8..e837991 100644
--- a/sage_core/operator/comap_operator.py
+++ b/sage_core/operator/comap_operator.py
@@ -1,7 +1,7 @@
 from .base_operator import BaseOperator
 from typing import Union, Any
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class CoMapOperator(BaseOperator):
@@ -56,7 +56,7 @@ class CoMapOperator(BaseOperator):
             if result is not None:
                 # ç»§æ‰¿åŸpacketçš„åˆ†åŒºä¿¡æ¯
                 result_packet = packet.inherit_partition_info(result)
-                self.router.send(result_packet)
+                self.emit_packet(result_packet)
                 
         except Exception as e:
             self.logger.error(f"Error in CoMapOperator {self.name}: {e}", exc_info=True)
diff --git a/sage_core/operator/filter_operator.py b/sage_core/operator/filter_operator.py
index 5d61b05..2062cd2 100644
--- a/sage_core/operator/filter_operator.py
+++ b/sage_core/operator/filter_operator.py
@@ -2,11 +2,11 @@ from typing import Any, Optional
 from typing import Any, List, Dict, Optional, Set, TYPE_CHECKING, Type, Tuple
 from sage_core.operator.base_operator import BaseOperator
 from sage_core.function.filter_function import FilterFunction
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_runtime.router.connection import Connection
+    from sage_runtime.io.connection import Connection
     
 
 class FilterOperator(BaseOperator):
@@ -40,7 +40,7 @@ class FilterOperator(BaseOperator):
             
             if should_pass:
                 # é€šè¿‡è¿‡æ»¤ï¼Œç»§æ‰¿åˆ†åŒºä¿¡æ¯
-                self.router.send(packet)
+                self.emit_packet(packet)
             # ä¸é€šè¿‡è¿‡æ»¤ï¼šä¸å‘é€ä»»ä½•packet
             
         except Exception as e:
diff --git a/sage_core/operator/flatmap_operator.py b/sage_core/operator/flatmap_operator.py
index 084bdef..a8a3bfa 100644
--- a/sage_core/operator/flatmap_operator.py
+++ b/sage_core/operator/flatmap_operator.py
@@ -2,7 +2,7 @@ from typing import Any, Iterable, Optional
 from sage_core.operator.base_operator import BaseOperator
 from sage_core.function.flatmap_function import FlatMapFunction
 from sage_core.function.flatmap_collector import Collector
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class FlatMapOperator(BaseOperator):
@@ -30,8 +30,9 @@ class FlatMapOperator(BaseOperator):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.out: Collector = Collector(
-            self.ctx,
-            self.router
+            operator=self,
+            session_folder=self.runtime_context.session_folder,
+            name=self.name
         )
         self.function.insert_collector(self.out)
         self.logger.info(f"FlatMapOperator '{self.name}' initialized with collector")
@@ -55,7 +56,7 @@ class FlatMapOperator(BaseOperator):
             
             # å¤„ç†functionçš„è¿”å›å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
             if result is not None:
-                self._flatmap_send(result, packet)
+                self._emit_iterable_with_partition_info(result, packet)
             
             # å¤„ç†é€šè¿‡collectoræ”¶é›†çš„æ•°æ®
             collected_data = self.out.get_collected_data()
@@ -64,7 +65,7 @@ class FlatMapOperator(BaseOperator):
                 for item_data in collected_data:
                     # ä¸ºæ¯ä¸ªæ”¶é›†çš„itemåˆ›å»ºæ–°packetï¼Œç»§æ‰¿åˆ†åŒºä¿¡æ¯
                     result_packet = packet.inherit_partition_info(item_data)
-                    self.router.send(result_packet)
+                    self.emit_packet(result_packet)
                 # æ¸…ç©ºcollector
                 self.out.clear()
             
@@ -78,7 +79,7 @@ class FlatMapOperator(BaseOperator):
         except Exception as e:
             self.logger.error(f"Error in FlatMapOperator '{self.name}'.process_packet(): {e}", exc_info=True)
 
-    def _flatmap_send(self, result: Any, source_packet: 'Packet'):
+    def _emit_iterable_with_partition_info(self, result: Any, source_packet: 'Packet'):
         """
         å°†å¯è¿­ä»£å¯¹è±¡å±•å¼€å¹¶å‘é€ç»™ä¸‹æ¸¸ï¼Œä¿æŒåˆ†åŒºä¿¡æ¯
         
@@ -93,15 +94,67 @@ class FlatMapOperator(BaseOperator):
                 for item in result:
                     # ä¸ºæ¯ä¸ªitemåˆ›å»ºæ–°packetï¼Œç»§æ‰¿åˆ†åŒºä¿¡æ¯
                     result_packet = source_packet.inherit_partition_info(item)
-                    self.router.send(result_packet)
+                    self.emit_packet(result_packet)
                     count += 1
                 self.logger.debug(f"FlatMapOperator '{self.name}' emitted {count} items from iterable")
             else:
                 # å¦‚æœä¸æ˜¯å¯è¿­ä»£å¯¹è±¡ï¼Œç›´æ¥å‘é€
                 result_packet = source_packet.inherit_partition_info(result)
-                self.router.send(result_packet)
+                self.emit_packet(result_packet)
                 self.logger.debug(f"FlatMapOperator '{self.name}' emitted single item: {result}")
                 
         except Exception as e:
             self.logger.error(f"Error in FlatMapOperator '{self.name}'._emit_iterable_with_partition_info(): {e}", exc_info=True)
-            raise
\ No newline at end of file
+            raise
+
+    def get_collector_statistics(self) -> dict:
+        """
+        è·å–collectorçš„ç»Ÿè®¡ä¿¡æ¯
+        
+        Returns:
+            dict: ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰collectoråˆ™è¿”å›ç©ºå­—å…¸
+        """
+        if self.out:
+            return self.out.get_statistics()
+        return {}
+
+    def debug_print_collector_info(self):
+        """
+        æ‰“å°collectorçš„è°ƒè¯•ä¿¡æ¯
+        """
+        if self.out:
+            print(f"\nğŸ” FlatMapOperator '{self.name}' Collector Info:")
+            self.out.debug_print_collected_data()
+        else:
+            print(f"\nğŸ” FlatMapOperator '{self.name}' has no collector")
+
+    def emit_data_with_key(self, data: Any, partition_key: Any = None, partition_strategy: str = None):
+        """
+        ä¾¿åˆ©æ–¹æ³•ï¼šç›´æ¥å‘é€å¸¦æœ‰åˆ†åŒºä¿¡æ¯çš„æ•°æ®
+        
+        Args:
+            data: è¦å‘é€çš„æ•°æ®
+            partition_key: åˆ†åŒºé”®
+            partition_strategy: åˆ†åŒºç­–ç•¥
+        """
+        packet = Packet(
+            payload=data,
+            partition_key=partition_key,
+            partition_strategy=partition_strategy
+        )
+        self.emit_packet(packet)
+
+    def emit_collected_data_with_source_partition(self, source_packet: 'Packet'):
+        """
+        ä¾¿åˆ©æ–¹æ³•ï¼šå‘é€collectorä¸­çš„æ‰€æœ‰æ•°æ®ï¼Œç»§æ‰¿æºpacketçš„åˆ†åŒºä¿¡æ¯
+        
+        Args:
+            source_packet: æºpacketï¼Œç”¨äºç»§æ‰¿åˆ†åŒºä¿¡æ¯
+        """
+        collected_data = self.out.get_collected_data()
+        if collected_data:
+            for item_data in collected_data:
+                result_packet = source_packet.inherit_partition_info(item_data)
+                self.emit_packet(result_packet)
+            self.out.clear()
+            self.logger.debug(f"Emitted {len(collected_data)} collected items with inherited partition info")
\ No newline at end of file
diff --git a/sage_core/operator/future_operator.py b/sage_core/operator/future_operator.py
index efd10cc..f75d3a2 100644
--- a/sage_core/operator/future_operator.py
+++ b/sage_core/operator/future_operator.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 from typing import Any, List
 from .base_operator import BaseOperator
-from sage_jobmanager.factory.function_factory import FunctionFactory
+from sage_runtime.function.factory import FunctionFactory
 
 
 class FutureOperator(BaseOperator):
diff --git a/sage_core/operator/join_operator.py b/sage_core/operator/join_operator.py
index f9cf41b..3e05fb8 100644
--- a/sage_core/operator/join_operator.py
+++ b/sage_core/operator/join_operator.py
@@ -1,7 +1,7 @@
 from .base_operator import BaseOperator
 from typing import Union, Any, List
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class JoinOperator(BaseOperator):
@@ -123,7 +123,7 @@ class JoinOperator(BaseOperator):
                 partition_strategy=original_packet.partition_strategy or "hash",
             )
             
-            self.router.send(result_packet)
+            self.emit_packet(result_packet)
             
             self.logger.debug(
                 f"JoinOperator '{self.name}' emitted result for key '{join_key}': "
diff --git a/sage_core/operator/keyby_operator.py b/sage_core/operator/keyby_operator.py
index c395ca4..ff6b120 100644
--- a/sage_core/operator/keyby_operator.py
+++ b/sage_core/operator/keyby_operator.py
@@ -1,8 +1,8 @@
 from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Optional
 from sage_core.operator.base_operator import BaseOperator
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 if TYPE_CHECKING:
-    from sage_runtime.router.connection import Connection
+    from sage_runtime.io.connection import Connection
 
 class KeyByOperator(BaseOperator):
     """
@@ -35,13 +35,13 @@ class KeyByOperator(BaseOperator):
             self.logger.debug(f"KeyByOperator '{self.name}' added key '{extracted_key}' to packet")
             
             # ç›´æ¥å‘é€å¸¦æœ‰åˆ†åŒºä¿¡æ¯çš„packet
-            self.router.send(keyed_packet)
+            self.emit_packet(keyed_packet)
             
         except Exception as e:
             self.logger.error(f"Error in KeyByOperator {self.name}: {e}", exc_info=True)
             # å›é€€ï¼šå‘é€åŸå§‹packet
             if packet:
-                self.router.send(packet)
+                self.emit_packet(packet)
 
 
     def process(self, raw_data: Any, input_index: int = 0) -> Any:
diff --git a/sage_core/operator/map_operator.py b/sage_core/operator/map_operator.py
index 4c45cc0..2f49e1f 100644
--- a/sage_core/operator/map_operator.py
+++ b/sage_core/operator/map_operator.py
@@ -4,7 +4,7 @@ from sage_core.function.map_function import MapFunction
 from typing import Union, Any
 from sage_core.function.map_function import MapFunction
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class MapOperator(BaseOperator):
@@ -23,6 +23,6 @@ class MapOperator(BaseOperator):
                 self.logger.debug(f"Operator {self.name} processed data with result: {result}")
                 result_packet = packet.inherit_partition_info(result) if (result is not None) else None
                 if result_packet is not None:
-                    self.router.send(result_packet)
+                    self.emit_packet(result_packet)
         except Exception as e:
             self.logger.error(f"Error in {self.name}.process(): {e}", exc_info=True)
\ No newline at end of file
diff --git a/sage_core/operator/sink_operator.py b/sage_core/operator/sink_operator.py
index f9ffc62..6b920ee 100644
--- a/sage_core/operator/sink_operator.py
+++ b/sage_core/operator/sink_operator.py
@@ -1,17 +1,14 @@
-from sage_core.function.source_function import StopSignal
 from sage_core.operator.base_operator import BaseOperator
 from sage_core.function.sink_function import SinkFunction
 from sage_utils.custom_logger import CustomLogger
 from collections import deque
 from typing import Union, Dict, Deque, Tuple, Any
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class SinkOperator(BaseOperator):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        self.received_stop_signals = set()
-        self.stop_signal_count = 0
         # # éªŒè¯å‡½æ•°ç±»å‹
         # if not isinstance(self.function, SinkFunction):
         #     raise TypeError(f"SinkOperator requires SinkFunction, got {type(self.function)}")
@@ -24,19 +21,4 @@ class SinkOperator(BaseOperator):
                 result = self.function.execute(packet.payload)
                 self.logger.debug(f"Operator {self.name} processed data with result: {result}")
         except Exception as e:
-            self.logger.error(f"Error in {self.name}.process(): {e}", exc_info=True)
-
-    def handle_stop_signal(self, stop_signal: StopSignal):
-        """
-        å¤„ç†åœæ­¢ä¿¡å·
-        """
-        if stop_signal.name in self.received_stop_signals:
-            self.logger.debug(f"Already received stop signal from {stop_signal.name}")
-            return
-        
-        self.received_stop_signals.add(stop_signal.name)
-        self.logger.info(f"Handling stop signal from {stop_signal.name}")
-
-        self.stop_signal_count += 1
-        if self.stop_signal_count >= self.ctx.stop_signal_num:
-            self.ctx.jobmanager.receive_stop_signal(self.ctx.env_uuid)
\ No newline at end of file
+            self.logger.error(f"Error in {self.name}.process(): {e}", exc_info=True)
\ No newline at end of file
diff --git a/sage_core/operator/source_operator.py b/sage_core/operator/source_operator.py
index 467ed37..2bd2067 100644
--- a/sage_core/operator/source_operator.py
+++ b/sage_core/operator/source_operator.py
@@ -2,31 +2,20 @@ from sage_core.operator.base_operator import BaseOperator
 from sage_core.function.source_function import SourceFunction
 from sage_utils.custom_logger import CustomLogger
 from collections import deque
-from typing import Union, Dict, Deque, Tuple, Any, TYPE_CHECKING
-from sage_runtime.router.packet import Packet
-from sage_core.function.source_function import StopSignal
-if TYPE_CHECKING:
-    from sage_runtime.task.base_task import BaseTask
+from typing import Union, Dict, Deque, Tuple, Any
+from sage_runtime.io.packet import Packet
 
 class SourceOperator(BaseOperator):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
-    def receive_packet(self, packet: 'Packet'):
-        self.process_packet(packet)
-
+        
     def process_packet(self, packet: 'Packet' = None):
         try:
             
             result = self.function.execute()
             self.logger.debug(f"Operator {self.name} processed data with result: {result}")
-            if isinstance(result, StopSignal):
-                self.logger.info(f"Source Operator {self.name} received stop signal: {result}")
-                result.name = self.name
-                self.router.send_stop_signal(result)
-                self.task.stop()
-                return
             if result is not None:
-                self.router.send(Packet(result))
+                self.emit_packet(Packet(result))
         except Exception as e:
             self.logger.error(f"Error in {self.name}.process(): {e}", exc_info=True)
diff --git a/sage_core/transformation/base_transformation.py b/sage_core/transformation/base_transformation.py
index 09bbc29..f24fb26 100644
--- a/sage_core/transformation/base_transformation.py
+++ b/sage_core/transformation/base_transformation.py
@@ -1,15 +1,17 @@
 from __future__ import annotations
-from typing import List, Type, Union, TYPE_CHECKING, Any
+from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Optional
+from enum import Enum
+from abc import ABC, abstractmethod
 from sage_utils.custom_logger import CustomLogger
-from sage_jobmanager.utils.name_server import get_name
-from sage_jobmanager.factory.operator_factory import OperatorFactory
-from sage_jobmanager.factory.function_factory import FunctionFactory
-from sage_jobmanager.factory.task_factory import TaskFactory
+from sage_utils.name_server import get_name
+from sage_runtime.operator.factory import OperatorFactory
+from sage_runtime.function.factory import FunctionFactory
+from sage_runtime.dagnode.factory import DAGNodeFactory
 from ray.actor import ActorHandle
 if TYPE_CHECKING:
     from sage_core.operator.base_operator import BaseOperator
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class BaseTransformation:
@@ -25,8 +27,6 @@ class BaseTransformation:
         self.operator_class:Type[BaseOperator]  # ç”±å­ç±»è®¾ç½®
 
         self.remote = (env.platform == "remote")
-        self.env_name = env.name
-        self.memory_collection: Union[Any, ActorHandle] = env.memory_collection
         self.env = env
         self.function_class = function
         self.function_args = args
@@ -35,9 +35,14 @@ class BaseTransformation:
         self.basename = get_name(name) if name else get_name(self.function_class.__name__)
             
 
-        self.logger = CustomLogger()
-        if self.remote and (env.memory_collection is not None) and (not isinstance(env.memory_collection, ActorHandle)):
-            raise Exception(f"Memory collection must be a Ray Actor handle for remote transformation, but got {type(env.memory_collection)}")
+        self.logger = CustomLogger(
+            filename=f"{self.basename}_{self.__class__.__name__}",
+            env_name = env.name,
+            console_output=False,
+            file_output=True
+        )
+        if self.remote and not isinstance(env.memory_collection, ActorHandle):
+            raise Exception("Memory collection must be a Ray Actor handle for remote transformation")
 
         self.logger.debug(f"Creating BaseTransformation of type {type} with rag {self.function_class.__name__}")
 
@@ -47,7 +52,7 @@ class BaseTransformation:
 
         
         # æ‡’åŠ è½½å·¥å‚
-        self._dag_node_factory: TaskFactory = None
+        self._dag_node_factory: DAGNodeFactory = None
         self._operator_factory: OperatorFactory = None
         self._function_factory: FunctionFactory = None
         # ç”Ÿæˆçš„å¹³è¡ŒèŠ‚ç‚¹åå­—ï¼šf"{transformation.function_class.__name__}_{i}"
@@ -93,16 +98,16 @@ class BaseTransformation:
                 operator_class=self.operator_class,
                 function_factory=self.function_factory,
                 basename=self.basename,
-                env_name=self.env_name,
+                env_name=self.env.name,
                 remote=self.remote
             )   
         return self._operator_factory
 
     @property
-    def task_factory(self) -> TaskFactory:
+    def dag_node_factory(self) -> DAGNodeFactory:
         """æ‡’åŠ è½½åˆ›å»ºDAGèŠ‚ç‚¹å·¥å‚"""
         if self._dag_node_factory is None:
-            self._dag_node_factory = TaskFactory(self)
+            self._dag_node_factory = DAGNodeFactory(self)
         return self._dag_node_factory
 
     @property
@@ -113,10 +118,6 @@ class BaseTransformation:
     def is_spout(self) -> bool:
         return False
 
-    @property
-    def is_sink(self) -> bool:
-        return False
-
     @property
     def is_merge_operation(self) -> bool:
         """
diff --git a/sage_core/transformation/comap_transformation.py b/sage_core/transformation/comap_transformation.py
index 079ada4..78309e4 100644
--- a/sage_core/transformation/comap_transformation.py
+++ b/sage_core/transformation/comap_transformation.py
@@ -3,7 +3,7 @@ from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Opti
 from sage_core.transformation.base_transformation import BaseTransformation
 if TYPE_CHECKING:
     from sage_core.function.comap_function import BaseCoMapFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class CoMapTransformation(BaseTransformation):
diff --git a/sage_core/transformation/filter_transformation.py b/sage_core/transformation/filter_transformation.py
index 709ea2e..e627709 100644
--- a/sage_core/transformation/filter_transformation.py
+++ b/sage_core/transformation/filter_transformation.py
@@ -5,7 +5,7 @@ from sage_core.operator.filter_operator import FilterOperator
 if TYPE_CHECKING:
     from sage_core.operator.base_operator import BaseOperator
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 
diff --git a/sage_core/transformation/flatmap_transformation.py b/sage_core/transformation/flatmap_transformation.py
index ea7286b..0597729 100644
--- a/sage_core/transformation/flatmap_transformation.py
+++ b/sage_core/transformation/flatmap_transformation.py
@@ -4,7 +4,7 @@ from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.flatmap_operator import FlatMapOperator
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class FlatMapTransformation(BaseTransformation):
diff --git a/sage_core/transformation/future_transformation.py b/sage_core/transformation/future_transformation.py
index be226bd..e581934 100644
--- a/sage_core/transformation/future_transformation.py
+++ b/sage_core/transformation/future_transformation.py
@@ -4,7 +4,7 @@ from .base_transformation import BaseTransformation
 from sage_core.operator.future_operator import FutureOperator
 
 if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class FutureTransformation(BaseTransformation):
@@ -53,31 +53,31 @@ class FutureTransformation(BaseTransformation):
         
         # æ ‡è®°ä¸ºå·²å¡«å……ï¼Œä½†ä¿ç•™åœ¨pipelineä¸­ä»¥ä¾¿compilerèƒ½å¤Ÿå¤„ç†
         # compilerä¼šæ£€æŸ¥filledçŠ¶æ€æ¥å†³å®šå¦‚ä½•å¤„ç†è¿™ä¸ªtransformation
-        # self._mark_as_filled_in_pipeline()
+        self._mark_as_filled_in_pipeline()
         
         self.logger.debug(f"Filled FutureTransformation '{self.future_name}' with {actual_transformation.basename}")
     
-    # def _mark_as_filled_in_pipeline(self) -> None:
-    #     """
-    #     å°†å·²å¡«å……çš„future transformationä»pipelineä¸­ç§»é™¤ï¼Œå¹¶ä¿å­˜åˆ°filled_futuresä¸­
-    #     è¿™æ ·compilerå°±çœ‹ä¸åˆ°future transformationsï¼Œåªçœ‹åˆ°å®é™…çš„åé¦ˆè¾¹è¿æ¥
-    #     """
-    #     # å°†è¯¥future transformationä»pipelineä¸­ç§»é™¤
-    #     if self in self.env._pipeline:
-    #         self.env._pipeline.remove(self)
-    #         self.logger.debug(f"Removed FutureTransformation '{self.future_name}' from pipeline")
+    def _mark_as_filled_in_pipeline(self) -> None:
+        """
+        å°†å·²å¡«å……çš„future transformationä»pipelineä¸­ç§»é™¤ï¼Œå¹¶ä¿å­˜åˆ°filled_futuresä¸­
+        è¿™æ ·compilerå°±çœ‹ä¸åˆ°future transformationsï¼Œåªçœ‹åˆ°å®é™…çš„åé¦ˆè¾¹è¿æ¥
+        """
+        # å°†è¯¥future transformationä»pipelineä¸­ç§»é™¤
+        if self in self.env._pipeline:
+            self.env._pipeline.remove(self)
+            self.logger.debug(f"Removed FutureTransformation '{self.future_name}' from pipeline")
         
-    #     # ä¿å­˜å¡«å……ä¿¡æ¯åˆ°ç¯å¢ƒä¸­ï¼Œä¾›è°ƒè¯•å’Œç®¡ç†ä½¿ç”¨
-    #     if not hasattr(self.env, '_filled_futures'):
-    #         self.env._filled_futures = {}
+        # ä¿å­˜å¡«å……ä¿¡æ¯åˆ°ç¯å¢ƒä¸­ï¼Œä¾›è°ƒè¯•å’Œç®¡ç†ä½¿ç”¨
+        if not hasattr(self.env, '_filled_futures'):
+            self.env._filled_futures = {}
         
-    #     self.env._filled_futures[self.future_name] = {
-    #         'future_transformation': self,
-    #         'actual_transformation': self.actual_transformation,
-    #         'filled_at': self._get_current_timestamp()
-    #     }
+        self.env._filled_futures[self.future_name] = {
+            'future_transformation': self,
+            'actual_transformation': self.actual_transformation,
+            'filled_at': self._get_current_timestamp()
+        }
         
-    #     self.logger.info(f"Future transformation '{self.future_name}' filled and removed from pipeline")
+        self.logger.info(f"Future transformation '{self.future_name}' filled and removed from pipeline")
     
     def _get_current_timestamp(self) -> str:
         """è·å–å½“å‰æ—¶é—´æˆ³"""
@@ -113,7 +113,7 @@ class FutureTransformation(BaseTransformation):
         """
         åœ¨pipelineä¸­æŸ¥æ‰¾æŒ‡å®šåç§°çš„transformation
         """
-        for trans in self.env.pipeline:
+        for trans in self.env._pipeline:
             if trans.basename == name:
                 return trans
         return None
diff --git a/sage_core/transformation/join_transformation.py b/sage_core/transformation/join_transformation.py
index cf62d67..3d1b348 100644
--- a/sage_core/transformation/join_transformation.py
+++ b/sage_core/transformation/join_transformation.py
@@ -3,7 +3,7 @@ from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Opti
 from sage_core.transformation.base_transformation import BaseTransformation
 if TYPE_CHECKING:
     from sage_core.function.join_function import BaseJoinFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class JoinTransformation(BaseTransformation):
diff --git a/sage_core/transformation/keyby_transformation.py b/sage_core/transformation/keyby_transformation.py
index 72fe096..d7cba9a 100644
--- a/sage_core/transformation/keyby_transformation.py
+++ b/sage_core/transformation/keyby_transformation.py
@@ -1,6 +1,6 @@
 from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.keyby_operator import KeyByOperator
-from sage_jobmanager.factory.operator_factory import OperatorFactory
+from sage_runtime.operator.factory import OperatorFactory
 
 class KeyByTransformation(BaseTransformation):
     """
@@ -30,7 +30,7 @@ class KeyByTransformation(BaseTransformation):
             operator_class=self.operator_class,
             function_factory=self.function_factory,
             basename=self.basename,
-            env_name=self.env_name,
+            env_name=self.env.name,
             remote=self.remote,
             partition_strategy=self.partition_strategy  # KeyByç‰¹æœ‰å‚æ•°
         )
diff --git a/sage_core/transformation/map_transformation.py b/sage_core/transformation/map_transformation.py
index cc34f5c..9d979d2 100644
--- a/sage_core/transformation/map_transformation.py
+++ b/sage_core/transformation/map_transformation.py
@@ -4,7 +4,7 @@ from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.map_operator import MapOperator
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 class MapTransformation(BaseTransformation):
     """æ˜ å°„å˜æ¢ - ä¸€å¯¹ä¸€æ•°æ®å˜æ¢"""
diff --git a/sage_core/transformation/sink_transformation.py b/sage_core/transformation/sink_transformation.py
index 765687e..a40f02c 100644
--- a/sage_core/transformation/sink_transformation.py
+++ b/sage_core/transformation/sink_transformation.py
@@ -4,7 +4,7 @@ from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.sink_operator import SinkOperator
 if TYPE_CHECKING:
     from sage_core.function.sink_function import SinkFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class SinkTransformation(BaseTransformation):
@@ -23,7 +23,5 @@ class SinkTransformation(BaseTransformation):
         super().__init__(env, function, *args, **kwargs)
 
 
-    @property
-    def is_sink(self) -> bool:
-        return True
+
 
diff --git a/sage_core/transformation/source_transformation.py b/sage_core/transformation/source_transformation.py
index 8fccc84..1405e4f 100644
--- a/sage_core/transformation/source_transformation.py
+++ b/sage_core/transformation/source_transformation.py
@@ -4,7 +4,7 @@ from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.source_operator import SourceOperator
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class SourceTransformation(BaseTransformation):
diff --git a/sage_examples/batch_example.py b/sage_examples/batch_example.py
deleted file mode 100644
index 712a2d2..0000000
--- a/sage_examples/batch_example.py
+++ /dev/null
@@ -1,242 +0,0 @@
-from sage_core.api.local_environment import LocalEnvironment
-from sage_core.api.remote_environment import RemoteEnvironment
-from sage_core.function.sink_function import SinkFunction
-from sage_core.function.source_function import SourceFunction, StopSignal
-import time
-import random
-
-class NumberSequenceSource(SourceFunction):
-    """
-    æ•°å­—åºåˆ—æº - ç”Ÿæˆæœ‰é™æ•°é‡çš„æ•°å­—ï¼Œç„¶åå‘é€åœæ­¢ä¿¡å·
-    """
-    def __init__(self, max_count=10, **kwargs):
-        super().__init__(**kwargs)
-        self.counter = 0
-        self.max_count = max_count
-        
-    def execute(self):
-        if self.counter >= self.max_count:
-            # æ•°æ®è€—å°½ï¼Œå‘é€åœæ­¢ä¿¡å·
-            return StopSignal(f"NumberSequence_{self.counter}")
-        
-        self.counter += 1
-        number = self.counter * 10 + random.randint(1, 9)
-        print(f"[Source] Generating number {self.counter}/{self.max_count}: {number}")
-        return number
-
-class FileLineSource(SourceFunction):
-    """
-    æ–‡ä»¶è¡Œæº - é€è¡Œè¯»å–æ–‡ä»¶ï¼Œè¯»å®Œåå‘é€åœæ­¢ä¿¡å·
-    """
-    def __init__(self, lines_data=None, **kwargs):
-        super().__init__(**kwargs)
-        # æ¨¡æ‹Ÿæ–‡ä»¶å†…å®¹
-        self.lines = lines_data or [
-            "Hello, SAGE batch processing!",
-            "Processing line by line...",
-            "Each line is processed independently.",
-            "This is a test of batch termination.",
-            "End of file reached."
-        ]
-        self.current_index = 0
-        
-    def execute(self):
-        if self.current_index >= len(self.lines):
-            # æ–‡ä»¶è¯»å®Œï¼Œå‘é€åœæ­¢ä¿¡å·
-            return StopSignal(f"FileReader_EOF")
-        
-        line = self.lines[self.current_index]
-        self.current_index += 1
-        print(f"[FileSource] Reading line {self.current_index}/{len(self.lines)}: {line}")
-        return line
-
-class CountdownSource(SourceFunction):
-    """
-    å€’è®¡æ—¶æº - ä»æŒ‡å®šæ•°å­—å€’æ•°åˆ°0ï¼Œç„¶åå‘é€åœæ­¢ä¿¡å·
-    """
-    def __init__(self, start_from=5, **kwargs):
-        super().__init__(**kwargs)
-        self.current_number = start_from
-        
-    def execute(self):
-        if self.current_number < 0:
-            # å€’è®¡æ—¶ç»“æŸï¼Œå‘é€åœæ­¢ä¿¡å·
-            return StopSignal(f"Countdown_Finished")
-        
-        result = self.current_number
-        print(f"[Countdown] T-minus {self.current_number}")
-        self.current_number -= 1
-        return result
-
-class BatchProcessor(SinkFunction):
-    """
-    æ‰¹å¤„ç†æ•°æ®æ¥æ”¶å™¨
-    """
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-        self.processed_count = 0
-        
-    def execute(self, data):
-        self.processed_count += 1
-        print(f"[Processor-{self.name}] Processed item #{self.processed_count}: {data}")
-        return data
-
-def run_simple_batch_test():
-    """æµ‹è¯•1: ç®€å•çš„æ•°å­—åºåˆ—æ‰¹å¤„ç†"""
-    print("ğŸ”¢ Test 1: Simple Number Sequence Batch Processing")
-    print("=" * 50)
-    
-    env = LocalEnvironment("simple_batch_test")
-    
-    # åˆ›å»ºæœ‰é™æ•°æ®æº
-    source_stream = env.from_source(NumberSequenceSource, max_count=5, delay=0.5)
-    
-    # å¤„ç†ç®¡é“
-    result = (source_stream
-        .map(lambda x: x * 2)  # æ•°å­—ç¿»å€
-        .filter(lambda x: x > 50)  # è¿‡æ»¤å¤§äº50çš„æ•°å­—
-        .sink(BatchProcessor, name="NumberProcessor")
-    )
-    
-    print("ğŸš€ Starting simple batch processing...")
-    print("ğŸ“Š Processing sequence: generate â†’ double â†’ filter â†’ sink")
-    print("â¹ï¸  Source will automatically stop after 5 numbers\n")
-    
-    # æäº¤å¹¶è¿è¡Œ
-    env.submit()
-    
-    print("\nâœ… Simple batch test completed!\n")
-
-def run_file_processing_test():
-    """æµ‹è¯•2: æ–‡ä»¶è¡Œæ‰¹å¤„ç†"""
-    print("ğŸ“„ Test 2: File Line Batch Processing") 
-    print("=" * 50)
-    
-    env = LocalEnvironment("file_batch_test")
-    
-    # æ¨¡æ‹Ÿæ–‡ä»¶æ•°æ®
-    file_data = [
-        "SAGE Framework",
-        "Distributed Stream Processing", 
-        "Batch Processing Support",
-        "Ray-based Architecture",
-        "Python Implementation"
-    ]
-    
-    source_stream = env.from_source(FileLineSource, lines_data=file_data, delay=0.8)
-    
-    # æ–‡æœ¬å¤„ç†ç®¡é“
-    result = (source_stream
-        .map(lambda line: line.upper())  # è½¬å¤§å†™
-        .map(lambda line: f"ğŸ“ {line}")   # æ·»åŠ å‰ç¼€
-        .sink(BatchProcessor, name="TextProcessor")
-    )
-    
-    print("ğŸš€ Starting file batch processing...")
-    print("ğŸ“Š Processing pipeline: read â†’ uppercase â†’ prefix â†’ sink")  
-    print("â¹ï¸  Source will automatically stop after reading all lines\n")
-    
-    # æäº¤å¹¶è¿è¡Œ
-    env.submit()
-    
-    print("\nâœ… File batch test completed!\n")
-
-def run_multi_source_batch_test():
-    """æµ‹è¯•3: å¤šæºæ‰¹å¤„ç†ï¼ˆå±•ç¤ºä¸åŒæºçš„ç»ˆæ­¢æ—¶æœºï¼‰"""
-    print("ğŸ”€ Test 3: Multi-Source Batch Processing")
-    print("=" * 50)
-    
-    env = LocalEnvironment("multi_source_batch_test")
-    
-    # åˆ›å»ºå¤šä¸ªä¸åŒé€Ÿåº¦çš„æ•°æ®æº
-    numbers_stream = env.from_source(NumberSequenceSource, max_count=3, delay=0.5)
-    countdown_stream = env.from_source(CountdownSource, start_from=2, delay=0.7)
-    
-    # åˆå¹¶æµå¤„ç†
-    combined_result = (numbers_stream
-        .connect(countdown_stream)  # åˆå¹¶ä¸¤ä¸ªæµ
-        .map(lambda x: f"Combined: {x}")
-        .sink(BatchProcessor, name="MultiSourceProcessor")
-    )
-    
-    print("ğŸš€ Starting multi-source batch processing...")
-    print("ğŸ“Š Two independent sources will terminate at different times")
-    print("â¹ï¸  Job will complete when ALL sources send stop signals\n")
-    
-    # æäº¤å¹¶è¿è¡Œ
-    env.submit()
-    
-    print("\nâœ… Multi-source batch test completed!\n")
-
-def run_processing_chain_test():
-    """æµ‹è¯•4: å¤æ‚å¤„ç†é“¾æ‰¹å¤„ç†"""
-    print("â›“ï¸  Test 4: Complex Processing Chain Batch")
-    print("=" * 50)
-    
-    env = RemoteEnvironment("complex_batch_test")  # ä½¿ç”¨è¿œç¨‹ç¯å¢ƒæµ‹è¯•åˆ†å¸ƒå¼æ‰¹å¤„ç†
-    
-    source_stream = env.from_source(NumberSequenceSource, max_count=8, delay=0.3)
-    
-    # å¤æ‚çš„å¤„ç†é“¾
-    result = (source_stream
-        .map(lambda x: x + 100)           # +100
-        .filter(lambda x: x % 2 == 0)     # åªä¿ç•™å¶æ•°
-        .map(lambda x: x / 2)             # é™¤ä»¥2
-        .map(lambda x: f"Result: {int(x)}")  # æ ¼å¼åŒ–
-        .sink(BatchProcessor, name="ChainProcessor")
-    )
-    
-    print("ğŸš€ Starting complex processing chain...")
-    print("ğŸ“Š Chain: source â†’ +100 â†’ filter_even â†’ /2 â†’ format â†’ sink")
-    print("ğŸŒ Running on distributed Ray cluster")
-    print("â¹ï¸  Automatic termination with batch lifecycle management\n")
-    
-    # æäº¤å¹¶è¿è¡Œ
-    env.submit()
-    
-    print("\nâœ… Complex batch test completed!\n")
-
-def main():
-    """ä¸»æµ‹è¯•å‡½æ•°"""
-    print("ğŸ¯ SAGE Batch Processing Tests with StopSignal")
-    print("=" * 60)
-    print("ğŸ§ª Testing automatic batch termination using StopSignal interface")
-    print("ğŸ“ˆ Each test demonstrates different batch processing scenarios\n")
-    
-    try:
-        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
-        run_simple_batch_test()
-        time.sleep(2)
-        
-        run_file_processing_test() 
-        time.sleep(2)
-        
-        run_multi_source_batch_test()
-        time.sleep(2)
-        
-        run_processing_chain_test()
-        
-    except KeyboardInterrupt:
-        print("\n\nğŸ›‘ Tests interrupted by user")
-        
-    finally:
-        print("\nğŸ“‹ Batch Processing Tests Summary:")
-        print("âœ… Test 1: Simple sequence - PASSED")
-        print("âœ… Test 2: File processing - PASSED") 
-        print("âœ… Test 3: Multi-source - PASSED")
-        print("âœ… Test 4: Complex chain - PASSED")
-        print("\nğŸ’¡ Key Features Demonstrated:")
-        print("   - StopSignal automatic termination")
-        print("   - Source-driven batch lifecycle")
-        print("   - Multi-source coordination")
-        print("   - Distributed batch processing")
-        print("   - Graceful job completion")
-        print("\nğŸ”„ StopSignal Workflow:")
-        print("   1. Source detects data exhaustion")
-        print("   2. Source returns StopSignal")
-        print("   3. SourceOperator propagates signal")
-        print("   4. Downstream nodes receive termination")
-        print("   5. Job gracefully completes")
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/sage_examples/comap_function_example.py b/sage_examples/comap_function_example.py
index 8b30897..618554d 100644
--- a/sage_examples/comap_function_example.py
+++ b/sage_examples/comap_function_example.py
@@ -1,5 +1,4 @@
-from sage_core.api.local_environment import LocalEnvironment
-from sage_core.api.remote_environment import RemoteEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.sink_function import SinkFunction
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.comap_function import BaseCoMapFunction
@@ -102,6 +101,8 @@ class TypeSpecificProcessor(BaseCoMapFunction):
 
 # æ±‡æ€»è¾“å‡ºå‡½æ•°
 class SensorSink(SinkFunction):
+    def __init__(self, **kwargs):
+        self.name = kwargs.get('name', 'SensorSink')
         
     def execute(self, data):
         if isinstance(data, dict) and 'alert' in data:
@@ -113,7 +114,7 @@ class SensorSink(SinkFunction):
 
 def main():
     # åˆ›å»ºç¯å¢ƒ
-    env = RemoteEnvironment("comap_function_example")
+    env = LocalEnvironment("comap_function_example")
     
     print("ğŸš€ Starting CoMap Function Example")
     print("ğŸŒ¡ï¸  Demonstrating multi-sensor data processing with CoMap")
@@ -145,7 +146,6 @@ def main():
     simple_result = (connected_sensors
         .comap(TypeSpecificProcessor)
         .print("ğŸ¯ Formatted Output")
-        
     )
     
     print("\nğŸ“ˆ All sensors connected and processing with CoMap...\n")
@@ -159,8 +159,8 @@ def main():
     try:
         # è¿è¡Œæµå¤„ç†
         env.submit()
-        
-        # time.sleep(40)  # è¿è¡Œ15ç§’ä»¥è§‚å¯Ÿä¸åŒé¢‘ç‡çš„æ•°æ®
+        env.run_streaming()
+        time.sleep(15)  # è¿è¡Œ15ç§’ä»¥è§‚å¯Ÿä¸åŒé¢‘ç‡çš„æ•°æ®
         
     except KeyboardInterrupt:
         print("\n\nğŸ›‘ Stopping CoMap Function Example...")
@@ -176,6 +176,7 @@ def main():
         print("\nğŸ”„ Comparison with regular map():")
         print("   - Regular map(): All inputs merged â†’ single execute() method")
         print("   - CoMap: Each input stream â†’ dedicated mapN() method")
+        env.close()
 
 if __name__ == "__main__":
     main()
diff --git a/sage_examples/comap_lambda_example.py b/sage_examples/comap_lambda_example.py
index b9a45ba..2893f8e 100644
--- a/sage_examples/comap_lambda_example.py
+++ b/sage_examples/comap_lambda_example.py
@@ -14,7 +14,7 @@ from typing import List, Any
 # Add the project root to Python path for imports
 sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 
 
@@ -41,7 +41,7 @@ def main():
     print("=" * 60)
     
     # Create environment
-    env1 = LocalStreamEnvironment()
+    env1 = LocalEnvironment()
     
     # Example 1: Lambda List Approach
     print("\nğŸ“‹ Example 1: Lambda List Approach")
@@ -75,7 +75,7 @@ def main():
     print("-" * 40)
     
     # Reset environment for new example
-    env2 = LocalStreamEnvironment()
+    env2 = LocalEnvironment()
     
     # Define named functions
     def format_temperature(data: float) -> str:
@@ -116,7 +116,7 @@ def main():
     print("-" * 40)
     
     # Reset environment for new example
-    env3 = LocalStreamEnvironment()
+    env3 = LocalEnvironment()
     
     # Define a complex processing function
     def process_numeric_data(data: float) -> str:
@@ -161,7 +161,7 @@ def main():
     print("-" * 40)
     
     # Reset environment for new example
-    env4 = LocalStreamEnvironment()
+    env4 = LocalEnvironment()
     
     # Create numeric data sources
     input1_source = env4.from_source(ListSource, [1, 2, 3, 4, 5])
@@ -196,7 +196,7 @@ def main():
     print("-" * 40)
     
     # Reset environment for new example
-    env5 = LocalStreamEnvironment()
+    env5 = LocalEnvironment()
     
     # Create data with potential issues
     mixed_data1 = env5.from_source(ListSource, [5, -3, 0, 12, -1])
diff --git a/sage_examples/connected_stream_example.py b/sage_examples/connected_stream_example.py
index 2d29b1a..f62dec8 100644
--- a/sage_examples/connected_stream_example.py
+++ b/sage_examples/connected_stream_example.py
@@ -1,8 +1,8 @@
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.sink_function import SinkFunction
 from sage_core.function.source_function import SourceFunction
 import time
-
+import random
 
 # ç®€å•çš„æ•°å­—æº
 class NumberSource(SourceFunction):
@@ -28,7 +28,7 @@ class StatsSink(SinkFunction):
 
 def main():
     # åˆ›å»ºç¯å¢ƒ
-    env = LocalStreamEnvironment("simple_connected_example")
+    env = LocalEnvironment("simple_connected_example")
     
     print("ğŸš€ Starting Simple Connected Streams Example")
     print("ğŸ“Š Demonstrating multiple stream processing and connection")
@@ -85,7 +85,7 @@ def main():
     try:
         # è¿è¡Œæµå¤„ç†
         env.submit()
-        
+        env.run_streaming()
         time.sleep(5)  # è¿è¡Œ5ç§’
         
     except KeyboardInterrupt:
diff --git a/sage_examples/external_memory_ingestion_pipeline.py b/sage_examples/external_memory_ingestion_pipeline.py
index bfc37f9..11157aa 100644
--- a/sage_examples/external_memory_ingestion_pipeline.py
+++ b/sage_examples/external_memory_ingestion_pipeline.py
@@ -1,16 +1,15 @@
 import logging
 import time
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.sink import MemWriteSink
 from sage_common_funs.io.source import FileSource
-
-from sage_libs.rag.chunk import CharacterSplitter
-from sage_libs.rag.writer import MemoryWriter
+from sage_libs.rag import CharacterSplitter
+from sage_libs.rag import MemoryWriter
 from sage_utils.config_loader import load_config
 
 
 def pipeline_run():
-    env = LocalStreamEnvironment(name="example_pipeline")
+    env = LocalEnvironment(name="example_pipeline")
     env.set_memory(config=None)  # åˆå§‹åŒ–å†…å­˜é…ç½®
 
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
@@ -19,7 +18,7 @@ def pipeline_run():
     memwrite_stream= chunk_stream.map(MemoryWriter,config["writer"])
     sink_stream= memwrite_stream.sink(MemWriteSink,config["sink"])
     env.submit()
-      # å¯åŠ¨ç®¡é“
+    env.run_streaming()  # å¯åŠ¨ç®¡é“
     time.sleep(100)  # ç­‰å¾…ç®¡é“è¿è¡Œ
 
 if __name__ == '__main__':
diff --git a/sage_examples/future_stream_example.py b/sage_examples/future_stream_example.py
index 4c2069c..407808a 100644
--- a/sage_examples/future_stream_example.py
+++ b/sage_examples/future_stream_example.py
@@ -1,4 +1,4 @@
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.sink_function import SinkFunction
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.comap_function import BaseCoMapFunction
@@ -93,7 +93,7 @@ class CounterSink(SinkFunction):
 
 def main():
     # åˆ›å»ºç¯å¢ƒ
-    env = LocalStreamEnvironment("future_stream_example")
+    env = LocalEnvironment("future_stream_example")
     
     print("ğŸš€ Starting Future Stream Example")
     print("ğŸ”„ Demonstrating feedback edges with a counting loop")
@@ -145,16 +145,21 @@ def main():
     print()
     
     print("âœ… Pipeline validation:")
-    print(f"   - Pipeline transformations: {len(env.pipeline)}")
+    print(f"   - Pipeline transformations: {len(env._pipeline)}")
+    print(f"   - Filled future streams: {list(env.get_filled_futures().keys())}")
+    print(f"   - Has unfilled futures: {env.has_unfilled_futures()}")
     
     try:
+        # éªŒè¯pipelineå¯ç¼–è¯‘æ€§
+        env.validate_pipeline_for_compilation()
+        print("âœ… Pipeline validation passed - ready to run!\n")
         
         print("ğŸ¬ Starting feedback loop execution...")
         print("ğŸ“ˆ Watch the counter increment in a feedback loop:\n")
         
         # è¿è¡Œæµå¤„ç†
         env.submit()
-        
+        env.run_streaming()
         time.sleep(15)  # è¿è¡Œ15ç§’ï¼Œè¶³å¤Ÿè®¡æ•°åˆ°10
         
     except KeyboardInterrupt:
diff --git a/sage_examples/kafka_query.py b/sage_examples/kafka_query.py
index ce0a343..4915348 100644
--- a/sage_examples/kafka_query.py
+++ b/sage_examples/kafka_query.py
@@ -2,13 +2,15 @@ import logging
 import json
 import threading
 import time
+from kafka import KafkaProducer
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
 from sage_utils.config_loader import load_config
+from sage_utils.custom_logger import CustomLogger
 from sage_utils.logging_utils import configure_logging
 
 
@@ -111,7 +113,7 @@ def extract_query_from_kafka(kafka_data):
 
 def pipeline_run():
     """åˆ›å»ºå¹¶è¿è¡ŒåŸºäºKafkaçš„æ•°æ®å¤„ç†ç®¡é“"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)  # åˆå§‹åŒ–å†…å­˜é…ç½®
     
     # åˆ›å»ºKafkaæ•°æ®æº
@@ -134,12 +136,23 @@ def pipeline_run():
     response_stream.sink(TerminalSink, config["sink"])
 
     # æäº¤ç®¡é“å¹¶è¿è¡Œ
-    env.submit()
-
+    env.submit(name="kafka_rag_pipeline")
+    
+    # åœ¨åå°çº¿ç¨‹å¯åŠ¨æµå¤„ç†
+    def run_pipeline():
+        try:
+            env.run_streaming()
+        except Exception as e:
+            logging.error(f"Pipeline error: {e}")
+    
+    pipeline_thread = threading.Thread(target=run_pipeline, daemon=True)
+    pipeline_thread.start()
     
     # ç­‰å¾…pipelineå¯åŠ¨
     time.sleep(2)
     logging.info("Kafka RAG pipeline started successfully")
+    
+    return pipeline_thread
 
 
 def interactive_mode():
@@ -211,6 +224,7 @@ def main():
 
 if __name__ == '__main__':
     # é…ç½®æ—¥å¿—
+    CustomLogger.disable_global_console_debug()
     configure_logging(level=logging.INFO)
     
     # åŠ è½½é…ç½®
diff --git a/sage_examples/multiagent_app.py b/sage_examples/multiagent_app.py
index 83bc86d..6291986 100644
--- a/sage_examples/multiagent_app.py
+++ b/sage_examples/multiagent_app.py
@@ -1,5 +1,6 @@
-import time
-from sage_core.api.local_environment import LocalStreamEnvironment
+from dotenv import load_dotenv
+import os, time
+from sage_core.api.env import LocalEnvironment, RemoteEnvironment
 from sage_utils.config_loader import load_config
 from sage_libs.agents.question_bot import QuestionBot
 from sage_libs.agents.chief_bot import ChiefBot
@@ -10,9 +11,11 @@ from sage_libs.agents.answer_bot import AnswerBot
 from sage_libs.agents.critic_bot import CriticBot
 from sage_libs.utils.tool_filter import ToolFilter
 
+
+
 def pipeline_run():
     """åˆ›å»ºå¹¶è¿è¡Œæ•°æ®å¤„ç†ç®¡é“"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     chief_stream = (
@@ -36,12 +39,25 @@ def pipeline_run():
 
 
     env.submit()
+    env.run_once()
     try:
         time.sleep(60)
     except KeyboardInterrupt:
         print("\nğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
     env.stop()
     env.close()
+    # try:
+    #     env.submit()
+    #     env.run_streaming() # å¼€é”€æœ‰ç‚¹å¤§ï¼Œæœ€å¥½åªæ¶¦ä¸€æ¬¡åšæµ‹è¯•
+    #     print("ğŸŒ± ç®¡é“å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C ä¸­æ–­")
+    #     while True:
+    #         time.sleep(1)  # æŒç»­è¿è¡Œç›´åˆ°è¢«æ‰“æ–­
+    # except KeyboardInterrupt:
+    #     print("\nğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
+    #     env.stop()
+    # finally:
+    #     env.close()
+    #     print("âœ… ç®¡é“å·²å®‰å…¨å…³é—­")
 
 if __name__ == '__main__':
     config = load_config("multiagent_config.yaml")
diff --git a/sage_examples/multiple_pipeline.py b/sage_examples/multiple_pipeline.py
index e032771..5f0330a 100644
--- a/sage_examples/multiple_pipeline.py
+++ b/sage_examples/multiple_pipeline.py
@@ -1,30 +1,29 @@
 import logging
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.sink import MemWriteSink, FileSink
 from sage_common_funs.io.source import FileSource
-
-from sage_libs.rag.chunk import CharacterSplitter
+from sage_libs.rag import CharacterSplitter
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
-from sage_libs.rag.writer import MemoryWriter
+from sage_libs.rag import MemoryWriter
 
 from sage_utils.config_loader import load_config
 
 def ingest_pipeline_run():
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
     source_stream = env.from_source(FileSource, config_for_ingest["source"])
     chunk_stream = source_stream.map(CharacterSplitter,config_for_ingest["chunk"])
     memwrite_stream= chunk_stream.map(MemoryWriter,config_for_ingest["writer"])
     sink_stream= memwrite_stream.sink(MemWriteSink,config_for_ingest["sink"])
     env.submit()
-      # å¯åŠ¨ç®¡é“
+    env.run_streaming()  # å¯åŠ¨ç®¡é“
 
 def qa_pipeline_run():
     """åˆ›å»ºå¹¶è¿è¡Œæ•°æ®å¤„ç†ç®¡é“"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory()
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
     query_stream = env.from_source(FileSource, config_for_qa["source"])
@@ -34,7 +33,7 @@ def qa_pipeline_run():
     response_stream.sink(FileSink, config_for_qa["sink"])
     # æäº¤ç®¡é“å¹¶è¿è¡Œ
     env.submit()
-      # å¯åŠ¨ç®¡é“
+    env.run_streaming()  # å¯åŠ¨ç®¡é“
 
 if __name__ == '__main__':
     # åŠ è½½é…ç½®å¹¶åˆå§‹åŒ–æ—¥å¿—
diff --git a/sage_examples/neuromem_examples/experiment/memrun/locomo_memprompt_run.py b/sage_examples/neuromem_examples/experiment/memrun/locomo_memprompt_run.py
index 2344aa9..570ba1b 100644
--- a/sage_examples/neuromem_examples/experiment/memrun/locomo_memprompt_run.py
+++ b/sage_examples/neuromem_examples/experiment/memrun/locomo_memprompt_run.py
@@ -6,6 +6,9 @@ import os
 os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
 
 from tqdm import tqdm
+from sage_memory.api import get_memory, get_manager
+from sage_utils.config_loader import load_config
+from sage_utils.embedding_model import apply_embedding_model
 from data.neuromem_datasets.locomo_dataloader import LocomoDataLoader
 
 # manager = get_manager()
diff --git a/sage_examples/neuromem_examples/experiment/prefill/locomo_memprompt_prefill.py b/sage_examples/neuromem_examples/experiment/prefill/locomo_memprompt_prefill.py
index 9bbb9d3..5af70ab 100644
--- a/sage_examples/neuromem_examples/experiment/prefill/locomo_memprompt_prefill.py
+++ b/sage_examples/neuromem_examples/experiment/prefill/locomo_memprompt_prefill.py
@@ -83,7 +83,7 @@ manager.store_collection()
 #     response_stream.sink(TerminalSink, config["sink"])
 #     # æäº¤ç®¡é“å¹¶è¿è¡Œ
 #     env.submit()
-#       # å¯åŠ¨ç®¡é“
+#     env.run_streaming()  # å¯åŠ¨ç®¡é“
 
 #     # time.sleep(100)  # ç­‰å¾…ç®¡é“è¿è¡Œ
 
diff --git a/sage_examples/persistent_jobmanager_example.py b/sage_examples/persistent_jobmanager_example.py
deleted file mode 100644
index 085f7da..0000000
--- a/sage_examples/persistent_jobmanager_example.py
+++ /dev/null
@@ -1,297 +0,0 @@
-"""
-JobManageræŒä¹…åŒ–Actorå®ç°ç¤ºä¾‹
-å±•ç¤ºå¦‚ä½•åœ¨SAGEç³»ç»Ÿä¸­åˆ›å»ºæŒä¹…åŒ–çš„JobManager Actor
-"""
-import ray
-import time
-import threading
-from typing import Dict, Any, Optional
-from sage_jobmanager.job_manager import JobManager
-from draft.persistent_actor_manager import create_persistent_actor
-from sage_utils.custom_logger import CustomLogger
-
-
-@ray.remote
-class PersistentJobManagerActor(JobManager):
-    """æŒä¹…åŒ–JobManager Actor - ç»•è¿‡_ray_weak_refé™åˆ¶"""
-    
-    def __init__(self, *args, **kwargs):
-        # ç¦ç”¨TCPæœåŠ¡å™¨ï¼Œä½¿ç”¨Rayé€šä¿¡
-        kwargs['enable_tcp_server'] = False
-        super().__init__(*args, **kwargs)
-        
-        # Ray Actorç‰¹æœ‰çš„è®¾ç½®
-        self.actor_id = ray.get_runtime_context().get_actor_id()
-        self.start_time = time.time()
-        self.heartbeat_count = 0
-        
-        self.logger.info(f"PersistentJobManagerActor initialized: {self.actor_id}")
-
-    def keep_alive(self) -> Dict[str, Any]:
-        """å¿ƒè·³ä¿æ´»æ–¹æ³• - é˜²æ­¢Actorè¢«å›æ”¶"""
-        self.heartbeat_count += 1
-        current_time = time.time()
-        uptime = current_time - self.start_time
-        
-        return {
-            "status": "alive",
-            "actor_id": self.actor_id,
-            "timestamp": current_time,
-            "uptime_seconds": uptime,
-            "heartbeat_count": self.heartbeat_count
-        }
-
-    def get_actor_info(self) -> Dict[str, Any]:
-        """è·å–Actorä¿¡æ¯"""
-        return {
-            "actor_id": self.actor_id,
-            "start_time": self.start_time,
-            "uptime": time.time() - self.start_time,
-            "heartbeat_count": self.heartbeat_count,
-            "weak_ref_info": "Using persistent actor strategies to bypass _ray_weak_ref"
-        }
-
-    def health_check(self) -> Dict[str, Any]:
-        """å¥åº·æ£€æŸ¥"""
-        try:
-            # æ‰§è¡Œä¸€äº›åŸºæœ¬çš„å¥åº·æ£€æŸ¥
-            status = {
-                "status": "healthy",
-                "actor_id": self.actor_id,
-                "uptime": time.time() - self.start_time,
-                "active_environments": len(getattr(self, 'environments', {})),
-                "memory_usage": "N/A"  # å¯ä»¥æ·»åŠ å†…å­˜ä½¿ç”¨æƒ…å†µ
-            }
-            return status
-        except Exception as e:
-            return {
-                "status": "unhealthy",
-                "error": str(e),
-                "actor_id": self.actor_id
-            }
-
-    def graceful_shutdown(self) -> Dict[str, Any]:
-        """ä¼˜é›…å…³é—­Actor"""
-        self.logger.info(f"PersistentJobManagerActor {self.actor_id} shutting down...")
-        try:
-            # æ¸…ç†èµ„æº
-            if hasattr(self, 'environments'):
-                for env_uuid in list(self.jobs.keys()):
-                    try:
-                        self.pause_job(env_uuid)
-                    except Exception as e:
-                        self.logger.error(f"Error stopping environment {env_uuid}: {e}")
-            
-            return {
-                "status": "shutdown_complete",
-                "actor_id": self.actor_id,
-                "final_heartbeat_count": self.heartbeat_count
-            }
-        except Exception as e:
-            return {
-                "status": "shutdown_error",
-                "error": str(e),
-                "actor_id": self.actor_id
-            }
-
-
-class JobManagerActorService:
-    """JobManager ActoræœåŠ¡ç®¡ç†å™¨"""
-    
-    def __init__(self, actor_name: str = "sage_jobmanager", namespace: str = "sage"):
-        self.actor_name = actor_name
-        self.namespace = namespace
-        self.logger = CustomLogger()
-        self._actor_handle: Optional[ray.actor.ActorHandle] = None
-        self._persistence_method = "detached"  # é»˜è®¤ä½¿ç”¨detachedæ–¹æ³•
-
-    def start_persistent_jobmanager(
-        self,
-        method: str = "detached",
-        **actor_options
-    ) -> ray.actor.ActorHandle:
-        """
-        å¯åŠ¨æŒä¹…åŒ–JobManager Actor
-        
-        Args:
-            method: æŒä¹…åŒ–æ–¹æ³• ("detached", "named", "resource_locked")
-            **actor_options: å…¶ä»–Actoré€‰é¡¹
-        
-        Returns:
-            Ray ActorHandle
-        """
-        try:
-            self.logger.info(f"Starting persistent JobManager using method: {method}")
-            
-            # æ ¹æ®ä¸åŒæ–¹æ³•è®¾ç½®ä¸åŒçš„é€‰é¡¹
-            if method == "detached":
-                # æ–¹æ¡ˆ1ï¼šä½¿ç”¨detachedç”Ÿå‘½å‘¨æœŸï¼ˆæœ€æ¨èï¼‰
-                actor_handle = create_persistent_actor(
-                    PersistentJobManagerActor,
-                    name=self.actor_name,
-                    method="detached",
-                    namespace=self.namespace,
-                    max_restarts=3,
-                    resources={"jobmanager": 1.0},  # èµ„æºå ç”¨é˜²æ­¢å›æ”¶
-                    **actor_options
-                )
-                
-            elif method == "named":
-                # æ–¹æ¡ˆ2ï¼šå‘½åActor + å¿ƒè·³ä¿æ´»
-                actor_handle = create_persistent_actor(
-                    PersistentJobManagerActor,
-                    name=self.actor_name,
-                    method="named",
-                    namespace=self.namespace,
-                    keepalive_interval=30,  # 30ç§’å¿ƒè·³
-                    **actor_options
-                )
-                
-            elif method == "resource_locked":
-                # æ–¹æ¡ˆ3ï¼šèµ„æºé”å®š
-                actor_handle = create_persistent_actor(
-                    PersistentJobManagerActor,
-                    name=self.actor_name,
-                    method="resource_locked",
-                    namespace=self.namespace,
-                    resource_name="jobmanager_lock",
-                    resource_amount=1.0,
-                    **actor_options
-                )
-            else:
-                raise ValueError(f"Unknown persistence method: {method}")
-
-            self._actor_handle = actor_handle
-            self._persistence_method = method
-            
-            # éªŒè¯Actoræ˜¯å¦æ­£å¸¸å¯åŠ¨
-            actor_info = ray.get(actor_handle.get_actor_info.remote(), timeout=10)
-            self.logger.info(f"JobManager Actor started successfully: {actor_info}")
-            
-            return actor_handle
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start persistent JobManager: {e}")
-            raise
-
-    def get_or_create_jobmanager(self, **options) -> ray.actor.ActorHandle:
-        """è·å–ç°æœ‰JobManageræˆ–åˆ›å»ºæ–°çš„"""
-        try:
-            # å°è¯•è·å–ç°æœ‰Actor
-            existing_actor = ray.get_actor(self.actor_name, namespace=self.namespace)
-            
-            # æ£€æŸ¥Actoræ˜¯å¦å¥åº·
-            health_status = ray.get(existing_actor.health_check.remote(), timeout=5)
-            if health_status.get("status") == "healthy":
-                self.logger.info("Found existing healthy JobManager Actor")
-                self._actor_handle = existing_actor
-                return existing_actor
-            else:
-                self.logger.warning(f"Existing Actor unhealthy: {health_status}")
-                
-        except (ValueError, Exception) as e:
-            self.logger.info(f"No existing JobManager Actor found: {e}")
-        
-        # åˆ›å»ºæ–°çš„æŒä¹…åŒ–Actor
-        return self.start_persistent_jobmanager(**options)
-
-    def stop_jobmanager(self, graceful: bool = True) -> bool:
-        """åœæ­¢JobManager Actor"""
-        if not self._actor_handle:
-            self.logger.warning("No JobManager Actor to stop")
-            return True
-            
-        try:
-            if graceful:
-                # ä¼˜é›…å…³é—­
-                shutdown_info = ray.get(
-                    self._actor_handle.graceful_shutdown.remote(), 
-                    timeout=30
-                )
-                self.logger.info(f"JobManager graceful shutdown: {shutdown_info}")
-            
-            # æ€æ­»Actor
-            ray.kill(self._actor_handle)
-            self._actor_handle = None
-            
-            self.logger.info("JobManager Actor stopped successfully")
-            return True
-            
-        except Exception as e:
-            self.logger.error(f"Error stopping JobManager Actor: {e}")
-            return False
-
-    def get_actor_handle(self) -> Optional[ray.actor.ActorHandle]:
-        """è·å–å½“å‰Actorå¥æŸ„"""
-        return self._actor_handle
-
-    def monitor_actor_health(self, interval: int = 60) -> threading.Thread:
-        """å¯åŠ¨Actorå¥åº·ç›‘æ§çº¿ç¨‹"""
-        def monitor_worker():
-            while self._actor_handle:
-                try:
-                    health_status = ray.get(
-                        self._actor_handle.health_check.remote(), 
-                        timeout=10
-                    )
-                    
-                    if health_status.get("status") != "healthy":
-                        self.logger.warning(f"JobManager Actor unhealthy: {health_status}")
-                        # å¯ä»¥åœ¨è¿™é‡Œå®ç°é‡å¯é€»è¾‘
-                    else:
-                        self.logger.debug(f"JobManager Actor healthy: {health_status}")
-                    
-                    time.sleep(interval)
-                    
-                except Exception as e:
-                    self.logger.error(f"Health check failed: {e}")
-                    time.sleep(interval)
-        
-        thread = threading.Thread(
-            target=monitor_worker,
-            name="JobManagerHealthMonitor",
-            daemon=True
-        )
-        thread.start()
-        self.logger.info("Started JobManager health monitoring")
-        return thread
-
-
-# ä½¿ç”¨ç¤ºä¾‹
-def example_usage():
-    """å±•ç¤ºå¦‚ä½•ä½¿ç”¨æŒä¹…åŒ–JobManager Actor"""
-    
-    # ç¡®ä¿Rayå·²åˆå§‹åŒ–
-    if not ray.is_initialized():
-        ray.init(address="auto", ignore_reinit_error=True)
-    
-    # åˆ›å»ºJobManageræœåŠ¡
-    jm_service = JobManagerActorService()
-    
-    # æ–¹æ³•1ï¼šä½¿ç”¨detachedç”Ÿå‘½å‘¨æœŸï¼ˆæ¨èï¼‰
-    print("=== æ–¹æ³•1ï¼šDetachedç”Ÿå‘½å‘¨æœŸ ===")
-    jobmanager_actor = jm_service.start_persistent_jobmanager(method="detached")
-    
-    # æµ‹è¯•ActoråŠŸèƒ½
-    actor_info = ray.get(jobmanager_actor.get_actor_info.remote())
-    print(f"Actor Info: {actor_info}")
-    
-    # å¿ƒè·³æµ‹è¯•
-    heartbeat = ray.get(jobmanager_actor.keep_alive.remote())
-    print(f"Heartbeat: {heartbeat}")
-    
-    # å¥åº·æ£€æŸ¥
-    health = ray.get(jobmanager_actor.health_check.remote())
-    print(f"Health: {health}")
-    
-    # å¯åŠ¨å¥åº·ç›‘æ§
-    monitor_thread = jm_service.monitor_actor_health(interval=30)
-    
-    print("JobManager Actor is now persistent and resistant to _ray_weak_ref issues")
-    print("The actor will survive even if the original handle goes out of scope")
-    
-    return jobmanager_actor
-
-
-if __name__ == "__main__":
-    example_usage()
diff --git a/sage_examples/qa_bm25_retrieval.py b/sage_examples/qa_bm25_retrieval.py
index 7f80180..abff4bb 100644
--- a/sage_examples/qa_bm25_retrieval.py
+++ b/sage_examples/qa_bm25_retrieval.py
@@ -1,7 +1,7 @@
 
 import logging
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.sink import TerminalSink
 from sage_common_funs.io.source import FileSource
 from sage_libs.rag.generator import OpenAIGenerator
@@ -13,7 +13,7 @@ from sage_utils.logging_utils import configure_logging
 
 def pipeline_run():
     """åˆ›å»ºå¹¶è¿è¡Œæ•°æ®å¤„ç†ç®¡é“"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
     query_stream = env.from_source(FileSource, config["source"])
@@ -23,7 +23,7 @@ def pipeline_run():
     response_stream.sink(TerminalSink, config["sink"])
     # æäº¤ç®¡é“å¹¶è¿è¡Œ
     env.submit()
-      # å¯åŠ¨ç®¡é“
+    env.run_streaming()  # å¯åŠ¨ç®¡é“
 
     # time.sleep(100)  # ç­‰å¾…ç®¡é“è¿è¡Œ
 
diff --git a/sage_examples/qa_bm25_retrieval_edit.py b/sage_examples/qa_bm25_retrieval_edit.py
deleted file mode 100644
index 7f80180..0000000
--- a/sage_examples/qa_bm25_retrieval_edit.py
+++ /dev/null
@@ -1,35 +0,0 @@
-
-import logging
-
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_common_funs.io.sink import TerminalSink
-from sage_common_funs.io.source import FileSource
-from sage_libs.rag.generator import OpenAIGenerator
-from sage_libs.rag.promptor import QAPromptor
-from sage_libs.rag.retriever import BM25sRetriever
-from sage_utils.config_loader import load_config
-from sage_utils.logging_utils import configure_logging
-
-
-def pipeline_run():
-    """åˆ›å»ºå¹¶è¿è¡Œæ•°æ®å¤„ç†ç®¡é“"""
-    env = LocalStreamEnvironment()
-    env.set_memory(config=None)
-    # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
-    query_stream = env.from_source(FileSource, config["source"])
-    query_and_chunks_stream = query_stream.map(BM25sRetriever, config["retriever"])
-    prompt_stream = query_and_chunks_stream.map(QAPromptor, config["promptor"])
-    response_stream = prompt_stream.map(OpenAIGenerator, config["generator"])
-    response_stream.sink(TerminalSink, config["sink"])
-    # æäº¤ç®¡é“å¹¶è¿è¡Œ
-    env.submit()
-      # å¯åŠ¨ç®¡é“
-
-    # time.sleep(100)  # ç­‰å¾…ç®¡é“è¿è¡Œ
-
-if __name__ == '__main__':
-    # åŠ è½½é…ç½®å¹¶åˆå§‹åŒ–æ—¥å¿—
-    config = load_config('config_bm25s.yaml')
-    configure_logging(level=logging.INFO)
-    # åˆå§‹åŒ–å†…å­˜å¹¶è¿è¡Œç®¡é“
-    pipeline_run()
\ No newline at end of file
diff --git a/sage_examples/qa_dense_retrieval.py b/sage_examples/qa_dense_retrieval.py
index 25a0d37..316d73e 100644
--- a/sage_examples/qa_dense_retrieval.py
+++ b/sage_examples/qa_dense_retrieval.py
@@ -1,6 +1,7 @@
+import time
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
-from sage_core.api.remote_environment import RemoteBatchEnvironment
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -9,34 +10,21 @@ from sage_utils.config_loader import load_config
 
 def pipeline_run():
     """åˆ›å»ºå¹¶è¿è¡Œæ•°æ®å¤„ç†ç®¡é“"""
-    # env = LocalBatchEnvironment() #DEBUG and Batch -- Client æ‹¥æœ‰åç»­ç¨‹åºçš„å…¨éƒ¨handlerï¼ˆåŒ…æ‹¬JMï¼‰
-    env = RemoteBatchEnvironment("JM-IP")  # Deployment to JM. -- Client ä¸æ‹¥æœ‰åç»­ç¨‹åºçš„å…¨éƒ¨handlerï¼ˆåŒ…æ‹¬JMï¼‰
-
-    # Batch Environment.
-
-    query_stream = (env
-                    .process(FileSource, config["source"]) # å¤„ç†ä¸”å¤„ç†ä¸€æ•´ä¸ªfile ä¸€æ¬¡ã€‚
-                    .map(DenseRetriever, config["retriever"])
-                    .map(QAPromptor, config["promptor"])
-                    .map(OpenAIGenerator, config["generator"])
-                    .sink(TerminalSink, config["sink"]) # TM (JVM) --> ä¼šæ‰“å°åœ¨æŸä¸€å°æœºå™¨çš„consoleé‡Œ
-                    )
-
-    # Streaming Environment.
+    env = LocalEnvironment()
     env.set_memory(config=None)
-    # env = LocalStreamEnvironment() #DEBUG and Streaming
-    # env = RemoteStreamEnvironment("JM-IP")  # Deployment to JM.
-
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
     query_stream = (env
                     .from_source(FileSource, config["source"])
                     .map(DenseRetriever, config["retriever"])
                     .map(QAPromptor, config["promptor"])
                     .map(OpenAIGenerator, config["generator"])
-                    .sink(TerminalSink, config["sink"]) # TM (JVM) --> ä¼šæ‰“å°åœ¨æŸä¸€å°æœºå™¨çš„consoleé‡Œ
+                    .sink(TerminalSink, config["sink"])
                     )
     try:
         env.submit()
+        env.run_once()  # å¯åŠ¨ç®¡é“
+        time.sleep(15)  # ç­‰å¾…ç®¡é“è¿è¡Œ
+        env.stop()
     finally:
         env.close()
 
diff --git a/sage_examples/qa_dense_retrieval_mixed.py b/sage_examples/qa_dense_retrieval_mixed.py
index f91d5c3..756c5e6 100644
--- a/sage_examples/qa_dense_retrieval_mixed.py
+++ b/sage_examples/qa_dense_retrieval_mixed.py
@@ -2,7 +2,7 @@ import logging
 import time
 from dotenv import load_dotenv
 import os
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
@@ -13,7 +13,7 @@ from sage_utils.logging_utils import configure_logging
 
 def pipeline_run():
     """åˆ›å»ºå¹¶è¿è¡Œæ•°æ®å¤„ç†ç®¡é“"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
     query_stream = env.from_source(FileSource, config["source"])
@@ -23,6 +23,8 @@ def pipeline_run():
     response_stream.sink(TerminalSink, config["sink"])
     # æäº¤ç®¡é“å¹¶è¿è¡Œ
     env.submit()
+    env.run_streaming()  # å¯åŠ¨ç®¡é“
+
     time.sleep(100)  # ç­‰å¾…ç®¡é“è¿è¡Œ
 
 
diff --git a/sage_examples/qa_dense_retrieval_ray.py b/sage_examples/qa_dense_retrieval_ray.py
index 391a4a9..97c3e0f 100644
--- a/sage_examples/qa_dense_retrieval_ray.py
+++ b/sage_examples/qa_dense_retrieval_ray.py
@@ -2,9 +2,9 @@ import logging
 import time
 from dotenv import load_dotenv
 import os
-from sage_core.api.remote_environment import RemoteStreamEnvironment
+from sage_core.api.env import RemoteEnvironment
 from sage_common_funs.io.source import FileSource
-from sage_common_funs.io.sink import FileSink
+from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -13,32 +13,33 @@ from sage_utils.logging_utils import configure_logging
 
 def pipeline_run():
     """åˆ›å»ºå¹¶è¿è¡Œæ•°æ®å¤„ç†ç®¡é“"""
-    env = RemoteStreamEnvironment(name="example_pipeline")
+    env = RemoteEnvironment(name="example_pipeline")
     env.set_memory(config = {"collection_name": "example_collection"})
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
     query_stream = env.from_source(FileSource, config["source"])
     query_and_chunks_stream = query_stream.map(DenseRetriever, config["retriever"])
     prompt_stream = query_and_chunks_stream.map(QAPromptor, config["promptor"])
     response_stream = prompt_stream.map(OpenAIGenerator, config["generator"])
-    response_stream.sink(FileSink, config["sink"])
+    response_stream.sink(TerminalSink, config["sink"])
     # æäº¤ç®¡é“å¹¶è¿è¡Œ
     env.submit()
-      # å¯åŠ¨ç®¡é“
+    env.run_streaming()  # å¯åŠ¨ç®¡é“
     time.sleep(5)
-
-
-
-
-    env2 = RemoteStreamEnvironment(name="example_pipeline2")
+    env.stop()  # åœæ­¢ç®¡é“
+    time.sleep(2)
+    env2 = RemoteEnvironment(name="example_pipeline2")
     env2.set_memory(config={"collection_name": "example_collection2"})
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
     query_stream2 = env2.from_source(FileSource, config["source"])
     query_and_chunks_stream2 = query_stream2.map(DenseRetriever, config["retriever"])
     prompt_stream2 = query_and_chunks_stream2.map(QAPromptor, config["promptor"])
     response_stream2 = prompt_stream2.map(OpenAIGenerator, config["generator"])
-    response_stream2.sink(FileSink, config["sink"])
+    response_stream2.sink(TerminalSink, config["sink"])
     # æäº¤ç®¡é“å¹¶è¿è¡Œ
     env2.submit()
+    env2.run_streaming()  # å¯åŠ¨ç®¡é“
+    time.sleep(50)
+    env.stop()  # åœæ­¢ç®¡é“
     time.sleep(1000)
 
 if __name__ == '__main__':
diff --git a/sage_examples/qa_evaluate.py b/sage_examples/qa_evaluate.py
index 075bcdf..c146bd1 100644
--- a/sage_examples/qa_evaluate.py
+++ b/sage_examples/qa_evaluate.py
@@ -1,5 +1,5 @@
 import time
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.map_function import MapFunction
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
@@ -36,7 +36,7 @@ class ResultFormatter(MapFunction):
         return (reference, generated)
 
 def pipeline_run(config):
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     (env
@@ -47,6 +47,7 @@ def pipeline_run(config):
      )
     try:
         env.submit()
+        env.run_streaming()
         time.sleep(5)
         env.stop()
     finally:
diff --git a/sage_examples/qa_hf.py b/sage_examples/qa_hf.py
index 83f2cf6..baab21c 100644
--- a/sage_examples/qa_hf.py
+++ b/sage_examples/qa_hf.py
@@ -1,11 +1,10 @@
 import logging
 import time
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
-
-from sage_libs.rag.generator import HFGenerator
+from sage_libs.rag import HFGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
 from sage_utils.config_loader import load_config
@@ -19,7 +18,7 @@ def pipeline_run(config: dict) -> None:
     Args:
         config (dict): åŒ…å«å„ä¸ªç»„ä»¶é…ç½®çš„å­—å…¸ã€‚
     """
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
@@ -33,7 +32,7 @@ def pipeline_run(config: dict) -> None:
 
     # æäº¤ç®¡é“å¹¶è¿è¡Œä¸€æ¬¡
     env.submit()
-    
+    env.run_streaming()
     time.sleep(5)  # ç­‰å¾…ç®¡é“è¿è¡Œ
     env.close()
 
diff --git a/sage_examples/qa_multiplex.py b/sage_examples/qa_multiplex.py
index 88443e1..324e810 100644
--- a/sage_examples/qa_multiplex.py
+++ b/sage_examples/qa_multiplex.py
@@ -1,7 +1,7 @@
 import time
 from dotenv import load_dotenv
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink, FileSink
 from sage_libs.rag.generator import OpenAIGenerator
@@ -19,7 +19,7 @@ def pipeline_run(config):
         config (dict): The configuration parameters loaded from the config file.
     """
     try:
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         env.set_memory(config=None)  # Set environment memory if required.
 
         # Constructing the data processing pipeline
@@ -47,7 +47,7 @@ def pipeline_run(config):
 
         # Submit and run the pipeline
         env.submit()
-        
+        env.run_streaming()
 
         # Optional: Wait for 10 seconds before ending the pipeline (if necessary)
         time.sleep(10)
diff --git a/sage_examples/qa_openai.py b/sage_examples/qa_openai.py
index 1ce7de7..c47db74 100644
--- a/sage_examples/qa_openai.py
+++ b/sage_examples/qa_openai.py
@@ -1,9 +1,9 @@
 import time
 from dotenv import load_dotenv
 
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_libs.io.sink import TerminalSink
-from sage_libs.io.source import FileSource
+from sage_core.api.env import LocalEnvironment
+from sage_common_funs.io.source import FileSource
+from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -17,7 +17,7 @@ def pipeline_run(config: dict) -> None:
     Args:
         config (dict): åŒ…å«å„æ¨¡å—é…ç½®çš„é…ç½®å­—å…¸ã€‚
     """
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
@@ -30,9 +30,8 @@ def pipeline_run(config: dict) -> None:
     )
 
     env.submit()
-
-    time.sleep(5)  # ç­‰å¾…ç®¡é“è¿è¡Œ5ç§’
-
+    env.run_once()
+    time.sleep(5)  # ç­‰å¾…ç®¡é“è¿è¡Œ
     env.close()
 
 
diff --git a/sage_examples/qa_openai_chat_history.py b/sage_examples/qa_openai_chat_history.py
index 3addcde..56697c7 100644
--- a/sage_examples/qa_openai_chat_history.py
+++ b/sage_examples/qa_openai_chat_history.py
@@ -1,11 +1,10 @@
 import time
 from dotenv import load_dotenv
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
-
-from sage_libs.rag.generator import OpenAIGeneratorWithHistory
+from sage_libs.rag import OpenAIGeneratorWithHistory
 from sage_libs.rag.promptor import QAPromptor
 from sage_utils.config_loader import load_config
 
@@ -17,7 +16,7 @@ def pipeline_run(config: dict) -> None:
     Args:
         config (dict): åŒ…å«å„æ¨¡å—é…ç½®çš„é…ç½®å­—å…¸ã€‚
     """
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
@@ -29,7 +28,7 @@ def pipeline_run(config: dict) -> None:
     )
 
     env.submit()
-    
+    env.run_streaming()
     time.sleep(5)  # ç­‰å¾…ç®¡é“è¿è¡Œ
     env.close()
 
diff --git a/sage_tests/example_tests/qa_recycle_test.py b/sage_examples/qa_recycle_test.py
similarity index 95%
rename from sage_tests/example_tests/qa_recycle_test.py
rename to sage_examples/qa_recycle_test.py
index ea01a6a..21f16db 100644
--- a/sage_tests/example_tests/qa_recycle_test.py
+++ b/sage_examples/qa_recycle_test.py
@@ -1,6 +1,6 @@
 from dotenv import load_dotenv
 import os, time
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
@@ -44,7 +44,7 @@ def run_gc_report(verbose: bool = True):
 
 def pipeline_run():
     """åˆ›å»ºå¹¶è¿è¡Œæ•°æ®å¤„ç†ç®¡é“"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
     query_stream = (env
@@ -56,7 +56,7 @@ def pipeline_run():
                     )
     try:
         env.submit()
-          # å¯åŠ¨ç®¡é“
+        env.run_streaming()  # å¯åŠ¨ç®¡é“
         time.sleep(30)  # ç­‰å¾…ç®¡é“è¿è¡Œ
         env.stop()
         time.sleep(60)
diff --git a/sage_examples/qa_refiner.py b/sage_examples/qa_refiner.py
index ff1817d..182ec55 100644
--- a/sage_examples/qa_refiner.py
+++ b/sage_examples/qa_refiner.py
@@ -1,7 +1,7 @@
 import time
 
 # å¯¼å…¥ Sage ç›¸å…³æ¨¡å—
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -17,7 +17,7 @@ def pipeline_run():
     è¯¥å‡½æ•°ä¼šåˆå§‹åŒ–ç¯å¢ƒï¼ŒåŠ è½½é…ç½®ï¼Œè®¾ç½®æ•°æ®å¤„ç†æµç¨‹ï¼Œå¹¶å¯åŠ¨ç®¡é“ã€‚
     """
     # åˆå§‹åŒ–ç¯å¢ƒ
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)  # åˆå§‹åŒ–å†…å­˜é…ç½®
 
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
@@ -31,7 +31,7 @@ def pipeline_run():
 
     # æäº¤ç®¡é“å¹¶è¿è¡Œ
     env.submit()
-    
+    env.run_streaming()
     time.sleep(5)
     env.close()
 
diff --git a/sage_examples/qa_refiner_batch.py b/sage_examples/qa_refiner_batch.py
deleted file mode 100644
index 6a7c0bc..0000000
--- a/sage_examples/qa_refiner_batch.py
+++ /dev/null
@@ -1,40 +0,0 @@
-import time
-
-# å¯¼å…¥ Sage ç›¸å…³æ¨¡å—
-from sage_core.api.local_environment import LocalBatchEnvironment
-from sage_libs.io.sink import TerminalSink
-from sage_libs.io.source import FileSource
-from sage_libs.rag.generator import OpenAIGenerator
-from sage_libs.rag.promptor import QAPromptor
-from sage_libs.rag.retriever import DenseRetriever
-from sage_libs.rag.refiner import AbstractiveRecompRefiner
-from sage_utils.config_loader import load_config
-
-
-def pipeline_run():
-    """åˆ›å»ºå¹¶è¿è¡Œæ•°æ®å¤„ç†ç®¡é“
-
-    è¯¥å‡½æ•°ä¼šåˆå§‹åŒ–ç¯å¢ƒï¼ŒåŠ è½½é…ç½®ï¼Œè®¾ç½®æ•°æ®å¤„ç†æµç¨‹ï¼Œå¹¶å¯åŠ¨ç®¡é“ã€‚
-    """
-    # åˆå§‹åŒ–ç¯å¢ƒ
-    env = LocalBatchEnvironment()
-    env.set_memory(config=None)  # åˆå§‹åŒ–å†…å­˜é…ç½®
-
-    # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
-    query_stream = (env.from_collection(FileSource, config["source"])
-                    .map(DenseRetriever, config["retriever"])
-                    .map(AbstractiveRecompRefiner, config["refiner"])  
-                    .map(QAPromptor, config["promptor"])
-                    .map(OpenAIGenerator, config["generator"])
-                    .sink(TerminalSink, config["sink"])
-                    )
-
-    # æäº¤ç®¡é“å¹¶è¿è¡Œ
-    env.submit()
-
-if __name__ == '__main__':
-    # åŠ è½½é…ç½®æ–‡ä»¶
-    config = load_config('config_refiner.yaml')
-    
-    # è¿è¡Œç®¡é“
-    pipeline_run()
diff --git a/sage_examples/qa_rerank.py b/sage_examples/qa_rerank.py
index ce1e083..42c97d4 100644
--- a/sage_examples/qa_rerank.py
+++ b/sage_examples/qa_rerank.py
@@ -1,7 +1,7 @@
 import time
 
 # å¯¼å…¥ Sage ç›¸å…³æ¨¡å—
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -17,7 +17,7 @@ def pipeline_run():
     è¯¥å‡½æ•°ä¼šåˆå§‹åŒ–ç¯å¢ƒï¼ŒåŠ è½½é…ç½®ï¼Œè®¾ç½®æ•°æ®å¤„ç†æµç¨‹ï¼Œå¹¶å¯åŠ¨ç®¡é“ã€‚
     """
     # åˆå§‹åŒ–ç¯å¢ƒ
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)  # åˆå§‹åŒ–å†…å­˜é…ç½®
 
     # æ„å»ºæ•°æ®å¤„ç†æµç¨‹
@@ -31,7 +31,8 @@ def pipeline_run():
 
     # æäº¤ç®¡é“å¹¶è¿è¡Œ
     env.submit()
-
+    env.run_once()
+    
     # ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿ä»»åŠ¡å®Œæˆ
     time.sleep(5)
     
diff --git a/sage_examples/ray_object_trimmer_usage_example.py b/sage_examples/ray_object_trimmer_usage_example.py
deleted file mode 100644
index c365172..0000000
--- a/sage_examples/ray_object_trimmer_usage_example.py
+++ /dev/null
@@ -1,347 +0,0 @@
-"""
-SAGEç³»ç»Ÿä¸­ä½¿ç”¨Rayå¯¹è±¡é¢„å¤„ç†å™¨çš„å®é™…ç¤ºä¾‹
-å±•ç¤ºå¦‚ä½•åœ¨Transformationå’ŒOperatorä¸­ä½¿ç”¨trim_object_for_ray
-"""
-import sys
-import os
-from pathlib import Path
-from typing import Any, Dict, List, Optional
-
-# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
-SAGE_ROOT = Path(__file__).parent.parent
-sys.path.insert(0, str(SAGE_ROOT))
-
-from sage_utils.serialization.dill_serializer import (
-    trim_object_for_ray,
-    RayObjectTrimmer
-)
-
-# æ¨¡æ‹ŸSAGEç»„ä»¶
-class MockTransformation:
-    """æ¨¡æ‹Ÿçš„Transformationç±»ï¼ŒåŒ…å«å…¸å‹çš„ä¸å¯åºåˆ—åŒ–å±æ€§"""
-    
-    def __init__(self, function_name: str):
-        # å¯åºåˆ—åŒ–çš„æ ¸å¿ƒå±æ€§
-        self.function_class_name = function_name
-        self.function_args = ["arg1", "arg2"]
-        self.function_kwargs = {"param1": "value1"}
-        self.basename = f"transform_{function_name}"
-        self.parallelism = 1
-        self.upstreams = []
-        self.downstreams = {"downstream1": 0}
-        
-        # ä¸å¯åºåˆ—åŒ–çš„å±æ€§ï¼ˆä¼šå¯¼è‡´Rayè°ƒç”¨å¤±è´¥ï¼‰
-        from sage_utils.custom_logger import CustomLogger
-        self.logger = CustomLogger()  # æ—¥å¿—å¯¹è±¡
-        self.env = self._create_mock_env()  # ç¯å¢ƒå¼•ç”¨
-        self.memory_collection = self._create_mock_memory()  # å¯èƒ½æ˜¯Actorå¥æŸ„
-        
-        # æ‡’åŠ è½½å·¥å‚ï¼ˆåŒ…å«å¤æ‚çŠ¶æ€ï¼‰
-        self._dag_node_factory = None
-        self._operator_factory = None
-        self._function_factory = None
-        
-        # è¿è¡Œæ—¶çŠ¶æ€
-        self.runtime_context = self._create_runtime_context()
-        
-        # å®šä¹‰åºåˆ—åŒ–æ’é™¤åˆ—è¡¨
-        self.__state_exclude__ = [
-            'logger', 'env', 'memory_collection', 'runtime_context',
-            '_dag_node_factory', '_operator_factory', '_function_factory'
-        ]
-    
-    def _create_mock_env(self):
-        """åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒå¯¹è±¡"""
-        class MockEnv:
-            def __init__(self):
-                self.name = "test_env"
-                self.platform = "remote"
-        return MockEnv()
-    
-    def _create_mock_memory(self):
-        """åˆ›å»ºæ¨¡æ‹Ÿå†…å­˜é›†åˆ"""
-        class MockMemory:
-            def __init__(self):
-                self.collection_type = "VDB"
-        return MockMemory()
-    
-    def _create_runtime_context(self):
-        """åˆ›å»ºæ¨¡æ‹Ÿè¿è¡Œæ—¶ä¸Šä¸‹æ–‡"""
-        class MockContext:
-            def __init__(self):
-                self.session_id = "test_session"
-        return MockContext()
-    
-    def get_serializable_state(self):
-        """è¿”å›å¯åºåˆ—åŒ–çš„çŠ¶æ€"""
-        return {
-            'function_class_name': self.function_class_name,
-            'function_args': self.function_args,
-            'function_kwargs': self.function_kwargs,
-            'basename': self.basename,
-            'parallelism': self.parallelism,
-            'upstreams': self.upstreams,
-            'downstreams': self.downstreams
-        }
-
-
-class MockOperator:
-    """æ¨¡æ‹Ÿçš„Operatorç±»"""
-    
-    def __init__(self, operator_name: str):
-        self.operator_name = operator_name
-        self.config = {"setting": "value"}
-        
-        # ä¸å¯åºåˆ—åŒ–çš„å±æ€§
-        import threading
-        self.logger = self._create_logger()
-        self.emit_context = self._create_emit_context()
-        self.server_thread = threading.Thread(target=lambda: None)
-        
-        # æ’é™¤åˆ—è¡¨
-        self.__state_exclude__ = [
-            'logger', 'emit_context', 'server_thread'
-        ]
-    
-    def _create_logger(self):
-        class Logger:
-            def info(self, msg): pass
-        return Logger()
-    
-    def _create_emit_context(self):
-        class EmitContext:
-            def __init__(self):
-                self.connections = {}
-        return EmitContext()
-
-
-def demonstrate_transformation_trimming():
-    """æ¼”ç¤ºTransformationå¯¹è±¡çš„é¢„å¤„ç†"""
-    print("=== Transformationå¯¹è±¡é¢„å¤„ç†ç¤ºä¾‹ ===")
-    
-    # åˆ›å»ºåŒ…å«é—®é¢˜å±æ€§çš„transformation
-    transformation = MockTransformation("map_function")
-    
-    print("åŸå§‹Transformationå±æ€§:")
-    for attr_name in sorted(transformation.__dict__.keys()):
-        attr_value = getattr(transformation, attr_name)
-        print(f"  {attr_name}: {type(attr_value)} = {str(attr_value)[:50]}...")
-    
-    # ä½¿ç”¨ä¸“é—¨çš„transformationæ¸…ç†æ–¹æ³•
-    print("\nä½¿ç”¨RayObjectTrimmer.trim_transformation_for_ray()...")
-    cleaned_transformation = RayObjectTrimmer.trim_transformation_for_ray(transformation)
-    
-    print("æ¸…ç†åTransformationå±æ€§:")
-    for attr_name in sorted(cleaned_transformation.__dict__.keys()):
-        attr_value = getattr(cleaned_transformation, attr_name)
-        print(f"  {attr_name}: {type(attr_value)} = {str(attr_value)[:50]}...")
-    
-    # éªŒè¯å…³é”®å±æ€§ä¿ç•™
-    assert hasattr(cleaned_transformation, 'function_class_name'), "æ ¸å¿ƒå±æ€§åº”è¯¥ä¿ç•™"
-    assert hasattr(cleaned_transformation, 'basename'), "åç§°åº”è¯¥ä¿ç•™"
-    assert not hasattr(cleaned_transformation, 'logger'), "loggeråº”è¯¥è¢«ç§»é™¤"
-    assert not hasattr(cleaned_transformation, 'env'), "envåº”è¯¥è¢«ç§»é™¤"
-    
-    print("âœ“ Transformationæ¸…ç†æˆåŠŸ")
-    return cleaned_transformation
-
-
-def demonstrate_operator_trimming():
-    """æ¼”ç¤ºOperatorå¯¹è±¡çš„é¢„å¤„ç†"""
-    print("\n=== Operatorå¯¹è±¡é¢„å¤„ç†ç¤ºä¾‹ ===")
-    
-    operator = MockOperator("map_operator")
-    
-    print("åŸå§‹Operatorå±æ€§:")
-    for attr_name in sorted(operator.__dict__.keys()):
-        print(f"  {attr_name}: {type(getattr(operator, attr_name))}")
-    
-    # ä½¿ç”¨ä¸“é—¨çš„operatoræ¸…ç†æ–¹æ³•
-    cleaned_operator = RayObjectTrimmer.trim_operator_for_ray(operator)
-    
-    print("æ¸…ç†åOperatorå±æ€§:")
-    for attr_name in sorted(cleaned_operator.__dict__.keys()):
-        print(f"  {attr_name}: {type(getattr(cleaned_operator, attr_name))}")
-    
-    assert hasattr(cleaned_operator, 'operator_name'), "æ ¸å¿ƒå±æ€§åº”è¯¥ä¿ç•™"
-    assert not hasattr(cleaned_operator, 'logger'), "loggeråº”è¯¥è¢«ç§»é™¤"
-    assert not hasattr(cleaned_operator, 'server_thread'), "çº¿ç¨‹åº”è¯¥è¢«ç§»é™¤"
-    
-    print("âœ“ Operatoræ¸…ç†æˆåŠŸ")
-    return cleaned_operator
-
-
-def demonstrate_custom_trimming():
-    """æ¼”ç¤ºè‡ªå®šä¹‰é¢„å¤„ç†è§„åˆ™"""
-    print("\n=== è‡ªå®šä¹‰é¢„å¤„ç†è§„åˆ™ç¤ºä¾‹ ===")
-    
-    transformation = MockTransformation("custom_function")
-    
-    # åœºæ™¯1ï¼šåªä¿ç•™ç‰¹å®šå±æ€§
-    print("åœºæ™¯1ï¼šåªä¿ç•™æ ¸å¿ƒæ‰§è¡Œå±æ€§...")
-    core_only = trim_object_for_ray(
-        transformation,
-        include=['function_class_name', 'function_args', 'function_kwargs', 'parallelism']
-    )
-    print(f"ä¿ç•™å±æ€§: {list(core_only.__dict__.keys())}")
-    
-    # åœºæ™¯2ï¼šæ’é™¤ç‰¹å®šå±æ€§
-    print("\nåœºæ™¯2ï¼šæ’é™¤è¿è¡Œæ—¶çŠ¶æ€...")
-    runtime_excluded = trim_object_for_ray(
-        transformation,
-        exclude=['upstreams', 'downstreams', 'runtime_context']
-    )
-    print(f"å‰©ä½™å±æ€§: {list(runtime_excluded.__dict__.keys())}")
-    
-    print("âœ“ è‡ªå®šä¹‰é¢„å¤„ç†è§„åˆ™æµ‹è¯•æˆåŠŸ")
-
-
-def simulate_ray_actor_workflow():
-    """æ¨¡æ‹Ÿå®Œæ•´çš„Ray Actorå·¥ä½œæµç¨‹"""
-    print("\n=== æ¨¡æ‹ŸRay Actorå·¥ä½œæµç¨‹ ===")
-    
-    try:
-        import ray
-        
-        if not ray.is_initialized():
-            ray.init(local_mode=True, ignore_reinit_error=True)
-        
-        @ray.remote
-        class TransformationProcessor:
-            """æ¨¡æ‹Ÿå¤„ç†Transformationçš„Ray Actor"""
-            
-            def process_transformation(self, transformation):
-                """å¤„ç†æ¸…ç†åçš„transformationå¯¹è±¡"""
-                return {
-                    "processed_type": type(transformation).__name__,
-                    "function_name": getattr(transformation, 'function_class_name', 'unknown'),
-                    "attributes_count": len(transformation.__dict__) if hasattr(transformation, '__dict__') else 0,
-                    "attributes": list(transformation.__dict__.keys()) if hasattr(transformation, '__dict__') else []
-                }
-            
-            def validate_object(self, obj):
-                """éªŒè¯å¯¹è±¡æ˜¯å¦å¯ä»¥æ­£å¸¸å¤„ç†"""
-                try:
-                    # å°è¯•è®¿é—®å¯¹è±¡å±æ€§
-                    attrs = obj.__dict__ if hasattr(obj, '__dict__') else {}
-                    return {
-                        "valid": True,
-                        "attribute_count": len(attrs),
-                        "sample_attributes": list(attrs.keys())[:5]
-                    }
-                except Exception as e:
-                    return {
-                        "valid": False,
-                        "error": str(e)
-                    }
-        
-        # åˆ›å»ºActor
-        processor = TransformationProcessor.remote()
-        
-        # å‡†å¤‡æµ‹è¯•å¯¹è±¡
-        original_transformation = MockTransformation("ray_workflow_test")
-        
-        print("æ­¥éª¤1ï¼šå°è¯•å‘é€åŸå§‹å¯¹è±¡ï¼ˆå¯èƒ½å¤±è´¥ï¼‰...")
-        try:
-            result = ray.get(processor.validate_object.remote(original_transformation))
-            print(f"åŸå§‹å¯¹è±¡éªŒè¯ç»“æœ: {result}")
-        except Exception as e:
-            print(f"åŸå§‹å¯¹è±¡å‘é€å¤±è´¥: {e}")
-        
-        print("\næ­¥éª¤2ï¼šé¢„å¤„ç†å¯¹è±¡å¹¶å‘é€...")
-        cleaned_transformation = RayObjectTrimmer.trim_transformation_for_ray(original_transformation)
-        
-        # å‘é€æ¸…ç†åçš„å¯¹è±¡
-        validation_result = ray.get(processor.validate_object.remote(cleaned_transformation))
-        print(f"æ¸…ç†åå¯¹è±¡éªŒè¯ç»“æœ: {validation_result}")
-        
-        processing_result = ray.get(processor.process_transformation.remote(cleaned_transformation))
-        print(f"å¤„ç†ç»“æœ: {processing_result}")
-        
-        print("âœ“ Ray Actorå·¥ä½œæµç¨‹æµ‹è¯•æˆåŠŸ")
-        
-        # æ¸…ç†
-        ray.shutdown()
-        
-    except ImportError:
-        print("Rayæœªå®‰è£…ï¼Œè·³è¿‡Actorå·¥ä½œæµç¨‹æµ‹è¯•")
-    except Exception as e:
-        print(f"Ray Actorå·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
-
-
-def performance_analysis():
-    """æ€§èƒ½åˆ†æï¼šå¯¹æ¯”å¤„ç†å‰åçš„å¯¹è±¡å¤§å°"""
-    print("\n=== æ€§èƒ½åˆ†æ ===")
-    
-    import pickle
-    import sys
-    
-    # åˆ›å»ºæµ‹è¯•å¯¹è±¡
-    transformation = MockTransformation("performance_test")
-    operator = MockOperator("performance_test")
-    
-    # åˆ†ætransformation
-    print("Transformationå¯¹è±¡åˆ†æ:")
-    try:
-        original_size = sys.getsizeof(pickle.dumps(transformation.__dict__))
-        print(f"  åŸå§‹å¯¹è±¡å¤§å°: {original_size} å­—èŠ‚")
-    except Exception as e:
-        print(f"  åŸå§‹å¯¹è±¡æ— æ³•åºåˆ—åŒ–: {e}")
-    
-    cleaned_trans = RayObjectTrimmer.trim_transformation_for_ray(transformation)
-    cleaned_size = sys.getsizeof(pickle.dumps(cleaned_trans.__dict__))
-    print(f"  æ¸…ç†åå¯¹è±¡å¤§å°: {cleaned_size} å­—èŠ‚")
-    print(f"  å±æ€§æ•°é‡: {len(transformation.__dict__)} -> {len(cleaned_trans.__dict__)}")
-    
-    # åˆ†æoperator  
-    print("\nOperatorå¯¹è±¡åˆ†æ:")
-    try:
-        original_op_size = sys.getsizeof(pickle.dumps(operator.__dict__))
-        print(f"  åŸå§‹å¯¹è±¡å¤§å°: {original_op_size} å­—èŠ‚")
-    except Exception as e:
-        print(f"  åŸå§‹å¯¹è±¡æ— æ³•åºåˆ—åŒ–: {e}")
-    
-    cleaned_op = RayObjectTrimmer.trim_operator_for_ray(operator)
-    cleaned_op_size = sys.getsizeof(pickle.dumps(cleaned_op.__dict__))
-    print(f"  æ¸…ç†åå¯¹è±¡å¤§å°: {cleaned_op_size} å­—èŠ‚")
-    print(f"  å±æ€§æ•°é‡: {len(operator.__dict__)} -> {len(cleaned_op.__dict__)}")
-
-
-def main():
-    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
-    print("SAGE Rayå¯¹è±¡é¢„å¤„ç†å™¨ä½¿ç”¨ç¤ºä¾‹")
-    print("=" * 60)
-    
-    try:
-        # åŸºæœ¬ç”¨æ³•ç¤ºä¾‹
-        demonstrate_transformation_trimming()
-        demonstrate_operator_trimming()
-        demonstrate_custom_trimming()
-        
-        # é«˜çº§ç”¨æ³•
-        simulate_ray_actor_workflow()
-        performance_analysis()
-        
-        print("\n" + "=" * 60)
-        print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
-        print("\næ€»ç»“:")
-        print("1. âœ“ trim_object_for_ray() å¯ä»¥æœ‰æ•ˆæ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å±æ€§")
-        print("2. âœ“ RayObjectTrimmer æä¾›äº†é’ˆå¯¹ä¸åŒå¯¹è±¡ç±»å‹çš„ä¸“é—¨æ¸…ç†æ–¹æ³•")
-        print("3. âœ“ æ”¯æŒè‡ªå®šä¹‰include/excludeè§„åˆ™")
-        print("4. âœ“ æ¸…ç†åçš„å¯¹è±¡å¯ä»¥å®‰å…¨åœ°ä¼ é€’ç»™Rayè¿›è¡Œåºåˆ—åŒ–")
-        print("5. âœ“ æ˜¾è‘—å‡å°‘äº†å¯¹è±¡å¤§å°å’Œåºåˆ—åŒ–å¤æ‚åº¦")
-        
-        print("\nä½¿ç”¨å»ºè®®:")
-        print("- åœ¨Rayè¿œç¨‹è°ƒç”¨å‰ä½¿ç”¨trim_object_for_ray()é¢„å¤„ç†å¯¹è±¡")
-        print("- ä¸ºå¸¸è§çš„SAGEç»„ä»¶ä½¿ç”¨ä¸“é—¨çš„æ¸…ç†æ–¹æ³•")
-        print("- æ ¹æ®å…·ä½“éœ€æ±‚å®šåˆ¶include/excludeåˆ—è¡¨")
-        print("- åœ¨å¼€å‘é˜¶æ®µä½¿ç”¨validate_ray_serializable()éªŒè¯å¯¹è±¡")
-        
-    except Exception as e:
-        print(f"ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
-        import traceback
-        traceback.print_exc()
-
-
-if __name__ == "__main__":
-    main()
diff --git a/sage_examples/test_jobmanager_architecture.py b/sage_examples/test_jobmanager_architecture.py
deleted file mode 100644
index 60fcf5b..0000000
--- a/sage_examples/test_jobmanager_architecture.py
+++ /dev/null
@@ -1,131 +0,0 @@
-#!/usr/bin/env python3
-"""
-æµ‹è¯•æ–°çš„JobManager Actoræ¶æ„çš„ç¤ºä¾‹
-"""
-
-import sys
-import time
-from pathlib import Path
-
-# æ·»åŠ é¡¹ç›®è·¯å¾„
-SAGE_ROOT = Path(__file__).parent.parent
-sys.path.insert(0, str(SAGE_ROOT))
-
-def test_local_environment():
-    """æµ‹è¯•æœ¬åœ°ç¯å¢ƒ"""
-    print("\n=== æµ‹è¯•æœ¬åœ°ç¯å¢ƒ ===")
-    
-    from sage_core.api.local_environment import LocalStreamEnvironment
-    
-    # åˆ›å»ºæœ¬åœ°ç¯å¢ƒ
-    env = LocalStreamEnvironment("test_local_env", config={"test": "local"})
-    
-    print(f"ç¯å¢ƒåç§°: {env.name}")
-    print(f"å¹³å°: {env.platform}")
-    
-    # æµ‹è¯•jobmanagerå±æ€§è®¿é—®
-    jobmanager = env.jobmanager
-    print(f"JobManagerç±»å‹: {type(jobmanager)}")
-    print(f"JobManageræ˜¯å¦æœ¬åœ°: {jobmanager.is_local()}")
-    print(f"JobManageræ˜¯å¦Ray Actor: {jobmanager.is_ray_actor()}")
-    
-    # æµ‹è¯•è°ƒç”¨jobmanageræ–¹æ³•
-    try:
-        server_info = jobmanager.get_server_info()
-        print(f"æœåŠ¡å™¨ä¿¡æ¯: {server_info}")
-    except Exception as e:
-        print(f"è·å–æœåŠ¡å™¨ä¿¡æ¯å¤±è´¥: {e}")
-    
-    print("âœ… æœ¬åœ°ç¯å¢ƒæµ‹è¯•å®Œæˆ")
-
-
-def test_remote_environment():
-    """æµ‹è¯•è¿œç¨‹ç¯å¢ƒ"""
-    print("\n=== æµ‹è¯•è¿œç¨‹ç¯å¢ƒ ===")
-    
-    try:
-        import ray
-        
-        # åˆå§‹åŒ–Rayï¼ˆå¦‚æœéœ€è¦ï¼‰
-        if not ray.is_initialized():
-            ray.init(ignore_reinit_error=True)
-        
-        from sage_core.api.remote_environment import RemoteStreamEnvironment
-        
-        # åˆ›å»ºè¿œç¨‹ç¯å¢ƒ
-        env = RemoteStreamEnvironment("test_remote_env", config={
-            "jobmanager_daemon_host": "127.0.0.1",
-            "jobmanager_daemon_port": 19000
-        })
-        
-        print(f"ç¯å¢ƒåç§°: {env.name}")
-        print(f"å¹³å°: {env.platform}")
-        
-        # å°è¯•è·å–JobManagerå¥æŸ„
-        # æ³¨æ„ï¼šè¿™éœ€è¦Ray JobManager Daemonåœ¨è¿è¡Œ
-        try:
-            jobmanager = env.jobmanager
-            print(f"JobManagerç±»å‹: {type(jobmanager)}")
-            print(f"JobManageræ˜¯å¦æœ¬åœ°: {jobmanager.is_local()}")
-            print(f"JobManageræ˜¯å¦Ray Actor: {jobmanager.is_ray_actor()}")
-            
-            # æµ‹è¯•è°ƒç”¨jobmanageræ–¹æ³•
-            server_info = jobmanager.get_server_info()
-            print(f"æœåŠ¡å™¨ä¿¡æ¯: {server_info}")
-            
-            print("âœ… è¿œç¨‹ç¯å¢ƒæµ‹è¯•å®Œæˆ")
-            
-        except Exception as e:
-            print(f"âš ï¸ è¿œç¨‹ç¯å¢ƒæµ‹è¯•å¤±è´¥ï¼ˆå¯èƒ½Ray JobManager Daemonæœªè¿è¡Œï¼‰: {e}")
-            print("æç¤ºï¼šè¯·å…ˆè¿è¡Œ 'python deployment/ray_jobmanager_daemon.py start'")
-    
-    except ImportError:
-        print("âš ï¸ Rayæœªå®‰è£…ï¼Œè·³è¿‡è¿œç¨‹ç¯å¢ƒæµ‹è¯•")
-
-
-def test_actor_wrapper():
-    """æµ‹è¯•ActorWrapperçš„åŸºæœ¬åŠŸèƒ½"""
-    print("\n=== æµ‹è¯•ActorWrapper ===")
-    
-    from sage_utils.actor_wrapper import ActorWrapper
-    from sage_jobmanager.job_manager import JobManager
-    
-    # æµ‹è¯•æœ¬åœ°å¯¹è±¡åŒ…è£…
-    jobmanager = JobManager()
-    wrapped = ActorWrapper(jobmanager)
-    
-    print(f"åŸå§‹å¯¹è±¡ç±»å‹: {type(jobmanager)}")
-    print(f"åŒ…è£…åç±»å‹: {type(wrapped)}")
-    print(f"æ˜¯å¦æœ¬åœ°: {wrapped.is_local()}")
-    print(f"æ˜¯å¦Ray Actor: {wrapped.is_ray_actor()}")
-    
-    # æµ‹è¯•æ–¹æ³•è°ƒç”¨
-    try:
-        server_info = wrapped.get_server_info()
-        print(f"é€šè¿‡wrapperè°ƒç”¨æ–¹æ³•æˆåŠŸ: {list(server_info.keys())}")
-    except Exception as e:
-        print(f"é€šè¿‡wrapperè°ƒç”¨æ–¹æ³•å¤±è´¥: {e}")
-    
-    print("âœ… ActorWrapperæµ‹è¯•å®Œæˆ")
-
-
-def main():
-    """ä¸»å‡½æ•°"""
-    print("SAGE JobManager Actoræ¶æ„æµ‹è¯•")
-    print("=" * 50)
-    
-    # åŸºç¡€ç»„ä»¶æµ‹è¯•
-    test_actor_wrapper()
-    
-    # æœ¬åœ°ç¯å¢ƒæµ‹è¯•
-    test_local_environment()
-    
-    # è¿œç¨‹ç¯å¢ƒæµ‹è¯•
-    test_remote_environment()
-    
-    print("\n" + "=" * 50)
-    print("æ‰€æœ‰æµ‹è¯•å®Œæˆ")
-
-
-if __name__ == "__main__":
-    main()
diff --git a/sage_examples/wordcount_lambda_example.py b/sage_examples/wordcount_lambda_example.py
index fb8a245..879ea05 100644
--- a/sage_examples/wordcount_lambda_example.py
+++ b/sage_examples/wordcount_lambda_example.py
@@ -1,8 +1,8 @@
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from collections import Counter
 import time
-
+import random
 
 # ç®€å•çš„å¥å­æºï¼Œé‡å¤è¾“å‡ºåŒä¸€å¥è¯
 class SentenceSource(SourceFunction):
@@ -26,7 +26,7 @@ class SentenceSource(SourceFunction):
 
 def main():
     # åˆ›å»ºç¯å¢ƒ
-    env = LocalStreamEnvironment("wordcount_example")
+    env = LocalEnvironment("wordcount_example")
     
     # å…¨å±€è¯æ±‡è®¡æ•°å™¨
     word_counts = Counter()
@@ -72,7 +72,7 @@ def main():
     try:
         # è¿è¡Œæµå¤„ç†
         env.submit()
-        
+        env.run_streaming()
         time.sleep(60)  # è¿è¡Œ60ç§’ä»¥è§‚å¯Ÿè¾“å‡º
     except KeyboardInterrupt:
         print("\n\nğŸ›‘ Stopping WordCount Example...")
diff --git a/sage_frontend/sage_server/data/jobinfo/c0755891-5744-49a1-9ca7-372cb32c5eee.json b/sage_frontend/sage_server/data/jobinfo/c0755891-5744-49a1-9ca7-372cb32c5eee.json
index a3df263..6cb5e75 100644
--- a/sage_frontend/sage_server/data/jobinfo/c0755891-5744-49a1-9ca7-372cb32c5eee.json
+++ b/sage_frontend/sage_server/data/jobinfo/c0755891-5744-49a1-9ca7-372cb32c5eee.json
@@ -5,7 +5,7 @@
     "cpu": "2",
     "ram": "4GB",
     "startTime": "2025-07-09 07:09:47",
-    "duration": "282s",
+    "duration": "281s",
     "isRunning": false,
     "nevents": 0,
     "minProcessTime": 0.0,
diff --git a/sage_frontend/sage_server/routers/signal.py b/sage_frontend/sage_server/routers/signal.py
index c18082f..a2a45c9 100644
--- a/sage_frontend/sage_server/routers/signal.py
+++ b/sage_frontend/sage_server/routers/signal.py
@@ -90,7 +90,7 @@ def update_json_field(file_path: str, *args):
 
 
 @router.post("/stop/{jobId}/{duration}")
-async def pause_job(jobId: str,duration:str ):
+async def stop_job(jobId: str,duration:str ):
     """
     åœæ­¢æŒ‡å®šIDçš„æµå¤„ç†ä½œä¸š
     """
diff --git a/sage_frontend/sage_server/start_a_pipeline.py b/sage_frontend/sage_server/start_a_pipeline.py
index 0934901..9a7ee1f 100644
--- a/sage_frontend/sage_server/start_a_pipeline.py
+++ b/sage_frontend/sage_server/start_a_pipeline.py
@@ -4,8 +4,8 @@
 # å¯¼å…¥ Sage ä¸­çš„ Environment å’Œç›¸å…³ç»„ä»¶
 import logging
 import re
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_core.environment.base_environment import RemoteEnvironment
+from sage_core.api.env import LocalEnvironment
+from sage_core.api.env import RemoteEnvironment
 
 def init_memory_and_pipeline(job_id=None,  config=None, operators=None,use_ray=False):
     """
@@ -40,7 +40,7 @@ def init_memory_and_pipeline(job_id=None,  config=None, operators=None,use_ray=F
 
         env = RemoteEnvironment(env_name)
     else:
-        env = LocalStreamEnvironment(env_name)
+        env = LocalEnvironment(env_name)
     env.set_memory(config={"collection_name":f"{env_name}_memory"})
     # å¦‚æœæ²¡æœ‰æä¾›operatorsé…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
     if not operators:
@@ -90,7 +90,7 @@ def init_memory_and_pipeline(job_id=None,  config=None, operators=None,use_ray=F
     # æäº¤ç®¡é“åˆ° SAGE è¿è¡Œæ—¶
     try:
         env.submit(name = f"env_{job_id}" if job_id else "dynamic_pipeline")
-        
+        env.run_streaming()
     except Exception as e:
         logging.error(f"Failed to submit pipeline: {e}")
         raise  Exception(f"Pipeline submission failed: {e}")
diff --git a/sage_jobmanager/__init__.py b/sage_jobmanager/__init__.py
deleted file mode 100644
index 016ef8e..0000000
--- a/sage_jobmanager/__init__.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# from .api import env, model, memory, operator, query
-
-# # åªæš´éœ²å››ä¸ªå­æ¨¡å—ï¼Œä¿æŒæ¸…æ™°çš„æ¨¡å—è¾¹ç•Œ
-# memory = api.memory
-# model = api.model
-# operator = api.operator
-# env = api.env
-# query = api.query
-
-# __all__ = ["memory", "model", "operator", "env", "query"]
\ No newline at end of file
diff --git a/sage_jobmanager/factory/__init__.py b/sage_jobmanager/factory/__init__.py
deleted file mode 100644
index 016ef8e..0000000
--- a/sage_jobmanager/factory/__init__.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# from .api import env, model, memory, operator, query
-
-# # åªæš´éœ²å››ä¸ªå­æ¨¡å—ï¼Œä¿æŒæ¸…æ™°çš„æ¨¡å—è¾¹ç•Œ
-# memory = api.memory
-# model = api.model
-# operator = api.operator
-# env = api.env
-# query = api.query
-
-# __all__ = ["memory", "model", "operator", "env", "query"]
\ No newline at end of file
diff --git a/sage_jobmanager/factory/operator_factory.py b/sage_jobmanager/factory/operator_factory.py
deleted file mode 100644
index 1826c66..0000000
--- a/sage_jobmanager/factory/operator_factory.py
+++ /dev/null
@@ -1,33 +0,0 @@
-from typing import Type, TYPE_CHECKING
-from sage_jobmanager.utils.name_server import get_name
-
-if TYPE_CHECKING:
-    from sage_core.operator.base_operator import BaseOperator
-    from sage_jobmanager.factory.function_factory import FunctionFactory
-    from sage_runtime.runtime_context import RuntimeContext
-
-
-class OperatorFactory:
-    # ç”±transformationåˆå§‹åŒ–
-    def __init__(self, 
-                 operator_class: Type['BaseOperator'],
-                 function_factory: 'FunctionFactory',
-                 basename: str = None,
-                 env_name:str = None,
-                 remote:bool = False,
-                 **operator_kwargs):
-        self.operator_class = operator_class
-        self.operator_kwargs = operator_kwargs  # ä¿å­˜é¢å¤–çš„operatorå‚æ•°
-        self.function_factory = function_factory
-        self.env_name = env_name
-        self.basename = get_name(basename) or get_name(self.function_factory.function_class.__name__)
-        self.remote = remote
-
-    def create_operator(self, runtime_context: 'RuntimeContext') -> 'BaseOperator':
-            Operator_class = self.operator_class
-            operator_instance = Operator_class(
-                self.function_factory,
-                runtime_context,
-                **self.operator_kwargs
-            )
-            return operator_instance
\ No newline at end of file
diff --git a/sage_jobmanager/job_info.py b/sage_jobmanager/job_info.py
deleted file mode 100644
index ab95242..0000000
--- a/sage_jobmanager/job_info.py
+++ /dev/null
@@ -1,102 +0,0 @@
-from datetime import datetime
-from typing import Dict, Any, Optional, TYPE_CHECKING
-import time
-
-if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment
-    from sage_jobmanager.execution_graph import ExecutionGraph
-    from sage_runtime.dispatcher import Dispatcher
-
-class JobInfo:
-    """ä½œä¸šä¿¡æ¯ç±»ï¼Œç”¨äºè·Ÿè¸ªå’Œç®¡ç†å•ä¸ªä½œä¸šçš„çŠ¶æ€"""
-    
-    def __init__(self, environment: 'BaseEnvironment', graph: 'ExecutionGraph', 
-                 dispatcher: 'Dispatcher', uuid: str):
-        self.environment = environment
-        self.graph = graph
-        self.dispatcher = dispatcher
-        self.uuid = uuid
-        
-        # çŠ¶æ€ä¿¡æ¯
-        self.status = "initializing"  # initializing, running, stopped, failed, restarting
-        self.start_time = datetime.now()
-        self.stop_time: Optional[datetime] = None
-        self.last_update_time = datetime.now()
-        self.error_message: Optional[str] = None
-        
-        # ç»Ÿè®¡ä¿¡æ¯
-        self.restart_count = 0
-        
-    def update_status(self, new_status: str, error: Optional[str] = None):
-        """æ›´æ–°ä½œä¸šçŠ¶æ€"""
-        old_status = self.status
-        self.status = new_status
-        self.last_update_time = datetime.now()
-        
-        if error:
-            self.error_message = error
-        
-        if new_status in ["stopped", "failed"]:
-            self.stop_time = datetime.now()
-        elif new_status == "running" and old_status in ["stopped", "failed", "restarting"]:
-            # é‡æ–°å¼€å§‹è¿è¡Œï¼Œé‡ç½®åœæ­¢æ—¶é—´
-            self.stop_time = None
-    
-    def get_runtime(self) -> str:
-        """è·å–è¿è¡Œæ—¶é—´å­—ç¬¦ä¸²"""
-        if self.stop_time:
-            runtime = self.stop_time - self.start_time
-        else:
-            runtime = datetime.now() - self.start_time
-        
-        total_seconds = int(runtime.total_seconds())
-        hours, remainder = divmod(total_seconds, 3600)
-        minutes, seconds = divmod(remainder, 60)
-        
-        if hours > 0:
-            return f"{hours}h {minutes}m {seconds}s"
-        elif minutes > 0:
-            return f"{minutes}m {seconds}s"
-        else:
-            return f"{seconds}s"
-    
-    def get_summary(self) -> Dict[str, Any]:
-        """è·å–ä½œä¸šæ‘˜è¦ä¿¡æ¯"""
-        return {
-            "uuid": self.uuid,
-            "name": self.environment.name,
-            "status": self.status,
-            "start_time": self.start_time.strftime("%Y-%m-%d %H:%M:%S"),
-            "runtime": self.get_runtime(),
-            "restart_count": self.restart_count,
-            "last_update": self.last_update_time.strftime("%Y-%m-%d %H:%M:%S")
-        }
-    
-    def get_status(self) -> Dict[str, Any]:
-        """è·å–è¯¦ç»†çŠ¶æ€ä¿¡æ¯"""
-        status_info = self.get_summary()
-        
-        # æ·»åŠ è¯¦ç»†ä¿¡æ¯
-        status_info.update({
-            "environment": {
-                "name": self.environment.name,
-                "platform": getattr(self.environment, 'platform', 'unknown'),
-                "description": getattr(self.environment, 'description', '')
-            },
-            "dispatcher": {
-                "task_count": len(self.dispatcher.tasks),
-                "is_running": self.dispatcher.is_running
-            }
-        })
-        
-        if self.error_message:
-            status_info["error"] = self.error_message
-        
-        if self.stop_time:
-            status_info["stop_time"] = self.stop_time.strftime("%Y-%m-%d %H:%M:%S")
-        
-        # è·å–ä»»åŠ¡ç»Ÿè®¡
-        if hasattr(self.dispatcher, 'get_statistics'):
-            status_info["task_statistics"] = self.dispatcher.get_statistics()
-        
-        return status_info
\ No newline at end of file
diff --git a/sage_jobmanager/job_manager.py b/sage_jobmanager/job_manager.py
deleted file mode 100644
index ece6e00..0000000
--- a/sage_jobmanager/job_manager.py
+++ /dev/null
@@ -1,329 +0,0 @@
-from datetime import datetime
-import os
-from pathlib import Path
-from typing import TYPE_CHECKING, Dict, Any, List, Optional
-import time, uuid
-from uuid import UUID
-from sage_jobmanager.job_info import JobInfo
-from sage_utils.custom_logger import CustomLogger
-from sage_runtime.dispatcher import Dispatcher
-import threading
-from sage_utils.serialization.dill_serializer import deserialize_object
-if TYPE_CHECKING:
-    from sage_jobmanager.execution_graph import ExecutionGraph
-    from sage_core.environment.base_environment import BaseEnvironment
-
-import ray
-class JobManager: #Job Manager
-    instance = None
-    instance_lock = threading.RLock()
-    def __new__(cls, *args, **kwargs):
-        if cls.instance is None:
-            with cls.instance_lock:
-                if cls.instance is None:
-                    cls.instance = super(JobManager, cls).__new__(cls)
-                    cls.instance._initialized = False
-        return cls.instance
-
-    def __init__(self):
-        with JobManager.instance_lock:
-            if self._initialized:
-                return
-            self._initialized = True
-            JobManager.instance = self
-            # æ–°å¢ï¼šUUID åˆ°ç¯å¢ƒä¿¡æ¯çš„æ˜ å°„
-            self.jobs: Dict[str, JobInfo] = {}  # uuid -> jobinfo
-            self.deleted_jobs: Dict[str, Dict[str, Any]] = {}
-            self.setup_logging_system()
-
-
-    def submit_job(self, env: 'BaseEnvironment') -> str:
-
-
-        env.setup_logging_system(self.log_base_dir)
-        # ç”Ÿæˆ UUID
-        env.uuid = str(uuid.uuid4())
-        # ç¼–è¯‘ç¯å¢ƒ
-        from sage_jobmanager.execution_graph import ExecutionGraph
-        graph = ExecutionGraph(env, self.handle) 
-
-
-
-        # TODO: å¦‚æœJobé‡Œé¢æœ‰ç”³æ˜'env.set_memory(config=None)'ï¼Œåˆ™è¯´æ˜è¯¥jobéœ€è¦ä¸€ä¸ªglobal memory manager.
-        # åˆ™åœ¨æ„å»ºexecutiongraphçš„æ—¶å€™è¦å•ç‹¬æ˜¯å®ä¾‹åŒ–ä¸€ä¸ªç‰¹æ®Šçš„operatorï¼Œå³ memory managerï¼Œå¹¶ä½¿å¾—æ‰€æœ‰è°ƒç”¨äº†memoryç›¸å…³æ“ä½œçš„
-        # ç®—å­ï¼ŒåŒå‘è¿åˆ°è¯¥memory managerç®—å­ä¸Šã€‚
-        dispatcher = Dispatcher(graph, env)
-
-            # åˆ›å»º JobInfo å¯¹è±¡
-        job_info = JobInfo(env, graph, dispatcher, env.uuid)
-
-        self.jobs[env.uuid] = job_info
-        
-        try:
-            # æäº¤ DAG
-            dispatcher.submit()
-            job_info.update_status("running")
-            
-            self.logger.info(f"Environment '{env.name}' submitted with UUID {env.uuid}")
-            
-        except Exception as e:
-            job_info.update_status("failed", error=str(e))
-            self.logger.error(f"Failed to submit environment {env.uuid}: {e}")
-
-        return env.uuid
-
-    def continue_job(self, env_uuid: str) -> Dict[str, Any]:
-        """é‡å¯ä½œä¸š"""
-        job_info = self.jobs.get(env_uuid)
-        
-        if not job_info:
-            self.logger.error(f"Job with UUID {env_uuid} not found")
-            return {
-                "uuid": env_uuid,
-                "status": "not_found",
-                "message": f"Job with UUID {env_uuid} not found"
-            }
-        
-        try:
-            current_status = job_info.status
-            
-            # å¦‚æœä½œä¸šæ­£åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢å®ƒ
-            if current_status == "running":
-                self.logger.info(f"Stopping running job {env_uuid} before restart")
-                stop_result = self.pause_job(env_uuid)
-                if stop_result.get("status") not in ["stopped", "error"]:
-                    return {
-                        "uuid": env_uuid,
-                        "status": "failed",
-                        "message": f"Failed to stop job before restart: {stop_result.get('message')}"
-                    }
-                
-                # ç­‰å¾…åœæ­¢å®Œæˆ
-                time.sleep(1.0)
-            
-            # é‡æ–°åˆ›å»º dispatcher
-            from sage_jobmanager.execution_graph import ExecutionGraph
-            new_graph = ExecutionGraph(job_info.environment)
-            new_dispatcher = Dispatcher(new_graph, job_info.environment)
-            
-            # æ›´æ–° job_info
-            job_info.graph = new_graph
-            job_info.dispatcher = new_dispatcher
-            job_info.update_status("restarting")
-            job_info.restart_count += 1
-            
-            # é‡æ–°æäº¤ä½œä¸š
-            new_dispatcher.submit()
-            job_info.update_status("running")
-            
-            self.logger.info(f"Job {env_uuid} restarted successfully (restart #{job_info.restart_count})")
-            
-            return {
-                "uuid": env_uuid,
-                "status": "running",
-                "message": f"Job restarted successfully (restart #{job_info.restart_count})"
-            }
-            
-        except Exception as e:
-            job_info.update_status("failed", error=str(e))
-            self.logger.error(f"Failed to restart job {env_uuid}: {e}")
-            return {
-                "uuid": env_uuid,
-                "status": "failed",
-                "message": f"Failed to restart job: {str(e)}"
-            }
-
-    def delete_job(self, env_uuid: str, force: bool = False) -> Dict[str, Any]:
-        """åˆ é™¤ä½œä¸š"""
-        job_info = self.jobs.get(env_uuid)
-        
-        if not job_info:
-            self.logger.error(f"Job with UUID {env_uuid} not found")
-            return {
-                "uuid": env_uuid,
-                "status": "not_found",
-                "message": f"Job with UUID {env_uuid} not found"
-            }
-        
-        try:
-            current_status = job_info.status
-            
-            # å¦‚æœä½œä¸šæ­£åœ¨è¿è¡Œä¸”æœªå¼ºåˆ¶åˆ é™¤ï¼Œå…ˆåœæ­¢å®ƒ
-            if current_status == "running" and not force:
-                self.logger.info(f"Stopping running job {env_uuid} before deletion")
-                stop_result = self.pause_job(env_uuid)
-                if stop_result.get("status") not in ["stopped", "error"]:
-                    return {
-                        "uuid": env_uuid,
-                        "status": "failed",
-                        "message": f"Failed to stop job before deletion: {stop_result.get('message')}"
-                    }
-                
-                # ç­‰å¾…åœæ­¢å®Œæˆ
-                time.sleep(0.5)
-            elif current_status == "running" and force:
-                # å¼ºåˆ¶åˆ é™¤ï¼šç›´æ¥åœæ­¢
-                self.logger.warning(f"Force deleting running job {env_uuid}")
-                job_info.dispatcher.stop()
-            
-            # æ¸…ç†èµ„æº
-            job_info.dispatcher.cleanup()
-            
-            # ä¿å­˜åˆ é™¤å†å²ï¼ˆå¯é€‰ï¼‰
-            deletion_info = {
-                "deleted_at": datetime.now().isoformat(),
-                "final_status": job_info.status,
-                "name": job_info.environment.name,
-                "runtime": job_info.get_runtime(),
-                "restart_count": job_info.restart_count
-            }
-            self.deleted_jobs[env_uuid] = deletion_info
-            
-            # ä»æ´»åŠ¨ä½œä¸šåˆ—è¡¨ä¸­ç§»é™¤
-            del self.jobs[env_uuid]
-            
-            self.logger.info(f"Job {env_uuid} deleted successfully")
-            
-            return {
-                "uuid": env_uuid,
-                "status": "deleted",
-                "message": "Job deleted successfully"
-            }
-            
-        except Exception as e:
-            self.logger.error(f"Failed to delete job {env_uuid}: {e}")
-            return {
-                "uuid": env_uuid,
-                "status": "failed",
-                "message": f"Failed to delete job: {str(e)}"
-            }
-
-    def receive_stop_signal(self, env_uuid: str):
-        """æ¥æ”¶åœæ­¢ä¿¡å·"""
-        job_info = self.jobs.get(env_uuid)
-        try:
-            # åœæ­¢ dispatcher
-            if (job_info.dispatcher.receive_stop_signal()) is True:
-                self.delete_job(env_uuid, force=True)
-                self.logger.info(f"Batch job: {env_uuid} completed ")
-            
-        except Exception as e:
-            job_info.update_status("failed", error=str(e))
-            self.logger.error(f"Failed to stop job {env_uuid}: {e}")
-
-
-    def pause_job(self, env_uuid: str) -> Dict[str, Any]:
-        """åœæ­¢Job"""
-        job_info = self.jobs.get(env_uuid, None)
-        
-        if not job_info:
-            self.logger.error(f"Job with UUID {env_uuid} not found")
-            return {
-                "uuid": env_uuid,
-                "status": "not_found",
-                "message": f"Job with UUID {env_uuid} not found"
-            }
-        
-        try:
-            # åœæ­¢ dispatcher
-            job_info.dispatcher.stop()
-            job_info.update_status("stopped")
-            
-            self.logger.info(f"Job {env_uuid} stopped successfully")
-            
-            return {
-                "uuid": env_uuid,
-                "status": "stopped",
-                "message": "Job stopped successfully"
-            }
-            
-        except Exception as e:
-            job_info.update_status("failed", error=str(e))
-            self.logger.error(f"Failed to stop job {env_uuid}: {e}")
-
-    def get_job_status(self, env_uuid: str) -> Dict[str, Any]:
-        job_info = self.jobs.get(env_uuid)
-        
-        if not job_info:
-            self.logger.warning(f"Job with UUID {env_uuid} not found")
-        
-        return job_info.get_status()
-
-    def list_jobs(self) -> List[Dict[str, Any]]:
-        return [job_info.get_summary() for job_info in self.jobs.values()]
-
-    def get_server_info(self) -> Dict[str, Any]:
-        job_summaries = [job_info.get_summary() for job_info in self.jobs.values()]
-            
-        return {
-            "session_id": self.session_id,
-            "log_base_dir": self.log_base_dir,
-            "environments_count": len(self.jobs),
-            "jobs": job_summaries
-        }
-    
-
-    def shutdown(self):
-        """
-        å®Œæ•´é‡Šæ”¾ Engine æŒæœ‰çš„æ‰€æœ‰èµ„æºï¼š
-        - åœæ‰ RuntimeManagerï¼ˆçº¿ç¨‹ã€Ray actor ç­‰ï¼‰
-        - åœæ‰å¯èƒ½çš„ TCP/HTTP server
-        - æ¸…ç©º DAG æ˜ å°„ä¸ç¼“å­˜
-        - é‡ç½® Engine å•ä¾‹
-        """
-        self.logger.info("Shutting down Engine and releasing resources")
-
-        JobManager._instance = None
-        self.logger.info("Engine shutdown complete")
-
-    def cleanup_all_jobs(self) -> Dict[str, Any]:
-        """æ¸…ç†æ‰€æœ‰ä½œä¸š"""
-        try:
-            cleanup_results = {}
-            
-            for env_uuid in list(self.jobs.keys()):
-                result = self.delete_job(env_uuid, force=True)
-                cleanup_results[env_uuid] = result
-            
-            self.logger.info(f"Cleaned up {len(cleanup_results)} jobs")
-            
-            return {
-                "status": "success",
-                "message": f"Cleaned up {len(cleanup_results)} jobs",
-                "results": cleanup_results
-            }
-            
-        except Exception as e:
-            self.logger.error(f"Failed to cleanup all jobs: {e}")
-            return {
-                "status": "failed",
-                "message": f"Failed to cleanup jobs: {str(e)}"
-            }
-
-    ########################################################
-    #                internal  methods                     #
-    ########################################################
-
-    def setup_logging_system(self):
-        """è®¾ç½®åˆ†å±‚æ—¥å¿—ç³»ç»Ÿ"""
-        # 1. ç”Ÿæˆæ—¶é—´æˆ³æ ‡è¯†
-        self.session_timestamp = datetime.now()
-        self.session_id = self.session_timestamp.strftime("%Y%m%d_%H%M%S")
-        
-        # 2. ç¡®å®šæ—¥å¿—åŸºç¡€ç›®å½•
-        # æ–¹æ¡ˆï¼š/tmp/sage/logs ä½œä¸ºå®é™…å­˜å‚¨ä½ç½®
-        project_root = Path(__file__).parent.parent
-        self.log_base_dir = project_root / "logs" / f"jobmanager_{self.session_id}"
-        Path(self.log_base_dir).mkdir(parents=True, exist_ok=True)
-
-        
-        # 3. åˆ›å»ºJobManagerä¸»æ—¥å¿—
-        self.logger = CustomLogger([
-            ("console", "INFO"),  # æ§åˆ¶å°æ˜¾ç¤ºé‡è¦ä¿¡æ¯
-            (os.path.join(self.log_base_dir, "jobmanager.log"), "DEBUG"),      # è¯¦ç»†æ—¥å¿—
-            (os.path.join(self.log_base_dir, "error.log"), "ERROR") # é”™è¯¯æ—¥å¿—
-        ], name="JobManager")
-
-    @property
-    def handle(self) -> 'JobManager':
-        return self
\ No newline at end of file
diff --git a/sage_jobmanager/remote_job_manager.py b/sage_jobmanager/remote_job_manager.py
deleted file mode 100644
index 52884c4..0000000
--- a/sage_jobmanager/remote_job_manager.py
+++ /dev/null
@@ -1,175 +0,0 @@
-import ray
-import time
-from typing import Dict, Any, List, TYPE_CHECKING
-from sage_jobmanager.job_manager import JobManager
-from ray.actor import ActorHandle
-if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment
-
-
-@ray.remote
-class RemoteJobManager(JobManager):
-    """
-    åŸºäºRay Actorçš„JobManagerï¼Œç»§æ‰¿æœ¬åœ°JobManagerçš„æ‰€æœ‰åŠŸèƒ½
-    é€šè¿‡Rayè£…é¥°å™¨è‡ªåŠ¨æ”¯æŒè¿œç¨‹è°ƒç”¨
-    """
-    
-    def __init__(self, *args, **kwargs):
-        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
-        super().__init__(*args, **kwargs)
-        
-        # Rayç‰¹æœ‰çš„Actorä¿¡æ¯
-        self.actor_id = ray.get_runtime_context().get_actor_id()
-        self.node_id = ray.get_runtime_context().get_node_id()
-        
-        self.logger.info(f"RemoteJobManager Actor initialized: {self.actor_id}")
-        self.logger.info(f"Running on Ray node: {self.node_id}")
-        self._actor_handle: ActorHandle = None
-    
-
-    # === ç»§æ‰¿çš„æ ¸å¿ƒæ–¹æ³•è‡ªåŠ¨æ”¯æŒRayè¿œç¨‹è°ƒç”¨ ===
-    # submit_job, pause_job, get_job_status, list_jobs ç­‰æ–¹æ³•
-    # æ— éœ€é‡å†™ï¼Œç›´æ¥ç»§æ‰¿çˆ¶ç±»å®ç°å³å¯
-    
-    # === Ray Actorä¸“ç”¨æ–¹æ³• ===
-    
-    def keep_alive(self) -> Dict[str, Any]:
-        """
-        é˜²æ­¢Actorè¢«å›æ”¶çš„å¿ƒè·³æ–¹æ³•
-        è¿”å›Actorçš„å¥åº·çŠ¶æ€ä¿¡æ¯
-        """
-        return {
-            "status": "alive",
-            "actor_id": self.actor_id,
-            "node_id": self.node_id,
-            "session_id": self.session_id,
-            "timestamp": time.time(),
-            "active_jobs": len(self.jobs),
-            "memory_info": self._get_memory_info()
-        }
-    
-    def get_actor_info(self) -> Dict[str, Any]:
-        """è·å–ActoråŸºæœ¬ä¿¡æ¯"""
-        return {
-            "actor_id": self.actor_id,
-            "node_id": self.node_id,
-            "session_id": self.session_id,
-            "log_base_dir": self.log_base_dir,
-            "actor_type": "RemoteJobManager"
-        }
-    
-    def _get_memory_info(self) -> Dict[str, Any]:
-        """è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
-        try:
-            import psutil
-            process = psutil.Process()
-            memory_info = process.memory_info()
-            return {
-                "rss_mb": round(memory_info.rss / 1024 / 1024, 2),
-                "vms_mb": round(memory_info.vms / 1024 / 1024, 2),
-                "available": True
-            }
-        except ImportError:
-            return {
-                "rss_mb": -1,
-                "vms_mb": -1,
-                "available": False,
-                "error": "psutil not available"
-            }
-    
-    def graceful_shutdown(self) -> Dict[str, Any]:
-        """
-        ä¼˜é›…å…³é—­Actor
-        åœæ­¢æ‰€æœ‰Jobå¹¶æ¸…ç†èµ„æº
-        """
-        self.logger.info("RemoteJobManager Actor shutting down...")
-        
-        shutdown_report = {
-            "jobs_stopped": 0,
-            "jobs_failed_to_stop": 0,
-            "errors": []
-        }
-        
-        # åœæ­¢æ‰€æœ‰æ´»è·ƒçš„Job
-        for env_uuid, job_info in list(self.jobs.items()):
-            try:
-                if job_info.status in ["running", "submitted"]:
-                    self.pause_job(env_uuid)
-                    shutdown_report["jobs_stopped"] += 1
-            except Exception as e:
-                shutdown_report["jobs_failed_to_stop"] += 1
-                shutdown_report["errors"].append(f"Failed to stop job {env_uuid}: {str(e)}")
-                self.logger.error(f"Error stopping job {env_uuid} during shutdown: {e}")
-        
-        # è°ƒç”¨çˆ¶ç±»shutdown
-        try:
-            super().shutdown()
-        except Exception as e:
-            shutdown_report["errors"].append(f"Parent shutdown error: {str(e)}")
-        
-        shutdown_report["status"] = "shutdown_complete"
-        shutdown_report["timestamp"] = time.time()
-        
-        self.logger.info(f"RemoteJobManager Actor shutdown complete: {shutdown_report}")
-        return shutdown_report
-    
-    # === å¢å¼ºçš„çŠ¶æ€ç›‘æ§æ–¹æ³• ===
-    
-    def get_extended_server_info(self) -> Dict[str, Any]:
-        """è·å–æ‰©å±•çš„æœåŠ¡å™¨ä¿¡æ¯ï¼ŒåŒ…å«Ray Actorç‰¹æœ‰ä¿¡æ¯"""
-        base_info = super().get_server_info()
-        
-        # æ·»åŠ Ray Actorä¿¡æ¯
-        base_info.update({
-            "actor_id": self.actor_id,
-            "node_id": self.node_id,
-            "actor_type": "RemoteJobManager",
-            "memory_info": self._get_memory_info(),
-            "uptime_seconds": time.time() - self.session_timestamp.timestamp()
-        })
-        
-        return base_info
-    
-    def health_check(self) -> Dict[str, Any]:
-        """å¥åº·æ£€æŸ¥æ–¹æ³•"""
-        try:
-            job_count = len(self.jobs)
-            running_jobs = len([j for j in self.jobs.values() if j.status == "running"])
-            
-            return {
-                "status": "healthy",
-                "actor_id": self.actor_id,
-                "total_jobs": job_count,
-                "running_jobs": running_jobs,
-                "timestamp": time.time(),
-                "session_uptime": time.time() - self.session_timestamp.timestamp()
-            }
-        except Exception as e:
-            return {
-                "status": "unhealthy",
-                "error": str(e),
-                "timestamp": time.time()
-            }
-    
-    def __repr__(self) -> str:
-        return f"RemoteJobManager(actor_id={self.actor_id[:8]}..., session={self.session_id}, jobs={len(self.jobs)})"
-
-    @property
-    def handle(self) -> 'ActorHandle':
-        if self._actor_handle is None:
-            try:
-                # æ–¹æ³•1: é€šè¿‡Actoråç§°è·å–ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
-                try:
-                    self._actor_handle = ray.get_actor("sage_global_jobmanager", namespace="sage_system")
-                    self.logger.debug("Got actor handle by name")
-                except ValueError:
-                    # æ–¹æ³•2: é€šè¿‡å…¨å±€æ³¨å†Œè¡¨è·å–ï¼ˆéœ€è¦äº‹å…ˆæ³¨å†Œï¼‰
-                    # æ–¹æ³•3: è¿”å›ä¸€ä¸ªä»£ç†å¯¹è±¡
-                    self._actor_handle = self._create_self_proxy()
-                    self.logger.debug("Created self proxy actor handle")
-                    
-            except Exception as e:
-                self.logger.error(f"Failed to get actor handle: {e}")
-                raise RuntimeError(f"Cannot obtain actor handle: {e}")
-        
-        return self._actor_handle
\ No newline at end of file
diff --git a/sage_libs/agents/agent.py b/sage_libs/agents/agent.py
index 47258bc..2e1c611 100644
--- a/sage_libs/agents/agent.py
+++ b/sage_libs/agents/agent.py
@@ -1,7 +1,10 @@
-from sage_utils.clients.generator_model import apply_generator_model
+from calendar import c
+from sage_common_funs.utils.generator_model import apply_generator_model
 from sage_core.function.map_function import MapFunction
+from jinja2 import Template
 
-from typing import Tuple
+from sage_utils.custom_logger import CustomLogger
+from typing import Any,Tuple
 import requests
 import json
 import re, time
diff --git a/sage_libs/agents/critic_bot.py b/sage_libs/agents/critic_bot.py
index 67a2c96..9c7fd31 100644
--- a/sage_libs/agents/critic_bot.py
+++ b/sage_libs/agents/critic_bot.py
@@ -1,9 +1,11 @@
 import json
 import re
 from typing import Dict, Any, Tuple
+from dataclasses import dataclass
 from jinja2 import Template
 from sage_core.function.map_function import MapFunction
-from sage_utils.clients.generator_model import apply_generator_model
+from sage_utils.custom_logger import CustomLogger
+from sage_common_funs.utils.generator_model import apply_generator_model
 from sage_libs.context.model_context import ModelContext
 from sage_libs.context.quality_label import QualityLabel
 from sage_libs.context.critic_evaluation import CriticEvaluation
diff --git a/sage_libs/agents/searcher_bot.py b/sage_libs/agents/searcher_bot.py
index c932e7b..5d1f706 100644
--- a/sage_libs/agents/searcher_bot.py
+++ b/sage_libs/agents/searcher_bot.py
@@ -1,9 +1,10 @@
-import time
-from typing import List, Dict, Any
+import os, time
+from typing import List, Dict, Union, Tuple, Any
 from jinja2 import Template
 from sage_core.function.map_function import MapFunction
+from sage_utils.custom_logger import CustomLogger
 from sage_libs.context.model_context import ModelContext
-from sage_utils.clients.generator_model import apply_generator_model
+from sage_common_funs.utils.generator_model import apply_generator_model
 
 # æœç´¢æŸ¥è¯¢ä¼˜åŒ–çš„promptæ¨¡æ¿
 SEARCH_QUERY_OPTIMIZATION_PROMPT = '''You are a search query optimization specialist. Your task is to analyze the user's original question and existing retrieved information, then design optimized search queries to fill information gaps.
diff --git a/sage_libs/rag/generator.py b/sage_libs/rag/generator.py
index 41c4b73..691ed09 100644
--- a/sage_libs/rag/generator.py
+++ b/sage_libs/rag/generator.py
@@ -3,7 +3,8 @@ from typing import Tuple,List
 from sage_common_funs.utils.generator_model import apply_generator_model
 from sage_core.function.map_function import MapFunction 
 from sage_core.function.base_function import StatefulFunction
-from sage_utils.state_persistence import load_function_state, save_function_state
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.state_persistence import load_function_state, save_function_state
 
 class OpenAIGenerator(MapFunction):
     """
@@ -82,9 +83,9 @@ class OpenAIGeneratorWithHistory(StatefulFunction):
         self.history_turns = config.get("max_history_turns", 5)
         self.num = 1
 
-        base = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
+        base = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
         os.makedirs(base, exist_ok=True)
-        path = os.path.join(base, f"{self.ctx.name}.chkpt")
+        path = os.path.join(base, f"{self.runtime_context.name}.chkpt")
         load_function_state(self, path)
 
     def execute(self, data: List, **kwargs) -> Tuple[str, str]:
@@ -116,9 +117,9 @@ class OpenAIGeneratorWithHistory(StatefulFunction):
         self.logger.info(f"\033[32m[{self.__class__.__name__}] Response: {response}\033[0m")
 
         # â€”â€” è‡ªåŠ¨æŒä¹…åŒ–ï¼šæ¯æ¬¡ execute åä¿å­˜çŠ¶æ€ â€”â€” 
-        base = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
+        base = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
         os.makedirs(base, exist_ok=True)
-        path = os.path.join(base, f"{self.ctx.name}.chkpt")
+        path = os.path.join(base, f"{self.runtime_context.name}.chkpt")
         save_function_state(self, path)
         return (user_query, response)
     
@@ -126,9 +127,9 @@ class OpenAIGeneratorWithHistory(StatefulFunction):
         """
         æ‰‹åŠ¨è§¦å‘ï¼šæŒä¹…åŒ–å½“å‰ dialogue_historyï¼Œç”¨äºæµ‹è¯•è°ƒç”¨ã€‚
         """
-        base = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
+        base = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
         os.makedirs(base, exist_ok=True)
-        path = os.path.join(base, f"{self.ctx.name}.chkpt")
+        path = os.path.join(base, f"{self.runtime_context.name}.chkpt")
         save_function_state(self, path)
 
 class HFGenerator(MapFunction):
diff --git a/sage_libs/rag/refiner.py b/sage_libs/rag/refiner.py
index c757acc..b8ad16c 100644
--- a/sage_libs/rag/refiner.py
+++ b/sage_libs/rag/refiner.py
@@ -1,7 +1,8 @@
 from sage_core.function.map_function import MapFunction
 
-from sage_utils.clients.generator_model import apply_generator_model
+from sage_common_funs.utils.generator_model import apply_generator_model
 from typing import Tuple,List
+import logging
 
 
 class AbstractiveRecompRefiner(MapFunction):
diff --git a/sage_libs/rag/retriever.py b/sage_libs/rag/retriever.py
index a65a6a7..b97f84d 100644
--- a/sage_libs/rag/retriever.py
+++ b/sage_libs/rag/retriever.py
@@ -3,6 +3,7 @@ import time  # æ›¿æ¢ asyncio ä¸º time ç”¨äºåŒæ­¥å»¶è¿Ÿ
 
 from sage_core.function.map_function import MapFunction
 from sage_core.function.base_function import MemoryFunction, StatefulFunction
+from sage_utils.custom_logger import CustomLogger
 from sage_runtime.runtime_context import RuntimeContext
 
 # æ›´æ–°åçš„ SimpleRetriever
@@ -29,7 +30,7 @@ class DenseRetriever(MapFunction):
             self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Retrieving from LTM \033[0m ")
             try:
                 # ä½¿ç”¨LTMé…ç½®å’Œè¾“å…¥æŸ¥è¯¢è°ƒç”¨æ£€ç´¢
-                ltm_results = self.ctx.retrieve(
+                ltm_results = self.runtime_context.retrieve(
                     query=input_query,
                     collection_config=self.ltm_config
                 )
@@ -60,7 +61,7 @@ class BM25sRetriever(MapFunction): # ç›®å‰runtime contextè¿˜åªæ”¯æŒltm
 
         try:
             # ä½¿ç”¨BM25sé…ç½®å’Œè¾“å…¥æŸ¥è¯¢è°ƒç”¨æ£€ç´¢
-            bm25s_results = self.ctx.retrieve(
+            bm25s_results = self.runtime_context.retrieve(
                 # self.bm25s_collection,
                 query=input_query,
                 collection_config=self.bm25s_config
diff --git a/sage_libs/rag/writer.py b/sage_libs/rag/writer.py
index 4027300..111c969 100644
--- a/sage_libs/rag/writer.py
+++ b/sage_libs/rag/writer.py
@@ -64,10 +64,6 @@ class MemoryWriter(MapFunction):
                 continue
 
             try:
-
-                # TODO: è¿™é‡Œçš„å®ç°å®é™…ä¸Šè¦æˆä¸ºç”±writer è¿™ä¸ªfunctionä¸»åŠ¨å¾€memory manager functionå‘é€ä¸€ä¸ªæ•°æ®ã€‚
-                # è€Œ memory manager functionæ‹¿åˆ°è¿™ä¸ªæ•°æ®ä¹‹åå°±ä¼šå»æ‰§è¡Œ `execute' method å³å¯å®ç°è®°å¿†çš„è¯»å†™ã€‚
-                # è¿™é‡Œå¯èƒ½ä¼šæœ‰ä¸€ä¸ªç”±äºè°ƒåº¦åŸå› å¯¼è‡´çš„é˜»å¡ -- å¯ä»¥è¢«ä¼˜åŒ–ï¼Œè¯·å‚è€ƒMorphStreamï¼
                 self.state.store(
                     collection=collection,
                     documents=processed_data,
diff --git a/sage_memory/api.py b/sage_memory/api.py
index b06ac88..295aa3d 100644
--- a/sage_memory/api.py
+++ b/sage_memory/api.py
@@ -1,9 +1,9 @@
 from typing import Optional, TYPE_CHECKING
-from sage_utils.embedding_methods.embedding_model import apply_embedding_model
+from sage_utils.embedding_model import apply_embedding_model
 from sage_memory.memory_manager import MemoryManager
 from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
-    pass
+    from sage_core.api.env import BaseEnvironment
 
 
 _manager = None
diff --git a/sage_jobmanager/execution_graph.py b/sage_runtime/compiler.py
similarity index 57%
rename from sage_jobmanager/execution_graph.py
rename to sage_runtime/compiler.py
index 5614c70..304de72 100644
--- a/sage_jobmanager/execution_graph.py
+++ b/sage_runtime/compiler.py
@@ -1,32 +1,31 @@
-
 from __future__ import annotations
-import os
 from typing import TYPE_CHECKING
-from typing import Dict, List, Set, Union
-from sage_core.environment.base_environment import BaseEnvironment
+from typing import Dict, List, Set
+from sage_core.api.env import BaseEnvironment
 from sage_core.transformation.base_transformation import BaseTransformation
 from sage_utils.custom_logger import CustomLogger
-from sage_jobmanager.utils.name_server import get_name
+from sage_utils.name_server import get_name
 from sage_runtime.runtime_context import RuntimeContext
 if TYPE_CHECKING:
-    from sage_jobmanager.job_manager import JobManager
-    from ray.actor import ActorHandle
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+    from sage_core.function.base_function import BaseFunction
+    from sage_runtime.function.factory import FunctionFactory
+    from sage_runtime.operator.factory import OperatorFactory
 
 
 class GraphNode:
-    def __init__(self, name: str, transformation: BaseTransformation, parallel_index: int, env:BaseEnvironment):
+    def __init__(self, name: str, transformation: BaseTransformation, parallel_index: int):
         self.name: str = name
         self.transformation: BaseTransformation = transformation
         self.parallel_index: int = parallel_index  # åœ¨è¯¥transformationä¸­çš„å¹¶è¡Œç´¢å¼•
         self.parallelism: int = transformation.parallelism
         self.is_spout: bool = transformation.is_spout
-        self.is_sink: bool = transformation.is_sink
+        # è¾“å…¥è¾“å‡ºchannelsï¼šæ¯ä¸ªchannelæ˜¯ä¸€ä¸ªè¾¹çš„åˆ—è¡¨
+
         self.input_channels:dict[int, List[GraphEdge]] = {}
         self.output_channels:List[List[GraphEdge]] = []
-
-        self.stop_signal_num: int = 0  # é¢„æœŸçš„æºèŠ‚ç‚¹æ•°é‡
-        self.runtime_context: RuntimeContext = None
-
+        self.runtime_context: RuntimeContext = RuntimeContext(self, transformation.env)
 
 class GraphEdge:
     def __init__(self,name:str,  output_node: GraphNode,  input_node:GraphNode = None, input_index:int = 0):
@@ -41,65 +40,24 @@ class GraphEdge:
         self.downstream_node:GraphNode = input_node
         self.input_index:int = input_index
 
-class ExecutionGraph:
-    def __init__(self, env:BaseEnvironment, jobmanager_handle:Union['JobManager', 'ActorHandle']):
+class Compiler:
+    def __init__(self, env:BaseEnvironment):
         self.env = env
+        self.name = env.name
         self.nodes:Dict[str, GraphNode] = {}
         self.edges:Dict[str, GraphEdge] = {}
         # æ„å»ºæ•°æ®æµä¹‹é—´çš„è¿æ¥æ˜ å°„
 
-        # self.log_base_dir = env.log_base_dir
-        # self.env_base_dir = env.env_base_dir
-        self.setup_logging_system()
+        self.logger = CustomLogger(
+            filename=f"Compiler_{env.name}",
+            console_output=False,
+            file_output=True
+        )
         # æ„å»ºåŸºç¡€å›¾ç»“æ„
         self._build_graph_from_pipeline(env)
-        self._calculate_source_dependencies()
-        self.generate_runtime_contexts(jobmanager_handle)
-        self.total_stop_signals = self.calculate_total_stop_signals()
+        
         self.logger.info(f"Successfully converted and optimized pipeline '{env.name}' to compiler with {len(self.nodes)} nodes and {len(self.edges)} edges")
 
-
-    def calculate_total_stop_signals(self):
-        """è®¡ç®—æ‰€æœ‰æºèŠ‚ç‚¹çš„åœæ­¢ä¿¡å·æ€»æ•°"""
-        total_signals = 0
-        for node in self.nodes.values():
-            if node.is_sink:
-                total_signals += node.stop_signal_num
-        return total_signals
-
-    def setup_logging_system(self): 
-        self.logger = CustomLogger([
-                ("console", "INFO"),  # æ§åˆ¶å°æ˜¾ç¤ºé‡è¦ä¿¡æ¯
-                (os.path.join(self.env.env_base_dir, "ExecutionGraph.log"), "DEBUG"),  # è¯¦ç»†æ—¥å¿—
-                (os.path.join(self.env.env_base_dir, "Error.log"), "ERROR")  # é”™è¯¯æ—¥å¿—
-            ],
-            name = f"ExecutionGraph_{self.env.name}",
-        )
-
-
-    def generate_runtime_contexts(self, jobmanager_handle):
-        """
-        ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆè¿è¡Œæ—¶ä¸Šä¸‹æ–‡
-        """
-        self.logger.debug("Generating runtime contexts for all nodes")
-        for node_name, node in self.nodes.items():
-            try:
-                node.runtime_context = RuntimeContext(node, node.transformation, self.env, jobmanager_handle)
-                self.logger.debug(f"Generated runtime context for node: {node_name}")
-            except Exception as e:
-                self.logger.error(f"Failed to generate runtime context for node {node_name}: {e}", exc_info=True)
-
-    def setup_logging_system(self):
-        self.logger = CustomLogger([
-                ("console", "INFO"),  # æ§åˆ¶å°æ˜¾ç¤ºé‡è¦ä¿¡æ¯
-                (os.path.join(self.env.env_base_dir, "ExecutionGraph.log"), "DEBUG"),  # è¯¦ç»†æ—¥å¿—
-                (os.path.join(self.env.env_base_dir, "Error.log"), "ERROR")  # é”™è¯¯æ—¥å¿—
-            ],
-            name = f"ExecutionGraph_{self.env.name}",
-        )
-
-
-
     def _build_graph_from_pipeline(self, env: BaseEnvironment):
         """
         æ ¹æ®transformation pipelineæ„å»ºå›¾, æ”¯æŒå¹¶è¡Œåº¦å’Œå¤šå¯¹å¤šè¿æ¥
@@ -111,20 +69,25 @@ class ExecutionGraph:
         self.logger.debug("Step 1: Generating parallel nodes for each transformation")
         for transformation in env.pipeline:
             # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœå‘ç°æœªå¡«å……çš„future transformationï¼ŒæŠ¥é”™
-            from sage_core.transformation.future_transformation import FutureTransformation
-            if isinstance(transformation, FutureTransformation):
+            if hasattr(transformation, 'is_future') and transformation.is_future:
                 if not transformation.filled:
                     raise RuntimeError(
-                        f"Unfilled future transformation '{transformation.future_name}' in pipeline. "
+                        f"Found unfilled future transformation '{transformation.future_name}' in pipeline. "
+                        f"All future streams must be filled with fill_future() before compilation."
+                    )
+                else:
+                    # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºå·²å¡«å……çš„future transformationåº”è¯¥è¢«ä»pipelineä¸­ç§»é™¤
+                    raise RuntimeError(
+                        f"Found filled future transformation '{transformation.future_name}' in pipeline. "
+                        f"This is unexpected - filled future transformations should be removed from pipeline."
                     )
-                continue
             
             node_names = []
             for i in range(transformation.parallelism):
                 try:
                     node_name = get_name(f"{transformation.basename}_{i}")
                     node_names.append(node_name)
-                    self.nodes[node_name] = GraphNode(node_name,   transformation, i, env)
+                    self.nodes[node_name] = GraphNode(node_name,   transformation, i)
                     self.logger.debug(f"Created node: {node_name} (parallel index: {i})")
                 except Exception as e:
                     self.logger.error(f"Error creating node {node_name}: {e}")
@@ -179,37 +142,4 @@ class ExecutionGraph:
                                     f"between {upstream_trans.operator_class.__name__} -> "
                                     f"{transformation.operator_class.__name__}")
         
-        self.logger.info(f"Graph construction completed: {len(self.nodes)} nodes, {len(self.edges)} edges")
-
-
-    def _calculate_source_dependencies(self):
-        """è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æºä¾èµ–å…³ç³»"""
-        self.logger.debug("Calculating source dependencies for all nodes")
-        
-        # ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹ä¾èµ–çš„æºèŠ‚ç‚¹
-        for node_name, node in self.nodes.items():
-            if node.is_sink:
-                # éæºèŠ‚ç‚¹é€šè¿‡BFSæ”¶é›†æ‰€æœ‰ä¸Šæ¸¸æºä¾èµ–
-                visited = set()
-                queue = [node_name]
-                source_deps = set()
-
-                while queue:
-                    current_name = queue.pop(0)
-                    if current_name in visited:
-                        continue
-                    visited.add(current_name)
-                    
-                    current_node = self.nodes[current_name]
-                    
-                    if current_node.is_spout:
-                        source_deps.add(current_node.transformation.basename)
-                        node.stop_signal_num += 1
-                    else:
-                        # æ·»åŠ æ‰€æœ‰ä¸Šæ¸¸èŠ‚ç‚¹åˆ°é˜Ÿåˆ—
-                        for input_channel in current_node.input_channels.values():
-                            for edge in input_channel:
-                                if edge.upstream_node.name not in visited:
-                                    queue.append(edge.upstream_node.name)
-            
-            self.logger.debug(f"Node {node_name} expects {node.stop_signal_num} source instances")
\ No newline at end of file
+        self.logger.info(f"Graph construction completed: {len(self.nodes)} nodes, {len(self.edges)} edges")
\ No newline at end of file
diff --git a/sage_runtime/dagnode/__init__.py b/sage_runtime/dagnode/__init__.py
new file mode 100644
index 0000000..0ff4321
--- /dev/null
+++ b/sage_runtime/dagnode/__init__.py
@@ -0,0 +1,4 @@
+# from .ray_runtime import RayRuntime
+# # from .ray_executor import RayDAGExecutor
+# # ä¾›é¡¶å±‚ sage/__init__.py ä½¿ç”¨
+# __all__ = ["RayRuntime"]
\ No newline at end of file
diff --git a/sage_runtime/dagnode/base_dag_node.py b/sage_runtime/dagnode/base_dag_node.py
new file mode 100644
index 0000000..97b6c1e
--- /dev/null
+++ b/sage_runtime/dagnode/base_dag_node.py
@@ -0,0 +1,91 @@
+from abc import ABC, abstractmethod
+import threading, copy
+from typing import Any, TYPE_CHECKING, Union
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.runtime_context import RuntimeContext
+from ray.actor import ActorHandle
+
+if TYPE_CHECKING:
+    from sage_runtime.io.connection import Connection
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_core.transformation.base_transformation import BaseTransformation, OperatorFactory
+    from sage_runtime.compiler import Compiler, GraphNode
+
+class BaseDAGNode(ABC):
+    def __init__(
+        self, 
+        name:str, 
+        runtime_context: 'RuntimeContext',
+        operator_factory: 'OperatorFactory'
+    ) -> None:
+        self.runtime_context: 'RuntimeContext'
+        self.operator:BaseOperator
+        self.delay: Union[int, float] = 1
+        self.is_spout: bool = False
+
+
+        self.operator_factory: 'OperatorFactory' = operator_factory
+        self.name = name
+        self._running = False
+        self.stop_event = threading.Event()
+        try:
+            self.runtime_context = runtime_context
+
+            self.operator = self.operator_factory.create_operator(name=self.name, runtime_context = runtime_context)
+            # Create logger first
+            self.logger = CustomLogger(
+                filename=f"Node_{runtime_context.name}",
+                env_name=runtime_context.env_name,
+                console_output="WARNING",
+                file_output="DEBUG",
+                global_output = "WARNING",
+                name = f"{runtime_context.name}_{self.__class__.__name__}"
+            )
+            self.logger.info(f"Node {self.name} initialized with type {self.__class__.__name__}")
+        except Exception as e:
+            self.logger.error(f"Failed to initialize node {self.name}: {e}", exc_info=True)
+
+    @abstractmethod
+    def run_loop(self) -> None:
+        """
+        Run the node's processing loop.
+        This method should be implemented by subclasses to define the node's behavior.
+        """
+        pass 
+
+    def add_connection(self, connection: 'Connection'):
+        """
+        æ·»åŠ è¿æ¥åˆ°DAGèŠ‚ç‚¹
+        :param connection: Connectionå¯¹è±¡ï¼ŒåŒ…å«è¿æ¥ä¿¡æ¯
+        """
+        self.operator.add_connection(connection)
+        self.logger.debug(f"Connection added to node '{self.name}': {connection}")
+
+    def submit(self):
+        pass
+
+
+    def trigger(self, input_tag: str = None, data:Any = None) -> None:
+        """
+        Execute the node once, processing any available input data.
+        This is typically used for spout nodes to emit initial data.
+        """
+        try:
+            self.logger.debug(f"Received data in node {self.name}, channel {input_tag}")
+            self.operator.receive_packet( data)
+        except Exception as e:
+            self.logger.error(f"Error processing data in node {self.name}: {e}", exc_info=True)
+            raise
+
+
+    def stop(self) -> None:
+        """Signal the worker loop to stop."""
+        if not self.stop_event.is_set():
+            self.stop_event.set()
+            self.logger.info(f"Node '{self.name}' received stop signal.")
+
+
+
+    def is_running(self):
+        """Check if the node is currently running."""
+        return self._running
\ No newline at end of file
diff --git a/sage_jobmanager/factory/task_factory.py b/sage_runtime/dagnode/factory.py
similarity index 54%
rename from sage_jobmanager/factory/task_factory.py
rename to sage_runtime/dagnode/factory.py
index be934ed..050f156 100644
--- a/sage_jobmanager/factory/task_factory.py
+++ b/sage_runtime/dagnode/factory.py
@@ -1,25 +1,28 @@
-from typing import Any, TYPE_CHECKING, Union
-from sage_runtime.task.ray_task import RayTask
-from sage_runtime.task.local_task import LocalTask
-from sage_runtime.task.base_task import BaseTask
-from sage_utils.actor_wrapper import ActorWrapper
+from typing import Type, Any, Dict, TYPE_CHECKING, Union
+from sage_runtime.function.factory import FunctionFactory
+from sage_runtime.dagnode.ray_dag_node import RayDAGNode
+from sage_runtime.dagnode.local_dag_node import LocalDAGNode
+
 if TYPE_CHECKING:
     from sage_core.transformation.base_transformation import BaseTransformation
+    from sage_core.api.env import BaseEnvironment
+    from sage_runtime.dagnode.base_dag_node import BaseDAGNode
     from ray.actor import ActorHandle
+    from sage_runtime.compiler import GraphNode
     from sage_runtime.runtime_context import RuntimeContext
     
-class TaskFactory:
+class DAGNodeFactory:
     def __init__(
         self,
         transformation: 'BaseTransformation',
         # parallel_index: int, # è¿™ä¸ªåœ¨create instanceæ—¶ä¼ å…¥
     ):
         self.basename = transformation.basename
-        self.env_name = transformation.env_name
+        self.env_name = transformation.env.name
         self.operator_factory = transformation.operator_factory
         self.delay = transformation.delay
         self.remote:bool = transformation.remote
-        self.memory_collection:Union[Any, ActorHandle] = transformation.memory_collection
+        self.memory_collection:Union[Any, ActorHandle] = transformation.env.memory_collection
         self.is_spout = transformation.is_spout
 
         # è¿™äº›å‚æ•°åœ¨åˆ›å»ºèŠ‚ç‚¹æ—¶æ³¨å…¥
@@ -27,17 +30,19 @@ class TaskFactory:
         # self.parallelism: int     # æ¥è‡ªå›¾ç¼–è¯‘
         # self.node_name: str       # æ¥è‡ªå›¾ç¼–è¯‘
 
-    def create_task(
+    def create_node(
         self,
         name: str,
         runtime_context: 'RuntimeContext' = None,
-    ) -> 'BaseTask':
+    ) -> 'BaseDAGNode':
         if self.remote:
-            node = RayTask.options(lifetime="detached").remote(runtime_context,  self.operator_factory)
-            node = ActorWrapper(node)
+            node = RayDAGNode(name, runtime_context,  self.operator_factory)
         else:
-            node = LocalTask(runtime_context, self.operator_factory)
+            node = LocalDAGNode(name, runtime_context, self.operator_factory)
+        node.delay = self.delay
+        node.is_spout = self.is_spout
+        # print(f"{name} is spout: {node.is_spout}")
         return node
     
     def __repr__(self) -> str:
-        return f"<TaskFactory {self.basename}>"
\ No newline at end of file
+        return f"<DAGNodeFactory {self.basename}>"
\ No newline at end of file
diff --git a/sage_runtime/dagnode/local_dag_node.py b/sage_runtime/dagnode/local_dag_node.py
new file mode 100644
index 0000000..08b75d7
--- /dev/null
+++ b/sage_runtime/dagnode/local_dag_node.py
@@ -0,0 +1,71 @@
+from __future__ import annotations
+import time, copy
+from typing import Any, Union, Tuple, TYPE_CHECKING
+from sage_runtime.io.local_message_queue import LocalMessageQueue
+from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+from ray.actor import ActorHandle
+from sage_memory.memory_collection.base_collection import BaseMemoryCollection
+from sage_utils.custom_logger import CustomLogger
+
+if TYPE_CHECKING:
+    from sage_core.transformation.base_transformation import BaseTransformation
+    from sage_runtime.operator.factory import OperatorFactory
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_runtime.operator.operator_wrapper import OperatorWrapper
+    from sage_runtime.compiler import Compiler, GraphNode
+    from sage_runtime.runtime_context import RuntimeContext
+
+
+
+class LocalDAGNode(BaseDAGNode):
+
+    def __init__(self, *args, **kwargs) -> None:
+        super().__init__(*args, **kwargs)
+        self.input_buffer = LocalMessageQueue(name = self.name, env_name=self.runtime_context.env_name)  # Local input buffer for this node
+        self.logger.info(f"Initialized LocalDAGNode: {self.name} (spout: {self.is_spout})")
+    
+    def put(self, data_packet: Any):
+        """
+        å‘è¾“å…¥ç¼“å†²åŒºæ”¾å…¥æ•°æ®åŒ…
+        
+        Args:
+            data_packet: (input_channel, data) å…ƒç»„
+        """
+        self.input_buffer.put(data_packet, timeout=1.0)
+        self.logger.debug(f"Put data packet into buffer")
+    
+
+    def run_loop(self) -> None:
+        """
+        Main worker loop that executes continuously until stop is signaled.
+        """
+
+        # Ensure all sage_runtime objects are initialized
+        self.stop_event.clear()
+        self._running = True
+        # Main execution loop
+        while not self.stop_event.is_set():
+            try:
+                if self.is_spout:
+                    self.logger.debug(f"Running spout node '{self.name}'")
+                    # For spout nodes, call operator.receive with dummy channel and data
+                    self.operator.receive_packet(None)
+                    time.sleep(self.delay)  # Sleep to avoid busy loop
+                else:
+                    # For non-spout nodes, fetch input and process
+                    # input_result = self.fetch_input()
+                    data_packet = self.input_buffer.get(timeout=0.5)
+                    if(data_packet is None):
+                        time.sleep(0.1)  # Short sleep when no data to process
+                        continue
+                    # Call operator's receive method with the channel_id and data
+                    self.operator.receive_packet(data_packet)
+            except Exception as e:
+                self.logger.error(
+                    f"Critical error in node '{self.name}': {str(e)}",
+                    exc_info=True
+                )
+                self.stop()
+                raise RuntimeError(f"Execution failed in node '{self.name}'")
+            finally:
+                self._running = False
\ No newline at end of file
diff --git a/sage_runtime/dagnode/ray_dag_node.py b/sage_runtime/dagnode/ray_dag_node.py
new file mode 100644
index 0000000..68a84d6
--- /dev/null
+++ b/sage_runtime/dagnode/ray_dag_node.py
@@ -0,0 +1,100 @@
+import ray
+import time
+from typing import Any, Dict, Union, TYPE_CHECKING
+from ray.actor import ActorHandle
+from sage_runtime.runtime_context import RuntimeContext
+from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+from sage_utils.custom_logger import CustomLogger
+if TYPE_CHECKING:   
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_runtime.operator.factory import OperatorFactory
+    from sage_runtime.operator.operator_wrapper import OperatorWrapper
+    from sage_runtime.compiler import Compiler, GraphNode
+
+class RayDAGNode(BaseDAGNode):
+    """
+    Ray Actor version of LocalDAGNode for distributed execution.
+    
+    Unlike local nodes, Ray actors don't need input buffers as Ray platform
+    maintains the request queue for actors automatically.
+    """
+    
+    def __init__(self, *args, **kwargs) -> None:
+        super().__init__(*args, **kwargs)
+
+    def run_loop(self):
+        """
+        Start the node. For spout nodes, this starts the generation loop.
+        For non-spout nodes, this just marks the node as ready to receive data.
+        """
+        self._running = True
+        if self.is_spout:
+            while not self.stop_event.is_set():
+    
+                # Start spout execution asynchronously
+                try:
+                    # For spout nodes, call operator.receive with dummy channel and data
+                    self.operator.receive_packet(None)
+                    time.sleep(self.delay)  # Small delay to prevent overwhelming
+                        
+                except Exception as e:
+                    self.logger.error(f"Error in spout node {self.name}: {e}", exc_info=True)
+                    raise
+            self._running = False
+            self.logger.info(f"Spout execution stopped for node {self.name}")
+        else:
+            # For non-spout nodes, just mark as running
+
+            self.logger.info(f"Ray node {self.name} started and ready to receive data")
+
+
+    ########################################################
+    #                inactive methods                      #
+    ########################################################
+
+    def get_node_info(self) -> Dict[str, Any]:
+        """Get comprehensive node information for debugging."""
+        return {
+            "name": self.name,
+            "is_spout": self.is_spout,
+            "is_running": self.is_running(),
+            "initialized": self._initialized,
+            "operator_class": self.function_class.__name__ if self.function_class else None,
+            "downstream_targets": len(self.emit_context.downstream_channels) if hasattr(self, 'emit_context') else 0
+        }
+
+    def health_check(self) -> Dict[str, Any]:
+        """Perform health check and return status."""
+        try:
+            return {
+                "status": "healthy",
+                "node_name": self.name,
+                "is_running": self.is_running(),
+                "initialized": self._initialized,
+                "timestamp": time.time_ns()
+            }
+        except Exception as e:
+            return {
+                "status": "unhealthy",
+                "node_name": self.name,
+                "error": str(e),
+                "timestamp": time.time_ns()
+            }
+
+    def __getstate__(self):
+        """
+        Custom serialization to exclude non-serializable objects.
+        Ray handles most serialization automatically, but this helps with debugging.
+        """
+        state = self.__dict__.copy()
+        # Ray actors typically don't need custom serialization,
+        # but we can exclude logger if needed
+        return state
+
+    def __setstate__(self, state):
+        """
+        Custom deserialization to restore state.
+        """
+        self.__dict__.update(state)
+        # Mark as not initialized so sage_runtime objects will be created when needed
+        self._initialized = False
\ No newline at end of file
diff --git a/sage_runtime/dispatcher.py b/sage_runtime/dispatcher.py
deleted file mode 100644
index b1795b2..0000000
--- a/sage_runtime/dispatcher.py
+++ /dev/null
@@ -1,259 +0,0 @@
-import os
-import time
-from typing import Dict, List, Any, Tuple, Union, TYPE_CHECKING
-from sage_runtime.task.base_task import BaseTask
-from sage_runtime.router.connection import Connection
-from sage_utils.custom_logger import CustomLogger
-import ray
-from ray.actor import ActorHandle
-if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment 
-    from sage_jobmanager.execution_graph import ExecutionGraph, GraphNode
-
-# è¿™ä¸ªdispatcherå¯ä»¥ç›´æ¥æ‰“åŒ…ä¼ ç»™ray sage daemon service
-class Dispatcher():
-    def __init__(self, graph: 'ExecutionGraph', env:'BaseEnvironment'):
-        self.total_stop_signals = graph.total_stop_signals
-        self.received_stop_signals = 0
-        self.graph = graph
-        self.env = env
-        self.name:str = env.name
-        self.remote = env.platform == "remote"
-        self.logger = CustomLogger([
-                ("console", "INFO"),  # æ§åˆ¶å°æ˜¾ç¤ºé‡è¦ä¿¡æ¯
-                (os.path.join(env.env_base_dir, "Dispatcher.log"), "DEBUG"),  # è¯¦ç»†æ—¥å¿—
-                (os.path.join(env.env_base_dir, "Error.log"), "ERROR")  # é”™è¯¯æ—¥å¿—
-            ],
-            name = f"Environment_{self.name}",
-        )
-        # self.nodes: Dict[str, Union[ActorHandle, LocalDAGNode]] = {}
-        self.tasks: Dict[str, BaseTask] = {}
-        self.is_running: bool = False
-        self.logger.info(f"Dispatcher '{self.name}' construction complete")
-        if env.platform is "remote" and not ray.is_initialized():
-            ray.init(address="auto", _temp_dir="/var/lib/ray_shared")
-        self.setup_logging_system()
-
-    def receive_stop_signal(self):
-        """
-        æ¥æ”¶åœæ­¢ä¿¡å·å¹¶å¤„ç†
-        """
-        self.logger.info(f"Dispatcher received stop signal.")
-        self.received_stop_signals += 1
-        if self.received_stop_signals >= self.total_stop_signals:
-            self.logger.info(f"Received all {self.total_stop_signals} stop signals, stopping dispatcher for batch job.")
-            self.cleanup()
-            return True
-        else:
-            return False
-
-
-    def setup_logging_system(self): 
-        self.logger = CustomLogger([
-                ("console", "INFO"),  # æ§åˆ¶å°æ˜¾ç¤ºé‡è¦ä¿¡æ¯
-                (os.path.join(self.env.env_base_dir, "Dispatcher.log"), "DEBUG"),  # è¯¦ç»†æ—¥å¿—
-                (os.path.join(self.env.env_base_dir, "Error.log"), "ERROR")  # é”™è¯¯æ—¥å¿—
-            ],
-            name = f"Dispatcher_{self.name}",
-        )
-
-    # Dispatcher will submit the job to LocalEngine or Ray Server.    
-    def submit(self):
-        """ç¼–è¯‘å›¾ç»“æ„ï¼Œåˆ›å»ºèŠ‚ç‚¹å¹¶å»ºç«‹è¿æ¥"""
-        self.logger.info(f"Compiling Job for graph: {self.name}")
-        
-        # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ‰€æœ‰èŠ‚ç‚¹å®ä¾‹
-        for node_name, graph_node in self.graph.nodes.items():
-            # task = graph_node.create_dag_node()
-            task = graph_node.transformation.task_factory.create_task(graph_node.name, graph_node.runtime_context)
-
-            self.tasks[node_name] = task
-
-            self.logger.debug(f"Added node '{node_name}' of type '{task.__class__.__name__}'")
-        
-        # ç¬¬äºŒæ­¥ï¼šå»ºç«‹èŠ‚ç‚¹é—´çš„è¿æ¥
-        for node_name, graph_node in self.graph.nodes.items():
-            self._setup_node_connections(node_name, graph_node)
-        
-        # ç¬¬ä¸‰æ­¥ï¼šæäº¤æ‰€æœ‰èŠ‚ç‚¹å¼€å§‹è¿è¡Œ
-        for node_name, task in self.tasks.items():
-            try:
-                task.start_running()
-                self.logger.debug(f"Started node: {node_name}")
-            except Exception as e:
-                self.logger.error(f"Failed to start node {node_name}: {e}", exc_info=True)
-        self.logger.info(f"Job submission completed: {len(self.tasks)} nodes")
-        self.is_running = True
-
-
-    def _setup_node_connections(self, node_name: str, graph_node: 'GraphNode'):
-        """
-        ä¸ºèŠ‚ç‚¹è®¾ç½®ä¸‹æ¸¸è¿æ¥
-        
-        Args:
-            node_name: èŠ‚ç‚¹åç§°
-            graph_node: å›¾èŠ‚ç‚¹å¯¹è±¡
-        """
-        output_handle = self.tasks[node_name]
-        
-        for broadcast_index, parallel_edges in enumerate(graph_node.output_channels):
-            for parallel_index, parallel_edge in enumerate(parallel_edges):
-                target_name = parallel_edge.downstream_node.name
-                target_input_index = parallel_edge.input_index
-                target_handle = self.tasks[target_name]
-
-                connection = Connection(
-                    broadcast_index=broadcast_index,
-                    parallel_index=parallel_index,
-                    target_name=target_name,
-                    target_input_buffer = target_handle.get_input_buffer(),
-                    target_input_index = target_input_index
-                )
-                try:
-                    output_handle.add_connection(connection)
-                    self.logger.debug(f"Setup connection: {node_name} -> {target_name}")
-                    
-                except Exception as e:
-                    self.logger.error(f"Error setting up connection {node_name} -> {target_name}: {e}", exc_info=True)
-
-    def stop(self):
-        """åœæ­¢æ‰€æœ‰ä»»åŠ¡"""
-        if not self.is_running:
-            self.logger.warning("Dispatcher is not running")
-            return
-            
-        self.logger.info(f"Stopping dispatcher '{self.name}'")
-        
-        # å‘é€åœæ­¢ä¿¡å·ç»™æ‰€æœ‰ä»»åŠ¡
-        for node_name, node_instance in self.tasks.items():
-            try:
-                node_instance.stop()
-                self.logger.debug(f"Sent stop signal to node: {node_name}")
-            except Exception as e:
-                self.logger.error(f"Error stopping node {node_name}: {e}")
-        
-        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡åœæ­¢ï¼ˆæœ€å¤šç­‰å¾…10ç§’ï¼‰
-        self._wait_for_tasks_stop(timeout=10.0)
-        
-        self.is_running = False
-        self.logger.info("Dispatcher stopped")
-
-    def _wait_for_tasks_stop(self, timeout: float = 10.0):
-        """ç­‰å¾…æ‰€æœ‰ä»»åŠ¡åœæ­¢"""
-        start_time = time.time()
-        
-        while time.time() - start_time < timeout:
-            all_stopped = True
-            
-            for node_name, task in self.tasks.items():
-                if hasattr(task, 'is_running') and task.is_running:
-                    all_stopped = False
-                    break
-            
-            if all_stopped:
-                self.logger.debug("All tasks stopped")
-                return
-                
-            time.sleep(0.1)
-        
-        self.logger.warning(f"Timeout waiting for tasks to stop after {timeout}s")
-
-    def cleanup(self):
-        """æ¸…ç†æ‰€æœ‰èµ„æº"""
-        self.logger.info(f"Cleaning up dispatcher '{self.name}'")
-        
-        try:
-            # åœæ­¢æ‰€æœ‰ä»»åŠ¡
-            if self.is_running:
-                self.stop()
-            
-            if self.remote:
-                # æ¸…ç† Ray Actors
-                self._cleanup_ray_actors()
-            else:
-                # æ¸…ç†ä»»åŠ¡å¼•ç”¨
-                for node_name, task in self.tasks.items():
-                    try:
-                        task.cleanup()
-                        self.logger.debug(f"Cleaned up task: {node_name}")
-                    except Exception as e:
-                        self.logger.error(f"Error cleaning up task {node_name}: {e}")
-            # æ¸…ç©ºä»»åŠ¡å­—å…¸
-            self.tasks.clear()
-            
-            self.logger.info("Dispatcher cleanup completed")
-            
-        except Exception as e:
-            self.logger.error(f"Error during dispatcher cleanup: {e}")
-
-    def _cleanup_ray_actors(self):
-        """æ¸…ç†æ‰€æœ‰ Ray Actors"""
-        
-        self.logger.info(f"Cleaning up {len(self.tasks)} Ray actors")
-        
-        # ç¬¬ä¸€é˜¶æ®µï¼šå‘é€æ¸…ç†ä¿¡å·
-        cleanup_futures = []
-        for actor in self.tasks:
-            try:
-                # è°ƒç”¨ Actor çš„ cleanup æ–¹æ³•
-                future = actor.cleanup.remote()
-                cleanup_futures.append((actor, future))
-            except Exception as e:
-                self.logger.warning(f"Failed to send cleanup signal to actor: {e}")
-        
-        # ç¬¬äºŒé˜¶æ®µï¼šç­‰å¾…æ¸…ç†å®Œæˆï¼ˆæœ‰è¶…æ—¶ï¼‰
-        if cleanup_futures:
-            self._wait_for_cleanup_completion(cleanup_futures, timeout=5.0)
-        
-        try:
-        # ç¬¬ä¸‰é˜¶æ®µï¼šå¼ºåˆ¶ç»ˆæ­¢æ‰€æœ‰ Actor
-            for actor in self.tasks:
-                ray.kill(actor)
-                self.logger.debug(f"Killed Ray actor: {actor}")
-        except Exception as e:
-            self.logger.warning(f"Failed to kill Ray actor {actor}: {e}")
-
-    def _wait_for_cleanup_completion(self, cleanup_futures: List[Tuple[Any, Any]], timeout: float = 5.0):
-        """ç­‰å¾…æ¸…ç†æ“ä½œå®Œæˆ"""
-        self.logger.debug(f"Waiting for {len(cleanup_futures)} actors cleanup (timeout: {timeout}s)")
-        
-        try:
-            futures = [future for _, future in cleanup_futures]
-            ray.get(futures, timeout=timeout)
-            self.logger.debug("All actors cleaned up successfully")
-        except ray.exceptions.RayTimeoutError:
-            self.logger.warning(f"Timeout waiting for actors cleanup after {timeout}s")
-        except Exception as e:
-            self.logger.error(f"Error waiting for cleanup completion: {e}")
-
-
-    def get_task_status(self) -> Dict[str, Any]:
-        """è·å–æ‰€æœ‰ä»»åŠ¡çš„çŠ¶æ€"""
-        status = {}
-        
-        for node_name, task in self.tasks.items():
-            try:
-                task_status = {
-                    "name": node_name,
-                    "running": getattr(task, 'is_running', False),
-                    "processed_count": getattr(task, '_processed_count', 0),
-                    "error_count": getattr(task, '_error_count', 0),
-                }
-                status[node_name] = task_status
-            except Exception as e:
-                status[node_name] = {
-                    "name": node_name,
-                    "error": str(e)
-                }
-        
-        return status
-
-
-    def get_statistics(self) -> Dict[str, Any]:
-        """è·å–dispatcherç»Ÿè®¡ä¿¡æ¯"""
-        return {
-            "name": self.name,
-            "is_running": self.is_running,
-            "task_count": len(self.tasks),
-            "task_status": self.get_task_status()
-        }
\ No newline at end of file
diff --git a/sage_runtime/function/__init__.py b/sage_runtime/function/__init__.py
new file mode 100644
index 0000000..0ff4321
--- /dev/null
+++ b/sage_runtime/function/__init__.py
@@ -0,0 +1,4 @@
+# from .ray_runtime import RayRuntime
+# # from .ray_executor import RayDAGExecutor
+# # ä¾›é¡¶å±‚ sage/__init__.py ä½¿ç”¨
+# __all__ = ["RayRuntime"]
\ No newline at end of file
diff --git a/sage_jobmanager/factory/function_factory.py b/sage_runtime/function/factory.py
similarity index 94%
rename from sage_jobmanager/factory/function_factory.py
rename to sage_runtime/function/factory.py
index 4f21e43..325e68e 100644
--- a/sage_jobmanager/factory/function_factory.py
+++ b/sage_runtime/function/factory.py
@@ -1,9 +1,9 @@
 from typing import Type, Any, Tuple,TYPE_CHECKING, Union
 from sage_core.function.base_function import BaseFunction
+from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
     from ray.actor import ActorHandle
     from sage_runtime.runtime_context import RuntimeContext
-    
 class FunctionFactory:
     # ç”±transformationåˆå§‹åŒ–
     def __init__(
diff --git a/sage_runtime/router/__init__.py b/sage_runtime/io/__init__.py
similarity index 100%
rename from sage_runtime/router/__init__.py
rename to sage_runtime/io/__init__.py
diff --git a/sage_runtime/io/connection.py b/sage_runtime/io/connection.py
new file mode 100644
index 0000000..8fc3d2f
--- /dev/null
+++ b/sage_runtime/io/connection.py
@@ -0,0 +1,129 @@
+from typing import Union, TYPE_CHECKING
+from dataclasses import dataclass
+from enum import Enum
+from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+from sage_runtime.dagnode.local_dag_node import LocalDAGNode
+from sage_runtime.dagnode.ray_dag_node import RayDAGNode
+from ray.actor import ActorHandle
+from sage_runtime.io.local_tcp_server import LocalTcpServer
+class NodeType(Enum):
+    LOCAL = "local"
+    RAY_ACTOR = "ray_actor"
+
+class ConnectionType(Enum):
+    LOCAL_TO_LOCAL = "local_to_local"
+    LOCAL_TO_RAY = "local_to_ray"
+    RAY_TO_LOCAL = "ray_to_local"
+    RAY_TO_RAY = "ray_to_ray"
+
+@dataclass
+class Connection:
+    """
+    ç”¨äºè¡¨ç¤ºæœ¬åœ°èŠ‚ç‚¹å’ŒRay Actorä¹‹é—´çš„è¿æ¥
+    """
+    def __init__(self,
+                 own_node: Union[ActorHandle, LocalDAGNode],
+                 broadcast_index: int,
+                 parallel_index: int,
+                 target_name: str,
+                 target_node: Union[ActorHandle, LocalDAGNode],
+                 target_input_index: int,
+                 tcp_server: LocalTcpServer):
+
+        self.broadcast_index: int = broadcast_index
+        self.parallel_index: int = parallel_index
+        self.target_name: str = target_name
+        self.target_input_index: int = target_input_index
+        # ç»Ÿä¸€çš„èŠ‚ç‚¹ç±»å‹æ£€æµ‹
+        self.own_type: NodeType = self._detect_node_type(own_node)
+        self.target_type: NodeType = self._detect_node_type(target_node)
+        
+        # æ ¹æ®è¿æ¥ç±»å‹æ„å»ºé…ç½®
+        self.connection_type: ConnectionType = self._get_connection_type()
+        self.target_config: dict = self._build_target_config(target_node, tcp_server)
+
+    def _detect_node_type(self, node: Union[ActorHandle, LocalDAGNode]) -> NodeType:
+        """
+        ç»Ÿä¸€çš„èŠ‚ç‚¹ç±»å‹æ£€æµ‹æ–¹æ³•
+        
+        Args:
+            node: è¦æ£€æµ‹çš„èŠ‚ç‚¹å¯¹è±¡
+            
+        Returns:
+            NodeType: èŠ‚ç‚¹ç±»å‹æšä¸¾
+            
+        Raises:
+            NotImplementedError: å½“èŠ‚ç‚¹ç±»å‹æœªçŸ¥æ—¶
+        """
+        if isinstance(node, LocalDAGNode):
+            return NodeType.LOCAL
+        elif isinstance(node, RayDAGNode):
+            return NodeType.RAY_ACTOR
+        else:
+            raise NotImplementedError(f"æœªçŸ¥èŠ‚ç‚¹ç±»å‹: {type(node)}")
+
+    def _get_connection_type(self) -> ConnectionType:
+        """
+        æ ¹æ®æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹ç±»å‹ç¡®å®šè¿æ¥ç±»å‹
+        
+        Returns:
+            ConnectionType: è¿æ¥ç±»å‹æšä¸¾
+        """
+        if self.own_type == NodeType.LOCAL and self.target_type == NodeType.LOCAL:
+            return ConnectionType.LOCAL_TO_LOCAL
+        elif self.own_type == NodeType.LOCAL and self.target_type == NodeType.RAY_ACTOR:
+            return ConnectionType.LOCAL_TO_RAY
+        elif self.own_type == NodeType.RAY_ACTOR and self.target_type == NodeType.LOCAL:
+            return ConnectionType.RAY_TO_LOCAL
+        elif self.own_type == NodeType.RAY_ACTOR and self.target_type == NodeType.RAY_ACTOR:
+            return ConnectionType.RAY_TO_RAY
+        else:
+            raise NotImplementedError(f"æœªçŸ¥è¿æ¥ç±»å‹: {self.own_type} â†’ {self.target_type}")
+
+    def _build_target_config(self, target_node: BaseDAGNode, 
+                           tcp_server: LocalTcpServer) -> dict:
+        """
+        æ ¹æ®è¿æ¥ç±»å‹æ„å»ºç›®æ ‡é…ç½®å­—å…¸
+        
+        Args:
+            target_node: ç›®æ ‡èŠ‚ç‚¹å¯¹è±¡
+            tcp_server: TCPæœåŠ¡å™¨å¯¹è±¡
+            
+        Returns:
+            dict: ç›®æ ‡é…ç½®å­—å…¸
+        """
+        if self.connection_type == ConnectionType.LOCAL_TO_LOCAL:
+            # æœ¬åœ°åˆ°æœ¬åœ°çš„è¿æ¥
+            return {
+                "type": "direct_local",
+                "dagnode": target_node,
+                "node_name": self.target_name
+            }
+
+        elif self.connection_type == ConnectionType.LOCAL_TO_RAY:
+            # æœ¬åœ°åˆ°Ray Actorçš„è¿æ¥
+            return {
+                "type": "actor_handle",
+                "actorhandle": target_node.operator.get_wrapped_operator(),
+                "node_name": self.target_name
+            }
+
+        elif self.connection_type == ConnectionType.RAY_TO_LOCAL:
+            # Ray Actoråˆ°æœ¬åœ°çš„è¿æ¥
+            return {
+                "type": "local_tcp",
+                "node_name": self.target_name,
+                "tcp_host": tcp_server.host,
+                "tcp_port": tcp_server.port
+            }
+
+        elif self.connection_type == ConnectionType.RAY_TO_RAY:
+            # Ray Actoråˆ°Ray Actorçš„è¿æ¥
+            return {
+                "type": "actor_handle",
+                "actorhandle": target_node.operator.get_wrapped_operator(),
+                "node_name": self.target_name
+            }
+
+        else:
+            raise NotImplementedError(f"æœªçŸ¥è¿æ¥ç±»å‹: {self.connection_type}")
diff --git a/sage_runtime/utils/local_message_queue.py b/sage_runtime/io/local_message_queue.py
similarity index 85%
rename from sage_runtime/utils/local_message_queue.py
rename to sage_runtime/io/local_message_queue.py
index d8e41cf..9738e7d 100644
--- a/sage_runtime/utils/local_message_queue.py
+++ b/sage_runtime/io/local_message_queue.py
@@ -4,20 +4,30 @@ import threading
 import time
 from collections import deque
 import sys
-from sage_runtime.runtime_context import RuntimeContext
+from sage_utils.custom_logger import CustomLogger
 
 
 class LocalMessageQueue:
 
-    def __init__(self, ctx:RuntimeContext):
+    def __init__(self, name="MessageQueue", max_buffer_size=30000, session_folder: str = None, env_name: str = None):
+        self.name = name
+        self.session_folder = session_folder
         self.queue = queue.Queue(maxsize=50000)
-        self.name = ctx.name
         self.total_task = 0
-        self.max_buffer_size = 30000  # æ€»å†…å­˜é™åˆ¶ï¼ˆå­—èŠ‚ï¼‰
+        self.max_buffer_size = max_buffer_size  # æ€»å†…å­˜é™åˆ¶ï¼ˆå­—èŠ‚ï¼‰
         self.current_buffer_usage = 0 # å½“å‰ä½¿ç”¨çš„å†…å­˜ï¼ˆå­—èŠ‚ï¼‰
         self.memory_tracker = {}  # è·Ÿè¸ªæ¯ä¸ªé¡¹ç›®çš„å†…å­˜å¤§å° {id(item): size}
         # self.task_per_minute = 0
-        self.logger = ctx.logger
+        self.logger = CustomLogger(
+            filename=f"Node_{name}",
+            env_name=env_name,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output = "WARNING",
+            name = f"{name}_LocalMessageQueue"
+        )
+        
+
         self.timestamps = deque()
         self.lock = threading.Lock()
         self.buffer_condition = threading.Condition(self.lock)  # ç”¨äºå†…å­˜ç©ºé—´é€šçŸ¥
@@ -164,29 +174,3 @@ class LocalMessageQueue:
             }
 
 
-    def put_nowait(self, item):
-        """
-        éé˜»å¡æ–¹å¼å°†é¡¹ç›®æ”¾å…¥é˜Ÿåˆ—ï¼ˆç­‰ä»·äº put(item, block=False)ï¼‰
-        å¦‚æœé˜Ÿåˆ—å·²æ»¡æˆ–å†…å­˜ä¸è¶³ï¼Œä¼šç«‹å³æŠ›å‡º queue.Full å¼‚å¸¸
-        
-        Args:
-            item: è¦æ”¾å…¥çš„æ•°æ®é¡¹
-            
-        Raises:
-            queue.Full: å½“é˜Ÿåˆ—å·²æ»¡æˆ–å†…å­˜é™åˆ¶è¶…å‡ºæ—¶
-        """
-        return self.put(item, block=False)
-
-    def put_no_wait(self, item):
-        """
-        put_nowaitçš„åˆ«åï¼Œä¸ºäº†ä¸Ray Queueæ¥å£ä¿æŒä¸€è‡´
-        
-        Args:
-            item: è¦æ”¾å…¥çš„æ•°æ®é¡¹
-            
-        Raises:
-            queue.Full: å½“é˜Ÿåˆ—å·²æ»¡æˆ–å†…å­˜é™åˆ¶è¶…å‡ºæ—¶
-        """
-        return self.put_nowait(item)
-
-    # ... ç°æœ‰çš„ _do_put, get, metrics æ–¹æ³•ä¿æŒä¸å˜ ...
\ No newline at end of file
diff --git a/sage_runtime/io/local_tcp_server.py b/sage_runtime/io/local_tcp_server.py
new file mode 100644
index 0000000..ec5a20d
--- /dev/null
+++ b/sage_runtime/io/local_tcp_server.py
@@ -0,0 +1,235 @@
+import socket
+import threading
+import pickle
+from typing import Dict, Any, Callable, Optional
+from sage_utils.custom_logger import CustomLogger
+
+
+class LocalTcpServer:
+    """
+    æœ¬åœ°TCPæœåŠ¡å™¨ï¼Œç”¨äºæ¥æ”¶Ray Actorå‘é€çš„æ•°æ®
+    """
+    
+    def __init__(self, 
+                 host: str = None, 
+                 port: int = None,
+                 message_handler: Optional[Callable[[Dict[str, Any], tuple], None]] = None):
+        """
+        åˆå§‹åŒ–TCPæœåŠ¡å™¨
+        
+        Args:
+            host: ç›‘å¬åœ°å€
+            port: ç›‘å¬ç«¯å£
+            message_handler: æ¶ˆæ¯å¤„ç†å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (message, client_address) å‚æ•°
+        """
+        self.host = host or self._get_host_ip()  # ç¡®å®šè‡ªå·±çš„ipåœ°å€
+        self.port = port or self._allocate_tcp_port()  # åˆ†é…ä¸€ä¸ªå¯ç”¨çš„ç«¯å£
+        self.message_handler = message_handler
+        self.server_socket: Optional[socket.socket] = None
+        self.server_thread: Optional[threading.Thread] = None
+        self.running = False
+        
+        self.logger = CustomLogger(
+            filename="LocalTcpServer",
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="WARNING"
+        )
+    
+        self.logger.info(f"Initializing LocalTcpServer on {self.host}:{self.port}")
+
+
+    def _get_host_ip(self):
+        """è‡ªåŠ¨è·å–æœ¬æœºå¯ç”¨äºå¤–éƒ¨è¿æ¥çš„ IP åœ°å€"""
+        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+        try:
+            # è¿æ¥åˆ°ä»»æ„å…¬ç½‘åœ°å€ï¼ˆä¸å¿…å¯è¾¾ï¼Œåªä¸ºå–å‡ºç»‘å®šIPï¼‰
+            s.connect(("8.8.8.8", 80))
+            ip = s.getsockname()[0]
+        except Exception:
+            self.logger.warning("Failed to get external IP, using localhost")
+            ip = "127.0.0.1"
+        finally:
+            s.close()
+        return ip
+    
+    def _allocate_tcp_port(self) -> int:
+        """ä¸º DAG åˆ†é…å¯ç”¨çš„ TCP ç«¯å£"""
+        import socket
+        
+        # å°è¯•ä»é¢„è®¾èŒƒå›´åˆ†é…ç«¯å£
+        for port in range(19000, 20000):  # DAG ä¸“ç”¨ç«¯å£èŒƒå›´
+            try:
+                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+                    s.bind((self.host, port))
+                    return port
+            except OSError:
+                continue
+        # å¦‚æœé¢„è®¾èŒƒå›´éƒ½è¢«å ç”¨ï¼Œä½¿ç”¨ç³»ç»Ÿåˆ†é…
+        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+            self.logger.warning("All predefined ports are occupied, using system-assigned port")
+            s.bind((self.host, 0))
+            return s.getsockname()[1]
+        
+
+    def set_message_handler(self, handler: Callable[[Dict[str, Any], tuple], None]):
+        """è®¾ç½®æ¶ˆæ¯å¤„ç†å™¨"""
+        self.message_handler = handler
+    
+    def start(self):
+        """å¯åŠ¨TCPæœåŠ¡å™¨"""
+        if self.running:
+            self.logger.warning("TCP server is already running")
+            return
+        
+        try:
+            self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+            self.server_socket.settimeout(5)  # è®¾ç½®è¶…æ—¶ï¼Œé¿å…é˜»å¡
+            self.server_socket.bind((self.host, self.port))
+            self.server_socket.listen(10)
+            
+            self.running = True
+            self.server_thread = threading.Thread(
+                target=self._server_loop,
+                name="LocalTcpServerThread"
+            )
+            self.server_thread.daemon = True
+            self.server_thread.start()
+            
+            self.logger.info(f"TCP server started on {self.host}:{self.port}")
+            
+        except Exception as e:
+            self.logger.error(f"Failed to start TCP server: {e}")
+            self.running = False
+            raise
+    
+    def stop(self):
+        """åœæ­¢TCPæœåŠ¡å™¨"""
+        if not self.running:
+            return
+        
+        self.logger.info("Stopping TCP server...")
+        self.running = False
+        
+        if self.server_socket:
+            self.server_socket.close()
+        
+        if self.server_thread and self.server_thread.is_alive():
+            for _ in range(5):  # æœ€å¤šç­‰5ç§’
+                self.server_thread.join(timeout=1.0)
+                if not self.server_thread.is_alive():
+                    break
+            else:
+                self.logger.warning("TCP server thread did not stop gracefully")
+        
+        self.logger.info("TCP server stopped")
+    
+    def _server_loop(self):
+        """TCPæœåŠ¡å™¨ä¸»å¾ªç¯"""
+        self.logger.debug("TCP server loop started")
+        
+        while self.running:
+            try:
+                if not self.server_socket:
+                    break
+                    
+                client_socket, address = self.server_socket.accept()
+                self.logger.debug(f"New TCP client connected from {address}")
+                
+                # åœ¨æ–°çº¿ç¨‹ä¸­å¤„ç†å®¢æˆ·ç«¯
+                client_thread = threading.Thread(
+                    target=self._handle_client,
+                    args=(client_socket, address),
+                    name=f"TcpClient-{address[0]}:{address[1]}"
+                )
+                client_thread.daemon = True
+                client_thread.start()
+            except socket.timeout:
+                # è¶…æ—¶ä¸å¤„ç†ï¼Œç»§ç»­å¾ªç¯
+                continue
+            except OSError as e:
+                # Socketè¢«å…³é—­æ—¶ä¼šæŠ›å‡ºOSError
+                if self.running:
+                    self.logger.error(f"Error accepting TCP connection: {e}")
+                break
+            except Exception as e:
+                if self.running:
+                    self.logger.error(f"Unexpected error in server loop: {e}")
+        
+        self.logger.debug("TCP server loop stopped")
+    
+    def _handle_client(self, client_socket: socket.socket, address: tuple):
+        """å¤„ç†TCPå®¢æˆ·ç«¯è¿æ¥å’Œæ¶ˆæ¯"""
+        try:
+            while self.running:
+                # è¯»å–æ¶ˆæ¯é•¿åº¦
+                size_data = client_socket.recv(4)
+                if not size_data:
+                    break
+                
+                message_size = int.from_bytes(size_data, byteorder='big')
+                if message_size <= 0 or message_size > 10 * 1024 * 1024:  # 10MB limit
+                    self.logger.warning(f"Invalid message size {message_size} from {address}")
+                    break
+                
+                # è¯»å–æ¶ˆæ¯å†…å®¹
+                message_data = self._receive_full_message(client_socket, message_size)
+                if not message_data:
+                    break
+                
+                # ååºåˆ—åŒ–å¹¶å¤„ç†æ¶ˆæ¯
+                try:
+                    message = pickle.loads(message_data)
+                    self._process_message(message, address)
+                except Exception as e:
+                    self.logger.error(f"Error processing message from {address}: {e}")
+                
+        except Exception as e:
+            self.logger.error(f"Error handling TCP client {address}: {e}")
+        finally:
+            try:
+                client_socket.close()
+            except:
+                pass
+            self.logger.debug(f"TCP client {address} disconnected")
+    
+    def _receive_full_message(self, client_socket: socket.socket, message_size: int) -> Optional[bytes]:
+        """æ¥æ”¶å®Œæ•´çš„æ¶ˆæ¯æ•°æ®"""
+        message_data = b''
+        while len(message_data) < message_size:
+            chunk_size = min(message_size - len(message_data), 8192)  # 8KB chunks
+            chunk = client_socket.recv(chunk_size)
+            if not chunk:
+                self.logger.warning("Connection closed while receiving message")
+                return None
+            message_data += chunk
+        
+        return message_data
+    
+    def _process_message(self, message: Dict[str, Any], client_address: tuple):
+        """å¤„ç†æ¥æ”¶åˆ°çš„æ¶ˆæ¯"""
+        try:
+            if self.message_handler:
+                self.message_handler(message, client_address)
+            else:
+                self.logger.warning(f"No message handler set, ignoring message from {client_address}")
+                
+        except Exception as e:
+            self.logger.error(f"Error in message handler: {e}", exc_info=True)
+    
+    def get_server_info(self) -> Dict[str, Any]:
+        """è·å–æœåŠ¡å™¨ä¿¡æ¯"""
+        return {
+            "host": self.host,
+            "port": self.port,
+            "running": self.running,
+            "address": f"{self.host}:{self.port}"
+        }
+    
+    def __del__(self):
+        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
+        try:
+            self.stop()
+        except:
+            pass
\ No newline at end of file
diff --git a/sage_runtime/router/packet.py b/sage_runtime/io/packet.py
similarity index 100%
rename from sage_runtime/router/packet.py
rename to sage_runtime/io/packet.py
diff --git a/sage_runtime/io/unified_emit_context.py b/sage_runtime/io/unified_emit_context.py
new file mode 100644
index 0000000..730d513
--- /dev/null
+++ b/sage_runtime/io/unified_emit_context.py
@@ -0,0 +1,195 @@
+from typing import Any , TYPE_CHECKING
+from ray.actor import ActorHandle
+import socket
+import pickle
+import threading
+import time
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.io.connection import Connection, ConnectionType
+from sage_runtime.io.packet import Packet
+
+if TYPE_CHECKING:
+    pass
+
+class UnifiedEmitContext:
+    """
+    ç»Ÿä¸€çš„Emit Contextï¼Œæ”¯æŒæ‰€æœ‰ç±»å‹çš„è¿æ¥
+    æ ¹æ®Connectionå¯¹è±¡ä¸­çš„é…ç½®è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å‘é€æ–¹å¼
+    """
+    
+    def __init__(self, session_folder: str = None, name: str = None,env_name = None, **kwargs):
+        self.logger = CustomLogger(
+            filename=f"Node_{name}",
+            env_name=env_name,
+            session_folder=session_folder,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="WARNING",
+            name=f"{name}_UnifiedEmitContext"
+        )
+        
+
+        self.name = name
+        
+        # TCPè¿æ¥ç®¡ç†ï¼ˆç”¨äºRayåˆ°Localçš„è¿æ¥ï¼‰
+        self._tcp_connections: dict = {}  # host:port -> socket
+        self._socket_lock = threading.Lock()
+
+    def send_packet_direct(self, connection: 'Connection', packet: 'Packet') -> None:
+        """
+        ç›´æ¥å‘é€å·²å°è£…çš„packetï¼Œä¸å†é‡æ–°å°è£…
+        
+        Args:
+            connection: Connectionå¯¹è±¡
+            packet: å·²å°è£…å¥½çš„Packetå¯¹è±¡
+        """
+        try:
+            connection_type = connection.connection_type
+            if connection_type == ConnectionType.LOCAL_TO_LOCAL:
+                self._send_local_to_local(connection, packet)
+            elif connection_type == ConnectionType.LOCAL_TO_RAY:
+                self._send_local_to_ray(connection, packet)
+            elif connection_type == ConnectionType.RAY_TO_LOCAL:
+                self._send_ray_to_local(connection, packet)
+            elif connection_type == ConnectionType.RAY_TO_RAY:
+                self._send_ray_to_ray(connection, packet)
+            else:
+                raise ValueError(f"Unknown connection type: {connection_type}")
+        except Exception as e:
+            self.logger.error(f"Failed to send packet via connection {connection}: {e}", exc_info=True)
+
+    def _send_local_to_local(self, connection: 'Connection', packet: 'Packet') -> None:
+        """æœ¬åœ°åˆ°æœ¬åœ°ï¼šç›´æ¥è°ƒç”¨putæ–¹æ³•"""
+        try:
+            target_node = connection.target_config["dagnode"]
+            
+            # å‘é€åˆ°ç›®æ ‡èŠ‚ç‚¹çš„è¾“å…¥ç¼“å†²åŒº
+            if hasattr(target_node, 'put'):
+                target_node.put(packet)
+            elif hasattr(target_node, 'input_buffer'):
+                target_node.input_buffer.put(packet)
+            else:
+                raise AttributeError(f"Local node {connection.target_name} has no put method or input_buffer")
+                
+            self.logger.debug(f"Sent local->local: {connection.target_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Error in local->local send: {e}")
+            raise
+
+    def _send_local_to_ray(self, connection: 'Connection', packet: 'Packet') -> None:
+        """æœ¬åœ°åˆ°Ray Actorï¼šè¿œç¨‹è°ƒç”¨"""
+        try:
+            actor_handle = connection.target_config["actorhandle"]
+            
+            if not isinstance(actor_handle, ActorHandle):
+                raise TypeError(f"Expected ActorHandle, got {type(actor_handle)}")
+            
+            # è°ƒç”¨Ray Actorçš„process_dataæ–¹æ³•
+            actor_handle.receive_packet.remote(packet)
+            
+            self.logger.debug(f"Sent local->ray: {connection.target_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Error in local->ray send: {e}")
+            raise
+
+    def _send_ray_to_local(self, connection: 'Connection', packet: 'Packet') -> None:
+        """Ray Actoråˆ°æœ¬åœ°ï¼šTCPè¿æ¥"""
+        try:
+            tcp_host = connection.target_config["tcp_host"]
+            tcp_port = connection.target_config["tcp_port"]
+            target_node_name = connection.target_config["node_name"]
+            
+            # æ„é€ TCPæ¶ˆæ¯åŒ…
+            message = {
+                "type": "ray_to_local",
+                "source_actor": self.name,
+                "target_node": target_node_name,
+                "data": packet,
+                "timestamp": time.time_ns()
+            }
+            
+            # è·å–TCPè¿æ¥å¹¶å‘é€
+            tcp_connection = self._get_tcp_connection(tcp_host, tcp_port)
+            serialized_data = pickle.dumps(message)
+            message_size = len(serialized_data)
+            
+            tcp_connection.sendall(message_size.to_bytes(4, byteorder='big'))
+            tcp_connection.sendall(serialized_data)
+            
+            self.logger.debug(f"Sent ray->local via TCP: {target_node_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Error in ray->local TCP send: {e}")
+            # é‡ç½®TCPè¿æ¥
+            self._reset_tcp_connection(connection.target_config["tcp_host"], 
+                                     connection.target_config["tcp_port"])
+            raise
+
+    def _send_ray_to_ray(self, connection: 'Connection', packet: 'Packet') -> None:
+        """Ray Actoråˆ°Ray Actorï¼šè¿œç¨‹è°ƒç”¨"""
+        try:
+            actor_handle = connection.target_config["actorhandle"]
+            
+            if not isinstance(actor_handle, ActorHandle):
+                raise TypeError(f"Expected ActorHandle, got {type(actor_handle)}")
+            
+            # è°ƒç”¨ç›®æ ‡Ray Actorçš„process_dataæ–¹æ³•
+            actor_handle.receive_packet.remote(packet)
+            
+            self.logger.debug(f"Sent ray->ray: {connection.target_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Error in ray->ray send: {e}")
+            raise
+
+    def _get_tcp_connection(self, host: str, port: int) -> socket.socket:
+        """è·å–TCPè¿æ¥ï¼ˆæ‡’åŠ è½½å’Œé‡ç”¨ï¼‰"""
+        connection_key = f"{host}:{port}"
+        
+        if connection_key not in self._tcp_connections:
+            with self._socket_lock:
+                if connection_key not in self._tcp_connections:
+                    try:
+                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+                        sock.connect((host, port))
+                        self._tcp_connections[connection_key] = sock
+                        self.logger.info(f"Established TCP connection to {host}:{port}")
+                    except Exception as e:
+                        self.logger.error(f"Failed to connect to TCP server {host}:{port}: {e}")
+                        raise
+        
+        return self._tcp_connections[connection_key]
+
+    def _reset_tcp_connection(self, host: str, port: int):
+        """é‡ç½®æŒ‡å®šçš„TCPè¿æ¥"""
+        connection_key = f"{host}:{port}"
+        with self._socket_lock:
+            if connection_key in self._tcp_connections:
+                try:
+                    self._tcp_connections[connection_key].close()
+                except:
+                    pass  # å¿½ç•¥å…³é—­é”™è¯¯
+                del self._tcp_connections[connection_key]
+                self.logger.debug(f"Reset TCP connection to {host}:{port}")
+
+    def close(self):
+        """å…³é—­æ‰€æœ‰TCPè¿æ¥"""
+        with self._socket_lock:
+            for connection_key, sock in self._tcp_connections.items():
+                try:
+                    sock.close()
+                    self.logger.debug(f"Closed TCP connection: {connection_key}")
+                except:
+                    pass  # å¿½ç•¥å…³é—­é”™è¯¯
+            self._tcp_connections.clear()
+            self.logger.info("Closed all TCP connections")
+
+    def get_connection_stats(self) -> dict:
+        """è·å–è¿æ¥ç»Ÿè®¡ä¿¡æ¯"""
+        with self._socket_lock:
+            return {
+                "active_tcp_connections": len(self._tcp_connections),
+                "tcp_endpoints": list(self._tcp_connections.keys())
+            }
\ No newline at end of file
diff --git a/sage_runtime/local_thread_pool.py b/sage_runtime/local_thread_pool.py
new file mode 100644
index 0000000..9d80769
--- /dev/null
+++ b/sage_runtime/local_thread_pool.py
@@ -0,0 +1,106 @@
+import os
+import threading
+import logging
+from typing import Dict, Optional, Any, List
+from concurrent.futures import ThreadPoolExecutor
+from sage_runtime.dagnode.local_dag_node import LocalDAGNode
+from sage_runtime.io.local_tcp_server import LocalTcpServer
+from sage_utils.custom_logger import CustomLogger
+import time
+import socket
+class LocalThreadPool:
+    _instance = None
+    _lock = threading.Lock()
+
+    def __init__(self):
+        if hasattr(self, "_initialized"):
+            return
+
+        self.logger = CustomLogger(
+            filename="LocalThreadPool",
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="WARNING",
+        )
+
+        self._initialized = True
+        self.name = "LocalThreadPool"
+        self.logger.debug(f"CPU count is {os.cpu_count()}")
+        self.thread_pool = ThreadPoolExecutor(
+            max_workers=os.cpu_count() * 3,
+            thread_name_prefix=None,
+            initializer=None,
+            initargs=None
+        )
+        # èŠ‚ç‚¹ç®¡ç†
+        # self.running_nodes: Dict[str, LocalDAGNode] = {}  # æ­£åœ¨è¿è¡Œçš„èŠ‚ç‚¹è¡¨
+        # self.handle_to_node: Dict[str, LocalDAGNode] = {}  # handleåˆ°èŠ‚ç‚¹çš„æ˜ å°„
+
+
+            
+    @classmethod
+    def get_instance(cls):
+        """è·å–LocalRuntimeçš„å”¯ä¸€å®ä¾‹"""
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # ç»•è¿‡ __new__ çš„å¼‚å¸¸ï¼Œç›´æ¥åˆ›å»ºå®ä¾‹
+                    instance = super().__new__(cls)
+                    instance.__init__()
+                    cls._instance = instance
+        return cls._instance
+
+
+
+    
+    def submit_node(self, node: LocalDAGNode) -> str:
+        self.logger.info(f"Submitting node '{node.name}' to {self.name}")
+        try:
+            future=self.thread_pool.submit(node.run_loop)
+        except Exception as e:
+            self.logger.error(f"Failed to submit node '{node.name}': {e}", exc_info=True)
+
+    
+    def shutdown(self):
+        """å…³é—­è¿è¡Œæ—¶å’Œæ‰€æœ‰èµ„æº"""
+        self.logger.info("Shutting down LocalThreadPool...")
+        
+        # åœæ­¢æ‰€æœ‰èŠ‚ç‚¹
+        self.thread_pool.shutdown(wait=True, cancel_futures=True)
+        self.__class__._instance = None  # æ¸…é™¤å®ä¾‹å¼•ç”¨
+        # # å…³é—­TCPæœåŠ¡å™¨
+        # if self.tcp_server:
+        #     self.tcp_server.stop()
+        
+        self.logger.info("LocalThreadPool shutdown completed")
+    
+
+
+
+    
+    ########################################################
+    #                inactive methods                      #
+    ########################################################
+
+    @classmethod
+    def reset_instance(cls):
+        """é‡ç½®å®ä¾‹ï¼ˆä¸»è¦ç”¨äºæµ‹è¯•ï¼‰"""
+        with cls._lock:
+            if cls._instance:
+                cls._instance.shutdown()
+                cls._instance = None
+
+    ########################################################
+    #                auxiliary methods                     #
+    ########################################################
+
+    def __new__(cls, *args, **kwargs):
+        # ç¦æ­¢ç›´æ¥å®ä¾‹åŒ–
+        raise RuntimeError("è¯·é€šè¿‡ get_instance() æ–¹æ³•è·å–å®ä¾‹")
+    
+    def __del__(self):
+        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
+        try:
+            self.shutdown()
+        except:
+            pass
\ No newline at end of file
diff --git a/sage_runtime/utils/__init__.py b/sage_runtime/metrics/__init__.py
similarity index 100%
rename from sage_runtime/utils/__init__.py
rename to sage_runtime/metrics/__init__.py
diff --git a/sage_runtime/mixed_dag.py b/sage_runtime/mixed_dag.py
new file mode 100644
index 0000000..782f371
--- /dev/null
+++ b/sage_runtime/mixed_dag.py
@@ -0,0 +1,236 @@
+from typing import Dict, List, Any, Tuple, Union, TYPE_CHECKING
+from ray.actor import ActorHandle
+
+from sage_runtime.local_thread_pool import LocalThreadPool
+from sage_runtime.runtime_context import RuntimeContext
+from sage_runtime.dagnode.local_dag_node import LocalDAGNode
+from sage_runtime.dagnode.ray_dag_node import RayDAGNode
+from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+from sage_runtime.io.local_tcp_server import LocalTcpServer
+from sage_runtime.io.connection import Connection
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.compiler import Compiler, GraphNode
+
+if TYPE_CHECKING:
+    from sage_core.api.env import BaseEnvironment 
+
+
+class MixedDAG():
+    def __init__(self, graph: Compiler, env:'BaseEnvironment'):
+        self.graph = graph
+        self.name:str = graph.name
+        self.logger = CustomLogger(
+            filename=f"MixedDAG_{self.name}",
+            env_name= env.name,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output = "DEBUG",
+        )
+        # self.nodes: Dict[str, Union[ActorHandle, LocalDAGNode]] = {}
+        self.nodes: Dict[str, BaseDAGNode] = {}
+
+        self.spout_nodes: Dict[str, BaseDAGNode] = {}
+
+        self.connections: List[Connection] = []
+
+        self.is_running: bool = False
+
+        # ä¸ºè¿™ä¸ª DAG åˆ†é…ç‹¬ç«‹çš„ TCP ç«¯å£
+        self.tcp_server = LocalTcpServer(
+            # host="localhost",
+            # port=self.tcp_port,
+            message_handler=self._handle_tcp_message
+        )
+        self.tcp_server.start()
+
+        self._compile_graph(graph, env)
+        # å¯åŠ¨ TCP æœåŠ¡å™¨
+        self.logger.info(f"MixedDAG '{self.name}' construction complete")
+    
+
+        
+
+    
+    def _compile_graph(self, graph: Compiler, env:'BaseEnvironment'):
+        """ç¼–è¯‘å›¾ç»“æ„ï¼Œåˆ›å»ºèŠ‚ç‚¹å¹¶å»ºç«‹è¿æ¥"""
+        self.logger.info(f"Compiling mixed DAG for graph: {self.name}")
+        
+        # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ‰€æœ‰èŠ‚ç‚¹å®ä¾‹
+        for node_name, graph_node in graph.nodes.items():
+            # node_instance = graph_node.create_dag_node()
+            node_instance = graph_node.transformation.dag_node_factory.create_node(graph_node.name, graph_node.runtime_context)
+            self.nodes[node_name] = node_instance
+            if graph_node.is_spout:
+                self.spout_nodes[node_name] = node_instance
+
+            self.logger.debug(f"Added node '{node_name}' of type '{node_instance.__class__.__name__}'")
+        
+        # ç¬¬äºŒæ­¥ï¼šå»ºç«‹èŠ‚ç‚¹é—´çš„è¿æ¥
+        for node_name, graph_node in graph.nodes.items():
+            self._setup_node_connections(node_name, graph_node)
+        
+        self.logger.info(f"Mixed DAG compilation completed: {len(self.nodes)} nodes, "f"{len(self.spout_nodes)} spout nodes")
+
+
+
+
+    def _setup_node_connections(self, node_name: str, graph_node: GraphNode):
+        """
+        ä¸ºèŠ‚ç‚¹è®¾ç½®ä¸‹æ¸¸è¿æ¥
+        
+        Args:
+            node_name: èŠ‚ç‚¹åç§°
+            graph_node: å›¾èŠ‚ç‚¹å¯¹è±¡
+        """
+        output_handle = self.nodes[node_name]
+        
+        for broadcast_index, parallel_edges in enumerate(graph_node.output_channels):
+            for parallel_index, parallel_edge in enumerate(parallel_edges):
+                target_name = parallel_edge.downstream_node.name
+                target_input_index = parallel_edge.input_index
+                target_handle = self.nodes[target_name]
+
+                connection = Connection(
+                    own_node=output_handle,
+                    broadcast_index=broadcast_index,
+                    parallel_index=parallel_index,
+                    target_name=target_name,
+                    target_node=target_handle,
+                    target_input_index = target_input_index,
+                    tcp_server=self.tcp_server
+                )
+                try:
+                    if isinstance(output_handle, ActorHandle):
+                        
+                        output_handle.add_connection.remote(connection)
+                        self.logger.debug(f"Setup Ray connection: {node_name} -> {target_name}")
+                    else:
+                        # æœ¬åœ°èŠ‚ç‚¹ç›´æ¥è°ƒç”¨
+                        output_handle.add_connection(connection)
+                        self.logger.debug(f"Setup local connection: {node_name} -> {target_name}")
+                        
+                    # è®°å½•è¿æ¥ä¿¡æ¯
+                    self.connections.append(connection)
+                    
+                except Exception as e:
+                    self.logger.error(f"Error setting up connection {node_name} -> {target_name}: {e}", exc_info=True)
+
+    def stop(self):
+        if(not self.is_running):
+            self.logger.warning(f"MixedDAG '{self.name}' is not running, nothing to stop")
+            return
+        
+        for node_name, node_instance in self.spout_nodes.items():
+            node_instance.stop()
+            self.logger.debug(f"Stopped spout node: {node_name}")
+        self.logger.info(f"Stopped all spout nodes in MixedDAG '{self.name}'")
+
+    def close(self):
+        """åœæ­¢æ‰€æœ‰èŠ‚ç‚¹"""
+        self.logger.info("Stopping all DAG nodes...")
+        self.tcp_server.stop()
+        for node_name, node in self.nodes.items():
+            try:
+                node.stop()
+                self.logger.debug(f"Stopped node: {node_name}")
+                # Ray actorsä¼šåœ¨è¿›ç¨‹ç»“æŸæ—¶è‡ªåŠ¨æ¸…ç†
+            except Exception as e:
+                self.logger.error(f"Error stopping node {node_name}: {e}")
+    
+    def submit(self):
+        self.logger.info(f"Submitting MixedDAG '{self.name}'")
+        try:
+            for node_name, node in self.nodes.items():
+                if node.is_spout is False:
+                    local_runtime = LocalThreadPool.get_instance()
+                    local_runtime.submit_node(node)
+        except Exception as e:
+            self.logger.error(f"Failed to submit MixedDAG '{self.name}': {e}", exc_info=True)
+
+    def execute_once(self, spout_node_name:str = None):
+        self.logger.info(f"executing once")
+        if(spout_node_name is None):
+            for node_name, node_instance in self.nodes.items():
+                # print(f"triggering spout node: {node_name}, is_spout:{node_instance.is_spout}")
+                if(node_instance.is_spout):
+                    node_instance.trigger()
+                    self.logger.debug(f"triggering spout node: {node_name}")
+
+        elif self.spout_nodes.get(spout_node_name, None) is not None:
+            node = self.spout_nodes[spout_node_name]
+            self.logger.debug(f"Running spout node: {node_name}")
+            node.trigger()
+        else:
+            self.logger.warning(f"Spout node '{spout_node_name}' not found in MixedDAG '{self.name}'")
+
+    def execute_streaming(self, spout_node_name:str = None):
+        self.logger.info(f"executing streaming")
+        self.is_running = True
+        if(spout_node_name is None):
+            for node_name, node_instance in self.spout_nodes.items():
+                local_runtime = LocalThreadPool.get_instance()
+                local_runtime.submit_node(node_instance)
+                self.logger.debug(f"Running spout node: {node_name}")
+        elif self.spout_nodes.get(spout_node_name, None) is not None:
+            node_handle = self.nodes[spout_node_name]
+            local_runtime = LocalThreadPool.get_instance()
+            local_runtime.submit_node(node_handle)
+            self.logger.debug(f"Running spout node: {node_name}")
+        else:
+            self.logger.warning(f"Spout node '{spout_node_name}' not found in MixedDAG '{self.name}'")
+
+
+    def _handle_tcp_message(self, message: Dict[str, Any], client_address: tuple):
+        """
+        å¤„ç†æ¥è‡ª Ray Actor çš„ TCP æ¶ˆæ¯
+        ç”±äºæ˜¯ DAG ä¸“ç”¨ç«¯å£ï¼Œæ‰€æœ‰æ¶ˆæ¯éƒ½å±äºå½“å‰ DAG
+        """
+        try:
+            message_type = message.get("type")
+            
+            if message_type == "ray_to_local":
+                # Ray Actor å‘é€ç»™æœ¬åœ°èŠ‚ç‚¹çš„æ•°æ®
+                target_node_name = message["target_node"]
+                data = message["data"]
+                source_actor = message.get("source_actor", "unknown")
+                
+                # æŸ¥æ‰¾ç›®æ ‡èŠ‚ç‚¹ï¼ˆåœ¨å½“å‰ DAG ä¸­ï¼‰
+                if target_node_name in self.nodes:
+                    target_node = self.nodes[target_node_name]
+                    
+                    # ç¡®ä¿æ˜¯æœ¬åœ°èŠ‚ç‚¹
+                    if isinstance(target_node, LocalDAGNode):
+                        # å°†æ•°æ®æ”¾å…¥ç›®æ ‡èŠ‚ç‚¹çš„è¾“å…¥ç¼“å†²åŒº
+                        target_node.put(data)
+                        
+                        self.logger.debug(f"[DAG {self.name}] Delivered TCP message: {source_actor} -> "
+                                        f"{target_node_name}")
+                    else:
+                        self.logger.warning(f"Target node '{target_node_name}' is not a local node")
+                else:
+                    self.logger.warning(f"Target node '{target_node_name}' not found in DAG '{self.name}'")
+            else:
+                self.logger.warning(f"Unknown TCP message type: {message_type} from {client_address}")
+                
+        except Exception as e:
+            self.logger.error(f"Error processing TCP message from {client_address}: {e}", exc_info=True)
+
+
+    def _detect_platform(self, executor: Any) -> str:
+        """
+        æ£€æµ‹æ‰§è¡Œå™¨çš„å¹³å°ç±»å‹
+        
+        Args:
+            executor: æ‰§è¡Œå™¨å¯¹è±¡
+            
+        Returns:
+            å¹³å°ç±»å‹å­—ç¬¦ä¸²
+        """
+        if isinstance(executor, ActorHandle):
+            return "remote"
+        elif hasattr(executor, 'remote'):
+            return "ray_function" 
+        elif isinstance(executor, LocalDAGNode):
+            return "local"
+        else:
+            return "unknown"
diff --git a/sage_runtime/operator/__init__.py b/sage_runtime/operator/__init__.py
new file mode 100644
index 0000000..0ff4321
--- /dev/null
+++ b/sage_runtime/operator/__init__.py
@@ -0,0 +1,4 @@
+# from .ray_runtime import RayRuntime
+# # from .ray_executor import RayDAGExecutor
+# # ä¾›é¡¶å±‚ sage/__init__.py ä½¿ç”¨
+# __all__ = ["RayRuntime"]
\ No newline at end of file
diff --git a/sage_runtime/operator/factory.py b/sage_runtime/operator/factory.py
new file mode 100644
index 0000000..7caebe7
--- /dev/null
+++ b/sage_runtime/operator/factory.py
@@ -0,0 +1,84 @@
+from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Optional
+from sage_utils.name_server import get_name
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.operator.operator_wrapper import OperatorWrapper
+if TYPE_CHECKING:
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_core.function.base_function import BaseFunction
+    from ray.actor import ActorHandle
+    from sage_runtime.function.factory import FunctionFactory
+    from sage_runtime.runtime_context import RuntimeContext
+import ray
+
+class OperatorFactory:
+    # ç”±transformationåˆå§‹åŒ–
+    def __init__(self, 
+                 operator_class: Type['BaseOperator'],
+                 function_factory: 'FunctionFactory',
+                 basename: str = None,
+                 env_name:str = None,
+                 remote:bool = False,
+                 **operator_kwargs):
+        self.operator_class = operator_class
+        self.operator_kwargs = operator_kwargs  # ä¿å­˜é¢å¤–çš„operatorå‚æ•°
+        self.function_factory = function_factory
+        self.env_name = env_name
+        self.basename = get_name(basename) or get_name(self.function_factory.function_class.__name__)
+        self.remote = remote
+
+    def create_operator(self, 
+                       name: str,
+                       runtime_context: 'RuntimeContext') -> 'OperatorWrapper':
+        """
+        åˆ›å»ºoperatorå®ä¾‹
+        
+        Args:
+            session_folder: ä¼šè¯æ–‡ä»¶å¤¹
+            name: èŠ‚ç‚¹åç§°
+            **additional_kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°
+            
+        Returns:
+            BaseOperator: åˆ›å»ºçš„operatorå®ä¾‹
+        """
+        # åˆ›å»ºloggerç”¨äºè°ƒè¯•
+        logger = CustomLogger(
+            filename=f"OperatorFactory_{name}",
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="WARNING",
+            name=f"OperatorFactory_{name}",
+            env_name = self.env_name
+        )
+        
+        try:
+            if self.remote:
+                Operator_class = ray.remote(self.operator_class)
+                operator_instance = Operator_class.remote(
+                    self.function_factory,
+                    runtime_context,
+                    **self.operator_kwargs
+                )
+                logger.debug(f"Building Ray Actor operator instance: {self.operator_class.__name__}")
+            else:
+                Operator_class = self.operator_class
+                operator_instance = Operator_class(
+                    self.function_factory,
+                    runtime_context,
+                    **self.operator_kwargs
+                )
+                logger.debug(f"Building local operator instance: {self.operator_class.__name__}")
+
+
+            # ç”¨OperatorWrapperåŒ…è£…
+            wrapped_operator = OperatorWrapper(operator_instance, name, self.env_name)
+            logger.debug(f"Wrapped operator with OperatorWrapper")
+            
+            return wrapped_operator
+            
+        except Exception as e:
+            logger.error(f"Failed to create operator: {e}")
+            raise
+            
+        except Exception as e:
+            logger.error(f"Failed to create operator: {e}")
+            raise
\ No newline at end of file
diff --git a/sage_runtime/operator/operator_wrapper.py b/sage_runtime/operator/operator_wrapper.py
new file mode 100644
index 0000000..044784e
--- /dev/null
+++ b/sage_runtime/operator/operator_wrapper.py
@@ -0,0 +1,231 @@
+import ray
+import asyncio
+import concurrent.futures
+from typing import Any, Union
+import logging
+from ray.actor import ActorHandle
+from sage_utils.custom_logger import CustomLogger
+
+class OperatorWrapper:
+    """é€æ˜çš„ Operator åŒ…è£…å™¨ï¼Œè‡ªåŠ¨é€‚é…æœ¬åœ° Operator å’Œ Ray Actor Operator"""
+
+    def __init__(self, operator: Union[Any, ActorHandle], name:str, env_name:str = None):
+        # ä½¿ç”¨ __dict__ ç›´æ¥è®¾ç½®ï¼Œé¿å…è§¦å‘ __setattr__
+        object.__setattr__(self, '_operator', operator)
+        object.__setattr__(self, '_execution_mode', self._detect_execution_mode())
+        object.__setattr__(self, '_method_cache', {})
+        object.__setattr__(self, '_attribute_cache', {})
+        
+        # åˆå§‹åŒ– logger
+        logger = CustomLogger(
+            filename=f"Node_{name}",
+            env_name=env_name,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="DEBUG",
+            name=f"{name}_OperatorWrapper"
+        )
+        
+
+        
+        object.__setattr__(self, 'logger', logger)
+        self.logger.debug(f"Created OperatorWrapper for {type(operator).__name__} in {self._execution_mode} mode")
+        
+    def _detect_execution_mode(self) -> str:
+        """æ£€æµ‹æ‰§è¡Œæ¨¡å¼"""
+        try:
+            # æ£€æŸ¥æ˜¯å¦æ˜¯Ray Actor
+            if isinstance(self._operator, ray.actor.ActorHandle):
+                return "ray_actor"
+        except (ImportError, AttributeError):
+            # å¦‚æœRayä¸å¯ç”¨ï¼Œå¿½ç•¥
+            pass
+
+        # é»˜è®¤ä½œä¸ºæœ¬åœ°å¯¹è±¡å¤„ç†
+        return "local"
+
+    def __getattr__(self, name: str):
+        """é€æ˜ä»£ç†å±æ€§è®¿é—®"""
+        # é˜²æ­¢å¾ªç¯å¼•ç”¨ - æ£€æŸ¥æ˜¯å¦è®¿é—®å†…éƒ¨å±æ€§
+        if name.startswith('_'):
+            raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
+        
+        # é˜²æ­¢è®¿é—® _operator æ—¶çš„å¾ªç¯å¼•ç”¨
+        if '_operator' not in self.__dict__:
+            raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
+        
+        # ç¼“å­˜æŸ¥æ‰¾
+        if hasattr(self, '_attribute_cache') and name in self._attribute_cache:
+            return self._attribute_cache[name]
+
+        # è·å–åŸå§‹å±æ€§
+        try:
+            if self._execution_mode == "ray_actor":
+                # å¯¹äº Ray Actorï¼Œç›´æ¥è·å–å±æ€§ï¼ˆä¸è°ƒç”¨ .remote()ï¼‰
+                original_attr = getattr(self._operator, name)
+            else:
+                # å¯¹äºæœ¬åœ°å¯¹è±¡ï¼Œæ­£å¸¸è·å–å±æ€§
+                original_attr = getattr(self._operator, name)
+        except AttributeError:
+            raise AttributeError(f"'{type(self._operator).__name__}' object has no attribute '{name}'")
+
+        # å¤„ç†æ–¹æ³•è°ƒç”¨
+        if callable(original_attr):
+            # åˆ›å»ºç»Ÿä¸€è°ƒç”¨æ–¹å¼
+            wrapped_method = self._create_unified_method(name, original_attr)
+            if hasattr(self, '_attribute_cache'):
+                self._attribute_cache[name] = wrapped_method
+            return wrapped_method
+
+        # æ™®é€šå±æ€§
+        if hasattr(self, '_attribute_cache'):
+            self._attribute_cache[name] = original_attr
+        return original_attr
+
+    def __setattr__(self, name: str, value: Any):
+        """ä»£ç†å±æ€§è®¾ç½®"""
+        if name.startswith('_'):
+            # å†…éƒ¨å±æ€§ç›´æ¥è®¾ç½®
+            object.__setattr__(self, name, value)
+        else:
+            # å¤–éƒ¨å±æ€§è®¾ç½®åˆ°åŸ operator
+            if self._execution_mode == "ray_actor":
+                # Ray Actor å±æ€§è®¾ç½®éœ€è¦é€šè¿‡è¿œç¨‹è°ƒç”¨
+                # æ³¨æ„ï¼šè¿™å¯èƒ½éœ€è¦åœ¨ operator ä¸­å®ç° set_attribute æ–¹æ³•
+                self.logger.warning(f"Setting attribute '{name}' on Ray Actor - this may not work as expected")
+                setattr(self._operator, name, value)
+            else:
+                setattr(self._operator, name, value)
+
+    def __dir__(self):
+        """ä»£ç†dir()è°ƒç”¨"""
+        if self._execution_mode == "ray_actor":
+            # Ray Actor çš„ dir() å¯èƒ½ä¸å®Œæ•´ï¼Œè¿”å›å¸¸è§çš„ operator æ–¹æ³•
+            return ['receive_packet', 'start_spout_loop', 'stop_actor', 'get_actor_status']
+        else:
+            return dir(self._operator)
+
+    def __repr__(self):
+        """ä»£ç†repr()"""
+        mode_info = f"[{self._execution_mode}]"
+        return f"OperatorWrapper{mode_info}({repr(self._operator)})"
+
+    def __str__(self):
+        """ä»£ç†str()"""
+        return f"OperatorWrapper({str(self._operator)})"
+
+    def _create_unified_method(self, method_name: str, original_method):
+        """åˆ›å»ºç»Ÿä¸€çš„æ–¹æ³•åŒ…è£… - å¯¹å¤–å§‹ç»ˆæä¾›åŒæ­¥æ¥å£"""
+        
+        if self._execution_mode == "ray_actor":
+            # Ray Actorå¤„ç†
+            def ray_actor_wrapper(*args, **kwargs):
+                try:
+                    self.logger.debug(f"Calling Ray Actor method '{method_name}' with args={args}, kwargs={kwargs}")
+                    
+                    # æ‰§è¡Œè¿œç¨‹è°ƒç”¨ï¼Œoriginal_method å·²ç»æ˜¯ remote æ–¹æ³•
+                    future = original_method.remote(*args, **kwargs)
+                    
+                    # åŒæ­¥è·å–ç»“æœ
+                    result = ray.get(future)
+                    self.logger.debug(f"Ray Actor method '{method_name}' completed successfully")
+                    return result
+                    
+                except Exception as e:
+                    self.logger.error(f"Ray Actor method '{method_name}' failed: {str(e)}")
+                    raise RuntimeError(f"Ray Actor method '{method_name}' failed: {str(e)}")
+
+            return ray_actor_wrapper
+
+        else:
+            # æœ¬åœ°æ–¹æ³•å¤„ç†
+            if asyncio.iscoroutinefunction(original_method):
+                # å¼‚æ­¥æ–¹æ³•è½¬åŒæ­¥
+                def async_wrapper(*args, **kwargs):
+                    try:
+                        self.logger.debug(f"Calling local async method '{method_name}'")
+                        
+                        # æ£€æŸ¥äº‹ä»¶å¾ªç¯
+                        try:
+                            loop = asyncio.get_running_loop()
+                            # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
+                            with concurrent.futures.ThreadPoolExecutor() as executor:
+                                result = executor.submit(
+                                    asyncio.run,
+                                    original_method(*args, **kwargs)
+                                ).result()
+                            return result
+                        except RuntimeError:
+                            # ç›´æ¥è¿è¡Œå¼‚æ­¥æ–¹æ³•
+                            return asyncio.run(original_method(*args, **kwargs))
+                    except Exception as e:
+                        self.logger.error(f"Local async method '{method_name}' failed: {str(e)}")
+                        raise RuntimeError(f"Local async method '{method_name}' failed: {str(e)}")
+
+                return async_wrapper
+            else:
+                # åŒæ­¥æ–¹æ³•ç›´æ¥è¿”å›
+                def sync_wrapper(*args, **kwargs):
+                    try:
+                        self.logger.debug(f"Calling local sync method '{method_name}'")
+                        result = original_method(*args, **kwargs)
+                        self.logger.debug(f"Local sync method '{method_name}' completed successfully")
+                        return result
+                    except Exception as e:
+                        self.logger.error(f"Local sync method '{method_name}' failed: {str(e)}")
+                        raise
+
+                return sync_wrapper
+
+    # æ·»åŠ ä¸€äº› operator ç‰¹æœ‰çš„ä¾¿åˆ©æ–¹æ³•
+    def is_ray_actor(self) -> bool:
+        """æ£€æŸ¥æ˜¯å¦ä¸º Ray Actor"""
+        return self._execution_mode == "ray_actor"
+
+    def is_local(self) -> bool:
+        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ° operator"""
+        return self._execution_mode == "local"
+
+    def get_execution_mode(self) -> str:
+        """è·å–æ‰§è¡Œæ¨¡å¼"""
+        return self._execution_mode
+
+    def get_wrapped_operator(self):
+        """è·å–è¢«åŒ…è£…çš„åŸå§‹ operatorï¼ˆè°¨æ…ä½¿ç”¨ï¼‰"""
+        return self._operator
+
+    # # ä¸ºå¸¸è§çš„ operator æ–¹æ³•æä¾›ç±»å‹æç¤ºå’Œæ–‡æ¡£
+    # def receive_packet(self, channel: str, data: Any) -> Any:
+    #     """
+    #     å¤„ç†æ•°æ®çš„ç»Ÿä¸€æ¥å£
+        
+    #     Args:
+    #         channel: è¾“å…¥é€šé“åç§°
+    #         data: è¾“å…¥æ•°æ®
+            
+    #     Returns:
+    #         å¤„ç†åçš„æ•°æ®
+    #     """
+    #     # è¿™ä¸ªæ–¹æ³•ä¼šè¢« __getattr__ æ‹¦æˆªå¹¶è¿”å›åŒ…è£…åçš„æ–¹æ³•
+    #     pass
+
+    # def start_spout_loop(self):
+    #     """
+    #     å¯åŠ¨ spout å¾ªç¯ï¼ˆä»…é€‚ç”¨äº spout operatorï¼‰
+    #     """
+    #     # è¿™ä¸ªæ–¹æ³•ä¼šè¢« __getattr__ æ‹¦æˆªå¹¶è¿”å›åŒ…è£…åçš„æ–¹æ³•
+    #     pass
+
+    # def stop_actor(self):
+    #     """
+    #     åœæ­¢ operatorï¼ˆé€‚ç”¨äº Ray Actorï¼‰
+    #     """
+    #     # è¿™ä¸ªæ–¹æ³•ä¼šè¢« __getattr__ æ‹¦æˆªå¹¶è¿”å›åŒ…è£…åçš„æ–¹æ³•
+    #     pass
+
+    # def get_actor_status(self) -> dict:
+    #     """
+    #     è·å– operator çŠ¶æ€
+    #     """
+    #     # è¿™ä¸ªæ–¹æ³•ä¼šè¢« __getattr__ æ‹¦æˆªå¹¶è¿”å›åŒ…è£…åçš„æ–¹æ³•
+    #     pass
\ No newline at end of file
diff --git a/sage_runtime/router/base_router.py b/sage_runtime/router/base_router.py
deleted file mode 100644
index 3501f1d..0000000
--- a/sage_runtime/router/base_router.py
+++ /dev/null
@@ -1,276 +0,0 @@
-# sage_runtime/base_router.py
-
-from abc import ABC, abstractmethod
-from typing import Dict, Any, TYPE_CHECKING
-from sage_core.function.source_function import StopSignal
-from sage_runtime.router.packet import Packet
-
-if TYPE_CHECKING:
-    from sage_runtime.router.connection import Connection
-    from sage_runtime.runtime_context import RuntimeContext
-
-class BaseRouter(ABC):
-    """
-    è·¯ç”±å™¨åŸºç±»ï¼Œè´Ÿè´£ç®¡ç†ä¸‹æ¸¸è¿æ¥å’Œæ•°æ®åŒ…è·¯ç”±
-    å­ç±»åªéœ€è¦å®ç°å…·ä½“çš„æ•°æ®å‘é€é€»è¾‘
-    """
-    
-    def __init__(self, ctx: 'RuntimeContext'):
-        self.name = ctx.name
-        self.ctx = ctx
-        
-        # ä¸‹æ¸¸è¿æ¥ç®¡ç†
-        self.downstream_groups: Dict[int, Dict[int, 'Connection']] = {}
-        self.downstream_group_roundrobin: Dict[int, int] = {}
-        
-        # Logger
-        self.logger = ctx.logger
-        self.logger.debug(f"Initialized {self.__class__.__name__} for {self.name}")
-    
-    def add_connection(self, connection: 'Connection') -> None:
-        """
-        æ·»åŠ ä¸‹æ¸¸è¿æ¥
-        
-        Args:
-            connection: Connectionå¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¿æ¥ä¿¡æ¯
-        """
-        broadcast_index = connection.broadcast_index
-        parallel_index = connection.parallel_index
-        
-        # Debug log
-        self.logger.debug(
-            f"Adding connection: broadcast_index={broadcast_index}, parallel_index={parallel_index}, target={connection.target_name}"
-        )
-        
-        # åˆå§‹åŒ–å¹¿æ’­ç»„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
-        if broadcast_index not in self.downstream_groups:
-            self.downstream_groups[broadcast_index] = {}
-            self.downstream_group_roundrobin[broadcast_index] = 0
-        
-        # ä¿å­˜å®Œæ•´çš„Connectionå¯¹è±¡
-        self.downstream_groups[broadcast_index][parallel_index] = connection
-        
-        self.logger.info(f"Added connection to {connection.target_name}")
-    
-    def remove_connection(self, broadcast_index: int, parallel_index: int) -> bool:
-        """
-        ç§»é™¤æŒ‡å®šçš„è¿æ¥
-        
-        Args:
-            broadcast_index: å¹¿æ’­ç´¢å¼•
-            parallel_index: å¹¶è¡Œç´¢å¼•
-            
-        Returns:
-            bool: æ˜¯å¦æˆåŠŸç§»é™¤
-        """
-        try:
-            if broadcast_index in self.downstream_groups:
-                if parallel_index in self.downstream_groups[broadcast_index]:
-                    connection = self.downstream_groups[broadcast_index][parallel_index]
-                    del self.downstream_groups[broadcast_index][parallel_index]
-                    
-                    # å¦‚æœè¿™ä¸ªå¹¿æ’­ç»„ç©ºäº†ï¼Œæ¸…ç†å®ƒ
-                    if not self.downstream_groups[broadcast_index]:
-                        del self.downstream_groups[broadcast_index]
-                        del self.downstream_group_roundrobin[broadcast_index]
-                    else:
-                        # é‡ç½®è½®è¯¢è®¡æ•°å™¨
-                        self.downstream_group_roundrobin[broadcast_index] = 0
-                    
-                    self.logger.info(f"Removed connection to {connection.target_name}")
-                    return True
-            
-            self.logger.warning(f"Connection not found: broadcast_index={broadcast_index}, parallel_index={parallel_index}")
-            return False
-            
-        except Exception as e:
-            self.logger.error(f"Error removing connection: {e}")
-            return False
-    
-    def get_connection_count(self) -> int:
-        """è·å–æ€»è¿æ¥æ•°"""
-        total = 0
-        for parallel_targets in self.downstream_groups.values():
-            total += len(parallel_targets)
-        return total
-    
-    def get_connections_info(self) -> Dict[str, Any]:
-        """è·å–è¿æ¥ä¿¡æ¯"""
-        info = {}
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            info[f"broadcast_group_{broadcast_index}"] = {
-                "count": len(parallel_targets),
-                "roundrobin_position": self.downstream_group_roundrobin[broadcast_index],
-                "targets": [
-                    {
-                        "parallel_index": parallel_index,
-                        "target_name": connection.target_name,
-                        "connection_type": connection.connection_type.value
-                    }
-                    for parallel_index, connection in parallel_targets.items()
-                ]
-            }
-        return info
-    
-    def send_stop_signal(self, stop_signal: 'StopSignal') -> None:
-        """
-        å‘é€åœæ­¢ä¿¡å·ç»™æ‰€æœ‰ä¸‹æ¸¸è¿æ¥
-        
-        Args:
-            stop_signal: åœæ­¢ä¿¡å·å¯¹è±¡
-        """
-        self.logger.info(f"Sending stop signal: {stop_signal}")
-        
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            for connection in parallel_targets.values():
-                try:
-                    connection.target_buffer.put_nowait(stop_signal)
-                    self.logger.debug(f"Sent stop signal to {connection.target_name}")
-                except Exception as e:
-                    self.logger.error(f"Failed to send stop signal to {connection.target_name}: {e}")
-
-    def send(self, packet: 'Packet') -> bool:
-        """
-        å‘é€æ•°æ®åŒ…ï¼Œæ ¹æ®å…¶åˆ†åŒºä¿¡æ¯é€‰æ‹©è·¯ç”±ç­–ç•¥
-        
-        Args:
-            packet: è¦å‘é€çš„packetï¼Œå¯èƒ½åŒ…å«åˆ†åŒºä¿¡æ¯
-            
-        Returns:
-            bool: æ˜¯å¦æˆåŠŸå‘é€
-        """
-        if not self.downstream_groups:
-            self.logger.warning(f"No downstream connections available for {self.name}")
-            return False
-        
-        try:
-            self.logger.debug(f"Emitting packet: {packet}")
-            
-            # æ ¹æ®packetçš„åˆ†åŒºä¿¡æ¯é€‰æ‹©è·¯ç”±ç­–ç•¥
-            if packet.is_keyed():
-                return self._route_packet(packet)
-            else:
-                return self._route_round_robin_packet(packet)
-                
-        except Exception as e:
-            self.logger.error(f"Error emitting packet: {e}")
-            return False
-    
-    def _route_packet(self, packet: 'Packet') -> bool:
-        """ä½¿ç”¨åˆ†åŒºä¿¡æ¯è¿›è¡Œè·¯ç”±"""
-        strategy = packet.partition_strategy
-        
-        if strategy == "hash":
-            return self._route_hashed_packet(packet)
-        elif strategy == "broadcast":
-            return self._route_broadcast_packet(packet)
-        else:
-            return self._route_round_robin_packet(packet)
-    
-    def _route_round_robin_packet(self, packet: 'Packet') -> bool:
-        """ä½¿ç”¨è½®è¯¢ç­–ç•¥è¿›è¡Œè·¯ç”±"""
-        success = True
-        
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            if not parallel_targets:  # ç©ºçš„å¹¶è¡Œç›®æ ‡ç»„
-                continue
-                
-            # è·å–å½“å‰è½®è¯¢ä½ç½®
-            current_round_robin = self.downstream_group_roundrobin[broadcast_index]
-            parallel_indices = list(parallel_targets.keys())
-            target_parallel_index = parallel_indices[current_round_robin % len(parallel_indices)]
-            
-            # æ›´æ–°è½®è¯¢ä½ç½®
-            self.downstream_group_roundrobin[broadcast_index] = (current_round_robin + 1) % len(parallel_indices)
-            
-            # å‘é€åˆ°é€‰ä¸­çš„è¿æ¥
-            connection = parallel_targets[target_parallel_index]
-            if not self._deliver_packet(connection, packet):
-                success = False
-        
-        return success
-    
-    def _route_broadcast_packet(self, packet: 'Packet') -> bool:
-        """ä½¿ç”¨å¹¿æ’­ç­–ç•¥è¿›è¡Œè·¯ç”±"""
-        success = True
-        
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            for connection in parallel_targets.values():
-                if not self._deliver_packet(connection, packet):
-                    success = False
-        
-        return success
-    
-    def _route_hashed_packet(self, packet: 'Packet') -> bool:
-        """ä½¿ç”¨å“ˆå¸Œåˆ†åŒºç­–ç•¥è¿›è¡Œè·¯ç”±"""
-        if not packet.partition_key:
-            self.logger.warning("Hash routing requested but no partition key provided, falling back to round-robin")
-            return self._route_round_robin_packet(packet)
-        
-        success = True
-        partition_key = packet.partition_key
-        
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            if not parallel_targets:
-                continue
-                
-            # åŸºäºåˆ†åŒºé”®è®¡ç®—ç›®æ ‡ç´¢å¼•
-            parallel_indices = list(parallel_targets.keys())
-            target_index = hash(partition_key) % len(parallel_indices)
-            target_parallel_index = parallel_indices[target_index]
-            
-            connection = parallel_targets[target_parallel_index]
-            if not self._deliver_packet(connection, packet):
-                success = False
-        
-        return success
-    
-    def _deliver_packet(self, connection: 'Connection', packet: 'Packet') -> bool:
-        try:
-            routed_packet = self._create_routed_packet(connection, packet)
-            target_buffer = connection.target_buffer
-            target_buffer.put_nowait(routed_packet)
-            self._log_delivery_success(connection, packet)
-            return True
-        except Exception as e:
-            self._log_delivery_failure(connection, e)
-            return False
-    
-    def clear_all_connections(self):
-        """æ¸…ç©ºæ‰€æœ‰è¿æ¥"""
-        cleared_count = self.get_connection_count()
-        self.downstream_groups.clear()
-        self.downstream_group_roundrobin.clear()
-        
-        self.logger.info(f"Cleared all connections ({cleared_count} connections removed)")
-    
-    def get_statistics(self) -> Dict[str, Any]:
-        """è·å–è·¯ç”±ç»Ÿè®¡ä¿¡æ¯"""
-        return {
-            "total_connections": self.get_connection_count(),
-            "broadcast_groups": len(self.downstream_groups),
-            "connections_by_group": {
-                broadcast_index: len(parallel_targets)
-                for broadcast_index, parallel_targets in self.downstream_groups.items()
-            }
-        }
-    
-    def _create_routed_packet(self, connection: 'Connection', packet: 'Packet') -> 'Packet':
-        """åˆ›å»ºè·¯ç”±åçš„æ•°æ®åŒ…"""
-        return Packet(
-            payload=packet.payload,
-            input_index=connection.target_input_index,
-            partition_key=packet.partition_key,
-            partition_strategy=packet.partition_strategy,
-        )
-    
-    def _log_delivery_success(self, connection: 'Connection', packet: 'Packet'):
-        """è®°å½•å‘é€æˆåŠŸæ—¥å¿—"""
-        self.logger.debug(
-            f"Sent {'keyed' if packet.is_keyed() else 'unkeyed'} packet "
-            f"to {connection.target_name} (strategy: {packet.partition_strategy or 'round-robin'})"
-        )
-    
-    def _log_delivery_failure(self, connection: 'Connection', error: Exception):
-        """è®°å½•å‘é€å¤±è´¥æ—¥å¿—"""
-        self.logger.error(f"Failed to send packet to {connection.target_name}: {error}", exc_info=True)
\ No newline at end of file
diff --git a/sage_runtime/router/connection.py b/sage_runtime/router/connection.py
deleted file mode 100644
index 1d21745..0000000
--- a/sage_runtime/router/connection.py
+++ /dev/null
@@ -1,22 +0,0 @@
-from typing import Union
-from dataclasses import dataclass
-from sage_runtime.utils.local_message_queue import LocalMessageQueue
-from ray.actor import ActorHandle
-
-@dataclass
-class Connection:
-    """
-    ç”¨äºè¡¨ç¤ºæœ¬åœ°èŠ‚ç‚¹å’ŒRay Actorä¹‹é—´çš„è¿æ¥
-    """
-    def __init__(self,
-                 broadcast_index: int,
-                 parallel_index: int,
-                 target_name: str,
-                 target_input_buffer: Union[ActorHandle, LocalMessageQueue],
-                 target_input_index: int):
-
-        self.broadcast_index: int = broadcast_index
-        self.parallel_index: int = parallel_index
-        self.target_name: str = target_name
-        self.target_buffer: Union[ActorHandle, LocalMessageQueue] = target_input_buffer
-        self.target_input_index: int = target_input_index
\ No newline at end of file
diff --git a/sage_runtime/router/local_router.py b/sage_runtime/router/local_router.py
deleted file mode 100644
index 7c63b63..0000000
--- a/sage_runtime/router/local_router.py
+++ /dev/null
@@ -1,17 +0,0 @@
-# sage_runtime/local_router.py
-
-from typing import TYPE_CHECKING
-from sage_runtime.router.base_router import BaseRouter
-from sage_runtime.router.packet import Packet
-
-if TYPE_CHECKING:
-    from sage_runtime.router.connection import Connection
-    from sage_runtime.runtime_context import RuntimeContext
-
-class LocalRouter(BaseRouter):
-    """
-    æœ¬åœ°ä»»åŠ¡çš„è·¯ç”±å™¨ï¼Œä½¿ç”¨ç›´æ¥æ–¹æ³•è°ƒç”¨è¿›è¡Œé€šä¿¡
-    """
-    
-    def __init__(self, ctx: 'RuntimeContext'):
-        super().__init__(ctx)
\ No newline at end of file
diff --git a/sage_runtime/router/ray_router.py b/sage_runtime/router/ray_router.py
deleted file mode 100644
index b92b025..0000000
--- a/sage_runtime/router/ray_router.py
+++ /dev/null
@@ -1,20 +0,0 @@
-# sage_runtime/ray_router.py
-
-from typing import TYPE_CHECKING
-from ray.util.queue import Queue as RayQueue
-from sage_runtime.router.base_router import BaseRouter
-from sage_runtime.router.packet import Packet
-
-if TYPE_CHECKING:
-    from sage_runtime.router.connection import Connection
-    from sage_runtime.runtime_context import RuntimeContext
-
-class RayRouter(BaseRouter):
-    """
-    Ray Actorä»»åŠ¡çš„è·¯ç”±å™¨ï¼Œä½¿ç”¨Ray Queueè¿›è¡Œè·¨è¿›ç¨‹é€šä¿¡
-    """
-    
-    def __init__(self, ctx: 'RuntimeContext'):
-        super().__init__(ctx)
-    
-    # ç›®å‰æ²¡æœ‰ä»€ä¹ˆç‰¹å¼‚åŠŸèƒ½
\ No newline at end of file
diff --git a/sage_runtime/runtime_context.py b/sage_runtime/runtime_context.py
index d569b22..361c759 100644
--- a/sage_runtime/runtime_context.py
+++ b/sage_runtime/runtime_context.py
@@ -1,49 +1,25 @@
-import os
 from typing import TYPE_CHECKING
 import ray
 from ray.actor import ActorHandle
-from typing import List,Dict,Optional, Any, Union
+from typing import List,Dict,Optional, Any
 from sage_memory.memory_collection.base_collection import BaseMemoryCollection
 from sage_memory.memory_collection.vdb_collection import VDBMemoryCollection
 from sage_utils.custom_logger import CustomLogger
-from sage_utils.actor_wrapper import ActorWrapper
-
 if TYPE_CHECKING:
-    from sage_jobmanager.execution_graph import ExecutionGraph, GraphNode
-    from sage_core.transformation.base_transformation import BaseTransformation
-    from sage_core.environment.base_environment import BaseEnvironment 
-    from sage_jobmanager.job_manager import JobManager
-# task, operatorå’Œfunction "å½¢å¼ä¸Šå…±äº«"çš„è¿è¡Œä¸Šä¸‹æ–‡
+    from sage_runtime.compiler import Compiler, GraphNode
+    from sage_core.api.env import BaseEnvironment 
+# dagnode, operatorå’Œfunction "å½¢å¼ä¸Šå…±äº«"çš„è¿è¡Œä¸Šä¸‹æ–‡
 
 class RuntimeContext:
-    # å®šä¹‰ä¸éœ€è¦åºåˆ—åŒ–çš„å±æ€§
-    __state_exclude__ = ["_logger", "env", "_env_logger_cache"]
-    def __init__(self, graph_node: 'GraphNode', transformation: 'BaseTransformation', env: 'BaseEnvironment', jobmanager_handle: Optional[Union['JobManager', 'ActorHandle']] = None):
-        
+    def __init__(self, graph_node: 'GraphNode', env: 'BaseEnvironment'):
         self.name:str = graph_node.name
-
-        self.env_name = env.name
-        self.env_base_dir:str = env.env_base_dir
-        self.env_uuid = env.uuid
-        self.jobmanager_handle: Optional[Union['JobManager', 'ActorHandle']] = jobmanager_handle
-
-        self.memory_collection:Any = transformation.memory_collection
-
+        self.env_name:str = env.name
+        self.session_folder:Optional[str] = CustomLogger.get_session_folder()
+        self.memory_collection:Any = env.memory_collection
         self.parallel_index:int = graph_node.parallel_index
         self.parallelism:int = graph_node.parallelism
-
         self._logger:Optional[CustomLogger] = None
 
-        self.is_spout = transformation.is_spout
-
-        self.delay = 0.01
-        self.stop_signal_num = graph_node.stop_signal_num
-
-    @property
-    def jobmanager(self) -> 'JobManager':
-        return ActorWrapper(self.jobmanager_handle)
-
-
     def retrieve(self,  query: Optional[str] = None, collection_config: Optional[Dict] = None) -> List[str]:
         """
         æ™ºèƒ½é€‰æ‹©æ£€ç´¢æ–¹å¼ï¼šRay Actorè¿œç¨‹è°ƒç”¨æˆ–æœ¬åœ°å¯¹è±¡è°ƒç”¨
@@ -389,11 +365,13 @@ class RuntimeContext:
     def logger(self) -> CustomLogger:
         """æ‡’åŠ è½½logger"""
         if self._logger is None:
-            self._logger = CustomLogger([
-                ("console", "INFO"),  # æ§åˆ¶å°æ˜¾ç¤ºé‡è¦ä¿¡æ¯
-                (os.path.join(self.env_base_dir, f"{self.name}.log"), "DEBUG"),  # è¯¦ç»†æ—¥å¿—
-                (os.path.join(self.env_base_dir, "Error.log"), "ERROR")  # é”™è¯¯æ—¥å¿—
-            ],
-            name = f"{self.name}",
-        )
+            self._logger = CustomLogger(
+                filename=f"Node_{self.name}",
+                console_output="WARNING",
+                file_output="DEBUG",
+                global_output="WARNING",
+                session_folder=self.session_folder,
+                name=f"{self.name}_RuntimeContext",
+                env_name=self.env_name
+            )
         return self._logger
diff --git a/sage_utils/state_persistence.py b/sage_runtime/state_persistence.py
similarity index 94%
rename from sage_utils/state_persistence.py
rename to sage_runtime/state_persistence.py
index 5a91b48..141d4a5 100644
--- a/sage_utils/state_persistence.py
+++ b/sage_runtime/state_persistence.py
@@ -4,7 +4,6 @@ import inspect
 import threading
 from collections.abc import Mapping, Sequence, Set
 
-# TODO: state çš„æŒä¹…åŒ–ç®¡ç†ä¸åº”è¯¥ç”± functionæ¥å®šä¹‰ï¼Œè€Œæ˜¯åº”è¯¥äº¤ç»™ç³»ç»Ÿè‡ªåŠ¨åœ¨operator / taské‡Œé¢ç”Ÿæˆã€‚
 # ä¸å¯åºåˆ—åŒ–ç±»å‹é»‘åå•
 _BLACKLIST = (
     type(open),        # æ–‡ä»¶å¥æŸ„
diff --git a/sage_runtime/task/base_task.py b/sage_runtime/task/base_task.py
deleted file mode 100644
index 858fcdc..0000000
--- a/sage_runtime/task/base_task.py
+++ /dev/null
@@ -1,164 +0,0 @@
-from abc import ABC, abstractmethod
-from queue import Empty
-import threading, copy, time
-from typing import Any, TYPE_CHECKING, Union, Optional
-from sage_runtime.runtime_context import RuntimeContext
-from sage_runtime.router.packet import Packet
-from ray.util.queue import Empty
-if TYPE_CHECKING:
-    from sage_runtime.router.base_router import BaseRouter
-    from sage_runtime.router.connection import Connection
-    from sage_core.operator.base_operator import BaseOperator
-    from sage_jobmanager.factory.operator_factory import OperatorFactory
-
-class BaseTask(ABC):
-    def __init__(self,runtime_context: 'RuntimeContext',operator_factory: 'OperatorFactory') -> None:
-        self.ctx = runtime_context
-        # === ç»§æ‰¿ç±»è®¾ç½® ===
-        self.router:BaseRouter
-        self.input_buffer: Any
-        # === çº¿ç¨‹æ§åˆ¶ ===
-        self._worker_thread: Optional[threading.Thread] = None
-        self.is_running = False
-        self._stop_event = threading.Event()
-        # === æ€§èƒ½ç›‘æ§ ===
-        self._processed_count = 0
-        self._error_count = 0
-        self._last_activity_time = time.time()
-        try:
-            self.operator:BaseOperator = operator_factory.create_operator(self.ctx)
-            self.operator.task = self
-        except Exception as e:
-            self.logger.error(f"Failed to initialize node {self.name}: {e}", exc_info=True)
-
-
-    def start_running(self):
-        """å¯åŠ¨ä»»åŠ¡çš„å·¥ä½œå¾ªç¯"""
-        if self.is_running:
-            self.logger.warning(f"Task {self.name} is already running")
-            return
-        
-        self.logger.info(f"Starting task {self.name}")
-        
-        # è®¾ç½®è¿è¡ŒçŠ¶æ€
-        self.is_running = True
-        self._stop_event.clear()
-        
-        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
-        self._worker_thread = threading.Thread(
-            target=self._worker_loop,
-            name=f"{self.name}_worker",
-            daemon=True
-        )
-        self._worker_thread.start()
-        
-        self.logger.info(f"Task {self.name} started with worker thread")
-
-    def add_connection(self, connection: 'Connection'):
-        self.router.add_connection(connection)
-        self.logger.debug(f"Connection added to node '{self.name}': {connection}")
-
-    def remove_connection(self, broadcast_index: int, parallel_index: int) -> bool:
-        return self.router.remove_connection(broadcast_index, parallel_index)
-
-
-    def trigger(self, input_tag: str = None, packet:'Packet' = None) -> None:
-        try:
-            self.logger.debug(f"Received data in node {self.name}, channel {input_tag}")
-            self.operator.process_packet(packet)
-        except Exception as e:
-            self.logger.error(f"Error processing data in node {self.name}: {e}", exc_info=True)
-            raise
-
-    def stop(self) -> None:
-        """Signal the worker loop to stop."""
-        if not self._stop_event.is_set():
-            self._stop_event.set()
-            self.logger.info(f"Node '{self.name}' received stop signal.")
-
-    def get_input_buffer(self):
-        """
-        è·å–è¾“å…¥ç¼“å†²åŒº
-        :return: è¾“å…¥ç¼“å†²åŒºå¯¹è±¡
-        """
-        return self.input_buffer
-
-    def _worker_loop(self) -> None:
-        """
-        Main worker loop that executes continuously until stop is signaled.
-        """
-        # Main execution loop
-        while not self._stop_event.is_set():
-            try:
-                if self.is_spout:
-                    self.logger.debug(f"Running spout node '{self.name}'")
-                    self.operator.receive_packet(None)
-                    # TODO: åšä¸€ä¸ªä¸‹æ¸¸ç¼“å†²åŒºåå‹æœºåˆ¶ï¼Œå› ä¸ºå¼•å…¥ä¸€ä¸ªæ‰‹åŠ¨å»¶è¿Ÿå®åœ¨æ˜¯å¤ªå‘†äº†
-                    time.sleep(self.delay)
-                else:
-                    
-                    # For non-spout nodes, fetch input and process
-                    # input_result = self.fetch_input()
-                    try:
-                        data_packet = self.input_buffer.get(timeout=0.5)
-                    except Empty as e:
-                        time.sleep(0.01)
-                        continue
-                    self.logger.debug(f"Node '{self.name}' received data packet: {data_packet}, type: {type(data_packet)}")
-                    if data_packet is None:
-                        time.sleep(0.01)
-                        continue
-                    self.operator.receive_packet(data_packet)
-            except Exception as e:
-                self.logger.error(f"Critical error in node '{self.name}': {str(e)}")
-            finally:
-                self._running = False
-
-    @property
-    def is_spout(self) -> bool:
-        """æ£€æŸ¥æ˜¯å¦ä¸º spout èŠ‚ç‚¹"""
-        return self.ctx.is_spout
-
-    @property
-    def delay(self) -> float:
-        """è·å–ä»»åŠ¡çš„å»¶è¿Ÿæ—¶é—´"""
-        return self.ctx.delay
-    
-    @property
-    def logger(self):
-        """è·å–å½“å‰ä»»åŠ¡çš„æ—¥å¿—è®°å½•å™¨"""
-        return self.ctx.logger
-
-    @property
-    def name(self) -> str:
-        """è·å–ä»»åŠ¡åç§°"""
-        return self.ctx.name
-
-
-    def cleanup(self):
-        """æ¸…ç†ä»»åŠ¡èµ„æº"""
-        self.logger.info(f"Cleaning up task {self.name}")
-        
-        try:
-            # åœæ­¢ä»»åŠ¡
-            if self.is_running:
-                self.stop()
-            
-            # # æ¸…ç†ç®—å­èµ„æº
-            # if hasattr(self.operator, 'cleanup'):
-            #     self.operator.cleanup()
-            # è¿™äº›å†…å®¹åº”è¯¥ä¼šè‡ªå·±æ¸…ç†æ‰
-            # # æ¸…ç†è·¯ç”±å™¨
-            # if hasattr(self.router, 'cleanup'):
-            #     self.router.cleanup()
-            
-            # æ¸…ç†è¾“å…¥ç¼“å†²åŒº
-            if hasattr(self.input_buffer, 'cleanup'):
-                self.input_buffer.cleanup()
-            elif hasattr(self.input_buffer, 'close'):
-                self.input_buffer.close()
-            
-            self.logger.debug(f"Task {self.name} cleanup completed")
-            
-        except Exception as e:
-            self.logger.error(f"Error during cleanup of task {self.name}: {e}")
\ No newline at end of file
diff --git a/sage_runtime/task/local_task.py b/sage_runtime/task/local_task.py
deleted file mode 100644
index 873ac68..0000000
--- a/sage_runtime/task/local_task.py
+++ /dev/null
@@ -1,36 +0,0 @@
-from typing import TYPE_CHECKING
-from sage_runtime.task.base_task import BaseTask
-from sage_runtime.utils.local_message_queue import LocalMessageQueue
-from sage_runtime.router.local_router import LocalRouter
-
-if TYPE_CHECKING:
-    from sage_jobmanager.factory.operator_factory import OperatorFactory
-    from sage_runtime.runtime_context import RuntimeContext
-
-
-class LocalTask(BaseTask):
-    """
-    æœ¬åœ°ä»»åŠ¡èŠ‚ç‚¹ï¼Œä½¿ç”¨LocalMessageQueueä½œä¸ºè¾“å…¥ç¼“å†²åŒº
-    å†…éƒ¨è¿è¡Œç‹¬ç«‹çš„å·¥ä½œçº¿ç¨‹ï¼Œå¤„ç†æ•°æ®æµ
-    """
-    
-    def __init__(self,
-                 runtime_context: 'RuntimeContext', 
-                 operator_factory: 'OperatorFactory',
-                 max_buffer_size: int = 30000,
-                 queue_maxsize: int = 50000) -> None:
-        
-        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
-        super().__init__(runtime_context, operator_factory)
-
-        # === Local Message Queue ç¼“å†²åŒº ===
-        # åˆ›å»ºæœ¬åœ°æ¶ˆæ¯é˜Ÿåˆ—ä½œä¸ºè¾“å…¥ç¼“å†²åŒº
-        self.input_buffer = LocalMessageQueue(runtime_context)
-    
-        
-        # === æœ¬åœ°è·¯ç”±å™¨ ===
-        self.router = LocalRouter(runtime_context)
-        self.operator.router = self.router
-
-        self.logger.info(f"Initialized LocalTask: {self.ctx.name}")
-        self.logger.debug(f"Buffer max size: {max_buffer_size} bytes, Queue max size: {queue_maxsize}")
\ No newline at end of file
diff --git a/sage_runtime/task/ray_task.py b/sage_runtime/task/ray_task.py
deleted file mode 100644
index 781bade..0000000
--- a/sage_runtime/task/ray_task.py
+++ /dev/null
@@ -1,66 +0,0 @@
-import ray
-import time
-import threading
-from typing import Any, Union, Tuple, TYPE_CHECKING, Dict, Optional
-from ray.util.queue import Queue as RayQueue
-from sage_runtime.task.base_task import BaseTask
-from sage_runtime.router.packet import Packet
-from sage_runtime.router.ray_router import RayRouter
-if TYPE_CHECKING:
-    from sage_jobmanager.factory.operator_factory import OperatorFactory
-    from sage_runtime.runtime_context import RuntimeContext
-
-
-
-@ray.remote
-class RayTask(BaseTask):
-    """
-    åŸºäºRay Actorçš„ä»»åŠ¡èŠ‚ç‚¹ï¼Œä½¿ç”¨Ray Queueä½œä¸ºè¾“å…¥è¾“å‡ºç¼“å†²åŒº
-    å†…éƒ¨è¿è¡Œç‹¬ç«‹çš„å·¥ä½œçº¿ç¨‹ï¼Œé¿å…é˜»å¡Ray Actorçš„äº‹ä»¶å¾ªç¯
-    """
-    
-    def __init__(self,
-                 runtime_context: 'RuntimeContext', 
-                 operator_factory: 'OperatorFactory') -> None:
-        
-        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
-        super().__init__(runtime_context, operator_factory)
-
-        # === Ray Queue ç¼“å†²åŒº ===
-        # åˆ›å»ºRay Queueï¼ˆè¿™æ˜¯ä¸€ä¸ªRayå¯¹è±¡ï¼Œè‡ªåŠ¨æ”¯æŒè·¨è¿›ç¨‹ï¼‰
-        self.input_buffer = RayQueue(maxsize=1000)
-        # === è·¯ç”±å™¨ ===
-        self.router = RayRouter(runtime_context)
-        self.operator.router = self.router
-
-        self.logger.info(f"Initialized RayTask: {self.ctx.name}")
-
-    def cleanup(self):
-        super().cleanup()
-        """æ¸…ç†ä»»åŠ¡èµ„æº - é‡å†™ä»¥æ”¯æŒ Ray ç‰¹å®šæ¸…ç†"""
-        self.logger.info(f"Cleaning up RayTask {self.name}")
-        
-        try:
-            # åœæ­¢ä»»åŠ¡
-            if self.is_running:
-                self.stop()
-            
-            # # æ¸…ç†ç®—å­èµ„æº
-            # if hasattr(self.operator, 'cleanup'):
-            #     self.operator.cleanup()
-            # è¿™äº›å†…å®¹åº”è¯¥ä¼šè‡ªå·±æ¸…ç†æ‰
-            # # æ¸…ç† Ray Router
-            # if hasattr(self.router, 'cleanup'):
-            #     self.router.cleanup()
-            
-            # æ¸…ç† Ray Queue
-            if hasattr(self.input_buffer, 'shutdown'):
-                try:
-                    self.input_buffer.shutdown()
-                except Exception as e:
-                    self.logger.warning(f"Error shutting down input buffer: {e}")
-            
-            self.logger.debug(f"RayTask {self.name} cleanup completed")
-            
-        except Exception as e:
-            self.logger.error(f"Error during cleanup of RayTask {self.name}: {e}")
\ No newline at end of file
diff --git a/sage_tests/core_tests/comap_test.py b/sage_tests/core_tests/comap_test.py
index ec6a5ae..3303bb3 100644
--- a/sage_tests/core_tests/comap_test.py
+++ b/sage_tests/core_tests/comap_test.py
@@ -1,12 +1,12 @@
+import pytest
 import time
-from typing import Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+import threading
+from typing import List, Dict, Any
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.comap_function import BaseCoMapFunction
 from sage_core.function.sink_function import SinkFunction
-import json
-import tempfile
-from pathlib import Path
+
 
 class OrderDataSource(SourceFunction):
     """ç”Ÿæˆè®¢å•æ•°æ®"""
@@ -78,38 +78,30 @@ class InventoryDataSource(SourceFunction):
 
 
 class CoMapDebugSink(SinkFunction):
-    """è°ƒè¯•ç”¨çš„Sinkï¼Œé€šè¿‡æ–‡ä»¶ç³»ç»Ÿè®°å½•CoMapå¤„ç†ç»“æœ"""
+    """è°ƒè¯•ç”¨çš„Sinkï¼Œè®°å½•CoMapå¤„ç†ç»“æœ"""
+    
+    _received_data: Dict[int, List[Dict]] = {}
+    _lock = threading.Lock()
     
-    def __init__(self, output_file=None, **kwargs):
+    def __init__(self, **kwargs):
         super().__init__(**kwargs)
-        
         self.parallel_index = None
         self.received_count = 0
-        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
-        if output_file is None:
-            self.output_file = Path(tempfile.gettempdir()) / "comap_test_results.json"
-        else:
-            self.output_file = Path(output_file)
-        self.logger.info(f"CoMapDebugSink initialized, output file: {self.output_file}")
+    
     def execute(self, data: Any):
         if self.runtime_context:
             self.parallel_index = self.runtime_context.parallel_index
-        self.logctx!")
-        self.received_count += 1ctx
         
-        result_type = data.get('type', 'unknown')
-        source_stream = data.get('source_stream', -1)
+        with self._lock:
+            if self.parallel_index not in self._received_data:
+                self._received_data[self.parallel_index] = []
+            
+            self._received_data[self.parallel_index].append(data)
         
-        # å‡†å¤‡è¦å†™å…¥æ–‡ä»¶çš„è®°å½•
-        record = {
-            'timestamp': time.time(),
-            'parallel_index': self.parallel_index,
-            'received_count': self.received_count,
-            'data': data
-        }
+        self.received_count += 1
         
-        # åŸå­æ€§åœ°è¿½åŠ åˆ°æ–‡ä»¶
-        self._append_record(record)
+        result_type = data.get('type', 'unknown')
+        source_stream = data.get('source_stream', -1)
         
         self.logger.info(
             f"[Instance {self.parallel_index}] "
@@ -122,59 +114,15 @@ class CoMapDebugSink(SinkFunction):
         
         return data
     
-    def _append_record(self, record):
-        """åŸå­æ€§åœ°è¿½åŠ è®°å½•åˆ°æ–‡ä»¶"""
-        import fcntl  # Unixæ–‡ä»¶é”
-        
-        try:
-            # ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€æ–‡ä»¶
-            with open(self.output_file, 'a') as f:
-                # è·å–æ–‡ä»¶é”
-                fcntl.flock(f.fileno(), fcntl.LOCK_EX)
-                # å†™å…¥ä¸€è¡ŒJSON
-                f.write(json.dumps(record) + '\n')
-                f.flush()
-                # é”ä¼šåœ¨withå—ç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
-        except Exception as e:
-            self.logger.error(f"Failed to write record: {e}")
-    
-    @staticmethod
-    def read_results(output_file=None):
-        """è¯»å–æµ‹è¯•ç»“æœ"""
-        if output_file is None:
-            output_file = Path(tempfile.gettempdir()) / "comap_test_results.json"
-        else:
-            output_file = Path(output_file)
-        
-        if not output_file.exists():
-            return {}
-        
-        results = {}
-        try:
-            with open(output_file, 'r') as f:
-                for line in f:
-                    line = line.strip()
-                    if line:
-                        record = json.loads(line)
-                        parallel_index = record.get('parallel_index', 0)
-                        if parallel_index not in results:
-                            results[parallel_index] = []
-                        results[parallel_index].append(record['data'])
-        except Exception as e:
-            print(f"Error reading results: {e}")
-        print(f"ğŸ“‚ Read {len(results)} parallel instances from results file: {output_file}")
-        return results
+    @classmethod
+    def get_received_data(cls) -> Dict[int, List[Dict]]:
+        with cls._lock:
+            return dict(cls._received_data)
     
-    @staticmethod
-    def clear_results(output_file=None):
-        """æ¸…ç†ç»“æœæ–‡ä»¶"""
-        if output_file is None:
-            output_file = Path(tempfile.gettempdir()) / "comap_test_results.json"
-        else:
-            output_file = Path(output_file)
-        
-        if output_file.exists():
-            output_file.unlink()
+    @classmethod
+    def clear_data(cls):
+        with cls._lock:
+            cls._received_data.clear()
 
 
 class OrderPaymentCoMapFunction(BaseCoMapFunction):
@@ -355,14 +303,13 @@ class TestCoMapFunctionality:
     """æµ‹è¯•CoMapåŠŸèƒ½"""
     
     def setup_method(self):
-        # æ¸…ç†ç»“æœæ–‡ä»¶
-        CoMapDebugSink.clear_results()
+        CoMapDebugSink.clear_data()
     
     def test_basic_two_stream_comap(self):
         """æµ‹è¯•åŸºæœ¬çš„ä¸¤è·¯CoMapå¤„ç†"""
         print("\nğŸš€ Testing Basic Two-Stream CoMap")
         
-        env = LocalStreamEnvironment("basic_comap_test")
+        env = LocalEnvironment("basic_comap_test")
         
         order_stream = env.from_source(OrderDataSource, delay=0.3)
         payment_stream = env.from_source(PaymentDataSource, delay=0.4)
@@ -372,7 +319,6 @@ class TestCoMapFunctionality:
             .connect(payment_stream)
             .comap(OrderPaymentCoMapFunction)
             .sink(CoMapDebugSink, parallelism=2)
-            .print()
         )
         
         print("ğŸ“Š Pipeline: OrderStream + PaymentStream -> comap(OrderPaymentCoMapFunction) -> Sink(parallelism=2)")
@@ -380,18 +326,107 @@ class TestCoMapFunctionality:
         
         try:
             env.submit()
-            
-            time.sleep(10)
+            env.run_streaming()
+            time.sleep(3)
         finally:
             env.close()
         
-        # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ–‡ä»¶å†™å…¥å®Œæˆ
-        time.sleep(1)
         self._verify_two_stream_comap_results()
     
+    def test_three_stream_comap(self):
+        """æµ‹è¯•ä¸‰è·¯CoMapå¤„ç†"""
+        print("\nğŸš€ Testing Three-Stream CoMap")
+        
+        env = LocalEnvironment("three_stream_comap_test")
+        
+        order_stream = env.from_source(OrderDataSource, delay=0.3)
+        payment_stream = env.from_source(PaymentDataSource, delay=0.4)
+        inventory_stream = env.from_source(InventoryDataSource, delay=0.5)
+        
+        result_stream = (
+            order_stream
+            .connect(payment_stream)
+            .connect(inventory_stream)
+            .comap(TripleStreamCoMapFunction)
+            .sink(CoMapDebugSink, parallelism=3)
+        )
+        
+        print("ğŸ“Š Pipeline: OrderStream + PaymentStream + InventoryStream -> comap(TripleStreamCoMapFunction) -> Sink")
+        print("ğŸ¯ Expected: Each stream processed by corresponding mapN method\n")
+        
+        try:
+            env.submit()
+            env.run_streaming()
+            time.sleep(4)
+        finally:
+            env.close()
+        
+        self._verify_three_stream_comap_results()
+    
+    def test_stateful_comap(self):
+        """æµ‹è¯•æœ‰çŠ¶æ€çš„CoMapå¤„ç†"""
+        print("\nğŸš€ Testing Stateful CoMap")
+        
+        env = LocalEnvironment("stateful_comap_test")
+        
+        order_stream = env.from_source(OrderDataSource, delay=0.3)
+        payment_stream = env.from_source(PaymentDataSource, delay=0.6)  # å»¶è¿Ÿæ›´å¤šï¼Œè®©è®¢å•å…ˆåˆ°è¾¾
+        
+        result_stream = (
+            order_stream
+            .connect(payment_stream)
+            .comap(StatefulCoMapFunction)
+            .sink(CoMapDebugSink, parallelism=1)  # å•å®ä¾‹ä»¥ä¾¿è§‚å¯ŸçŠ¶æ€
+        )
+        
+        print("ğŸ“Š Pipeline: OrderStream + PaymentStream -> comap(StatefulCoMapFunction) -> Sink")
+        print("ğŸ¯ Expected: Orders cached, Payments enriched with order info and stats\n")
+        
+        try:
+            env.submit()
+            env.run_streaming()
+            time.sleep(5)  # æ›´å¤šæ—¶é—´è®©çŠ¶æ€ç§¯ç´¯
+        finally:
+            env.close()
+        
+        self._verify_stateful_comap_results()
+    
+    def test_comap_validation_errors(self):
+        """æµ‹è¯•CoMapå‡½æ•°éªŒè¯é”™è¯¯"""
+        print("\nğŸš€ Testing CoMap Validation Errors")
+        
+        env = LocalEnvironment("comap_validation_test")
+        
+        order_stream = env.from_source(OrderDataSource, delay=0.5)
+        payment_stream = env.from_source(PaymentDataSource, delay=0.5)
+        connected = order_stream.connect(payment_stream)
+        
+        # æµ‹è¯•1ï¼šä½¿ç”¨æ™®é€šå‡½æ•°è€Œä¸æ˜¯CoMapå‡½æ•°
+        from sage_core.function.base_function import BaseFunction
+        
+        class RegularFunction(BaseFunction):
+            def execute(self, data):
+                return data
+        
+        with pytest.raises(TypeError, match="must inherit from BaseCoMapFunction"):
+            connected.comap(RegularFunction)
+        
+        # æµ‹è¯•2ï¼šCoMapå‡½æ•°ç¼ºå°‘å¿…éœ€æ–¹æ³•
+        with pytest.raises(TypeError, match="with abstract method map1"):
+            connected.comap(InvalidCoMapFunction)
+        
+        # æµ‹è¯•3ï¼šè¾“å…¥æµæ•°é‡ä¸è¶³
+        single_stream = env.from_source(OrderDataSource, delay=0.5)
+        with pytest.raises(AttributeError, match=" object has no attribute"):
+            single_stream.comap(OrderPaymentCoMapFunction)
+        
+        print("âœ… CoMap validation tests passed")
+        
+        env.close()
+    
     def _verify_two_stream_comap_results(self):
         """éªŒè¯ä¸¤è·¯CoMapçš„ç»“æœ"""
-        received_data = CoMapDebugSink.read_results()
+        received_data = CoMapDebugSink.get_received_data()
         
         print("\nğŸ“‹ Two-Stream CoMap Results:")
         print("=" * 50)
@@ -437,6 +472,87 @@ class TestCoMapFunctionality:
             assert payment.get("source_stream") == 1, f"âŒ Payment from wrong stream: {payment.get('source_stream')}"
         
         print("âœ… Two-stream CoMap test passed: Correct stream routing and processing")
+    
+    def _verify_three_stream_comap_results(self):
+        """éªŒè¯ä¸‰è·¯CoMapçš„ç»“æœ"""
+        received_data = CoMapDebugSink.get_received_data()
+        
+        print("\nğŸ“‹ Three-Stream CoMap Results:")
+        print("=" * 50)
+        
+        stream_results = {0: [], 1: [], 2: []}
+        
+        for instance_id, data_list in received_data.items():
+            print(f"\nğŸ”¹ Parallel Instance {instance_id}:")
+            
+            for data in data_list:
+                source_stream = data.get("source_stream", -1)
+                enrichment = data.get("enrichment", "unknown")
+                stream_sequence = data.get("stream_sequence", 0)
+                
+                if 0 <= source_stream <= 2:
+                    stream_results[source_stream].append(data)
+                
+                print(f"   - Stream {source_stream}: {enrichment} (seq #{stream_sequence})")
+        
+        print(f"\nğŸ¯ Three-Stream Processing Summary:")
+        for stream_id, results in stream_results.items():
+            stream_name = ["Order", "Payment", "Inventory"][stream_id]
+            print(f"   - Stream {stream_id} ({stream_name}): {len(results)} items")
+        
+        # éªŒè¯ï¼šæ¯ä¸ªæµéƒ½åº”è¯¥æœ‰å¤„ç†ç»“æœ
+        for stream_id in range(3):
+            assert len(stream_results[stream_id]) > 0, f"âŒ No results from stream {stream_id}"
+        
+        print("âœ… Three-stream CoMap test passed: All streams processed correctly")
+    
+    def _verify_stateful_comap_results(self):
+        """éªŒè¯æœ‰çŠ¶æ€CoMapçš„ç»“æœ"""
+        received_data = CoMapDebugSink.get_received_data()
+        
+        print("\nğŸ“‹ Stateful CoMap Results:")
+        print("=" * 50)
+        
+        cached_orders = []
+        enriched_payments = []
+        
+        for instance_id, data_list in received_data.items():
+            print(f"\nğŸ”¹ Parallel Instance {instance_id}:")
+            
+            for data in data_list:
+                result_type = data.get("type", "unknown")
+                
+                if result_type == "cached_order":
+                    cached_orders.append(data)
+                    order_id = data.get("order_id", "unknown")
+                    cache_size = data.get("cache_size", 0)
+                    print(f"   - Cached Order: {order_id} (cache size: {cache_size})")
+                    
+                elif result_type == "enriched_payment_with_stats":
+                    enriched_payments.append(data)
+                    payment_id = data.get("payment_id", "unknown")
+                    status = data.get("status", "unknown")
+                    stats = data.get("payment_stats", {})
+                    order_info = data.get("order_info", {})
+                    
+                    print(f"   - Enriched Payment: {payment_id} ({status})")
+                    print(f"     Stats: {stats}")
+                    print(f"     Order Info: {order_info.get('product', 'N/A')} - ${order_info.get('amount', 0)}")
+        
+        print(f"\nğŸ¯ Stateful Processing Summary:")
+        print(f"   - Cached orders: {len(cached_orders)}")
+        print(f"   - Enriched payments: {len(enriched_payments)}")
+        
+        # éªŒè¯çŠ¶æ€ç»´æŠ¤
+        if enriched_payments:
+            final_payment = enriched_payments[-1]
+            final_stats = final_payment.get("payment_stats", {})
+            print(f"   - Final payment stats: {final_stats}")
+            
+            assert final_stats.get("success_count", 0) > 0 or final_stats.get("failed_count", 0) > 0, \
+                "âŒ Payment statistics not maintained"
+        
+        print("âœ… Stateful CoMap test passed: State correctly maintained across streams")
 
 
 if __name__ == "__main__":
diff --git a/sage_tests/core_tests/connected_keyby_test.py b/sage_tests/core_tests/connected_keyby_test.py
index 58645ea..d2289c0 100644
--- a/sage_tests/core_tests/connected_keyby_test.py
+++ b/sage_tests/core_tests/connected_keyby_test.py
@@ -2,9 +2,10 @@ import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.keyby_function import KeyByFunction
+from sage_core.function.base_function import BaseFunction
 from sage_core.function.comap_function import BaseCoMapFunction
 from sage_core.function.sink_function import SinkFunction
 
@@ -92,8 +93,8 @@ class ConnectedDebugSink(SinkFunction):
     
     def execute(self, data: Any):
         if self.runtime_context:
-            selfctx = self.runtime_context.parallel_index
-        ctx
+            self.parallel_index = self.runtime_context.parallel_index
+        
         with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
@@ -177,7 +178,7 @@ class TestConnectedStreamsKeyBy:
         """æµ‹è¯•ç»Ÿä¸€çš„KeyBy - ä¸¤ä¸ªæµä½¿ç”¨ç›¸åŒçš„key selector"""
         print("\nğŸš€ Testing Connected Streams Unified KeyBy")
         
-        env = LocalStreamEnvironment("connected_unified_keyby_test")
+        env = LocalEnvironment("connected_unified_keyby_test")
         
         # åˆ›å»ºä¸¤ä¸ªæ•°æ®æº
         user_stream = env.from_source(UserDataSource, delay=0.3)
@@ -197,7 +198,7 @@ class TestConnectedStreamsKeyBy:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -208,7 +209,7 @@ class TestConnectedStreamsKeyBy:
         """æµ‹è¯•Flinké£æ ¼çš„per-stream KeyBy - æ¯ä¸ªæµä½¿ç”¨ä¸åŒçš„key selector"""
         print("\nğŸš€ Testing Connected Streams Per-Stream KeyBy (Flink-style)")
         
-        env = LocalStreamEnvironment("connected_per_stream_keyby_test")
+        env = LocalEnvironment("connected_per_stream_keyby_test")
         
         user_stream = env.from_source(UserDataSource, delay=0.3)
         event_stream = env.from_source(EventDataSource, delay=0.4)
@@ -227,7 +228,7 @@ class TestConnectedStreamsKeyBy:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -238,7 +239,7 @@ class TestConnectedStreamsKeyBy:
         """æµ‹è¯•KeyByåæ¥CoMapæ“ä½œ"""
         print("\nğŸš€ Testing Connected Streams KeyBy + CoMap")
         
-        env = LocalStreamEnvironment("connected_keyby_comap_test")
+        env = LocalEnvironment("connected_keyby_comap_test")
         
         user_stream = env.from_source(UserDataSource, delay=0.3)
         event_stream = env.from_source(EventDataSource, delay=0.4)
@@ -257,7 +258,7 @@ class TestConnectedStreamsKeyBy:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)  # ç»™æ›´å¤šæ—¶é—´è®©joinæ“ä½œå®Œæˆ
         finally:
             env.close()
@@ -268,7 +269,7 @@ class TestConnectedStreamsKeyBy:
         """æµ‹è¯•æ— æ•ˆçš„KeyByé…ç½®"""
         print("\nğŸš€ Testing Invalid KeyBy Configurations")
         
-        env = LocalStreamEnvironment("invalid_keyby_test")
+        env = LocalEnvironment("invalid_keyby_test")
         
         user_stream = env.from_source(UserDataSource, delay=0.5)
         event_stream = env.from_source(EventDataSource, delay=0.5)
diff --git a/sage_tests/core_tests/filter_test.py b/sage_tests/core_tests/filter_test.py
index a259bbd..253ed7d 100644
--- a/sage_tests/core_tests/filter_test.py
+++ b/sage_tests/core_tests/filter_test.py
@@ -1,10 +1,12 @@
+import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.filter_function import FilterFunction
 from sage_core.function.sink_function import SinkFunction
+from sage_core.function.base_function import BaseFunction
 
 
 class NumberDataSource(SourceFunction):
@@ -73,8 +75,8 @@ class FilterDebugSink(SinkFunction):
     def execute(self, data: Any):
         if self.runtime_context:
             self.parallel_index = self.runtime_context.parallel_index
-        ctx
-        with self._lock:ctx
+        
+        with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
             
@@ -187,7 +189,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•åŸºæœ¬çš„æ­£æ•°è¿‡æ»¤"""
         print("\nğŸš€ Testing Basic Positive Number Filter")
         
-        env = LocalStreamEnvironment("positive_filter_test")
+        env = LocalEnvironment("positive_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -200,7 +202,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -211,7 +213,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•é“¾å¼è¿‡æ»¤å™¨"""
         print("\nğŸš€ Testing Chained Filters")
         
-        env = LocalStreamEnvironment("chained_filter_test")
+        env = LocalEnvironment("chained_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -225,7 +227,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -236,7 +238,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•ç”¨æˆ·æ•°æ®è¿‡æ»¤"""
         print("\nğŸš€ Testing User Data Filters")
         
-        env = LocalStreamEnvironment("user_filter_test")
+        env = LocalEnvironment("user_filter_test")
         
         result_stream = (
             env.from_source(UserDataSource, delay=0.3)
@@ -250,7 +252,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
@@ -261,7 +263,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•Lambdaå‡½æ•°è¿‡æ»¤"""
         print("\nğŸš€ Testing Lambda Function Filter")
         
-        env = LocalStreamEnvironment("lambda_filter_test")
+        env = LocalEnvironment("lambda_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -274,7 +276,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -285,7 +287,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•æç«¯æƒ…å†µçš„è¿‡æ»¤å™¨"""
         print("\nğŸš€ Testing Extreme Filter Cases")
         
-        env = LocalStreamEnvironment("extreme_filter_test")
+        env = LocalEnvironment("extreme_filter_test")
         
         # æµ‹è¯•1ï¼šæ‰€æœ‰æ•°æ®éƒ½é€šè¿‡
         always_true_stream = (
@@ -298,7 +300,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(2)
         finally:
             env.close()
@@ -307,7 +309,7 @@ class TestFilterFunctionality:
         FilterDebugSink.clear_data()
         
         # æµ‹è¯•2ï¼šæ‰€æœ‰æ•°æ®éƒ½è¢«è¿‡æ»¤
-        env2 = LocalStreamEnvironment("always_false_filter_test")
+        env2 = LocalEnvironment("always_false_filter_test")
         
         always_false_stream = (
             env2.from_source(NumberDataSource, delay=0.2)
@@ -332,7 +334,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•Filterä¸Mapçš„é›†æˆ"""
         print("\nğŸš€ Testing Filter + Map Integration")
         
-        env = LocalStreamEnvironment("filter_map_integration_test")
+        env = LocalEnvironment("filter_map_integration_test")
         
         result_stream = (
             env.from_source(UserDataSource, delay=0.3)
@@ -351,7 +353,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
@@ -362,7 +364,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•Filterçš„é”™è¯¯å¤„ç†"""
         print("\nğŸš€ Testing Filter Error Handling")
         
-        env = LocalStreamEnvironment("filter_error_test")
+        env = LocalEnvironment("filter_error_test")
         
         # æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•å¯èƒ½ä¼šäº§ç”Ÿé”™è¯¯æ—¥å¿—ï¼Œè¿™æ˜¯é¢„æœŸçš„
         result_stream = (
@@ -376,7 +378,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
diff --git a/sage_tests/core_tests/flatmap_test.py b/sage_tests/core_tests/flatmap_test.py
index a259bbd..253ed7d 100644
--- a/sage_tests/core_tests/flatmap_test.py
+++ b/sage_tests/core_tests/flatmap_test.py
@@ -1,10 +1,12 @@
+import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.filter_function import FilterFunction
 from sage_core.function.sink_function import SinkFunction
+from sage_core.function.base_function import BaseFunction
 
 
 class NumberDataSource(SourceFunction):
@@ -73,8 +75,8 @@ class FilterDebugSink(SinkFunction):
     def execute(self, data: Any):
         if self.runtime_context:
             self.parallel_index = self.runtime_context.parallel_index
-        ctx
-        with self._lock:ctx
+        
+        with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
             
@@ -187,7 +189,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•åŸºæœ¬çš„æ­£æ•°è¿‡æ»¤"""
         print("\nğŸš€ Testing Basic Positive Number Filter")
         
-        env = LocalStreamEnvironment("positive_filter_test")
+        env = LocalEnvironment("positive_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -200,7 +202,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -211,7 +213,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•é“¾å¼è¿‡æ»¤å™¨"""
         print("\nğŸš€ Testing Chained Filters")
         
-        env = LocalStreamEnvironment("chained_filter_test")
+        env = LocalEnvironment("chained_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -225,7 +227,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -236,7 +238,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•ç”¨æˆ·æ•°æ®è¿‡æ»¤"""
         print("\nğŸš€ Testing User Data Filters")
         
-        env = LocalStreamEnvironment("user_filter_test")
+        env = LocalEnvironment("user_filter_test")
         
         result_stream = (
             env.from_source(UserDataSource, delay=0.3)
@@ -250,7 +252,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
@@ -261,7 +263,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•Lambdaå‡½æ•°è¿‡æ»¤"""
         print("\nğŸš€ Testing Lambda Function Filter")
         
-        env = LocalStreamEnvironment("lambda_filter_test")
+        env = LocalEnvironment("lambda_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -274,7 +276,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -285,7 +287,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•æç«¯æƒ…å†µçš„è¿‡æ»¤å™¨"""
         print("\nğŸš€ Testing Extreme Filter Cases")
         
-        env = LocalStreamEnvironment("extreme_filter_test")
+        env = LocalEnvironment("extreme_filter_test")
         
         # æµ‹è¯•1ï¼šæ‰€æœ‰æ•°æ®éƒ½é€šè¿‡
         always_true_stream = (
@@ -298,7 +300,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(2)
         finally:
             env.close()
@@ -307,7 +309,7 @@ class TestFilterFunctionality:
         FilterDebugSink.clear_data()
         
         # æµ‹è¯•2ï¼šæ‰€æœ‰æ•°æ®éƒ½è¢«è¿‡æ»¤
-        env2 = LocalStreamEnvironment("always_false_filter_test")
+        env2 = LocalEnvironment("always_false_filter_test")
         
         always_false_stream = (
             env2.from_source(NumberDataSource, delay=0.2)
@@ -332,7 +334,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•Filterä¸Mapçš„é›†æˆ"""
         print("\nğŸš€ Testing Filter + Map Integration")
         
-        env = LocalStreamEnvironment("filter_map_integration_test")
+        env = LocalEnvironment("filter_map_integration_test")
         
         result_stream = (
             env.from_source(UserDataSource, delay=0.3)
@@ -351,7 +353,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
@@ -362,7 +364,7 @@ class TestFilterFunctionality:
         """æµ‹è¯•Filterçš„é”™è¯¯å¤„ç†"""
         print("\nğŸš€ Testing Filter Error Handling")
         
-        env = LocalStreamEnvironment("filter_error_test")
+        env = LocalEnvironment("filter_error_test")
         
         # æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•å¯èƒ½ä¼šäº§ç”Ÿé”™è¯¯æ—¥å¿—ï¼Œè¿™æ˜¯é¢„æœŸçš„
         result_stream = (
@@ -376,7 +378,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
diff --git a/sage_tests/core_tests/join_test.py b/sage_tests/core_tests/join_test.py
index 3ae3a15..10909de 100644
--- a/sage_tests/core_tests/join_test.py
+++ b/sage_tests/core_tests/join_test.py
@@ -1,7 +1,8 @@
+import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.flatmap_function import FlatMapFunction
 from sage_core.function.filter_function import FilterFunction
@@ -489,8 +490,8 @@ class JoinResultSink(SinkFunction):
     
     def execute(self, data: Any):
         if self.runtime_context:
-            selfctx = self.runtime_context.parallel_index
-        ctx
+            self.parallel_index = self.runtime_context.parallel_index
+        
         with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
@@ -538,7 +539,7 @@ class TestJoinFunctionality:
         """æµ‹è¯•å®Œæ•´çš„FlatMap -> Filter -> Joinç®¡é“"""
         print("\nğŸš€ Testing Complete FlatMap -> Filter -> Join Pipeline")
         
-        env = LocalStreamEnvironment("flatmap_filter_join_test")
+        env = LocalEnvironment("flatmap_filter_join_test")
         
         # 1. åˆ›å»ºæºæ•°æ®æµ
         order_source = env.from_source(OrderEventSource, delay=0.2)
@@ -571,7 +572,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(6)
         finally:
             env.close()
@@ -582,7 +583,7 @@ class TestJoinFunctionality:
         """æµ‹è¯•å¤šé˜¶æ®µJoinç®¡é“"""
         print("\nğŸš€ Testing Multi-Stage Join Pipeline")
         
-        env = LocalStreamEnvironment("multi_stage_join_test")
+        env = LocalEnvironment("multi_stage_join_test")
         
         # ç¬¬ä¸€é˜¶æ®µï¼šè®¢å•äº‹ä»¶æµå¤„ç†
         order_source = env.from_source(OrderEventSource, delay=0.2)
@@ -628,7 +629,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(6)
         finally:
             env.close()
@@ -639,7 +640,7 @@ class TestJoinFunctionality:
         """æµ‹è¯•åŸºäºæ—¶é—´çª—å£çš„Join"""
         print("\nğŸš€ Testing Windowed Join Pipeline")
         
-        env = LocalStreamEnvironment("windowed_join_test")
+        env = LocalEnvironment("windowed_join_test")
         
         order_source = env.from_source(OrderEventSource, delay=0.15)
         
@@ -671,7 +672,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(5)
         finally:
             env.close()
@@ -682,7 +683,7 @@ class TestJoinFunctionality:
         """æµ‹è¯•åŒ…å«å¤šä¸ªJoinçš„å¤æ‚ç®¡é“"""
         print("\nğŸš€ Testing Complex Pipeline with Multiple Joins")
         
-        env = LocalStreamEnvironment("complex_multi_join_test")
+        env = LocalEnvironment("complex_multi_join_test")
         
         # æ•°æ®æº
         order_source = env.from_source(OrderEventSource, delay=0.2)
@@ -735,7 +736,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(7)
         finally:
             env.close()
@@ -746,7 +747,7 @@ class TestJoinFunctionality:
         """æµ‹è¯•ç©ºæµçš„Joinå¤„ç†"""
         print("\nğŸš€ Testing Join with Empty/Filtered Streams")
         
-        env = LocalStreamEnvironment("empty_stream_join_test")
+        env = LocalEnvironment("empty_stream_join_test")
         
         order_source = env.from_source(OrderEventSource, delay=0.2)
         user_source = env.from_source(UserProfileSource, delay=0.3)
@@ -779,7 +780,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
diff --git a/sage_tests/core_tests/kafka_test.py b/sage_tests/core_tests/kafka_test.py
index a26ebfa..404317f 100644
--- a/sage_tests/core_tests/kafka_test.py
+++ b/sage_tests/core_tests/kafka_test.py
@@ -1,8 +1,9 @@
 import pytest
 import json
 import time
-from unittest.mock import Mock, patch
-from sage_core.api.local_environment import LocalStreamEnvironment, RemoteEnvironment
+import threading
+from unittest.mock import Mock, patch, MagicMock
+from sage_core.api.env import LocalEnvironment, RemoteEnvironment
 from sage_core.function.kafka_source import KafkaSourceFunction
 from sage_core.function.base_function import BaseFunction
 
@@ -308,7 +309,7 @@ class TestKafkaSourceIntegration:
     
     def test_environment_kafka_source_creation(self):
         """æµ‹è¯•Environmentåˆ›å»ºKafkaæº"""
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="localhost:9092",
@@ -320,10 +321,10 @@ class TestKafkaSourceIntegration:
         
         # éªŒè¯DataStreamåˆ›å»º
         assert kafka_stream is not None
-        assert len(env.pipeline) == 1
+        assert len(env._pipeline) == 1
         
         # éªŒè¯Transformationç±»å‹
-        transformation = env.pipeline[0]
+        transformation = env._pipeline[0]
         assert transformation.function_class == KafkaSourceFunction
         
         # éªŒè¯å‚æ•°ä¼ é€’
@@ -396,7 +397,7 @@ class TestKafkaSourcePipeline:
         mock_kafka_consumer.return_value = mock_consumer_instance
         
         # åˆ›å»ºæµ‹è¯•pipeline
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="localhost:9092",
@@ -412,15 +413,15 @@ class TestKafkaSourcePipeline:
         processed_stream = kafka_stream.map(ProcessTestEvent)
         
         # éªŒè¯pipelineæ„å»º
-        assert len(env.pipeline) == 2  # source + map
+        assert len(env._pipeline) == 2  # source + map
         
         # éªŒè¯sourceé…ç½®
-        source_transformation = env.pipeline[0]
+        source_transformation = env._pipeline[0]
         assert source_transformation.function_class == KafkaSourceFunction
         assert source_transformation.function_kwargs['topic'] == "test_topic"
         
         # éªŒè¯mapé…ç½®
-        map_transformation = env.pipeline[1]
+        map_transformation = env._pipeline[1]
         assert map_transformation.function_class == ProcessTestEvent
     
     def test_kafka_source_with_custom_deserializer(self):
@@ -431,7 +432,7 @@ class TestKafkaSourcePipeline:
             json_data['custom_prefix'] = 'CUSTOM_'
             return json_data
         
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="localhost:9092",
@@ -441,7 +442,7 @@ class TestKafkaSourcePipeline:
         )
         
         # éªŒè¯è‡ªå®šä¹‰ååºåˆ—åŒ–å™¨ä¼ é€’
-        transformation = env.pipeline[0]
+        transformation = env._pipeline[0]
         assert transformation.function_kwargs['value_deserializer'] == custom_deserializer
 
 
@@ -450,7 +451,7 @@ class TestKafkaSourceConfiguration:
     
     def test_default_configuration(self):
         """æµ‹è¯•é»˜è®¤é…ç½®"""
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="localhost:9092",
@@ -458,7 +459,7 @@ class TestKafkaSourceConfiguration:
             group_id="test_group"
         )
         
-        transformation = env.pipeline[0]
+        transformation = env._pipeline[0]
         
         # éªŒè¯é»˜è®¤å€¼
         assert transformation.function_kwargs['auto_offset_reset'] == 'latest'
@@ -468,7 +469,7 @@ class TestKafkaSourceConfiguration:
     
     def test_custom_configuration(self):
         """æµ‹è¯•è‡ªå®šä¹‰é…ç½®"""
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="custom:9092",
@@ -482,7 +483,7 @@ class TestKafkaSourceConfiguration:
             security_protocol="SSL"
         )
         
-        transformation = env.pipeline[0]
+        transformation = env._pipeline[0]
         
         # éªŒè¯è‡ªå®šä¹‰å€¼
         assert transformation.function_kwargs['bootstrap_servers'] == "custom:9092"
diff --git a/sage_tests/core_tests/keyby_test.py b/sage_tests/core_tests/keyby_test.py
index 999a87d..e380cc4 100644
--- a/sage_tests/core_tests/keyby_test.py
+++ b/sage_tests/core_tests/keyby_test.py
@@ -1,9 +1,11 @@
+import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.keyby_function import KeyByFunction
+from sage_core.function.base_function import BaseFunction
 from sage_core.function.sink_function import SinkFunction
 
 
@@ -54,8 +56,8 @@ class ParallelDebugSink(SinkFunction):
         # ä»runtime_contextè·å–parallel_index
         if self.runtime_context:
             self.parallel_index = self.runtime_context.parallel_index
-        ctx
-        with self._lock:ctx
+        
+        with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
             
@@ -99,7 +101,7 @@ class TestKeyByFunctionality:
         print("\nğŸš€ Testing KeyBy Hash Partitioning")
         
         # åˆ›å»ºç¯å¢ƒ
-        env = LocalStreamEnvironment("keyby_test")
+        env = LocalEnvironment("keyby_test")
         
         # æ„å»ºæ•°æ®æµï¼šsource -> keyby -> parallel sink
         result_stream = (
@@ -114,7 +116,7 @@ class TestKeyByFunctionality:
         try:
             # æäº¤å¹¶è¿è¡Œ
             env.submit()
-            
+            env.run_streaming()
             
             # è¿è¡Œä¸€æ®µæ—¶é—´è®©æ•°æ®æµè¿‡
             time.sleep(3)
@@ -132,7 +134,7 @@ class TestKeyByFunctionality:
         """æµ‹è¯•å¹¿æ’­ç­–ç•¥"""
         print("\nğŸš€ Testing KeyBy Broadcast Strategy")
         
-        env = LocalStreamEnvironment("keyby_broadcast_test")
+        env = LocalEnvironment("keyby_broadcast_test")
         
         result_stream = (
             env.from_source(TestDataSource, delay=0.3)
@@ -145,7 +147,7 @@ class TestKeyByFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(2)
         finally:
             env.close()
@@ -241,7 +243,7 @@ class TestAdvancedKeyBy:
         """æµ‹è¯•å¤æ‚çš„keyæå–é€»è¾‘"""
         print("\nğŸš€ Testing Advanced Key Extraction")
         
-        env = LocalStreamEnvironment("advanced_keyby_test")
+        env = LocalEnvironment("advanced_keyby_test")
         
         result_stream = (
             env.from_source(TestDataSource, delay=0.4)
@@ -254,7 +256,7 @@ class TestAdvancedKeyBy:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
diff --git a/sage_tests/example_tests/pipeline_test.py b/sage_tests/example_tests/pipeline_test.py
index 6f614ad..d18dbaa 100644
--- a/sage_tests/example_tests/pipeline_test.py
+++ b/sage_tests/example_tests/pipeline_test.py
@@ -6,9 +6,9 @@ import logging
 import time
 from typing import TYPE_CHECKING
 
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_libs.io.sink import FileSink
-from sage_libs.io.source import FileSource
+from sage_core.api.env import LocalEnvironment
+from sage_common_funs.io.sink import FileSink
+from sage_common_funs.io.source import FileSource
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.refiner import AbstractiveRecompRefiner
@@ -21,7 +21,7 @@ if TYPE_CHECKING:
 
 def init_memory_and_pipeline():
     # åˆ›å»ºä¸€ä¸ªæ–°çš„ç®¡é“å®ä¾‹
-    pipeline = LocalStreamEnvironment()
+    pipeline = LocalEnvironment()
 
     # æ­¥éª¤ 1: å®šä¹‰æ•°æ®æºï¼ˆä¾‹å¦‚ï¼Œæ¥è‡ªç”¨æˆ·çš„æŸ¥è¯¢ï¼‰
     query_stream: DataStream = pipeline.from_source(FileSource,config["source"])  # ä»æ–‡ä»¶æºè¯»å–æ•°æ®
diff --git a/sage_tests/function_tests/io_tests/__init__.py b/sage_tests/function_tests/io_tests/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/sage_tests/function_tests/io_tests/sink_test.py b/sage_tests/function_tests/io_tests/sink_test.py
index ad5c24e..5bde7e1 100644
--- a/sage_tests/function_tests/io_tests/sink_test.py
+++ b/sage_tests/function_tests/io_tests/sink_test.py
@@ -1,10 +1,10 @@
 import pytest
 
+from sage_common_funs.io.sink import (
+    TerminalSink, RetriveSink, FileSink, MemWriteSink
+)
 import os
 
-from sage_libs.io.sink import TerminalSink, RetriveSink, FileSink, MemWriteSink
-
-
 @pytest.fixture
 def sample_qa_data():
     return ("What is AI?", "Artificial Intelligence")
diff --git a/sage_tests/function_tests/io_tests/source_test.py b/sage_tests/function_tests/io_tests/source_test.py
index 9b2b23e..8ca547a 100644
--- a/sage_tests/function_tests/io_tests/source_test.py
+++ b/sage_tests/function_tests/io_tests/source_test.py
@@ -1,7 +1,9 @@
 import pytest
 
-from sage_libs.io.source import FileSource
-
+from sage_common_funs.io.source import (
+    FileSource
+)
+import os
 
 @pytest.fixture
 def sample_file(tmp_path):
diff --git a/sage_tests/function_tests/rag_tests/retriever_test.py b/sage_tests/function_tests/rag_tests/retriever_test.py
index 1196c28..501c9f4 100644
--- a/sage_tests/function_tests/rag_tests/retriever_test.py
+++ b/sage_tests/function_tests/rag_tests/retriever_test.py
@@ -21,7 +21,7 @@ def test_dense_retriever_execute(dense_retriever_config):
 
     # mock runtime_context
     mock_runtime_context = MagicMock()
-    retriever.ctx = mock_runtime_context
+    retriever.runtime_context = mock_runtime_context
     mock_runtime_context.retrieve.return_value = ["doc1", "doc2"]
 
     input_query = "test query"
@@ -62,7 +62,7 @@ def test_bm25s_retriever_execute(bm25s_retriever_config, caplog):
     # æ³¨å…¥ runtime_context mockï¼ˆä¸éœ€è¦ memory äº†ï¼‰
     mock_runtime_context = MagicMock()
     mock_runtime_context.retrieve.return_value = ["doc1", "doc2"]
-    retriever.ctx = mock_runtime_context
+    retriever.runtime_context = mock_runtime_context
 
     data = "some query"
 
@@ -82,7 +82,7 @@ def test_bm25s_retriever_execute_no_collection(bm25s_retriever_config):
     mock_runtime_context = MagicMock()
     mock_memory = MagicMock()
     mock_runtime_context.memory = mock_memory
-    retriever.ctx = mock_runtime_context
+    retriever.runtime_context = mock_runtime_context
 
     data = "some query"
 
diff --git a/sage_tests/runtime_tests/test_message_queue.py b/sage_tests/runtime_tests/test_message_queue.py
index f18b588..083843d 100644
--- a/sage_tests/runtime_tests/test_message_queue.py
+++ b/sage_tests/runtime_tests/test_message_queue.py
@@ -3,7 +3,7 @@ import threading
 import random
 import time
 import queue
-from sage_runtime.utils.local_message_queue import LocalMessageQueue
+from sage_runtime.io.local_message_queue import LocalMessageQueue
 
 
 class TestMessageQueue(unittest.TestCase):
diff --git a/sage_tests/runtime_tests/test_qa_oneshot.py b/sage_tests/runtime_tests/test_qa_oneshot.py
index 0aae8b5..a5823ed 100644
--- a/sage_tests/runtime_tests/test_qa_oneshot.py
+++ b/sage_tests/runtime_tests/test_qa_oneshot.py
@@ -1,11 +1,73 @@
+# import unittest
+# import logging
+# import time
+# import io
+# from contextlib import redirect_stdout
+# from pathlib import Path
+# import os
+# import yaml
+# from dotenv import load_dotenv
+#
+# from sage_core.api.env import LocalEnvironment
+# from sage_common_funs.io.sink import TerminalSink
+# from sage_common_funs.io.source import FileSource
+# from sage_common_funs.rag.generator import OpenAIGenerator
+# from sage_common_funs.rag.promptor import QAPromptor
+# from sage_common_funs.rag.retriever import DenseRetriever
+# from sage_utils.config_loader import load_config
+# from sage_utils.logging_utils import configure_logging
+#
+#
+# class TestFullLocalPipeline(unittest.TestCase):
+#     def setUp(self):
+#         configure_logging(level=logging.INFO)
+#         load_dotenv(override=False)
+#         self.config = load_config('config_mixed.yaml')
+#         api_key = os.environ.get("ALIBABA_API_KEY")
+#         if api_key:
+#             self.config.setdefault("generator", {})["api_key"] = api_key
+#
+#     def test_local_pipeline_run(self):
+#         """æµ‹è¯• Remote pipeline æ‰§è¡Œï¼Œå¹¶éªŒè¯æ˜¯å¦è¾“å‡ºè‡³å°‘ 5 æ¬¡ Q/A"""
+#         env = LocalEnvironment()
+#         env.set_memory(config=None)
+#
+#         query_stream = (env
+#             .from_source(FileSource, self.config["source"])
+#             .map(DenseRetriever, self.config["retriever"])
+#             .map(QAPromptor, self.config["promptor"])
+#             .map(OpenAIGenerator, self.config["generator"])
+#             .sink(TerminalSink, self.config["sink"])
+#         )
+#
+#         env.submit()
+#
+#         with io.StringIO() as buf, redirect_stdout(buf):
+#             env.run_once()
+#             env.run_once()
+#             env.run_once()
+#             time.sleep(15)
+#             output = buf.getvalue()
+#
+#         q_count = output.count("[Q] Question :")
+#         a_count = output.count("[A] Answer :")
+#
+#         self.assertGreaterEqual(q_count, 3, f"Question è¾“å‡ºä¸è¶³ 3 æ¬¡ï¼Œå®é™…ä¸º {q_count}")
+#         self.assertGreaterEqual(a_count, 3, f"Answer è¾“å‡ºä¸è¶³ 3 æ¬¡ï¼Œå®é™…ä¸º {a_count}")
+#
+#
+#
+# if __name__ == '__main__':
+#     unittest.main()
+
 import logging
 import pytest
 from dotenv import load_dotenv
 import os
 
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_libs.io.sink import TerminalSink
-from sage_libs.io.source import FileSource
+from sage_core.api.env import LocalEnvironment
+from sage_common_funs.io.sink import TerminalSink
+from sage_common_funs.io.source import FileSource
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -26,7 +88,7 @@ def config():
 
 @pytest.fixture(scope="function")
 def env():
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
     yield env
     # teardown: ä¸»åŠ¨æ¸…ç†èµ„æº
diff --git a/sage_tests/utils_tests/test_name_server.py b/sage_tests/utils_tests/test_name_server.py
index f18dba0..684fba4 100644
--- a/sage_tests/utils_tests/test_name_server.py
+++ b/sage_tests/utils_tests/test_name_server.py
@@ -1,6 +1,6 @@
 import unittest
 import threading
-from sage_jobmanager.utils.name_server import NameServer
+from sage_utils.name_server import NameServer
 
 class TestNameServer(unittest.TestCase):
     
diff --git a/sage_tests/utils_tests/test_ray_object_trimmer.py b/sage_tests/utils_tests/test_ray_object_trimmer.py
deleted file mode 100644
index 68c2cc1..0000000
--- a/sage_tests/utils_tests/test_ray_object_trimmer.py
+++ /dev/null
@@ -1,389 +0,0 @@
-"""
-æµ‹è¯•Rayå¯¹è±¡é¢„å¤„ç†å™¨çš„æ•ˆæœ
-éªŒè¯trim_object_for_rayå‡½æ•°èƒ½å¦æ­£ç¡®æ¸…ç†å¯¹è±¡ä»¥ä¾›Rayåºåˆ—åŒ–
-"""
-import sys
-import os
-import time
-import threading
-import pickle
-from typing import Any, Dict, List, Optional
-from pathlib import Path
-
-# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
-SAGE_ROOT = Path(__file__).parent.parent
-sys.path.insert(0, str(SAGE_ROOT))
-
-from sage_utils.serialization.dill_serializer import (
-    trim_object_for_ray,
-    RayObjectTrimmer,
-    SerializationError
-)
-
-# å°è¯•å¯¼å…¥Ray
-try:
-    import ray
-    RAY_AVAILABLE = True
-    print("Ray is available for testing")
-except ImportError:
-    RAY_AVAILABLE = False
-    print("Ray not available, will only test object trimming")
-
-
-class ProblematicClass:
-    """åŒ…å«ä¸å¯åºåˆ—åŒ–å†…å®¹çš„æµ‹è¯•ç±»"""
-    
-    def __init__(self, name: str):
-        self.name = name
-        self.data = {"key": "value", "number": 42}
-        
-        # ä¸å¯åºåˆ—åŒ–çš„å±æ€§
-        self.logger = self._create_mock_logger()
-        self.thread = threading.Thread(target=lambda: None)
-        self.lock = threading.Lock()
-        self.file_handle = open(__file__, 'r')  # æ–‡ä»¶å¥æŸ„
-        self.socket_mock = self._create_socket_mock()
-        
-        # å¾ªç¯å¼•ç”¨
-        self.self_ref = self
-        
-        # å¯åºåˆ—åŒ–çš„å±æ€§
-        self.config = {
-            "setting1": "value1",
-            "setting2": 100,
-            "nested": {"inner": "data"}
-        }
-        self.items = [1, 2, 3, "test"]
-        
-        # å®šä¹‰åºåˆ—åŒ–æ’é™¤åˆ—è¡¨
-        self.__state_exclude__ = [
-            'logger', 'thread', 'lock', 'file_handle', 'socket_mock', 'self_ref'
-        ]
-    
-    def _create_mock_logger(self):
-        """åˆ›å»ºæ¨¡æ‹Ÿæ—¥å¿—å¯¹è±¡"""
-        class MockLogger:
-            def info(self, msg): pass
-            def error(self, msg): pass
-        return MockLogger()
-    
-    def _create_socket_mock(self):
-        """åˆ›å»ºæ¨¡æ‹Ÿsocketå¯¹è±¡"""
-        class MockSocket:
-            def __init__(self):
-                self.connected = False
-        return MockSocket()
-    
-    def get_safe_data(self):
-        """è¿”å›å¯ä»¥å®‰å…¨åºåˆ—åŒ–çš„æ•°æ®"""
-        return {
-            "name": self.name,
-            "data": self.data,
-            "config": self.config,
-            "items": self.items
-        }
-    
-    def __del__(self):
-        # æ¸…ç†æ–‡ä»¶å¥æŸ„
-        try:
-            if hasattr(self, 'file_handle') and self.file_handle:
-                self.file_handle.close()
-        except:
-            pass
-
-
-class NestedProblematicClass:
-    """åŒ…å«åµŒå¥—é—®é¢˜çš„æµ‹è¯•ç±»"""
-    
-    def __init__(self):
-        self.name = "nested_test"
-        self.problematic_child = ProblematicClass("child")
-        self.safe_data = {"nested": "safe"}
-        self.thread_list = [threading.Thread(target=lambda: None) for _ in range(3)]
-
-
-def test_basic_trimming():
-    """æµ‹è¯•åŸºæœ¬çš„å¯¹è±¡trimåŠŸèƒ½"""
-    print("\n=== æµ‹è¯•åŸºæœ¬TrimåŠŸèƒ½ ===")
-    
-    # åˆ›å»ºæœ‰é—®é¢˜çš„å¯¹è±¡
-    obj = ProblematicClass("test_object")
-    
-    print(f"åŸå§‹å¯¹è±¡å±æ€§æ•°é‡: {len(obj.__dict__)}")
-    print("åŸå§‹å¯¹è±¡å±æ€§:", list(obj.__dict__.keys()))
-    
-    # æ‰§è¡Œtrim
-    try:
-        trimmed_obj = trim_object_for_ray(obj)
-        
-        print(f"æ¸…ç†åå¯¹è±¡å±æ€§æ•°é‡: {len(trimmed_obj.__dict__)}")
-        print("æ¸…ç†åå¯¹è±¡å±æ€§:", list(trimmed_obj.__dict__.keys()))
-        
-        # éªŒè¯æ¸…ç†æ•ˆæœ
-        assert hasattr(trimmed_obj, 'name'), "nameå±æ€§åº”è¯¥è¢«ä¿ç•™"
-        assert hasattr(trimmed_obj, 'data'), "dataå±æ€§åº”è¯¥è¢«ä¿ç•™"
-        assert hasattr(trimmed_obj, 'config'), "configå±æ€§åº”è¯¥è¢«ä¿ç•™"
-        assert not hasattr(trimmed_obj, 'logger'), "loggerå±æ€§åº”è¯¥è¢«ç§»é™¤"
-        assert not hasattr(trimmed_obj, 'thread'), "threadå±æ€§åº”è¯¥è¢«ç§»é™¤"
-        assert not hasattr(trimmed_obj, 'lock'), "lockå±æ€§åº”è¯¥è¢«ç§»é™¤"
-        
-        print("âœ“ åŸºæœ¬trimæµ‹è¯•é€šè¿‡")
-        return trimmed_obj
-        
-    except Exception as e:
-        print(f"âœ— åŸºæœ¬trimæµ‹è¯•å¤±è´¥: {e}")
-        raise
-
-
-def test_custom_include_exclude():
-    """æµ‹è¯•è‡ªå®šä¹‰include/excludeåŠŸèƒ½"""
-    print("\n=== æµ‹è¯•è‡ªå®šä¹‰Include/Exclude ===")
-    
-    obj = ProblematicClass("test_custom")
-    
-    # æµ‹è¯•exclude
-    trimmed_obj = trim_object_for_ray(obj, exclude=['data', 'config'])
-    assert hasattr(trimmed_obj, 'name'), "nameåº”è¯¥ä¿ç•™"
-    assert not hasattr(trimmed_obj, 'data'), "dataåº”è¯¥è¢«æ’é™¤"
-    assert not hasattr(trimmed_obj, 'config'), "configåº”è¯¥è¢«æ’é™¤"
-    print("âœ“ Excludeæµ‹è¯•é€šè¿‡")
-    
-    # æµ‹è¯•include
-    trimmed_obj = trim_object_for_ray(obj, include=['name', 'items'])
-    assert hasattr(trimmed_obj, 'name'), "nameåº”è¯¥åŒ…å«"
-    assert hasattr(trimmed_obj, 'items'), "itemsåº”è¯¥åŒ…å«"
-    assert not hasattr(trimmed_obj, 'data'), "dataä¸åœ¨includeä¸­åº”è¯¥è¢«æ’é™¤"
-    print("âœ“ Includeæµ‹è¯•é€šè¿‡")
-
-
-def test_ray_object_trimmer():
-    """æµ‹è¯•RayObjectTrimmerç±»çš„åŠŸèƒ½"""
-    print("\n=== æµ‹è¯•RayObjectTrimmerç±» ===")
-    
-    obj = ProblematicClass("trimmer_test")
-    
-    # æµ‹è¯•æµ…å±‚æ¸…ç†
-    shallow_cleaned = RayObjectTrimmer.trim_for_remote_call(obj, deep_clean=False)
-    print(f"æµ…å±‚æ¸…ç†åå±æ€§: {list(shallow_cleaned.__dict__.keys())}")
-    
-    # æµ‹è¯•æ·±åº¦æ¸…ç†
-    deep_cleaned = RayObjectTrimmer.trim_for_remote_call(obj, deep_clean=True)
-    print(f"æ·±åº¦æ¸…ç†åå±æ€§: {list(deep_cleaned.__dict__.keys())}")
-    
-    # éªŒè¯æ¸…ç†æ•ˆæœ
-    assert not hasattr(deep_cleaned, 'logger'), "loggeråº”è¯¥è¢«ç§»é™¤"
-    assert hasattr(deep_cleaned, 'name'), "nameåº”è¯¥ä¿ç•™"
-    
-    print("âœ“ RayObjectTrimmeræµ‹è¯•é€šè¿‡")
-
-
-def test_nested_object_trimming():
-    """æµ‹è¯•åµŒå¥—å¯¹è±¡çš„æ¸…ç†"""
-    print("\n=== æµ‹è¯•åµŒå¥—å¯¹è±¡æ¸…ç† ===")
-    
-    nested_obj = NestedProblematicClass()
-    
-    print("åŸå§‹åµŒå¥—å¯¹è±¡ç»“æ„:")
-    print(f"  - é¡¶å±‚å±æ€§: {list(nested_obj.__dict__.keys())}")
-    if hasattr(nested_obj, 'problematic_child'):
-        print(f"  - å­å¯¹è±¡å±æ€§: {list(nested_obj.problematic_child.__dict__.keys())}")
-    
-    # æ‰§è¡ŒåµŒå¥—æ¸…ç†
-    trimmed_nested = trim_object_for_ray(nested_obj)
-    
-    print("æ¸…ç†ååµŒå¥—å¯¹è±¡ç»“æ„:")
-    print(f"  - é¡¶å±‚å±æ€§: {list(trimmed_nested.__dict__.keys())}")
-    if hasattr(trimmed_nested, 'problematic_child'):
-        print(f"  - å­å¯¹è±¡å±æ€§: {list(trimmed_nested.problematic_child.__dict__.keys())}")
-    
-    # éªŒè¯åµŒå¥—æ¸…ç†
-    assert hasattr(trimmed_nested, 'name'), "é¡¶å±‚nameåº”è¯¥ä¿ç•™"
-    assert hasattr(trimmed_nested, 'safe_data'), "safe_dataåº”è¯¥ä¿ç•™"
-    assert not hasattr(trimmed_nested, 'thread_list'), "thread_liståº”è¯¥è¢«ç§»é™¤"
-    
-    # éªŒè¯å­å¯¹è±¡ä¹Ÿè¢«æ¸…ç†
-    if hasattr(trimmed_nested, 'problematic_child'):
-        child = trimmed_nested.problematic_child
-        assert hasattr(child, 'name'), "å­å¯¹è±¡nameåº”è¯¥ä¿ç•™"
-        assert not hasattr(child, 'logger'), "å­å¯¹è±¡loggeråº”è¯¥è¢«ç§»é™¤"
-    
-    print("âœ“ åµŒå¥—å¯¹è±¡æ¸…ç†æµ‹è¯•é€šè¿‡")
-
-
-def test_ray_serialization_validation():
-    """æµ‹è¯•Rayåºåˆ—åŒ–éªŒè¯åŠŸèƒ½"""
-    print("\n=== æµ‹è¯•Rayåºåˆ—åŒ–éªŒè¯ ===")
-    
-    if not RAY_AVAILABLE:
-        print("âš  Rayä¸å¯ç”¨ï¼Œè·³è¿‡åºåˆ—åŒ–éªŒè¯æµ‹è¯•")
-        return
-    
-    obj = ProblematicClass("validation_test")
-    
-    # éªŒè¯åŸå§‹å¯¹è±¡
-    print("éªŒè¯åŸå§‹å¯¹è±¡...")
-    original_result = RayObjectTrimmer.validate_ray_serializable(obj)
-    print(f"åŸå§‹å¯¹è±¡å¯åºåˆ—åŒ–: {original_result['is_serializable']}")
-    if original_result['issues']:
-        print("é—®é¢˜åˆ—è¡¨:")
-        for issue in original_result['issues']:
-            print(f"  - {issue}")
-    
-    # éªŒè¯æ¸…ç†åçš„å¯¹è±¡
-    print("éªŒè¯æ¸…ç†åå¯¹è±¡...")
-    trimmed_obj = trim_object_for_ray(obj)
-    trimmed_result = RayObjectTrimmer.validate_ray_serializable(trimmed_obj)
-    print(f"æ¸…ç†åå¯¹è±¡å¯åºåˆ—åŒ–: {trimmed_result['is_serializable']}")
-    if trimmed_result['issues']:
-        print("é—®é¢˜åˆ—è¡¨:")
-        for issue in trimmed_result['issues']:
-            print(f"  - {issue}")
-    else:
-        print(f"åºåˆ—åŒ–å¤§å°ä¼°è®¡: {trimmed_result['size_estimate']} å­—èŠ‚")
-    
-    # éªŒè¯æ”¹è¿›æ•ˆæœ
-    if not original_result['is_serializable'] and trimmed_result['is_serializable']:
-        print("âœ“ æ¸…ç†æˆåŠŸè§£å†³äº†åºåˆ—åŒ–é—®é¢˜")
-    elif original_result['is_serializable'] and trimmed_result['is_serializable']:
-        print("âœ“ å¯¹è±¡æ¸…ç†åä»ç„¶å¯åºåˆ—åŒ–")
-    else:
-        print("âš  æ¸…ç†å¯èƒ½æ²¡æœ‰å®Œå…¨è§£å†³åºåˆ—åŒ–é—®é¢˜")
-
-
-def test_ray_remote_call_simulation():
-    """æ¨¡æ‹ŸRayè¿œç¨‹è°ƒç”¨æµ‹è¯•"""
-    print("\n=== æ¨¡æ‹ŸRayè¿œç¨‹è°ƒç”¨æµ‹è¯• ===")
-    
-    if not RAY_AVAILABLE:
-        print("âš  Rayä¸å¯ç”¨ï¼Œè·³è¿‡è¿œç¨‹è°ƒç”¨æµ‹è¯•")
-        return
-    
-    # åˆå§‹åŒ–Ray
-    if not ray.is_initialized():
-        ray.init(address="auto", _temp_dir="/var/lib/ray_shared")
-    
-    @ray.remote
-    class TestActor:
-        def process_object(self, obj):
-            """å¤„ç†æ¥æ”¶åˆ°çš„å¯¹è±¡"""
-            return {
-                "received_type": type(obj).__name__,
-                "attributes": list(obj.__dict__.keys()) if hasattr(obj, '__dict__') else [],
-                "name": getattr(obj, 'name', 'unknown'),
-                "data": getattr(obj, 'data', None)
-            }
-    
-    # åˆ›å»ºæµ‹è¯•å¯¹è±¡å’ŒActor
-    obj = ProblematicClass("ray_test")
-    actor = TestActor.remote()
-    
-    # æµ‹è¯•åŸå§‹å¯¹è±¡ï¼ˆå¯èƒ½å¤±è´¥ï¼‰
-    print("æµ‹è¯•åŸå§‹å¯¹è±¡ä¼ è¾“...")
-    try:
-        original_result = ray.get(actor.process_object.remote(obj))
-        print(f"åŸå§‹å¯¹è±¡ä¼ è¾“æˆåŠŸ: {original_result}")
-    except Exception as e:
-        print(f"åŸå§‹å¯¹è±¡ä¼ è¾“å¤±è´¥: {e}")
-    
-    # æµ‹è¯•æ¸…ç†åçš„å¯¹è±¡
-    print("æµ‹è¯•æ¸…ç†åå¯¹è±¡ä¼ è¾“...")
-    try:
-        trimmed_obj = trim_object_for_ray(obj)
-        trimmed_result = ray.get(actor.process_object.remote(trimmed_obj))
-        print(f"æ¸…ç†åå¯¹è±¡ä¼ è¾“æˆåŠŸ: {trimmed_result}")
-        print("âœ“ Rayè¿œç¨‹è°ƒç”¨æµ‹è¯•é€šè¿‡")
-    except Exception as e:
-        print(f"æ¸…ç†åå¯¹è±¡ä¼ è¾“å¤±è´¥: {e}")
-        print("âœ— Rayè¿œç¨‹è°ƒç”¨æµ‹è¯•å¤±è´¥")
-    
-    # æ¸…ç†Ray
-    try:
-        ray.shutdown()
-    except:
-        pass
-
-
-def test_performance_comparison():
-    """æµ‹è¯•æ€§èƒ½å¯¹æ¯”"""
-    print("\n=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
-    
-    # åˆ›å»ºå¤§é‡æµ‹è¯•å¯¹è±¡
-    objects = [ProblematicClass(f"perf_test_{i}") for i in range(100)]
-    
-    # æµ‹è¯•trimæ€§èƒ½
-    start_time = time.time()
-    trimmed_objects = []
-    for obj in objects:
-        try:
-            trimmed = trim_object_for_ray(obj)
-            trimmed_objects.append(trimmed)
-        except Exception as e:
-            print(f"Trimå¤±è´¥: {e}")
-    
-    trim_time = time.time() - start_time
-    print(f"æ¸…ç†100ä¸ªå¯¹è±¡è€—æ—¶: {trim_time:.4f}ç§’")
-    print(f"å¹³å‡æ¯ä¸ªå¯¹è±¡: {trim_time/100:.4f}ç§’")
-    print(f"æˆåŠŸæ¸…ç†å¯¹è±¡æ•°é‡: {len(trimmed_objects)}")
-    
-    if RAY_AVAILABLE:
-        # æµ‹è¯•Rayåºåˆ—åŒ–æ€§èƒ½
-        import ray.cloudpickle as cloudpickle
-        
-        start_time = time.time()
-        serialized_count = 0
-        for obj in trimmed_objects[:10]:  # åªæµ‹è¯•å‰10ä¸ª
-            try:
-                cloudpickle.dumps(obj)
-                serialized_count += 1
-            except:
-                pass
-        
-        serialize_time = time.time() - start_time
-        print(f"Rayåºåˆ—åŒ–10ä¸ªæ¸…ç†å¯¹è±¡è€—æ—¶: {serialize_time:.4f}ç§’")
-        print(f"æˆåŠŸåºåˆ—åŒ–å¯¹è±¡æ•°é‡: {serialized_count}")
-
-
-def run_all_tests():
-    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
-    print("å¼€å§‹Rayå¯¹è±¡é¢„å¤„ç†å™¨æµ‹è¯•...")
-    print("=" * 50)
-    
-    try:
-        # åŸºæœ¬æµ‹è¯•
-        test_basic_trimming()
-        test_custom_include_exclude()
-        test_ray_object_trimmer()
-        test_nested_object_trimming()
-        
-        # Rayç›¸å…³æµ‹è¯•
-        test_ray_serialization_validation()
-        test_ray_remote_call_simulation()
-        
-        # æ€§èƒ½æµ‹è¯•
-        test_performance_comparison()
-        
-        print("\n" + "=" * 50)
-        print("âœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
-        
-    except Exception as e:
-        print(f"\nâœ— æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
-        import traceback
-        traceback.print_exc()
-        return False
-    
-    finally:
-        # æ¸…ç†èµ„æº
-        import gc
-        gc.collect()
-        
-    return True
-
-
-if __name__ == "__main__":
-    success = run_all_tests()
-    
-    if success:
-        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼trim_object_for_rayå‡½æ•°å·¥ä½œæ­£å¸¸")
-    else:
-        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
-        sys.exit(1)
diff --git a/sage_tests/utils_tests/test_serializer.py b/sage_tests/utils_tests/test_serializer.py
deleted file mode 100644
index 58ad9a8..0000000
--- a/sage_tests/utils_tests/test_serializer.py
+++ /dev/null
@@ -1,325 +0,0 @@
-import logging
-import pytest
-from dotenv import load_dotenv
-import os
-import tempfile
-
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_libs.io.sink import TerminalSink
-from sage_libs.io.source import FileSource
-from sage_libs.rag.generator import OpenAIGenerator
-from sage_libs.rag.promptor import QAPromptor
-from sage_libs.rag.retriever import DenseRetriever
-from sage_utils.config_loader import load_config
-from sage_utils.logging_utils import configure_logging
-from sage_utils.serialization.dill_serializer import (
-    serialize_object, deserialize_object, pack_object, unpack_object,
-    save_object_state, load_object_state
-)
-
-
-@pytest.fixture(scope="function")
-def config():
-    configure_logging(level=logging.INFO)
-    load_dotenv(override=False)
-    cfg = load_config("config_mixed.yaml")
-    api_key = os.environ.get("VLLM_API_KEY")
-    if api_key:
-        cfg.setdefault("generator", {})["api_key"] = api_key
-    return cfg
-
-
-@pytest.fixture(scope="function")
-def env():
-    env = LocalStreamEnvironment()
-    env.set_memory(config=None)
-    # return env
-    yield env
-    # teardown: ä¸»åŠ¨æ¸…ç†èµ„æº
-    try:
-        if hasattr(env, "executor"):
-            env.executor.shutdown(wait=False)
-        if hasattr(env, "actors"):
-            for a in env.actors:
-                a.kill()
-    except Exception as e:
-        logging.warning(f"env teardown failed: {e}")
-
-
-def test_env_serialization_and_reconstruction(env, config):
-    """æµ‹è¯•ç¯å¢ƒçš„åºåˆ—åŒ–å’Œé‡å»º"""
-    
-    # 1. åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„pipeline
-    query_stream = (env
-        .from_source(FileSource, config["source"])
-        .map(DenseRetriever, config["retriever"])
-        .map(QAPromptor, config["promptor"])
-        .map(OpenAIGenerator, config["generator"])
-        .sink(TerminalSink, config["sink"])
-    )
-    
-    # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥pipelineæ˜¯å¦æ­£ç¡®æ„å»º
-    logging.info(f"Original pipeline length: {len(env._pipeline)}")
-    for i, trans in enumerate(env._pipeline):
-        logging.info(f"Transformation {i}: {type(trans).__name__} - {trans.basename}")
-    
-    # æ·»åŠ ä¸€äº›è‡ªå®šä¹‰å±æ€§æ¥æµ‹è¯•åºåˆ—åŒ–
-    env.custom_metadata = {
-        "created_at": "2025-01-01",
-        "version": "1.0.0",
-        "description": "Test environment for serialization"
-    }
-    env.processing_stats = {
-        "total_processed": 0,
-        "errors": 0,
-        # "last_run": None
-    }
-    
-    # 2. åºåˆ—åŒ–ç¯å¢ƒ
-    logging.info("Serializing environment...")
-    serialized_data = serialize_object(env)
-    
-    # # éªŒè¯åºåˆ—åŒ–æ•°æ®åŒ…å«å¿…è¦ä¿¡æ¯
-    # assert '__class_path__' in serialized_data
-    # assert '__attributes__' in serialized_data
-    # assert serialized_data['__class_path__'] == 'sage_core.api.env.LocalEnvironment'
-    
-    # attributes = serialized_data['__attributes__']
-    # assert 'name' in attributes
-    # assert 'config' in attributes
-    # assert 'platform' in attributes
-    # assert '_pipeline' in attributes
-    # assert 'custom_metadata' in attributes
-    # assert 'processing_stats' in attributes
-    
-    # # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥åºåˆ—åŒ–åçš„pipeline
-    # pipeline_data = attributes['_pipeline']
-    # logging.info(f"Serialized pipeline length: {len(pipeline_data)}")
-    # for i, trans_data in enumerate(pipeline_data):
-    #     if isinstance(trans_data, dict) and '__class_path__' in trans_data:
-    #         logging.info(f"Serialized transformation {i}: {trans_data['__class_path__']}")
-    
-    # # éªŒè¯pipelineä¸­çš„transformationsä¹Ÿè¢«åºåˆ—åŒ–
-    # assert len(pipeline_data) == 5, f"Expected 5 transformations, got {len(pipeline_data)}"
-    
-    # logging.info(f"Environment serialized successfully with {len(attributes)} attributes")
-    # logging.info(f"Pipeline contains {len(pipeline_data)} transformations")
-    
-    # 3. é‡å»ºç¯å¢ƒ
-    logging.info("Reconstructing environment from serialized data...")
-    restored_env = deserialize_object(serialized_data)
-    print(f"Restored environment name: {restored_env.name}")
-    # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ¢å¤åçš„pipeline
-    logging.info(f"Restored pipeline length: {len(restored_env._pipeline)}")
-    for i, trans in enumerate(restored_env._pipeline):
-        logging.info(f"Restored transformation {i}: {type(trans).__name__} - {trans.basename}")
-    
-    # éªŒè¯ç¯å¢ƒåŸºæœ¬ä¿¡æ¯
-    assert restored_env.name == "local_environment"
-    assert restored_env.platform == env.platform
-    assert restored_env.config == env.config
-    assert restored_env.custom_metadata == env.custom_metadata
-    print( restored_env.processing_stats)
-    assert restored_env.processing_stats == env.processing_stats
-    
-    # éªŒè¯pipelineè¢«æ­£ç¡®é‡å»º
-    assert len(restored_env._pipeline) == len(env._pipeline)
-    
-    # éªŒè¯æ¯ä¸ªtransformationçš„åŸºæœ¬ä¿¡æ¯
-    for i, (original, restored) in enumerate(zip(env._pipeline, restored_env._pipeline)):
-        assert type(original) == type(restored)
-        assert original.basename == restored.basename
-        assert original.parallelism == restored.parallelism
-        assert original.function_class == restored.function_class
-        logging.info(f"Transformation {i}: {original.basename} restored successfully")
-    
-    # ... å…¶ä½™æµ‹è¯•ä»£ç ä¿æŒä¸å˜
-
-
-def test_env_binary_serialization(env, config):
-    """æµ‹è¯•ç¯å¢ƒçš„äºŒè¿›åˆ¶åºåˆ—åŒ–å’Œé‡å»º"""
-    
-    # åˆ›å»ºpipeline
-    query_stream = (env
-        .from_source(FileSource, config["source"])
-        .map(DenseRetriever, config["retriever"])
-        .print()
-    )
-    
-    # æ·»åŠ æµ‹è¯•æ•°æ®
-    env.test_data = {
-        "numbers": [1, 2, 3, 4, 5],
-        "settings": {"batch_size": 100, "timeout": 30}
-    }
-    
-    # äºŒè¿›åˆ¶åºåˆ—åŒ–
-    logging.info("Binary serializing environment...")
-    packed_data = pack_object(env)
-    
-    assert isinstance(packed_data, bytes)
-    assert len(packed_data) > 0
-    
-    logging.info(f"Environment packed to {len(packed_data)} bytes")
-    
-    # äºŒè¿›åˆ¶ååºåˆ—åŒ–
-    logging.info("Unpacking environment from binary data...")
-    restored_env = unpack_object(packed_data)
-    
-    # éªŒè¯é‡å»ºç»“æœ
-    assert restored_env.test_data == env.test_data
-    assert len(restored_env._pipeline) == len(env._pipeline)
-    
-    # æµ‹è¯•è¿è¡Œ
-    restored_env.set_memory(config=None)
-    restored_env.submit()
-    
-    try:
-        restored_env.run_once()
-        logging.info("Binary restored environment executed successfully")
-        restored_env.stop()
-    except Exception as e:
-        pytest.fail(f"Binary restored environment execution failed: {e}")
-    
-    restored_env.close()
-
-
-def test_env_file_persistence(env, config):
-    """æµ‹è¯•ç¯å¢ƒçš„æ–‡ä»¶æŒä¹…åŒ–"""
-    
-    # åˆ›å»ºç®€å•çš„pipeline
-    query_stream = (env
-        .from_source(FileSource, config["source"])
-        .map(QAPromptor, config["promptor"])
-        .print()
-    )
-    
-    # æ·»åŠ æŒä¹…åŒ–æ•°æ®
-    env.persistent_data = {
-        "session_id": "test_session_123",
-        "user_preferences": {"theme": "dark", "language": "en"},
-        "cache_settings": {"enabled": True, "size": 1000}
-    }
-    
-    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
-    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as tmp_file:
-        tmp_path = tmp_file.name
-    
-    try:
-        logging.info(f"Saving environment state to {tmp_path}")
-        save_object_state(env, tmp_path)
-        
-        # éªŒè¯æ–‡ä»¶å­˜åœ¨
-        assert os.path.exists(tmp_path)
-        assert os.path.getsize(tmp_path) > 0
-        
-        # åˆ›å»ºæ–°ç¯å¢ƒå¹¶åŠ è½½çŠ¶æ€
-        new_env = LocalStreamEnvironment("new_env")
-        new_env.set_memory(config=None)
-        
-        logging.info(f"Loading environment state from {tmp_path}")
-        load_success = load_object_state(new_env, tmp_path)
-        
-        assert load_success == True
-        assert new_env.persistent_data == env.persistent_data
-        assert len(new_env.pipeline) == len(env._pipeline)
-        
-        # æµ‹è¯•åŠ è½½çŠ¶æ€åçš„è¿è¡Œ
-        new_env.submit()
-        
-        try:
-            new_env.run_once()
-            logging.info("Environment with loaded state executed successfully")
-            new_env.stop()
-        except Exception as e:
-            pytest.fail(f"Environment with loaded state execution failed: {e}")
-        
-        new_env.close()
-        
-    finally:
-        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
-        if os.path.exists(tmp_path):
-            os.unlink(tmp_path)
-
-# åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ç¯å¢ƒç±»ï¼Œå¸¦æœ‰åºåˆ—åŒ–é…ç½®
-class CustomTestEnvironment(LocalStreamEnvironment):
-    # __state_include__ = ['name', 'config', 'platform', '_pipeline', 'important_data']
-    
-    def __init__(self, name: str = "custom_test_env", config: dict = None):
-        super().__init__(name, config)
-        self.important_data = "must be serialized"
-        self.temp_data = "should be ignored"
-        self.secret_key = "confidential"
-
-
-def test_env_serialization_with_custom_attributes(env, config):
-    """æµ‹è¯•å¸¦æœ‰è‡ªå®šä¹‰åºåˆ—åŒ–é…ç½®çš„ç¯å¢ƒ"""
-    
-
-    
-    # åˆ›å»ºè‡ªå®šä¹‰ç¯å¢ƒ
-    custom_env = CustomTestEnvironment("custom_test")
-    custom_env.set_memory(config=None)
-    
-    # åˆ›å»ºpipeline
-    query_stream = (custom_env
-        .from_source(FileSource, config["source"])
-        .print()
-    )
-    
-    # åºåˆ—åŒ–
-    serialized_data = serialize_object(custom_env)
-    # attributes = serialized_data['__attributes__']
-    
-    # # éªŒè¯åªæœ‰includeçš„å±æ€§è¢«åºåˆ—åŒ–
-    # assert 'important_data' in attributes
-    # assert 'temp_data' not in attributes
-    # assert 'secret_key' not in attributes
-    
-    # é‡å»º
-    restored_env = deserialize_object(serialized_data)
-    
-    assert restored_env.important_data == "must be serialized"
-    # assert not hasattr(restored_env, 'temp_data')
-    # assert not hasattr(restored_env, 'secret_key')
-    assert hasattr(restored_env, 'is_running')
-    
-    # æµ‹è¯•è¿è¡Œ
-    restored_env.set_memory(config=None)
-    restored_env.submit()
-    
-    try:
-        restored_env.run_once()
-        logging.info("Custom environment executed successfully")
-        restored_env.stop()
-    except Exception as e:
-        pytest.fail(f"Custom environment execution failed: {e}")
-    
-    restored_env.close()
-    custom_env.close()
-
-
-def test_env_serialization_error_handling():
-    """æµ‹è¯•åºåˆ—åŒ–è¿‡ç¨‹ä¸­çš„é”™è¯¯å¤„ç†"""
-    
-    # æµ‹è¯•æ— æ•ˆæ•°æ®çš„ååºåˆ—åŒ–
-    invalid_data = {'invalid': 'data'}
-    
-    with pytest.raises(Exception):
-        deserialize_object(invalid_data)
-    
-    # æµ‹è¯•ä¸å­˜åœ¨çš„ç±»è·¯å¾„
-    invalid_class_data = {
-        '__class_path__': 'non.existent.Class',
-        '__attributes__': {}
-    }
-    
-    with pytest.raises(Exception):
-        deserialize_object(invalid_class_data)
-    
-    logging.info("Error handling tests completed successfully")
-
-
-if __name__ == "__main__":
-    # test_env_serialization_with_custom_attributes(env(), config())
-    # è¿è¡Œæµ‹è¯•
-    pytest.main([__file__, "-v"])
\ No newline at end of file
diff --git a/sage_utils/actor_wrapper.py b/sage_utils/actor_wrapper.py
deleted file mode 100644
index 2b96691..0000000
--- a/sage_utils/actor_wrapper.py
+++ /dev/null
@@ -1,73 +0,0 @@
-import ray
-import asyncio
-import concurrent.futures
-from typing import Any, Union
-import logging
-from ray.actor import ActorHandle
-
-class ActorWrapper:
-    """ä¸‡èƒ½åŒ…è£…å™¨ï¼Œå¯ä»¥å°†ä»»æ„å¯¹è±¡åŒ…è£…æˆæœ¬åœ°å¯¹è±¡æˆ–Ray Actor"""
-    
-    def __init__(self, 
-                 obj: Union[Any, ActorHandle]):
-        # ä½¿ç”¨ __dict__ ç›´æ¥è®¾ç½®ï¼Œé¿å…è§¦å‘ __setattr__
-        object.__setattr__(self, '_obj', obj)
-        object.__setattr__(self, '_execution_mode', self._detect_execution_mode())
-    
-    def _detect_execution_mode(self) -> str:
-        """æ£€æµ‹æ‰§è¡Œæ¨¡å¼"""
-        try:
-            if isinstance(self._obj, ray.actor.ActorHandle):
-                return "ray_actor"
-        except (ImportError, AttributeError):
-            pass
-        return "local"
-    
-    def __getattr__(self, name: str):
-        """é€æ˜ä»£ç†å±æ€§è®¿é—®"""
-        if name.startswith('_'):
-            raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
-        
-        # è·å–åŸå§‹å±æ€§/æ–¹æ³•
-        try:
-            original_attr = getattr(self._obj, name)
-        except AttributeError:
-            raise AttributeError(f"'{type(self._obj).__name__}' object has no attribute '{name}'")
-        
-        # å¦‚æœæ˜¯æ–¹æ³•ï¼Œéœ€è¦åŒ…è£…
-        if callable(original_attr):
-            if self._execution_mode == "ray_actor":
-                # Ray Actoræ–¹æ³•ï¼šè¿”å›åŒæ­¥è°ƒç”¨åŒ…è£…å™¨
-                def ray_method_wrapper(*args, **kwargs):
-                    future = original_attr.remote(*args, **kwargs)
-                    result = ray.get(future)
-                    return result
-                return ray_method_wrapper
-            else:
-                # æœ¬åœ°æ–¹æ³•ï¼šç›´æ¥è¿”å›
-                return original_attr
-        else:
-            # æ™®é€šå±æ€§ï¼šç›´æ¥è¿”å›
-            return original_attr
-    
-    def __setattr__(self, name: str, value: Any):
-        """ä»£ç†å±æ€§è®¾ç½®"""
-        if name.startswith('_'):
-            object.__setattr__(self, name, value)
-        else:
-            setattr(self._obj, name, value)
-    
-    def __repr__(self):
-        return f"ActorWrapper[{self._execution_mode}]({repr(self._obj)})"
-    
-    def get_wrapped_object(self):
-        """è·å–è¢«åŒ…è£…çš„åŸå§‹å¯¹è±¡"""
-        return self._obj
-    
-    def is_ray_actor(self) -> bool:
-        """æ£€æŸ¥æ˜¯å¦ä¸ºRay Actor"""
-        return self._execution_mode == "ray_actor"
-    
-    def is_local(self) -> bool:
-        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°å¯¹è±¡"""
-        return self._execution_mode == "local"
\ No newline at end of file
diff --git a/sage_utils/clients/__init__.py b/sage_utils/clients/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/sage_utils/custom_formatter.py b/sage_utils/custom_formatter.py
deleted file mode 100644
index d9dbe2e..0000000
--- a/sage_utils/custom_formatter.py
+++ /dev/null
@@ -1,64 +0,0 @@
-import logging
-import os
-import sys
-from datetime import datetime
-from pathlib import Path
-from typing import Union, Optional
-import threading
-import inspect
-
-
-
-class CustomFormatter(logging.Formatter):
-    """
-    è‡ªå®šä¹‰æ ¼å¼åŒ–å™¨ï¼Œåˆå¹¶IDEæ ¼å¼å’Œä¸¤è¡Œæ ¼å¼ï¼š
-    ç¬¬ä¸€è¡Œï¼šæ—¶é—´ | çº§åˆ« | å¯¹è±¡å | æ–‡ä»¶è·¯å¾„:è¡Œå·
-    ç¬¬äºŒè¡Œï¼š	â†’ æ—¥å¿—æ¶ˆæ¯
-    ç¬¬ä¸‰è¡Œï¼š ç•™ç©º
-    """
-    COLOR_RESET = "\033[0m"
-    COLOR_DEBUG = "\033[36m"  # é’è‰²
-    COLOR_INFO = "\033[32m"   # ç»¿è‰²
-    COLOR_WARNING = "\033[33m" # é»„è‰²
-    COLOR_ERROR = "\033[31m"  # çº¢è‰²
-    COLOR_CRITICAL = "\033[35m" # ç´«è‰²
-
-
-    def format(self, record):
-
-        if record.levelno == logging.DEBUG:
-            color = self.COLOR_DEBUG
-        elif record.levelno == logging.INFO:
-            color = self.COLOR_INFO
-        elif record.levelno == logging.WARNING:
-            color = self.COLOR_WARNING
-        elif record.levelno == logging.ERROR:
-            color = self.COLOR_ERROR
-        elif record.levelno == logging.CRITICAL:
-            color = self.COLOR_CRITICAL
-        else:
-            color = self.COLOR_RESET
-
-        # ç¬¬ä¸€è¡Œï¼šæ—¶é—´ | çº§åˆ« | å¯¹è±¡å | æ–‡ä»¶è·¯å¾„:è¡Œå·
-        timestamp = self.formatTime(record, '%Y-%m-%d %H:%M:%S')
-        level = record.levelname
-        name = record.name
-        pathname = record.pathname
-        lineno = record.lineno
-
-        # ç¬¬äºŒè¡Œï¼šâ†’ æ¶ˆæ¯å†…å®¹
-        message = record.getMessage()
-
-        # å¦‚æœæœ‰å¼‚å¸¸ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯åé¢
-        if record.exc_info:
-            if not record.exc_text:
-                record.exc_text = self.formatException(record.exc_info)
-        if record.exc_text:
-            message = message + '\n' + record.exc_text
-        if record.stack_info:
-            message = message + '\n' + self.formatStack(record.stack_info)
-
-        # ç»„åˆæ ¼å¼ï¼šæ—¢ç¾è§‚åˆæ”¯æŒIDEç‚¹å‡»
-        formatted_message = f"{timestamp} | {level:<5} | {name} | {pathname}:{lineno} â†’\n\t {color}{message}{self.COLOR_RESET}\n"
-
-        return formatted_message
\ No newline at end of file
diff --git a/sage_utils/custom_logger.py b/sage_utils/custom_logger.py
index 7f2c7fa..0951605 100644
--- a/sage_utils/custom_logger.py
+++ b/sage_utils/custom_logger.py
@@ -3,27 +3,81 @@ import os
 import sys
 from datetime import datetime
 from pathlib import Path
-from typing import List, Union, Optional, Tuple
+from typing import Union, Optional
 import threading
 import inspect
-from .custom_formatter import CustomFormatter  # å‡è®¾æœ‰ä¸€ä¸ªè‡ªå®šä¹‰æ ¼å¼åŒ–å™¨
+from .name_server import get_name
 
+class CustomFormatter(logging.Formatter):
+    """
+    è‡ªå®šä¹‰æ ¼å¼åŒ–å™¨ï¼Œåˆå¹¶IDEæ ¼å¼å’Œä¸¤è¡Œæ ¼å¼ï¼š
+    ç¬¬ä¸€è¡Œï¼šæ—¶é—´ | çº§åˆ« | å¯¹è±¡å | æ–‡ä»¶è·¯å¾„:è¡Œå·
+    ç¬¬äºŒè¡Œï¼š	â†’ æ—¥å¿—æ¶ˆæ¯
+    ç¬¬ä¸‰è¡Œï¼š ç•™ç©º
+    """
+    COLOR_RESET = "\033[0m"
+    COLOR_DEBUG = "\033[36m"  # é’è‰²
+    COLOR_INFO = "\033[32m"   # ç»¿è‰²
+    COLOR_WARNING = "\033[33m" # é»„è‰²
+    COLOR_ERROR = "\033[31m"  # çº¢è‰²
+    COLOR_CRITICAL = "\033[35m" # ç´«è‰²
+
+
+    def format(self, record):
+
+        if record.levelno == logging.DEBUG:
+            color = self.COLOR_DEBUG
+        elif record.levelno == logging.INFO:
+            color = self.COLOR_INFO
+        elif record.levelno == logging.WARNING:
+            color = self.COLOR_WARNING
+        elif record.levelno == logging.ERROR:
+            color = self.COLOR_ERROR
+        elif record.levelno == logging.CRITICAL:
+            color = self.COLOR_CRITICAL
+        else:
+            color = self.COLOR_RESET
+
+        # ç¬¬ä¸€è¡Œï¼šæ—¶é—´ | çº§åˆ« | å¯¹è±¡å | æ–‡ä»¶è·¯å¾„:è¡Œå·
+        timestamp = self.formatTime(record, '%Y-%m-%d %H:%M:%S')
+        level = record.levelname
+        name = record.name
+        pathname = record.pathname
+        lineno = record.lineno
+
+        # ç¬¬äºŒè¡Œï¼šâ†’ æ¶ˆæ¯å†…å®¹
+        message = record.getMessage()
 
+        # å¦‚æœæœ‰å¼‚å¸¸ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯åé¢
+        if record.exc_info:
+            if not record.exc_text:
+                record.exc_text = self.formatException(record.exc_info)
+        if record.exc_text:
+            message = message + '\n' + record.exc_text
+        if record.stack_info:
+            message = message + '\n' + self.formatStack(record.stack_info)
+
+        # ç»„åˆæ ¼å¼ï¼šæ—¢ç¾è§‚åˆæ”¯æŒIDEç‚¹å‡»
+        formatted_message = f"{timestamp} | {level:<5} | {name} | {pathname}:{lineno} â†’\n\t {color}{message}{self.COLOR_RESET}\n"
+
+        return formatted_message
 
 
 
 class CustomLogger:
     """
     ç®€åŒ–çš„è‡ªå®šä¹‰Loggerç±»
-    æ”¯æŒå¤šç§è¾“å‡ºç›®æ ‡é…ç½®ï¼š
-    - "console": æ§åˆ¶å°è¾“å‡º
-    - ç›¸å¯¹è·¯å¾„: ç›¸å¯¹äºlog_base_folderçš„è·¯å¾„
-    - ç»å¯¹è·¯å¾„: å®Œæ•´è·¯å¾„çš„æ–‡ä»¶è¾“å‡º
+    æ¯ä¸ªLoggeräº§ç”Ÿä¸‰ä»½è¾“å‡ºï¼š
+    1. å¯¹è±¡ä¸“ç”¨æ–‡ä»¶ï¼š{object_name}.log
+    2. å…¨å±€æ—¶é—´é¡ºåºæ–‡ä»¶ï¼šall_logs.log
+    3. æ§åˆ¶å°è¾“å‡º
+    æ”¯æŒåŠ¨æ€è°ƒæ•´å„è¾“å‡ºæ¸ é“çš„æ—¥å¿—ç­‰çº§
     """
+    # ç±»çº§åˆ«çš„é»˜è®¤sessionç®¡ç†
+    _default_session_folder: Optional[str] = None
+    _lock = threading.Lock()
     # å…¨å±€console debugå¼€å…³
     _global_console_debug_enabled: bool = True
-    _lock = threading.Lock()
-    
     # æ—¥å¿—çº§åˆ«æ˜ å°„
     _LEVEL_MAPPING = {
         'DEBUG': logging.DEBUG,
@@ -36,296 +90,319 @@ class CustomLogger:
     }
 
     def __init__(self,
-                 outputs: List[Tuple[str, Union[str, int]]] = [("console", "INFO")],
-                 name: str = None,
-                 log_base_folder: str = None):
+                 filename: str,
+                 session_folder: str = None,
+                 env_name: Optional[str] = None,
+                 console_output: Union[bool, str, int] = False,
+                 file_output: Union[bool, str, int] = True,
+                 global_output: Union[bool, str, int] = True,
+                 name: str = None):
         """
         åˆå§‹åŒ–è‡ªå®šä¹‰Logger
         
         Args:
-            outputs: è¾“å‡ºé…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (output_target, level) å…ƒç»„
-                    - output_target å¯ä»¥æ˜¯:
-                      - "console": æ§åˆ¶å°è¾“å‡º
-                      - ç›¸å¯¹è·¯å¾„: ç›¸å¯¹äºlog_base_folderçš„è·¯å¾„ï¼Œå¦‚ "app.log", "logs/error.log"
-                      - ç»å¯¹è·¯å¾„: å®Œæ•´è·¯å¾„ï¼Œå¦‚ "/tmp/app.log"
-                    - level å¯ä»¥æ˜¯å­—ç¬¦ä¸²("DEBUG", "INFO"ç­‰) æˆ–æ•°å­—
-            name: loggeråç§°ï¼Œé»˜è®¤ä½¿ç”¨ "Logger"
-            log_base_folder: æ—¥å¿—åŸºç¡€æ–‡ä»¶å¤¹ï¼Œç”¨äºè§£æç›¸å¯¹è·¯å¾„
-        
-        Examples:
-            # JobManagerç¤ºä¾‹
-            logger = CustomLogger([
-                ("console", "INFO"),
-                ("jobmanager.log", "DEBUG"),           # ç›¸å¯¹è·¯å¾„
-                ("error.log", "ERROR"),               # ç›¸å¯¹è·¯å¾„
-            ], name="JobManager", log_base_folder="/tmp/sage/logs")
-            
-            # æ··åˆè·¯å¾„ç¤ºä¾‹
-            logger = CustomLogger([
-                ("console", "INFO"),
-                ("app.log", "DEBUG"),                 # ç›¸å¯¹äºlog_base_folder
-                ("/var/log/system.log", "ERROR")      # ç»å¯¹è·¯å¾„
-            ], name="MyApp", log_base_folder="./logs")
+            filename: å¯¹è±¡åç§°ï¼Œç”¨ä½œloggeråç§°å’Œæ–‡ä»¶å
+            session_folder: sessionæ–‡ä»¶å¤¹è·¯å¾„
+            console_output: æ§åˆ¶å°è¾“å‡ºè®¾ç½®
+                          - False: ä¸è¾“å‡ºåˆ°æ§åˆ¶å°
+                          - True: è¾“å‡ºæ‰€æœ‰çº§åˆ«åˆ°æ§åˆ¶å° (ç›¸å½“äº DEBUG)
+                          - str/int: æŒ‡å®šæ—¥å¿—çº§åˆ«ï¼Œåªè¾“å‡º >= è¯¥çº§åˆ«çš„æ—¥å¿—
+            file_output: å¯¹è±¡ä¸“ç”¨æ–‡ä»¶è¾“å‡ºè®¾ç½®
+                        - False: ä¸è¾“å‡ºåˆ°ä¸“ç”¨æ–‡ä»¶
+                        - True: è¾“å‡ºæ‰€æœ‰çº§åˆ«åˆ°ä¸“ç”¨æ–‡ä»¶ (ç›¸å½“äº DEBUG)
+                        - str/int: æŒ‡å®šæ—¥å¿—çº§åˆ«ï¼Œåªè¾“å‡º >= è¯¥çº§åˆ«çš„æ—¥å¿—
+            global_output: å…¨å±€æ±‡æ€»æ–‡ä»¶è¾“å‡ºè®¾ç½®
+                          - False: ä¸è¾“å‡ºåˆ°å…¨å±€æ–‡ä»¶
+                          - True: è¾“å‡ºæ‰€æœ‰çº§åˆ«åˆ°å…¨å±€æ–‡ä»¶ (ç›¸å½“äº DEBUG)
+                          - str/int: æŒ‡å®šæ—¥å¿—çº§åˆ«ï¼Œåªè¾“å‡º >= è¯¥çº§åˆ«çš„æ—¥å¿—
+            name: è‡ªå®šä¹‰loggeråç§°ï¼Œé»˜è®¤ä½¿ç”¨filename
         """
-        self.name = name or "Logger"
-        self.log_base_folder = log_base_folder or os.getcwd()
-        
-        # ç¡®ä¿log_base_folderå­˜åœ¨
-        Path(self.log_base_folder).mkdir(parents=True, exist_ok=True)
+        self.object_name = filename if name is None else name
+        self.filename = filename
+        self.env_name = env_name or None
+        # å¤„ç†session_folderï¼šç©ºå­—ç¬¦ä¸²æ£€æŸ¥å’Œé»˜è®¤å€¼å¤„ç†
+        if not session_folder:  # None æˆ–ç©ºå­—ç¬¦ä¸²
+            if self._default_session_folder is None:
+                # å¦‚æœæ²¡æœ‰é»˜è®¤sessionï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
+                with self._lock:
+                    if self._default_session_folder is None:
+                        self._default_session_folder = self.create_session_folder()
+            self.session_folder = self._default_session_folder
+        else:
+            self.session_folder = session_folder
+            # å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ¬¡è®¾ç½®session_folderï¼Œå°†å…¶è®¾ä¸ºé»˜è®¤å€¼
+            if self._default_session_folder is None:
+                self.set_default_session_folder(session_folder)
+        if self.env_name:
+            self.session_folder = os.path.join(self.session_folder, self.env_name)
 
-        self.logger = logging.getLogger(self.name)
+        self.logger = logging.getLogger(f"{self.object_name}")
 
         # é¿å…é‡å¤åˆå§‹åŒ–åŒä¸€ä¸ªlogger
         if self.logger.handlers:
             return
 
-        # è§£æè¾“å‡ºé…ç½®
-        self.output_configs = []
-        enabled_levels = []
+        # æå–å„è¾“å‡ºæ¸ é“çš„æ—¥å¿—çº§åˆ«
+        self.console_level = self._extract_log_level(console_output, default_level=logging.DEBUG)
+        self.file_level = self._extract_log_level(file_output, default_level=logging.DEBUG)
+        self.global_level = self._extract_log_level(global_output, default_level=logging.DEBUG)
+        
+        # è®°å½•è¾“å‡ºå¼€å…³çŠ¶æ€
+        self.console_enabled = console_output is not False
+        self.file_enabled = file_output is not False
+        self.global_enabled = global_output is not False
         
-        for output_target, level in outputs:
-            level_int = self._extract_log_level(level)
-            self.output_configs.append({
-                'target': output_target,
-                'level': level_int,
-                'level_str': logging.getLevelName(level_int),
-                'handler': None,
-                'resolved_path': self._resolve_path(output_target)
-            })
-            enabled_levels.append(level_int)
-
-        # è®¾ç½®loggerçš„æœ€ä½çº§åˆ«
+        # è®¾ç½®loggerçš„æœ€ä½çº§åˆ«ï¼ˆå–æ‰€æœ‰å¯ç”¨è¾“å‡ºä¸­çš„æœ€ä½çº§åˆ«ï¼‰
+        enabled_levels = []
+        if self.console_enabled:
+            enabled_levels.append(self.console_level)
+        if self.file_enabled:
+            enabled_levels.append(self.file_level)
+        if self.global_enabled:
+            enabled_levels.append(self.global_level)
+            
         min_level = min(enabled_levels) if enabled_levels else logging.INFO
         self.logger.setLevel(min_level)
 
         # åˆ›å»ºç»Ÿä¸€çš„è‡ªå®šä¹‰æ ¼å¼åŒ–å™¨
         formatter = CustomFormatter()
 
-        # ä¸ºæ¯ä¸ªè¾“å‡ºç›®æ ‡åˆ›å»ºhandler
-        for config in self.output_configs:
-            handler = self._create_handler(config, formatter)
-            if handler:
-                handler.setLevel(config['level'])
-                self.logger.addHandler(handler)
-                config['handler'] = handler
+        # å­˜å‚¨handlerå¼•ç”¨ä»¥ä¾¿åŠ¨æ€è°ƒæ•´
+        self.console_handler = None
+        self.file_handler = None
+        self.global_handler = None
+
+        # æ§åˆ¶å°è¾“å‡º
+        if self.console_enabled and self._global_console_debug_enabled:
+            self.console_handler = logging.StreamHandler()
+            self.console_handler.setLevel(self.console_level)
+            self.console_handler.setFormatter(formatter)
+            self.logger.addHandler(self.console_handler)
+
+        # æ–‡ä»¶è¾“å‡º
+        if self.file_enabled or self.global_enabled:
+            # ç¡®ä¿sessionæ–‡ä»¶å¤¹å­˜åœ¨
+            Path(self.session_folder).mkdir(parents=True, exist_ok=True)
+
+            # 1. å¯¹è±¡ä¸“ç”¨æ—¥å¿—æ–‡ä»¶
+            if self.file_enabled:
+                object_log_file_path = os.path.join(self.session_folder, f"{self.filename}.log")
+                log_dir = os.path.dirname(object_log_file_path)
+                os.makedirs(log_dir, exist_ok=True)
+
+                self.file_handler = logging.FileHandler(object_log_file_path, encoding='utf-8')
+                self.file_handler.setLevel(self.file_level)
+                self.file_handler.setFormatter(formatter)
+                self.logger.addHandler(self.file_handler)
+
+            # 2. å…¨å±€æ—¶é—´é¡ºåºæ—¥å¿—æ–‡ä»¶
+            if self.global_enabled:
+                global_log_file_path = os.path.join(self.session_folder, "all_logs.log")
+                log_dir = os.path.dirname(global_log_file_path)
+                os.makedirs(log_dir, exist_ok=True)
+                
+                self.global_handler = logging.FileHandler(global_log_file_path, encoding='utf-8')
+                self.global_handler.setLevel(self.global_level)
+                self.global_handler.setFormatter(formatter)
+                self.logger.addHandler(self.global_handler)
 
         # ä¸ä¼ æ’­åˆ°çˆ¶logger
         self.logger.propagate = False
 
-    def _resolve_path(self, output_target: str) -> str:
+    def _extract_log_level(self, output_setting: Union[bool, str, int], default_level: int = logging.DEBUG) -> int:
         """
-        è§£æè¾“å‡ºè·¯å¾„
+        ä»è¾“å‡ºè®¾ç½®ä¸­æå–æ—¥å¿—çº§åˆ«
         
         Args:
-            output_target: è¾“å‡ºç›®æ ‡
+            output_setting: è¾“å‡ºè®¾ç½®
+                          - False: è¿”å›æœ€é«˜çº§åˆ«ï¼ˆä¸ä¼šå®é™…ä½¿ç”¨ï¼Œå› ä¸ºä¸ä¼šåˆ›å»ºhandlerï¼‰
+                          - True: è¿”å›é»˜è®¤çº§åˆ«
+                          - str: æ—¥å¿—çº§åˆ«åç§°ï¼Œè½¬æ¢ä¸ºå¯¹åº”çš„æ•°å€¼
+                          - int: ç›´æ¥è¿”å›è¯¥æ•°å€¼
+            default_level: å½“ output_setting ä¸º True æ—¶ä½¿ç”¨çš„é»˜è®¤çº§åˆ«
             
-        Returns:
-            str: è§£æåçš„è·¯å¾„
-        """
-        if output_target == "console":
-            return "console"
-        
-        # æ£€æŸ¥æ˜¯å¦ä¸ºç»å¯¹è·¯å¾„
-        if os.path.isabs(output_target):
-            return output_target
-        else:
-            # ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äºlog_base_folder
-            return os.path.join(self.log_base_folder, output_target)
-
-    def _extract_log_level(self, level_setting: Union[str, int]) -> int:
-        """
-        ä»çº§åˆ«è®¾ç½®ä¸­æå–æ—¥å¿—çº§åˆ«
-        
-        Args:
-            level_setting: çº§åˆ«è®¾ç½®
-        
         Returns:
             int: å¯¹åº”çš„æ—¥å¿—çº§åˆ«æ•°å€¼
+            
+        Raises:
+            ValueError: å½“å­—ç¬¦ä¸²çº§åˆ«åç§°æ— æ•ˆæ—¶
+            TypeError: å½“ç±»å‹ä¸æ”¯æŒæ—¶
         """
-        if isinstance(level_setting, str):
-            level_str = level_setting.upper()
+        if output_setting is False:
+            # è¿”å›æœ€é«˜çº§åˆ«ï¼Œå®é™…ä¸Šä¸ä¼šä½¿ç”¨å› ä¸ºä¸ä¼šåˆ›å»ºhandler
+            return logging.CRITICAL + 1
+        elif output_setting is True:
+            return default_level
+        elif isinstance(output_setting, str):
+            level_str = output_setting.upper()
             if level_str not in self._LEVEL_MAPPING:
-                raise ValueError(f"Invalid log level: {level_setting}. "
+                raise ValueError(f"Invalid log level: {output_setting}. "
                                f"Valid levels are: {list(self._LEVEL_MAPPING.keys())}")
             return self._LEVEL_MAPPING[level_str]
-        elif isinstance(level_setting, int):
-            return level_setting
+        elif isinstance(output_setting, int):
+            return output_setting
         else:
-            raise TypeError(f"level_setting must be str or int, got {type(level_setting)}")
+            raise TypeError(f"output_setting must be bool, str or int, got {type(output_setting)}")
 
-    def _create_handler(self, config: dict, formatter: CustomFormatter) -> Optional[logging.Handler]:
-        """
-        æ ¹æ®è¾“å‡ºé…ç½®åˆ›å»ºå¯¹åº”çš„handler
-        
-        Args:
-            config: è¾“å‡ºé…ç½®å­—å…¸
-            formatter: æ ¼å¼åŒ–å™¨
-            
-        Returns:
-            logging.Handler: åˆ›å»ºçš„handlerï¼Œå¦‚æœåˆ›å»ºå¤±è´¥è¿”å›None
-        """
-        try:
-            if config['target'] == "console":
-                # æ§åˆ¶å°è¾“å‡º
-                if self._global_console_debug_enabled:
-                    handler = logging.StreamHandler()
-                    handler.setFormatter(formatter)
-                    return handler
-                else:
-                    return None
-            else:
-                # æ–‡ä»¶è¾“å‡º
-                file_path = config['resolved_path']
-                log_dir = os.path.dirname(file_path)
-                if log_dir:  # å¦‚æœæœ‰ç›®å½•è·¯å¾„
-                    os.makedirs(log_dir, exist_ok=True)
-                handler = logging.FileHandler(file_path, encoding='utf-8')
-                handler.setFormatter(formatter)
-                return handler
-                
-        except Exception as e:
-            print(f"Failed to create handler for {config['target']}: {e}")
-            return None
-
-    def get_output_configs(self) -> List[dict]:
-        """è·å–å½“å‰è¾“å‡ºé…ç½®"""
-        return [
-            {
-                'target': config['target'],
-                'resolved_path': config['resolved_path'],
-                'level': config['level_str'],
-                'level_num': config['level'],
-                'handler_active': config['handler'] is not None
-            }
-            for config in self.output_configs
-        ]
-
-    def print_current_configs(self):
-        """æ‰“å°å½“å‰è¾“å‡ºé…ç½®"""
-        configs = self.get_output_configs()
-        print(f"\n=== Logger '{self.name}' Output Configurations ===")
-        print(f"Log base folder: {self.log_base_folder}")
-        for i, config in enumerate(configs, 1):
-            status = "ACTIVE" if config['handler_active'] else "INACTIVE"
-            print(f"{i}. Target: {config['target']}")
-            if config['target'] != "console":
-                print(f"   Resolved Path: {config['resolved_path']}")
-            print(f"   Level: {config['level']} ({config['level_num']}) - {status}")
-        print(f"Logger minimum level: {logging.getLevelName(self.logger.level)} ({self.logger.level})")
-        print("=" * 60)
-
-    def update_output_level(self, target_index_or_name: Union[int, str], new_level: Union[str, int]):
+    def set_console_level(self, level: Union[str, int, bool]):
         """
-        åŠ¨æ€æ›´æ–°æŒ‡å®šè¾“å‡ºçš„çº§åˆ«
+        åŠ¨æ€è®¾ç½®æ§åˆ¶å°è¾“å‡ºçº§åˆ«
         
         Args:
-            target_index_or_name: ç›®æ ‡ç´¢å¼•(0å¼€å§‹)æˆ–ç›®æ ‡åç§°
-            new_level: æ–°çš„æ—¥å¿—çº§åˆ«
+            level: æ–°çš„æ—¥å¿—çº§åˆ«
+                  - False: ç¦ç”¨æ§åˆ¶å°è¾“å‡º
+                  - True: å¯ç”¨æ§åˆ¶å°è¾“å‡ºï¼Œçº§åˆ«ä¸ºDEBUG
+                  - str/int: æŒ‡å®šå…·ä½“çº§åˆ«
         """
-        # æŸ¥æ‰¾ç›®æ ‡é…ç½®
-        target_config = None
-        if isinstance(target_index_or_name, int):
-            if 0 <= target_index_or_name < len(self.output_configs):
-                target_config = self.output_configs[target_index_or_name]
-        else:
-            for config in self.output_configs:
-                if config['target'] == target_index_or_name:
-                    target_config = config
-                    break
-        
-        if not target_config:
-            raise ValueError(f"Output target not found: {target_index_or_name}")
+        old_enabled = self.console_enabled
+        new_level = self._extract_log_level(level)
+        self.console_enabled = level is not False
+        self.console_level = new_level
         
-        # æ›´æ–°çº§åˆ«
-        new_level_int = self._extract_log_level(new_level)
-        target_config['level'] = new_level_int
-        target_config['level_str'] = logging.getLevelName(new_level_int)
+        # ç§»é™¤ç°æœ‰çš„æ§åˆ¶å°handler
+        if self.console_handler:
+            self.logger.removeHandler(self.console_handler)
+            self.console_handler = None
         
-        # æ›´æ–°handlerçº§åˆ«
-        if target_config['handler']:
-            target_config['handler'].setLevel(new_level_int)
+        # å¦‚æœå¯ç”¨æ§åˆ¶å°è¾“å‡ºï¼Œæ·»åŠ æ–°çš„handler
+        if self.console_enabled and self._global_console_debug_enabled:
+            self.console_handler = logging.StreamHandler()
+            self.console_handler.setLevel(self.console_level)
+            self.console_handler.setFormatter(CustomFormatter())
+            self.logger.addHandler(self.console_handler)
         
         # æ›´æ–°loggerçš„æœ€ä½çº§åˆ«
-        enabled_levels = [config['level'] for config in self.output_configs if config['handler']]
-        min_level = min(enabled_levels) if enabled_levels else logging.INFO
-        self.logger.setLevel(min_level)
+        self._update_logger_level()
         
-        print(f"Updated {target_config['target']} level to {target_config['level_str']}")
+        status = "enabled" if self.console_enabled else "disabled"
+        level_str = logging.getLevelName(self.console_level) if self.console_enabled else "N/A"
+        print(f"Console output {status} with level: {level_str}")
 
-    def add_output(self, output_target: str, level: Union[str, int]):
+    def set_file_level(self, level: Union[str, int, bool]):
         """
-        åŠ¨æ€æ·»åŠ æ–°çš„è¾“å‡ºç›®æ ‡
+        åŠ¨æ€è®¾ç½®å¯¹è±¡ä¸“ç”¨æ–‡ä»¶è¾“å‡ºçº§åˆ«
         
         Args:
-            output_target: è¾“å‡ºç›®æ ‡
-            level: æ—¥å¿—çº§åˆ«
+            level: æ–°çš„æ—¥å¿—çº§åˆ«
         """
-        level_int = self._extract_log_level(level)
+        old_enabled = self.file_enabled
+        new_level = self._extract_log_level(level)
+        self.file_enabled = level is not False
+        self.file_level = new_level
         
-        # åˆ›å»ºæ–°é…ç½®
-        new_config = {
-            'target': output_target,
-            'level': level_int,
-            'level_str': logging.getLevelName(level_int),
-            'handler': None,
-            'resolved_path': self._resolve_path(output_target)
-        }
+        # ç§»é™¤ç°æœ‰çš„æ–‡ä»¶handler
+        if self.file_handler:
+            self.logger.removeHandler(self.file_handler)
+            self.file_handler = None
         
-        # åˆ›å»ºhandler
-        formatter = CustomFormatter()
-        handler = self._create_handler(new_config, formatter)
-        if handler:
-            handler.setLevel(level_int)
-            self.logger.addHandler(handler)
-            new_config['handler'] = handler
+        # å¦‚æœå¯ç”¨æ–‡ä»¶è¾“å‡ºï¼Œæ·»åŠ æ–°çš„handler
+        if self.file_enabled:
+            Path(self.session_folder).mkdir(parents=True, exist_ok=True)
+            object_log_file_path = os.path.join(self.session_folder, f"{self.filename}.log")
             
-        self.output_configs.append(new_config)
+            self.file_handler = logging.FileHandler(object_log_file_path, encoding='utf-8')
+            self.file_handler.setLevel(self.file_level)
+            self.file_handler.setFormatter(CustomFormatter())
+            self.logger.addHandler(self.file_handler)
         
-        # æ›´æ–°loggeræœ€ä½çº§åˆ«
-        enabled_levels = [config['level'] for config in self.output_configs if config['handler']]
-        min_level = min(enabled_levels) if enabled_levels else logging.INFO
-        self.logger.setLevel(min_level)
+        # æ›´æ–°loggerçš„æœ€ä½çº§åˆ«
+        self._update_logger_level()
         
-        print(f"Added output: {output_target} -> {new_config['resolved_path']} with level {new_config['level_str']}")
+        status = "enabled" if self.file_enabled else "disabled"
+        level_str = logging.getLevelName(self.file_level) if self.file_enabled else "N/A"
+        self.info(f"File output {status} with level: {level_str}")
 
-    def remove_output(self, target_index_or_name: Union[int, str]):
+    def set_global_level(self, level: Union[str, int, bool]):
         """
-        ç§»é™¤æŒ‡å®šçš„è¾“å‡ºç›®æ ‡
+        åŠ¨æ€è®¾ç½®å…¨å±€æ±‡æ€»æ–‡ä»¶è¾“å‡ºçº§åˆ«
         
         Args:
-            target_index_or_name: ç›®æ ‡ç´¢å¼•æˆ–åç§°
+            level: æ–°çš„æ—¥å¿—çº§åˆ«
         """
-        # æŸ¥æ‰¾å¹¶ç§»é™¤é…ç½®
-        target_config = None
-        target_index = None
-        
-        if isinstance(target_index_or_name, int):
-            if 0 <= target_index_or_name < len(self.output_configs):
-                target_index = target_index_or_name
-                target_config = self.output_configs[target_index]
-        else:
-            for i, config in enumerate(self.output_configs):
-                if config['target'] == target_index_or_name:
-                    target_index = i
-                    target_config = config
-                    break
+        old_enabled = self.global_enabled
+        new_level = self._extract_log_level(level)
+        self.global_enabled = level is not False
+        self.global_level = new_level
         
-        if not target_config:
-            raise ValueError(f"Output target not found: {target_index_or_name}")
+        # ç§»é™¤ç°æœ‰çš„å…¨å±€handler
+        if self.global_handler:
+            self.logger.removeHandler(self.global_handler)
+            self.global_handler = None
         
-        # ç§»é™¤handler
-        if target_config['handler']:
-            self.logger.removeHandler(target_config['handler'])
+        # å¦‚æœå¯ç”¨å…¨å±€è¾“å‡ºï¼Œæ·»åŠ æ–°çš„handler
+        if self.global_enabled:
+            Path(self.session_folder).mkdir(parents=True, exist_ok=True)
+            global_log_file_path = os.path.join(self.session_folder, "all_logs.log")
+            
+            self.global_handler = logging.FileHandler(global_log_file_path, encoding='utf-8')
+            self.global_handler.setLevel(self.global_level)
+            self.global_handler.setFormatter(CustomFormatter())
+            self.logger.addHandler(self.global_handler)
         
-        # ç§»é™¤é…ç½®
-        self.output_configs.pop(target_index)
+        # æ›´æ–°loggerçš„æœ€ä½çº§åˆ«
+        self._update_logger_level()
         
-        # æ›´æ–°loggeræœ€ä½çº§åˆ«
-        enabled_levels = [config['level'] for config in self.output_configs if config['handler']]
+        status = "enabled" if self.global_enabled else "disabled"
+        level_str = logging.getLevelName(self.global_level) if self.global_enabled else "N/A"
+        self.info(f"Global output {status} with level: {level_str}")
+
+    def _update_logger_level(self):
+        """æ›´æ–°loggerçš„æœ€ä½çº§åˆ«"""
+        enabled_levels = []
+        if self.console_enabled:
+            enabled_levels.append(self.console_level)
+        if self.file_enabled:
+            enabled_levels.append(self.file_level)
+        if self.global_enabled:
+            enabled_levels.append(self.global_level)
+            
         min_level = min(enabled_levels) if enabled_levels else logging.INFO
         self.logger.setLevel(min_level)
+
+    def get_current_levels(self) -> dict:
+        """
+        è·å–å½“å‰å„è¾“å‡ºæ¸ é“çš„çº§åˆ«è®¾ç½®
         
-        print(f"Removed output: {target_config['target']}")
+        Returns:
+            dict: åŒ…å«å„æ¸ é“çº§åˆ«ä¿¡æ¯çš„å­—å…¸
+        """
+        return {
+            'console': {
+                'enabled': self.console_enabled,
+                'level': logging.getLevelName(self.console_level) if self.console_enabled else None,
+                'level_num': self.console_level if self.console_enabled else None
+            },
+            'file': {
+                'enabled': self.file_enabled,
+                'level': logging.getLevelName(self.file_level) if self.file_enabled else None,
+                'level_num': self.file_level if self.file_enabled else None
+            },
+            'global': {
+                'enabled': self.global_enabled,
+                'level': logging.getLevelName(self.global_level) if self.global_enabled else None,
+                'level_num': self.global_level if self.global_enabled else None
+            },
+            'logger_min_level': {
+                'level': logging.getLevelName(self.logger.level),
+                'level_num': self.logger.level
+            }
+        }
+
+    def print_current_levels(self):
+        """æ‰“å°å½“å‰å„è¾“å‡ºæ¸ é“çš„çº§åˆ«è®¾ç½®"""
+        levels = self.get_current_levels()
+        print(f"\n=== Logger '{self.object_name}' Current Levels ===")
+        for channel, info in levels.items():
+            if channel == 'logger_min_level':
+                print(f"Logger minimum level: {info['level']} ({info['level_num']})")
+            else:
+                if info['enabled']:
+                    print(f"{channel.capitalize()} output: {info['level']} ({info['level_num']})")
+                else:
+                    print(f"{channel.capitalize()} output: DISABLED")
+        print("=" * 50)
+
+    # ...existing code...
 
     def _log_with_caller_info(self, level: int, message: str, exc_info: bool = False):
         """
@@ -380,15 +457,54 @@ class CustomLogger:
         """Criticalçº§åˆ«æ—¥å¿—"""
         self._log_with_caller_info(logging.CRITICAL, message)
 
-    def exception(self, message: str):
-        """å¼‚å¸¸çº§åˆ«æ—¥å¿—ï¼Œè‡ªåŠ¨åŒ…å«å¼‚å¸¸ä¿¡æ¯"""
-        self.error(message, exc_info=True)
+    def get_log_file_path(self) -> str:
+        """è·å–å½“å‰å¯¹è±¡çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
+        return os.path.join(self.session_folder, f"{self.filename}.log")
+
+    def get_global_log_file_path(self) -> str:
+        """è·å–å…¨å±€æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
+        return os.path.join(self.session_folder, "all_logs.log")
+
+    @staticmethod
+    def create_session_folder(base_path: str = "logs") -> str:
+        """
+        åˆ›å»ºsessionæ–‡ä»¶å¤¹çš„å·¥å…·æ–¹æ³•, å§‹ç»ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»º.
+        é¡¹ç›®æ ¹ç›®å½•è¢«å®šä¹‰ä¸ºæœ¬æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ä¸Šä¸¤çº§ç›®å½•.
+        """
+        # å°†é¡¹ç›®æ ¹ç›®å½•å®šä¹‰ä¸ºå½“å‰æ–‡ä»¶çš„ä¸Šä¸¤çº§ç›®å½•
+        project_root = Path(os.getcwd())  # è·å–å½“å‰å·¥ä½œç›®å½•
+        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+        # å°† base_path (é»˜è®¤ä¸º "logs") ç½®äºé¡¹ç›®æ ¹ç›®å½•ä¸‹
+        session_folder = project_root / base_path / timestamp
+        session_folder.mkdir(parents=True, exist_ok=True)
+        return str(session_folder)
 
     @classmethod
     def get_available_levels(cls) -> list:
         """è·å–æ‰€æœ‰å¯ç”¨çš„æ—¥å¿—çº§åˆ«"""
         return list(cls._LEVEL_MAPPING.keys())
 
+    @classmethod
+    def set_default_session_folder(cls, session_folder: str):
+        """è®¾ç½®é»˜è®¤çš„sessionæ–‡ä»¶å¤¹"""
+        with cls._lock:
+            cls._default_session_folder = session_folder
+            Path(session_folder).mkdir(parents=True, exist_ok=True)
+
+    @classmethod
+    def get_session_folder(cls) -> Optional[str]:
+        if cls._default_session_folder is None:
+            # å¦‚æœæ²¡æœ‰è®¾ç½®é»˜è®¤sessionæ–‡ä»¶å¤¹ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
+            cls._default_session_folder = cls.create_session_folder()
+        """è·å–é»˜è®¤çš„sessionæ–‡ä»¶å¤¹"""
+        return cls._default_session_folder
+
+    @classmethod
+    def reset_default_session(cls):
+        """é‡ç½®é»˜è®¤sessionï¼ˆç”¨äºæµ‹è¯•æˆ–é‡æ–°å¼€å§‹ï¼‰"""
+        with cls._lock:
+            cls._default_session_folder = None
+
     @classmethod
     def disable_global_console_debug(cls):
         """å…¨å±€ç¦ç”¨æ‰€æœ‰console debugè¾“å‡º"""
@@ -404,4 +520,7 @@ class CustomLogger:
     @classmethod
     def is_global_console_debug_enabled(cls) -> bool:
         """æ£€æŸ¥å…¨å±€console debugæ˜¯å¦å¯ç”¨"""
-        return cls._global_console_debug_enabled
\ No newline at end of file
+        return cls._global_console_debug_enabled
+
+    def exception(self, param):
+        self.error(param)
diff --git a/sage_libs/io/utils/data_loader.py b/sage_utils/data_loader.py
similarity index 100%
rename from sage_libs/io/utils/data_loader.py
rename to sage_utils/data_loader.py
diff --git a/sage_utils/embedding_methods/embedding_api.py b/sage_utils/embedding_methods/embedding_api.py
index f44ce8b..794e68b 100644
--- a/sage_utils/embedding_methods/embedding_api.py
+++ b/sage_utils/embedding_methods/embedding_api.py
@@ -1,4 +1,4 @@
-from sage_utils.embedding_methods.embedding_model import EmbeddingModel
+from sage_utils.embedding_model import EmbeddingModel
 
 
 def apply_embedding_model(name: str = "default",**kwargs) -> EmbeddingModel:
diff --git a/sage_utils/embedding_methods/hf.py b/sage_utils/embedding_methods/hf.py
index 33990ba..17f9cb4 100644
--- a/sage_utils/embedding_methods/hf.py
+++ b/sage_utils/embedding_methods/hf.py
@@ -18,9 +18,19 @@ if not pm.is_installed("tenacity"):
     pm.install("tenacity")
 
 from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+import torch
+import numpy as np
 
 os.environ["TOKENIZERS_PARALLELISM"] = "false"
 
+
 @lru_cache(maxsize=1)
 def initialize_hf_model(model_name):
     hf_tokenizer = AutoTokenizer.from_pretrained(
@@ -35,6 +45,26 @@ def initialize_hf_model(model_name):
     return hf_model, hf_tokenizer
 
 
+
+
+
+# async def hf_embed(text: str, tokenizer, embed_model) -> list[float]:
+#     device = next(embed_model.parameters()).device
+#     encoded_texts = tokenizer(
+#         text, return_tensors="pt", padding=True, truncation=True
+#     ).to(device)
+#     with torch.no_grad():
+#         outputs = embed_model(
+#             input_ids=encoded_texts["input_ids"],
+#             attention_mask=encoded_texts["attention_mask"],
+#         )
+#         embeddings = outputs.last_hidden_state.mean(dim=1)
+#     if embeddings.dtype == torch.bfloat16:
+#         return embeddings.detach().to(torch.float32).cpu()[0].tolist()
+#     else:
+#         return embeddings.detach().cpu()[0].tolist()
+
+
 import torch
 
 def hf_embed_sync(text: str, tokenizer, embed_model) -> list[float]:
diff --git a/sage_utils/embedding_methods/lollms.py b/sage_utils/embedding_methods/lollms.py
index a43751d..d47e1e4 100644
--- a/sage_utils/embedding_methods/lollms.py
+++ b/sage_utils/embedding_methods/lollms.py
@@ -1,7 +1,5 @@
 import sys
 
-import aiohttp
-
 if sys.version_info < (3, 9):
     from typing import AsyncIterator
 else:
diff --git a/sage_utils/embedding_methods/embedding_model.py b/sage_utils/embedding_model.py
similarity index 100%
rename from sage_utils/embedding_methods/embedding_model.py
rename to sage_utils/embedding_model.py
diff --git a/sage_utils/local_tcp_server.py b/sage_utils/local_tcp_server.py
deleted file mode 100644
index 614981a..0000000
--- a/sage_utils/local_tcp_server.py
+++ /dev/null
@@ -1,416 +0,0 @@
-import socket
-import threading
-import pickle
-from typing import Dict, Any, Callable, Optional
-import os
-
-class LocalTcpServer:
-    """
-    æœ¬åœ°TCPæœåŠ¡å™¨ï¼Œç”¨äºæ¥æ”¶Ray Actorå‘é€çš„æ•°æ®
-    æ”¯æŒåŸºäºæ¶ˆæ¯ç±»å‹çš„å¤šä¸ªå¤„ç†å™¨
-    """
-    
-    def __init__(self, 
-                 host: str = None, 
-                 port: int = None,
-                 default_handler: Optional[Callable[[Dict[str, Any], tuple], None]] = None):
-        """
-        åˆå§‹åŒ–TCPæœåŠ¡å™¨
-        
-        Args:
-            host: ç›‘å¬åœ°å€
-            port: ç›‘å¬ç«¯å£
-            default_handler: é»˜è®¤æ¶ˆæ¯å¤„ç†å›è°ƒå‡½æ•°ï¼Œç”¨äºå¤„ç†æœªçŸ¥ç±»å‹çš„æ¶ˆæ¯
-        """
-        # ... ç°æœ‰åˆå§‹åŒ– ...
-        self.server_cwd = os.getcwd()
-        self.host = host or self._get_host_ip()
-        self.port = port or self._allocate_tcp_port()
-        self.server_socket: Optional[socket.socket] = None
-        self.server_thread: Optional[threading.Thread] = None
-        self.running = False
-        
-        # æ¶ˆæ¯å¤„ç†å™¨å­—å…¸ï¼šæ¶ˆæ¯ç±»å‹ -> å¤„ç†å‡½æ•°
-        self.message_handlers: Dict[str, Callable[[Dict[str, Any], tuple], None]] = {}
-        self.default_handler = default_handler
-        
-        # æ·»åŠ é”ä¿æŠ¤å¤„ç†å™¨å­—å…¸
-        self._handlers_lock = threading.RLock()
-        
-        # å®¢æˆ·ç«¯è¿æ¥ç®¡ç†
-        self.client_connections: Dict[str, socket.socket] = {}  # client_id -> socket
-        self.client_lock = threading.Lock()
-
-    def _get_host_ip(self):
-        """è‡ªåŠ¨è·å–æœ¬æœºå¯ç”¨äºå¤–éƒ¨è¿æ¥çš„ IP åœ°å€"""
-        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
-        try:
-            s.connect(("8.8.8.8", 80))
-            ip = s.getsockname()[0]
-        except Exception:
-            self.logger.warning("Failed to get external IP, using localhost")
-            ip = "127.0.0.1"
-        finally:
-            s.close()
-        return ip
-    
-    def _allocate_tcp_port(self) -> int:
-        print( "Allocating TCP port..." )
-        """ä¸º DAG åˆ†é…å¯ç”¨çš„ TCP ç«¯å£"""
-        # å°è¯•ä»é¢„è®¾èŒƒå›´åˆ†é…ç«¯å£
-        for port in range(19200, 20000):
-            try:
-                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
-                    s.bind((self.host, port))
-                    print("Allocated port:", port)
-                    return port
-            except OSError:
-                continue
-        
-        # å¦‚æœé¢„è®¾èŒƒå›´éƒ½è¢«å ç”¨ï¼Œä½¿ç”¨ç³»ç»Ÿåˆ†é…
-        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
-            self.logger.warning("All predefined ports are occupied, using system-assigned port")
-            s.bind((self.host, 0))
-            return s.getsockname()[1]
-
-
-
-    def start(self):
-        """å¯åŠ¨TCPæœåŠ¡å™¨"""
-        if self.running:
-            self.logger.warning("TCP server is already running")
-            return
-        
-        try:
-            self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-            self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
-            self.server_socket.settimeout(5)
-            self.server_socket.bind((self.host, self.port))
-            self.server_socket.listen(10)
-            
-            self.running = True
-            self.server_thread = threading.Thread(
-                target=self._server_loop,
-                name="LocalTcpServerThread"
-            )
-            self.server_thread.daemon = True
-            self.server_thread.start()
-            
-            self.logger.info(f"TCP server started on {self.host}:{self.port}")
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start TCP server: {e}")
-            self.running = False
-            raise
-    
-    def stop(self):
-        """åœæ­¢TCPæœåŠ¡å™¨"""
-        if not self.running:
-            return
-        
-        self.logger.info("Stopping TCP server...")
-        self.running = False
-        
-        if self.server_socket:
-            self.server_socket.close()
-        
-        if self.server_thread and self.server_thread.is_alive():
-            for _ in range(5):
-                self.server_thread.join(timeout=1.0)
-                if not self.server_thread.is_alive():
-                    break
-            else:
-                self.logger.warning("TCP server thread did not stop gracefully")
-        
-        self.logger.info("TCP server stopped")
-    
-    def _server_loop(self):
-        """TCPæœåŠ¡å™¨ä¸»å¾ªç¯"""
-        self.logger.debug("TCP server loop started")
-        
-        while self.running:
-            try:
-                if not self.server_socket:
-                    break
-                    
-                client_socket, address = self.server_socket.accept()
-                self.logger.debug(f"New TCP client connected from {address}")
-                
-                # åœ¨æ–°çº¿ç¨‹ä¸­å¤„ç†å®¢æˆ·ç«¯
-                client_thread = threading.Thread(
-                    target=self._handle_client,
-                    args=(client_socket, address),
-                    name=f"TcpClient-{address[0]}:{address[1]}"
-                )
-                client_thread.daemon = True
-                client_thread.start()
-            except socket.timeout:
-                continue
-            except OSError as e:
-                if self.running:
-                    self.logger.error(f"Error accepting TCP connection: {e}")
-                break
-            except Exception as e:
-                if self.running:
-                    self.logger.error(f"Unexpected error in server loop: {e}")
-        
-        self.logger.debug("TCP server loop stopped")
-    
-    def _handle_client(self, client_socket: socket.socket, address: tuple):
-        """å¤„ç†å®¢æˆ·ç«¯è¿æ¥å’Œæ¶ˆæ¯"""
-        try:
-            while self.running:
-                # è¯»å–æ¶ˆæ¯é•¿åº¦
-                size_data = client_socket.recv(4)
-                if not size_data:
-                    break
-                
-                message_size = int.from_bytes(size_data, byteorder='big')
-                if message_size <= 0 or message_size > 10 * 1024 * 1024:  # 10MB limit
-                    self.logger.warning(f"Invalid message size {message_size} from {address}")
-                    break
-                
-                # è¯»å–æ¶ˆæ¯å†…å®¹
-                message_data = self._receive_full_message(client_socket, message_size)
-                if not message_data:
-                    break
-                
-                # ååºåˆ—åŒ–å¹¶å¤„ç†æ¶ˆæ¯
-                try:
-                    message = pickle.loads(message_data)
-                    response = self._process_message(message, address)
-                    
-                    # å‘é€å“åº”
-                    if response:
-                        self._send_response(client_socket, response)
-                        
-                except Exception as e:
-                    self.logger.error(f"Error processing message from {address}: {e}")
-                    # å‘é€é”™è¯¯å“åº”
-                    error_response = {
-                        "type": "error_response",
-                        "request_id": message.get("request_id") if 'message' in locals() else None,
-                        "timestamp": int(time.time()),
-                        "status": "error",
-                        "message": f"Internal server error: {str(e)}",
-                        "payload": {"error_code": "ERR_INTERNAL_ERROR"}
-                    }
-                    self._send_response(client_socket, error_response)
-                
-        except Exception as e:
-            self.logger.error(f"Error handling TCP client {address}: {e}")
-        finally:
-            try:
-                client_socket.close()
-            except:
-                pass
-            self.logger.debug(f"TCP client {address} disconnected")
-    
-    def _receive_full_message(self, client_socket: socket.socket, message_size: int) -> Optional[bytes]:
-        """æ¥æ”¶å®Œæ•´çš„æ¶ˆæ¯æ•°æ®"""
-        message_data = b''
-        while len(message_data) < message_size:
-            chunk_size = min(message_size - len(message_data), 8192)
-            chunk = client_socket.recv(chunk_size)
-            if not chunk:
-                self.logger.warning("Connection closed while receiving message")
-                return None
-            message_data += chunk
-        
-        return message_data
-    
-    def _process_message(self, message: Dict[str, Any], client_address: tuple) -> Optional[Dict[str, Any]]:
-        """
-        å¤„ç†æ¥æ”¶åˆ°çš„æ¶ˆæ¯ï¼Œæ ¹æ®æ¶ˆæ¯ç±»å‹åˆ†å‘ç»™å¯¹åº”çš„å¤„ç†å™¨
-        
-        Args:
-            message: æ¥æ”¶åˆ°çš„æ¶ˆæ¯å­—å…¸
-            client_address: å®¢æˆ·ç«¯åœ°å€
-            
-        Returns:
-            å“åº”å­—å…¸ï¼Œå¦‚æœå¤„ç†å™¨è¿”å› None åˆ™ä¸å‘é€å“åº”
-        """
-        try:
-            # å°è¯•è·å–æ¶ˆæ¯ç±»å‹
-            message_type = self._extract_message_type(message)
-            if message_type is None:
-                # æ— æ³•æå–æ¶ˆæ¯ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†å™¨
-                self.logger.warning(f"Could not extract message type from message, using default handler")
-                return self._use_default_handler(message, client_address, None)
-
-            self.logger.debug(f"Processing message type '{message_type}' from {client_address}")
-            
-            # æŸ¥æ‰¾å¯¹åº”çš„å¤„ç†å™¨
-            with self._handlers_lock:
-                handler = self.message_handlers.get(message_type, None)
-            
-            if handler is None:
-                # æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„å¤„ç†å™¨ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†å™¨
-                self.logger.warning(f"No handler found for message type '{message_type}', using default handler")
-                return self._use_default_handler(message, client_address, message_type)
-            
-
-            try:
-                response = handler(message, client_address)
-                self.logger.debug(f"Message type '{message_type}' processed successfully")
-                return response
-            except Exception as e:
-                self.logger.error(f"Error in handler for message type '{message_type}': {e}", exc_info=True)
-                return self._create_error_response(message, "ERR_HANDLER_FAILED", str(e))
-
-                
-        except Exception as e:
-            self.logger.error(f"Error in message processing: {e}", exc_info=True)
-            return self._create_error_response(message, "ERR_PROCESSING_FAILED", str(e))
-    
-    def _extract_message_type(self, message: Dict[str, Any]) -> Optional[str]:
-        """ä»æ¶ˆæ¯ä¸­æå–æ¶ˆæ¯ç±»å‹"""
-        if not isinstance(message, dict):
-            self.logger.warning(f"Message is not a dictionary: {type(message)}")
-            return None
-        
-        # å°è¯•å¤šç§å¯èƒ½çš„ç±»å‹å­—æ®µå
-        type_fields = ['type', 'message_type', 'msg_type', 'event_type', 'command']
-        
-        for field in type_fields:
-            if field in message:
-                msg_type = message[field]
-                if isinstance(msg_type, str) and msg_type.strip():
-                    return msg_type.strip()
-        
-        self.logger.debug(f"No valid type field found in message keys: {list(message.keys())}")
-        return None
-    
-    def _use_default_handler(self, message: Dict[str, Any], client_address: tuple, message_type: Optional[str]) -> Optional[Dict[str, Any]]:
-        """ä½¿ç”¨é»˜è®¤å¤„ç†å™¨å¤„ç†æ¶ˆæ¯"""
-        if self.default_handler:
-            try:
-                response = self.default_handler(message, client_address)
-                self.logger.debug("Message processed by default handler")
-                return response
-            except Exception as e:
-                self.logger.error(f"Error in default handler: {e}", exc_info=True)
-                return self._create_error_response(message, "ERR_DEFAULT_HANDLER_FAILED", str(e))
-        else:
-            self.logger.warning(f"No default handler set, ignoring message from {client_address}")
-            if message_type:
-                self.logger.info(f"Consider registering a handler for message type '{message_type}'")
-            return self._create_error_response(message, "ERR_NO_HANDLER", "No handler available for this message type")
-    
-    def _create_error_response(self, original_message: Dict[str, Any], error_code: str, error_message: str) -> Dict[str, Any]:
-        """åˆ›å»ºé”™è¯¯å“åº”"""
-        import time
-        return {
-            "type": f"{original_message.get('type', 'unknown')}_response",
-            "request_id": original_message.get("request_id"),
-            "env_name": original_message.get("env_name"),
-            "env_uuid": original_message.get("env_uuid"),
-            "timestamp": int(time.time()),
-            "status": "error",
-            "message": error_message,
-            "payload": {
-                "error_code": error_code,
-                "details": {}
-            }
-        }
-
-    def _send_response(self, client_socket: socket.socket, response: Dict[str, Any]):
-        """å‘é€å“åº”åˆ°å®¢æˆ·ç«¯"""
-        try:
-            # åºåˆ—åŒ–å“åº”
-            if isinstance(response, dict):
-                response["cwd"] = self.server_cwd  # æ·»åŠ æœåŠ¡å™¨å½“å‰å·¥ä½œç›®å½•
-            serialized = pickle.dumps(response)
-            message_size = len(serialized)
-            
-            # å‘é€æ¶ˆæ¯é•¿åº¦
-            client_socket.send(message_size.to_bytes(4, byteorder='big'))
-            
-            # å‘é€æ¶ˆæ¯å†…å®¹
-            client_socket.send(serialized)
-            
-            self.logger.debug(f"Sent response: {response.get('type')}")
-            
-        except Exception as e:
-            self.logger.error(f"Error sending response: {e}")
-
-    def get_server_info(self) -> Dict[str, Any]:
-        """è·å–æœåŠ¡å™¨ä¿¡æ¯"""
-        with self._handlers_lock:
-            registered_types = list(self.message_handlers.keys())
-        
-        return {
-            "host": self.host,
-            "port": self.port,
-            "running": self.running,
-            "address": f"{self.host}:{self.port}",
-            "registered_message_types": registered_types,
-            "has_default_handler": self.default_handler is not None
-        }
-    
-    def __del__(self):
-        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
-        try:
-            self.stop()
-        except:
-            pass
-
-    ########################################################
-    #                handler  registration                 #
-    ########################################################
-
-    # ä¿®æ”¹ register_handler æ–¹æ³•çš„ç±»å‹æ³¨è§£ï¼Œä½¿å…¶è¿”å› Dict[str, Any]
-    def register_handler(self, message_type: str, handler: Callable[[Dict[str, Any], tuple], Dict[str, Any]]):
-        with self._handlers_lock:
-            self.message_handlers[message_type] = handler
-            self.logger.info(f"Registered handler for message type: {message_type}")
-
-    def set_default_handler(self, handler: Callable[[Dict[str, Any], tuple], Dict[str, Any]]):
-        self.default_handler = handler
-        self.logger.info("Default message handler set")
-
-    def unregister_handler(self, message_type: str):
-        with self._handlers_lock:
-            if message_type in self.message_handlers:
-                del self.message_handlers[message_type]
-                self.logger.info(f"Unregistered handler for message type: {message_type}")
-            else:
-                self.logger.warning(f"No handler found for message type: {message_type}")
-
-    def get_registered_types(self) -> list[str]:
-        with self._handlers_lock:
-            return list(self.message_handlers.keys())
-
-
-
-# ä½¿ç”¨ç¤ºä¾‹
-if __name__ == "__main__":
-    def handle_status_message(message: Dict[str, Any], client_address: tuple):
-        print(f"Status message from {client_address}: {message}")
-    
-    def handle_data_message(message: Dict[str, Any], client_address: tuple):
-        print(f"Data message from {client_address}: {message}")
-    
-    def handle_unknown_message(message: Dict[str, Any], client_address: tuple):
-        print(f"Unknown message from {client_address}: {message}")
-    
-    # åˆ›å»ºæœåŠ¡å™¨
-    server = LocalTcpServer(default_handler=handle_unknown_message)
-    
-    # æ³¨å†Œä¸åŒç±»å‹çš„å¤„ç†å™¨
-    server.register_handler("status", handle_status_message)
-    server.register_handler("data", handle_data_message)
-    
-    # å¯åŠ¨æœåŠ¡å™¨
-    server.start()
-    
-    print(f"Server info: {server.get_server_info()}")
-    
-    try:
-        # ä¿æŒæœåŠ¡å™¨è¿è¡Œ
-        import time
-        while True:
-            time.sleep(1)
-    except KeyboardInterrupt:
-        print("Stopping server...")
-        server.stop()
\ No newline at end of file
diff --git a/sage_jobmanager/utils/name_server.py b/sage_utils/name_server.py
similarity index 100%
rename from sage_jobmanager/utils/name_server.py
rename to sage_utils/name_server.py
diff --git a/sage_utils/serialization/__init__.py b/sage_utils/serialization/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/sage_utils/serialization/dill_serializer.py b/sage_utils/serialization/dill_serializer.py
deleted file mode 100644
index 882e8b4..0000000
--- a/sage_utils/serialization/dill_serializer.py
+++ /dev/null
@@ -1,647 +0,0 @@
-import os
-import pickle
-import inspect
-import threading
-import importlib
-from typing import Any, Dict, List, Set, Type, Optional, Union
-from collections.abc import Mapping, Sequence, Set as AbstractSet
-import dill
-
-
-class SerializationError(Exception):
-    """åºåˆ—åŒ–ç›¸å…³é”™è¯¯"""
-    pass
-
-
-# ä¸å¯åºåˆ—åŒ–ç±»å‹é»‘åå•
-_BLACKLIST = [
-    threading.Thread,  # çº¿ç¨‹
-    type(open),        # æ–‡ä»¶å¥æŸ„
-    type(threading.Lock),    # é”
-    type(threading.RLock),   # é€’å½’é”
-    threading.Event,   # äº‹ä»¶
-    threading.Condition,  # æ¡ä»¶å˜é‡
-]
-
-# åºåˆ—åŒ–æ—¶éœ€è¦æ’é™¤çš„å±æ€§å
-_ATTRIBUTE_BLACKLIST = {
-    'logger',          # æ—¥å¿—å¯¹è±¡
-    '_logger',         # ç§æœ‰æ—¥å¿—å¯¹è±¡
-    'server_socket',   # socketå¯¹è±¡
-    'server_thread',   # çº¿ç¨‹å¯¹è±¡
-    '_server_thread',  # ç§æœ‰çº¿ç¨‹å¯¹è±¡
-    'client_socket',   # socketå¯¹è±¡
-    '__weakref__',     # å¼±å¼•ç”¨
-    'runtime_context', # è¿è¡Œæ—¶ä¸Šä¸‹æ–‡
-    # 'memory_collection', # å†…å­˜é›†åˆï¼ˆé€šå¸¸æ˜¯Ray Actorå¥æŸ„ï¼‰
-    'env',             # ç¯å¢ƒå¼•ç”¨ï¼ˆé¿å…å¾ªç¯å¼•ç”¨ï¼‰
-    # '_dag_node_factory',  # å·¥å‚å¯¹è±¡
-    # '_operator_factory',  # å·¥å‚å¯¹è±¡
-    # '_function_factory',  # å·¥å‚å¯¹è±¡
-}
-
-# å“¨å…µå€¼ï¼Œè¡¨ç¤ºåº”è¯¥è·³è¿‡çš„å€¼
-_SKIP_VALUE = object()
-
-
-def _gather_attrs(obj):
-    """æšä¸¾å®ä¾‹ __dict__ å’Œ @property å±æ€§ã€‚"""
-    attrs = dict(getattr(obj, "__dict__", {}))
-    for name, prop in inspect.getmembers(type(obj), lambda x: isinstance(x, property)):
-        try:
-            attrs[name] = getattr(obj, name)
-        except Exception:
-            pass
-    return attrs
-
-
-def _filter_attrs(attrs, include, exclude):
-    """æ ¹æ® include/exclude è¿‡æ»¤å­—æ®µå­—å…¸ã€‚"""
-    if include:
-        return {k: attrs[k] for k in include if k in attrs}
-    
-    # åˆå¹¶ç”¨æˆ·å®šä¹‰çš„excludeå’Œç³»ç»Ÿé»˜è®¤çš„exclude
-    all_exclude = set(exclude or []) | _ATTRIBUTE_BLACKLIST
-    return {k: v for k, v in attrs.items() if k not in all_exclude}
-
-
-def _should_skip(v):
-    """åˆ¤æ–­å¯¹è±¡æ˜¯å¦åº”è¯¥è·³è¿‡åºåˆ—åŒ–"""
-    # æ£€æŸ¥é»‘åå• - ä¿®æ”¹ä¸ºæ›´ç²¾ç¡®çš„æ£€æŸ¥
-    for i, blacklisted_type in enumerate(_BLACKLIST):
-        if isinstance(v, blacklisted_type):
-            # print(f"Skipping blacklisted instance {i}: {type(v)}, {v}")
-            return True
-    
-    # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å—ï¼ˆé€šå¸¸ä¸åº”è¯¥åºåˆ—åŒ–ï¼‰
-    if inspect.ismodule(v):
-        # print(f"Skipping module: {v}")
-        return True
-    
-    return False
-
-
-def _preprocess_for_dill(obj, _seen=None):
-    """
-    é€’å½’é¢„å¤„ç†å¯¹è±¡ï¼Œæ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å†…å®¹ï¼Œä¸ºdillåºåˆ—åŒ–åšå‡†å¤‡ã€‚
-    
-    Args:
-        obj: è¦é¢„å¤„ç†çš„å¯¹è±¡
-        _seen: å·²å¤„ç†å¯¹è±¡çš„é›†åˆï¼Œç”¨äºå¤„ç†å¾ªç¯å¼•ç”¨
-    
-    Returns:
-        é¢„å¤„ç†åçš„å¯¹è±¡ï¼Œå¯ä»¥å®‰å…¨åœ°äº¤ç»™dillåºåˆ—åŒ–
-    """
-    # print(f"_preprocess_for_dill called for object: {obj}")
-    if _seen is None:
-        _seen = set()
-    
-    # é˜²æ­¢å¾ªç¯å¼•ç”¨
-    obj_id = id(obj)
-    if obj_id in _seen:
-        # print(f"Skipping already seen object: {obj}")
-        return _SKIP_VALUE
-    
-    # åŸºæœ¬ç±»å‹ç›´æ¥è¿”å›
-    if isinstance(obj, (int, float, str, bool, type(None))):
-        return obj
-    
-    # ç±»å¯¹è±¡å¯ä»¥ç›´æ¥è¢«dillåºåˆ—åŒ–ï¼Œä¸éœ€è¦é¢„å¤„ç†
-    if inspect.isclass(obj):
-        # print(f"Processing class object: {obj}")
-        return obj
-    
-    # å‡½æ•°å¯¹è±¡ä¹Ÿå¯ä»¥ç›´æ¥è¢«dillåºåˆ—åŒ–
-    if inspect.isfunction(obj) or inspect.ismethod(obj):
-        # print(f"Processing function object: {obj}")
-        return obj
-
-    # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
-    if _should_skip(obj):
-        return _SKIP_VALUE
-    
-
-
-    # å¤„ç†å­—å…¸
-    if isinstance(obj, Mapping):
-        _seen.add(obj_id)
-        try:
-            cleaned = {}
-            for k, v in obj.items():
-                if not _should_skip(k) and not _should_skip(v):
-                    cleaned_k = _preprocess_for_dill(k, _seen)
-                    cleaned_v = _preprocess_for_dill(v, _seen)
-                    if cleaned_k is not _SKIP_VALUE and ((cleaned_v is not _SKIP_VALUE) or (cleaned_v is None)):
-                        cleaned[cleaned_k] = cleaned_v
-            return cleaned
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¤„ç†åºåˆ—ï¼ˆåˆ—è¡¨ã€å…ƒç»„ç­‰ï¼‰
-    if isinstance(obj, Sequence) and not isinstance(obj, str):
-        _seen.add(obj_id)
-        try:
-            cleaned = []
-            for item in obj:
-                if not _should_skip(item):
-                    cleaned_item = _preprocess_for_dill(item, _seen)
-                    if cleaned_item is not _SKIP_VALUE:
-                        cleaned.append(cleaned_item)
-            return type(obj)(cleaned) if cleaned else []
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¤„ç†é›†åˆ
-    if isinstance(obj, AbstractSet):
-        _seen.add(obj_id)
-        try:
-            cleaned = set()
-            for item in obj:
-                if not _should_skip(item):
-                    cleaned_item = _preprocess_for_dill(item, _seen)
-                    if cleaned_item is not _SKIP_VALUE:
-                        cleaned.add(cleaned_item)
-            return type(obj)(cleaned) if cleaned else set()
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¤„ç†å¤æ‚å¯¹è±¡
-    if hasattr(obj, '__dict__'):
-        # print(f"Processing complex object: {obj}")
-        # print(f"dict is {obj.__dict__}")
-        _seen.add(obj_id)
-        try:
-            # åˆ›å»ºä¸€ä¸ªæ–°çš„å¯¹è±¡å®ä¾‹
-            obj_class = type(obj)
-            
-            # å°è¯•åˆ›å»ºç©ºå®ä¾‹
-            try:
-                cleaned_obj = obj_class.__new__(obj_class)
-            except Exception:
-                # å¦‚æœæ— æ³•åˆ›å»ºç©ºå®ä¾‹ï¼Œè¿”å›åŸå¯¹è±¡è®©dillå¤„ç†
-                return obj
-            
-            # è·å–å’Œè¿‡æ»¤å±æ€§
-            custom_include = getattr(obj.__class__, "__state_include__", [])
-            custom_exclude = getattr(obj.__class__, "__state_exclude__", [])
-            # if len(custom_exclude) is not 0:
-            #     print(f"custom_exclude is {custom_exclude}")
-            # ä¸€èˆ¬ä¸ç”¨includeå­—æ®µï¼Œåªç”¨excludeå­—æ®µå°±è¡Œäº†    
-            
-            attrs = _gather_attrs(obj)
-            # if len(custom_exclude) is not 0:
-            #     print(f"attrs is {attrs}")
-
-            filtered_attrs = _filter_attrs(attrs, custom_include, custom_exclude)
-            # if len(custom_exclude) is not 0:
-            #     print(f"filtered_attrs is {filtered_attrs}")
-            
-            # é€’å½’æ¸…ç†å±æ€§
-            for attr_name, attr_value in filtered_attrs.items():
-                # print(f"Processing attribute: {attr_name} = {attr_value}")
-                if not _should_skip(attr_value):
-                    # print(f"Cleaning attribute: {attr_name}")
-                    cleaned_value = _preprocess_for_dill(attr_value, _seen)
-                    if cleaned_value is not _SKIP_VALUE:
-                        try:
-                            setattr(cleaned_obj, attr_name, cleaned_value)
-                        except Exception:
-                            # å¿½ç•¥è®¾ç½®å¤±è´¥çš„å±æ€§
-                            pass
-            
-            return cleaned_obj
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¯¹äºå…¶ä»–å¯¹è±¡ï¼Œç›´æ¥è¿”å›ç»™dillå¤„ç†
-    return obj
-
-
-def _postprocess_from_dill(obj, _seen=None):
-    """é€’å½’åå¤„ç†ä»dillååºåˆ—åŒ–çš„å¯¹è±¡ï¼Œæ¸…ç†å“¨å…µå€¼ã€‚"""
-    # print(f"_postprocess_from_dill called for object: {obj}")
-    if _seen is None:
-        _seen = set()
-    
-    # é˜²æ­¢å¾ªç¯å¼•ç”¨
-    obj_id = id(obj)
-    if obj_id in _seen:
-        return obj
-    
-    # åŸºæœ¬ç±»å‹ç›´æ¥è¿”å›
-    if isinstance(obj, (int, float, str, bool, type(None))):
-        return obj
-    
-    # è·³è¿‡å“¨å…µå€¼
-    if obj is _SKIP_VALUE:
-        return None
-    
-    # å¤„ç†å­—å…¸
-    if isinstance(obj, Mapping):
-        _seen.add(obj_id)
-        try:
-            cleaned = {}
-            for k, v in obj.items():
-                # print(f"Processing dict item: {k} = {v}")
-                # ä¿®å¤ï¼šåªè¿‡æ»¤æ‰å“¨å…µå€¼ï¼Œä¿ç•™æ‰€æœ‰åˆæ³•å€¼ï¼ˆåŒ…æ‹¬Noneã€Falseã€0ç­‰ï¼‰
-                if k is not _SKIP_VALUE and v is not _SKIP_VALUE:
-                    cleaned_k = _postprocess_from_dill(k, _seen)
-                    cleaned_v = _postprocess_from_dill(v, _seen)
-                    # ä¿ç•™æ‰€æœ‰å€¼ï¼ŒåŒ…æ‹¬Noneã€Falseã€0ã€ç©ºå­—å…¸ç­‰
-                    cleaned[cleaned_k] = cleaned_v
-                    # print(f"Cleaned dict item: {cleaned_k} = {cleaned_v}")
-            return cleaned
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¤„ç†åºåˆ—
-    if isinstance(obj, Sequence) and not isinstance(obj, str):
-        _seen.add(obj_id)
-        try:
-            cleaned = []
-            for item in obj:
-                if item is not _SKIP_VALUE:
-                    cleaned_item = _postprocess_from_dill(item, _seen)
-                    # ä¿ç•™æ‰€æœ‰å€¼ï¼ŒåŒ…æ‹¬Noneã€Falseã€0ç­‰
-                    cleaned.append(cleaned_item)
-            return type(obj)(cleaned)
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¤„ç†é›†åˆ
-    if isinstance(obj, AbstractSet):
-        _seen.add(obj_id)
-        try:
-            cleaned = set()
-            for item in obj:
-                if item is not _SKIP_VALUE:
-                    cleaned_item = _postprocess_from_dill(item, _seen)
-                    # é›†åˆä¸­ä¸èƒ½åŒ…å«Noneï¼Œä½†å¯ä»¥åŒ…å«Falseã€0ç­‰
-                    if cleaned_item is not None:
-                        cleaned.add(cleaned_item)
-            return type(obj)(cleaned)
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¤„ç†å¤æ‚å¯¹è±¡
-    if hasattr(obj, '__dict__'):
-        _seen.add(obj_id)
-        try:
-            # é€’å½’æ¸…ç†å±æ€§
-            for attr_name, attr_value in list(obj.__dict__.items()):
-                if attr_value is _SKIP_VALUE:
-                    # åˆ é™¤å“¨å…µå€¼å±æ€§
-                    try:
-                        delattr(obj, attr_name)
-                    except Exception:
-                        pass
-                else:
-                    # é€’å½’æ¸…ç†å±æ€§å€¼ï¼Œä¿ç•™æ‰€æœ‰åˆæ³•å€¼
-                    cleaned_value = _postprocess_from_dill(attr_value, _seen)
-                    try:
-                        setattr(obj, attr_name, cleaned_value)
-                    except Exception:
-                        pass
-            
-            return obj
-        finally:
-            _seen.remove(obj_id)
-    
-    return obj
-
-
-class UniversalSerializer:
-    """åŸºäºdillçš„é€šç”¨åºåˆ—åŒ–å™¨ï¼Œé¢„å¤„ç†æ¸…ç†ä¸å¯åºåˆ—åŒ–å†…å®¹"""
-    
-    @staticmethod
-    def serialize_object(obj: Any, 
-                        include: Optional[List[str]] = None,
-                        exclude: Optional[List[str]] = None) -> bytes:
-        """
-        åºåˆ—åŒ–ä»»æ„å¯¹è±¡
-        
-        Args:
-            obj: è¦åºåˆ—åŒ–çš„å¯¹è±¡
-            include: åŒ…å«çš„å±æ€§åˆ—è¡¨
-            exclude: æ’é™¤çš„å±æ€§åˆ—è¡¨
-            
-        Returns:
-            åºåˆ—åŒ–åçš„å­—èŠ‚æ•°æ®
-        """
-        if dill is None:
-            raise SerializationError("dill is required for serialization. Install with: pip install dill")
-        
-        try:
-            # é¢„å¤„ç†å¯¹è±¡ï¼Œæ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å†…å®¹
-            cleaned_obj = _preprocess_for_dill(obj)
-            
-            # ä½¿ç”¨dillåºåˆ—åŒ–
-            return dill.dumps(cleaned_obj)
-            
-        except Exception as e:
-            raise SerializationError(f"Object serialization failed: {e}")
-    
-    @staticmethod
-    def deserialize_object(data: bytes) -> Any:
-        """
-        ååºåˆ—åŒ–å¯¹è±¡
-        
-        Args:
-            data: åºåˆ—åŒ–çš„å­—èŠ‚æ•°æ®
-            
-        Returns:
-            ååºåˆ—åŒ–åçš„å¯¹è±¡
-        """
-        if dill is None:
-            raise SerializationError("dill is required for deserialization. Install with: pip install dill")
-        
-        try:
-            # ä½¿ç”¨dillååºåˆ—åŒ–
-            obj = dill.loads(data)
-            
-            # åå¤„ç†å¯¹è±¡ï¼Œæ¸…ç†å“¨å…µå€¼
-            return _postprocess_from_dill(obj)
-            
-        except Exception as e:
-            raise SerializationError(f"Object deserialization failed: {e}")
-    
-    @staticmethod
-    def save_object_state(obj: Any, path: str,
-                         include: Optional[List[str]] = None,
-                         exclude: Optional[List[str]] = None):
-        """å°†å¯¹è±¡çŠ¶æ€ä¿å­˜åˆ°æ–‡ä»¶"""
-        serialized_data = UniversalSerializer.serialize_object(obj, include, exclude)
-        
-        os.makedirs(os.path.dirname(path), exist_ok=True)
-        with open(path, 'wb') as f:
-            f.write(serialized_data)
-    
-    @staticmethod
-    def load_object_from_file(path: str) -> Any:
-        """ä»æ–‡ä»¶åŠ è½½å¯¹è±¡"""
-        if not os.path.isfile(path):
-            raise FileNotFoundError(f"File not found: {path}")
-        
-        with open(path, 'rb') as f:
-            data = f.read()
-        
-        return UniversalSerializer.deserialize_object(data)
-    
-    @staticmethod
-    def load_object_state(obj: Any, path: str) -> bool:
-        """ä»æ–‡ä»¶åŠ è½½å¯¹è±¡çŠ¶æ€åˆ°ç°æœ‰å¯¹è±¡"""
-        if not os.path.isfile(path):
-            return False
-        
-        try:
-            # åŠ è½½åºåˆ—åŒ–çš„å¯¹è±¡
-            loaded_obj = UniversalSerializer.load_object_from_file(path)
-            
-            # æ£€æŸ¥ç±»å‹æ˜¯å¦åŒ¹é…
-            if type(obj) != type(loaded_obj):
-                return False
-            
-            # å¤åˆ¶å±æ€§
-            if hasattr(loaded_obj, '__dict__'):
-                # æ£€æŸ¥å¯¹è±¡çš„include/excludeé…ç½®
-                include = getattr(obj, "__state_include__", [])
-                exclude = getattr(obj, "__state_exclude__", [])
-                
-                for attr_name, attr_value in loaded_obj.__dict__.items():
-                    # åº”ç”¨include/excludeè¿‡æ»¤
-                    if include and attr_name not in include:
-                        continue
-                    if attr_name in (exclude or []):
-                        continue
-                    
-                    try:
-                        setattr(obj, attr_name, attr_value)
-                    except Exception:
-                        pass
-            
-            return True
-            
-        except Exception:
-            return False
-
-
-# ä¾¿æ·å‡½æ•°
-def serialize_object(obj: Any, 
-                    include: Optional[List[str]] = None,
-                    exclude: Optional[List[str]] = None) -> bytes:
-    """åºåˆ—åŒ–å¯¹è±¡çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.serialize_object(obj, include, exclude)
-
-
-def deserialize_object(data: bytes) -> Any:
-    """ååºåˆ—åŒ–å¯¹è±¡çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.deserialize_object(data)
-
-
-def save_object_state(obj: Any, path: str,
-                     include: Optional[List[str]] = None,
-                     exclude: Optional[List[str]] = None):
-    """ä¿å­˜å¯¹è±¡çŠ¶æ€çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.save_object_state(obj, path, include, exclude)
-
-
-def load_object_from_file(path: str) -> Any:
-    """ä»æ–‡ä»¶åŠ è½½å¯¹è±¡çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.load_object_from_file(path)
-
-
-def load_object_state(obj: Any, path: str) -> bool:
-    """åŠ è½½å¯¹è±¡çŠ¶æ€çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.load_object_state(obj, path)
-
-
-# å‘åå…¼å®¹çš„å‡½æ•°
-def pack_object(obj: Any, 
-               include: Optional[List[str]] = None,
-               exclude: Optional[List[str]] = None) -> bytes:
-    """æ‰“åŒ…å¯¹è±¡çš„ä¾¿æ·å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰"""
-    return serialize_object(obj, include, exclude)
-
-
-def unpack_object(data: bytes) -> Any:
-    """è§£åŒ…å¯¹è±¡çš„ä¾¿æ·å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰"""
-    return deserialize_object(data)
-
-
-def trim_object_for_ray(obj: Any, 
-                       include: Optional[List[str]] = None,
-                       exclude: Optional[List[str]] = None) -> Any:
-    """
-    ä¸ºRayè¿œç¨‹è°ƒç”¨é¢„å¤„ç†å¯¹è±¡ï¼Œç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å†…å®¹
-    
-    è¿™ä¸ªå‡½æ•°åªåšæ¸…ç†å·¥ä½œï¼Œä¸è¿›è¡Œå®é™…çš„åºåˆ—åŒ–ï¼Œè®©Rayè‡ªå·±å¤„ç†åºåˆ—åŒ–è¿‡ç¨‹ã€‚
-    é€‚ç”¨äºåœ¨ray.remoteè°ƒç”¨å‰æ¸…ç†å¯¹è±¡ï¼Œé¿å…åºåˆ—åŒ–é”™è¯¯ã€‚
-    
-    Args:
-        obj: è¦é¢„å¤„ç†çš„å¯¹è±¡
-        include: åŒ…å«çš„å±æ€§åˆ—è¡¨ï¼ˆå¦‚æœæŒ‡å®šï¼Œåªä¿ç•™è¿™äº›å±æ€§ï¼‰
-        exclude: æ’é™¤çš„å±æ€§åˆ—è¡¨ï¼ˆè¿™äº›å±æ€§å°†è¢«ç§»é™¤ï¼‰
-        
-    Returns:
-        æ¸…ç†åçš„å¯¹è±¡ï¼Œå¯ä»¥å®‰å…¨åœ°ä¼ é€’ç»™Rayè¿›è¡Œåºåˆ—åŒ–
-        
-    Example:
-        # æ¸…ç†transformationå¯¹è±¡ç”¨äºRayè°ƒç”¨
-        cleaned_trans = trim_object_for_ray(transformation, 
-                                          exclude=['logger', 'env', '_operator_factory'])
-        
-        # ç°åœ¨å¯ä»¥å®‰å…¨åœ°ä¼ é€’ç»™Ray
-        result = ray_actor.process_transformation.remote(cleaned_trans)
-    """
-    try:
-        # ä½¿ç”¨ç°æœ‰çš„é¢„å¤„ç†å‡½æ•°ï¼Œä½†ä¸è¿›è¡Œdillåºåˆ—åŒ–
-        cleaned_obj = _preprocess_for_dill(obj)
-        
-        # å¦‚æœæœ‰é¢å¤–çš„include/excludeéœ€æ±‚ï¼Œå†æ¬¡è¿‡æ»¤
-        if cleaned_obj is not _SKIP_VALUE and hasattr(cleaned_obj, '__dict__'):
-            # åº”ç”¨ç”¨æˆ·æŒ‡å®šçš„include/exclude
-            if include or exclude:
-                attrs = _gather_attrs(cleaned_obj)
-                filtered_attrs = _filter_attrs(attrs, include, exclude)
-                
-                # åˆ›å»ºæ–°å¯¹è±¡å¹¶è®¾ç½®è¿‡æ»¤åçš„å±æ€§
-                obj_class = type(cleaned_obj)
-                try:
-                    final_obj = obj_class.__new__(obj_class)
-                    for attr_name, attr_value in filtered_attrs.items():
-                        try:
-                            setattr(final_obj, attr_name, attr_value)
-                        except Exception:
-                            pass  # å¿½ç•¥è®¾ç½®å¤±è´¥çš„å±æ€§
-                    return final_obj
-                except Exception:
-                    # å¦‚æœæ— æ³•åˆ›å»ºæ–°å®ä¾‹ï¼Œè¿”å›åŸå¯¹è±¡
-                    return cleaned_obj
-        
-        return cleaned_obj if cleaned_obj is not _SKIP_VALUE else None
-        
-    except Exception as e:
-        # å¦‚æœé¢„å¤„ç†å¤±è´¥ï¼Œè¿”å›Noneæˆ–æŠ›å‡ºå¼‚å¸¸
-        raise SerializationError(f"Object trimming for Ray failed: {e}")
-
-
-class RayObjectTrimmer:
-    """ä¸“é—¨ç”¨äºRayè¿œç¨‹è°ƒç”¨çš„å¯¹è±¡é¢„å¤„ç†å™¨"""
-    
-    @staticmethod
-    def trim_for_remote_call(obj: Any,
-                           include: Optional[List[str]] = None,
-                           exclude: Optional[List[str]] = None,
-                           deep_clean: bool = True) -> Any:
-        """
-        ä¸ºRayè¿œç¨‹è°ƒç”¨å‡†å¤‡å¯¹è±¡
-        
-        Args:
-            obj: è¦æ¸…ç†çš„å¯¹è±¡
-            include: åªä¿ç•™è¿™äº›å±æ€§
-            exclude: æ’é™¤è¿™äº›å±æ€§
-            deep_clean: æ˜¯å¦è¿›è¡Œæ·±åº¦æ¸…ç†ï¼ˆé€’å½’å¤„ç†åµŒå¥—å¯¹è±¡ï¼‰
-        
-        Returns:
-            æ¸…ç†åå¯ä»¥ä¼ é€’ç»™Rayçš„å¯¹è±¡
-        """
-        if not deep_clean:
-            # æµ…å±‚æ¸…ç†ï¼šåªå¤„ç†é¡¶å±‚å¯¹è±¡çš„å±æ€§
-            if hasattr(obj, '__dict__'):
-                attrs = _gather_attrs(obj)
-                filtered_attrs = _filter_attrs(attrs, include, exclude)
-                
-                obj_class = type(obj)
-                try:
-                    cleaned_obj = obj_class.__new__(obj_class)
-                    for attr_name, attr_value in filtered_attrs.items():
-                        if not _should_skip(attr_value):
-                            try:
-                                setattr(cleaned_obj, attr_name, attr_value)
-                            except Exception:
-                                pass
-                    return cleaned_obj
-                except Exception:
-                    return obj
-            return obj
-        else:
-            # æ·±åº¦æ¸…ç†ï¼šä½¿ç”¨å®Œæ•´çš„é¢„å¤„ç†æµç¨‹
-            return trim_object_for_ray(obj, include, exclude)
-    
-    @staticmethod
-    def trim_transformation_for_ray(transformation_obj) -> Any:
-        """
-        ä¸“é—¨ä¸ºTransformationå¯¹è±¡å®šåˆ¶çš„æ¸…ç†æ–¹æ³•
-        ç§»é™¤å¸¸è§çš„ä¸å¯åºåˆ—åŒ–å±æ€§
-        """
-        exclude_attrs = [
-            'logger', '_logger',           # æ—¥å¿—å¯¹è±¡
-            'env',                         # ç¯å¢ƒå¼•ç”¨ï¼ˆé¿å…å¾ªç¯å¼•ç”¨ï¼‰
-            'runtime_context',             # è¿è¡Œæ—¶ä¸Šä¸‹æ–‡
-            '_dag_node_factory',           # æ‡’åŠ è½½å·¥å‚
-            '_operator_factory',           # æ‡’åŠ è½½å·¥å‚  
-            '_function_factory',           # æ‡’åŠ è½½å·¥å‚
-            'server_socket',               # socketå¯¹è±¡
-            'server_thread', '_server_thread',  # çº¿ç¨‹å¯¹è±¡
-        ]
-        
-        return RayObjectTrimmer.trim_for_remote_call(
-            transformation_obj, 
-            exclude=exclude_attrs
-        )
-    
-    @staticmethod
-    def trim_operator_for_ray(operator_obj) -> Any:
-        """
-        ä¸“é—¨ä¸ºOperatorå¯¹è±¡å®šåˆ¶çš„æ¸…ç†æ–¹æ³•
-        """
-        exclude_attrs = [
-            'logger', '_logger',
-            'runtime_context',
-            'emit_context',
-            'server_socket', 'client_socket',
-            'server_thread', '_server_thread',
-            '__weakref__',
-        ]
-        
-        return RayObjectTrimmer.trim_for_remote_call(
-            operator_obj,
-            exclude=exclude_attrs
-        )
-    
-    @staticmethod
-    def validate_ray_serializable(obj: Any, max_depth: int = 3) -> Dict[str, Any]:
-        """
-        éªŒè¯å¯¹è±¡æ˜¯å¦å¯ä»¥è¢«Rayåºåˆ—åŒ–
-        
-        Args:
-            obj: è¦éªŒè¯çš„å¯¹è±¡
-            max_depth: æœ€å¤§æ£€æŸ¥æ·±åº¦
-        
-        Returns:
-            éªŒè¯ç»“æœå­—å…¸ï¼ŒåŒ…å«æ˜¯å¦å¯åºåˆ—åŒ–å’Œé—®é¢˜åˆ—è¡¨
-        """
-        import ray
-        
-        result = {
-            'is_serializable': False,
-            'issues': [],
-            'size_estimate': 0
-        }
-        
-        try:
-            # å°è¯•Rayçš„å†…éƒ¨åºåˆ—åŒ–
-            serialized = ray.cloudpickle.dumps(obj)
-            result['is_serializable'] = True
-            result['size_estimate'] = len(serialized)
-            
-        except Exception as e:
-            result['issues'].append(f"Ray serialization failed: {str(e)}")
-            
-            # å°è¯•è¯†åˆ«å…·ä½“çš„é—®é¢˜
-            if hasattr(obj, '__dict__'):
-                for attr_name, attr_value in obj.__dict__.items():
-                    if _should_skip(attr_value):
-                        result['issues'].append(f"Problematic attribute: {attr_name} = {type(attr_value)}")
-        
-        return result
\ No newline at end of file
diff --git a/sage_utils/serialization/universal_serializer.py b/sage_utils/serialization/universal_serializer.py
deleted file mode 100644
index 03177af..0000000
--- a/sage_utils/serialization/universal_serializer.py
+++ /dev/null
@@ -1,444 +0,0 @@
-import os
-import pickle
-import inspect
-import threading
-import importlib
-from typing import Any, Dict, List, Set, Type, Optional, Union
-from collections.abc import Mapping, Sequence, Set as AbstractSet
-
-
-class SerializationError(Exception):
-    """åºåˆ—åŒ–ç›¸å…³é”™è¯¯"""
-    pass
-
-
-# æ‰©å±•çš„ä¸å¯åºåˆ—åŒ–ç±»å‹é»‘åå•
-_BLACKLIST = (
-    type(open),        # æ–‡ä»¶å¥æŸ„
-    type(threading.Thread),  # çº¿ç¨‹
-    type(threading.Lock),    # é”
-    type(threading.RLock),   # é€’å½’é”
-    type(threading.Event),   # äº‹ä»¶
-    type(threading.Condition),  # æ¡ä»¶å˜é‡
-    type(lambda: None),   # å‡½æ•°ç±»å‹ï¼ˆé™¤éç‰¹æ®Šå¤„ç†ï¼‰
-    type(print),       # å†…ç½®å‡½æ•°
-    type(len),         # å†…ç½®å‡½æ•°
-)
-
-
-# åºåˆ—åŒ–æ—¶éœ€è¦æ’é™¤çš„å±æ€§å
-_ATTRIBUTE_BLACKLIST = {
-    'logger',          # æ—¥å¿—å¯¹è±¡
-    '_logger',         # ç§æœ‰æ—¥å¿—å¯¹è±¡
-    'server_socket',   # socketå¯¹è±¡
-    'server_thread',   # çº¿ç¨‹å¯¹è±¡
-    '_server_thread',  # ç§æœ‰çº¿ç¨‹å¯¹è±¡
-    'client_socket',   # socketå¯¹è±¡
-    '__weakref__',     # å¼±å¼•ç”¨
-    '__dict__',        # é˜²æ­¢é€’å½’
-    'runtime_context', # è¿è¡Œæ—¶ä¸Šä¸‹æ–‡
-    'memory_collection', # å†…å­˜é›†åˆï¼ˆé€šå¸¸æ˜¯Ray Actorå¥æŸ„ï¼‰
-    'env',             # ç¯å¢ƒå¼•ç”¨ï¼ˆé¿å…å¾ªç¯å¼•ç”¨ï¼‰
-    '_dag_node_factory',  # å·¥å‚å¯¹è±¡
-    '_operator_factory',  # å·¥å‚å¯¹è±¡
-    '_function_factory',  # å·¥å‚å¯¹è±¡
-}
-
-
-def _gather_attrs(obj):
-    """æšä¸¾å®ä¾‹ __dict__ å’Œ @property å±æ€§ã€‚"""
-    attrs = dict(getattr(obj, "__dict__", {}))
-    for name, prop in inspect.getmembers(type(obj), lambda x: isinstance(x, property)):
-        try:
-            attrs[name] = getattr(obj, name)
-        except Exception:
-            pass
-    return attrs
-
-
-def _filter_attrs(attrs, include, exclude):
-    """æ ¹æ® include/exclude è¿‡æ»¤å­—æ®µå­—å…¸ã€‚"""
-    if include:
-        return {k: attrs[k] for k in include if k in attrs}
-    
-    # åˆå¹¶ç”¨æˆ·å®šä¹‰çš„excludeå’Œç³»ç»Ÿé»˜è®¤çš„exclude
-    all_exclude = set(exclude or []) | _ATTRIBUTE_BLACKLIST
-    return {k: v for k, v in attrs.items() if k not in all_exclude}
-
-
-def _is_serializable(v):
-    print(f"Checking serializability of value: {v} (type: {type(v)})")
-    """åˆ¤æ–­å¯¹è±¡èƒ½å¦é€šè¿‡ pickle åºåˆ—åŒ–ï¼Œä¸”ä¸åœ¨é»‘åå•ä¸­ã€‚"""
-    if isinstance(v, _BLACKLIST):
-        print(f"Value {v} is in blacklist, not serializable")
-        return False
-    
-    # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å—ï¼ˆæ¨¡å—é€šå¸¸ä¸åº”è¯¥åºåˆ—åŒ–ï¼‰
-    if inspect.ismodule(v):
-        print(f"Value {v} is a module, not serializable")
-        return False
-    
-    # æ£€æŸ¥æ˜¯å¦æ˜¯ç±»ï¼ˆç±»å®šä¹‰é€šè¿‡ç±»è·¯å¾„åºåˆ—åŒ–ï¼Œè¿™é‡Œè¿”å›Falseè®©å…¶ä»–é€»è¾‘å¤„ç†ï¼‰
-    if inspect.isclass(v):
-        return False
-    
-    try:
-        pickle.dumps(v)
-        return True
-    except Exception:
-        print(f"exception not serializable")
-        return False
-
-
-def _is_complex_object(v):
-    """åˆ¤æ–­æ˜¯å¦æ˜¯éœ€è¦é€’å½’åºåˆ—åŒ–çš„å¤æ‚å¯¹è±¡"""
-    # åŸºæœ¬ç±»å‹ä¸éœ€è¦é€’å½’
-    if isinstance(v, (int, float, str, bool, type(None))):
-        return False
-    
-    # å®¹å™¨ç±»å‹éœ€è¦é€’å½’å¤„ç†å†…å®¹ï¼Œä½†ä¸æ˜¯å¤æ‚å¯¹è±¡æœ¬èº«
-    if isinstance(v, (Mapping, Sequence, AbstractSet)) and not isinstance(v, str):
-        return False
-    
-    # æœ‰__dict__å±æ€§çš„å¯¹è±¡é€šå¸¸æ˜¯å¤æ‚å¯¹è±¡
-    if hasattr(v, '__dict__'):
-        return True
-    
-    return False
-def _can_be_serialized(v):
-    """åˆ¤æ–­å¯¹è±¡æ˜¯å¦å¯ä»¥è¢«åºåˆ—åŒ–ï¼ˆç›´æ¥pickleæˆ–é€šè¿‡é€’å½’ç»“æ„ï¼‰"""
-    # å…ˆæ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
-    if isinstance(v, _BLACKLIST):
-        return False
-    
-    # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å—
-    if inspect.ismodule(v):
-        return False
-    
-    # æ£€æŸ¥æ˜¯å¦æ˜¯ç±»
-    if inspect.isclass(v):
-        return True
-    
-    # å¦‚æœå¯ä»¥ç›´æ¥pickleï¼Œå½“ç„¶å¯ä»¥åºåˆ—åŒ–
-    if _is_serializable(v):
-        return True
-    
-    # å¦‚æœæ˜¯å¤æ‚å¯¹è±¡ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥é€šè¿‡é€’å½’ç»“æ„åºåˆ—åŒ–
-    if _is_complex_object(v):
-        return True
-    
-    return False
-
-
-# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å“¨å…µå€¼
-_SKIP_VALUE = object()  # ç”¨ä½œå“¨å…µå€¼ï¼Œè¡¨ç¤ºåº”è¯¥è·³è¿‡çš„å€¼
-
-def _prepare(v, _seen=None):
-    """é€’å½’æ¸…æ´—å®¹å™¨ç±»å‹ï¼Œè¿‡æ»¤ä¸å¯åºåˆ—åŒ–å…ƒç´ ã€‚"""
-    if _seen is None:
-        _seen = set()
-    
-    # é˜²æ­¢å¾ªç¯å¼•ç”¨
-    obj_id = id(v)
-    if obj_id in _seen:
-        return _SKIP_VALUE
-    
-    # åŸºæœ¬ç±»å‹ç›´æ¥è¿”å›
-    if isinstance(v, (int, float, str, bool, type(None))):
-        return v
-
-    # å¤„ç†ç±»å¯¹è±¡ - åºåˆ—åŒ–ä¸ºç±»è·¯å¾„
-    if inspect.isclass(v):
-        return {
-            '__class_reference__': _get_class_path(v),
-            '__serializer_version__': '1.0'
-        }
-
-    # å¤„ç†å­—å…¸
-    if isinstance(v, Mapping):
-        _seen.add(obj_id)
-        try:
-            result = {}
-            for k, val in v.items():
-                print(f"Processing dict item: {k} -> {val}")
-                if _can_be_serialized(k) and _can_be_serialized(val):
-                    prepared_k = _prepare(k, _seen)
-                    prepared_v = _prepare(val, _seen)
-                    # åªè¿‡æ»¤æ‰å“¨å…µå€¼
-                    if prepared_k is not _SKIP_VALUE and prepared_v is not _SKIP_VALUE:
-                        result[prepared_k] = prepared_v
-                        print(f"Added to result: {prepared_k} -> {prepared_v}")
-            return result
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¤„ç†åºåˆ—ï¼ˆåˆ—è¡¨ã€å…ƒç»„ç­‰ï¼‰
-    if isinstance(v, Sequence) and not isinstance(v, str):
-        _seen.add(obj_id)
-        try:
-            cleaned = []
-            for x in v:
-                if _can_be_serialized(x):
-                    prepared_x = _prepare(x, _seen)
-                    # åªè¿‡æ»¤æ‰å“¨å…µå€¼
-                    if prepared_x is not _SKIP_VALUE:
-                        cleaned.append(prepared_x)
-            return type(v)(cleaned) if cleaned else []
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¤„ç†é›†åˆ
-    if isinstance(v, AbstractSet):
-        _seen.add(obj_id)
-        try:
-            cleaned = set()
-            for x in v:
-                if _can_be_serialized(x):
-                    prepared_x = _prepare(x, _seen)
-                    # åªè¿‡æ»¤æ‰å“¨å…µå€¼
-                    if prepared_x is not _SKIP_VALUE:
-                        cleaned.add(prepared_x)
-            return type(v)(cleaned) if cleaned else set()
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¤„ç†å¤æ‚å¯¹è±¡
-    if _is_complex_object(v):
-        _seen.add(obj_id)
-        try:
-            return _serialize_complex_object(v, _seen)
-        finally:
-            _seen.remove(obj_id)
-    
-    # å¯¹äºå…¶ä»–å¯ç›´æ¥åºåˆ—åŒ–çš„å¯¹è±¡ï¼Œç›´æ¥è¿”å›
-    if _is_serializable(v):
-        return v
-    
-    # ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡è¿”å›å“¨å…µå€¼
-    return _SKIP_VALUE
-
-
-def _serialize_complex_object(obj, _seen=None):
-    # print("serializing complex object:", obj)
-    """é€’å½’åºåˆ—åŒ–å¤æ‚å¯¹è±¡çš„å†…éƒ¨ç»“æ„"""
-    if _seen is None:
-        _seen = set()
-    
-    # è·å–å¯¹è±¡çš„ç±»ä¿¡æ¯
-    obj_class = type(obj)
-    
-    # æ£€æŸ¥å¯¹è±¡æ˜¯å¦æœ‰è‡ªå®šä¹‰çš„åºåˆ—åŒ–é…ç½®
-    custom_include = getattr(obj, "__state_include__", [])
-    custom_exclude = getattr(obj, "__state_exclude__", [])
-    # print("custom_exclude:", custom_exclude)
-    # æ”¶é›†æ‰€æœ‰å±æ€§
-    attrs = _gather_attrs(obj)
-    
-    # è¿‡æ»¤å±æ€§
-    filtered_attrs = _filter_attrs(attrs, custom_include, custom_exclude)
-    
-    # å‡†å¤‡åºåˆ—åŒ–æ•°æ® - é€’å½’å¤„ç†æ¯ä¸ªå±æ€§
-    prepared_attrs = {}
-    for k, v in filtered_attrs.items():
-        print(f"Preparing attribute: {k} -> {v}")
-        prepared_value = _prepare(v, _seen)
-        # åªè¿‡æ»¤æ‰å“¨å…µå€¼
-        if prepared_value is not _SKIP_VALUE:
-            prepared_attrs[k] = prepared_value
-            print(f"Prepared attribute: {k} -> {prepared_value}")
-    
-    # æ„å»ºåºåˆ—åŒ–æ•°æ®
-    return {
-        '__class_path__': _get_class_path(obj_class),
-        '__attributes__': prepared_attrs,
-        '__serializer_version__': '1.0'
-    }
-
-def _get_class_path(cls):
-    """è·å–ç±»çš„å®Œæ•´è·¯å¾„"""
-    return f"{cls.__module__}.{cls.__qualname__}"
-
-
-def _load_class(class_path: str) -> Type:
-    """åŠ¨æ€åŠ è½½ç±»"""
-    try:
-        module_name, class_name = class_path.rsplit('.', 1)
-        module = importlib.import_module(module_name)
-        return getattr(module, class_name)
-    except Exception as e:
-        raise SerializationError(f"Failed to load class {class_path}: {e}")
-
-
-def _restore_value(value):
-    """é€’å½’æ¢å¤å€¼"""
-    if isinstance(value, dict) and '__class_path__' in value:
-        # è¿™æ˜¯ä¸€ä¸ªåºåˆ—åŒ–çš„å¤æ‚å¯¹è±¡
-        return _deserialize_complex_object(value)
-    elif isinstance(value, dict) and '__class_reference__' in value:
-        # è¿™æ˜¯ä¸€ä¸ªç±»å¼•ç”¨
-        class_path = value['__class_reference__']
-        return _load_class(class_path)
-    elif isinstance(value, list):
-        return [_restore_value(item) for item in value]
-    elif isinstance(value, dict):
-        return {k: _restore_value(v) for k, v in value.items()}
-    elif isinstance(value, set):
-        return {_restore_value(item) for item in value}
-    else:
-        return value
-
-
-def _deserialize_complex_object(data: Dict[str, Any]) -> Any:
-    """ååºåˆ—åŒ–å¤æ‚å¯¹è±¡"""
-    # éªŒè¯æ•°æ®æ ¼å¼
-    if not isinstance(data, dict) or '__class_path__' not in data:
-        raise SerializationError("Invalid serialized data format")
-    
-    # åŠ è½½ç±»
-    class_path = data['__class_path__']
-    obj_class = _load_class(class_path)
-    
-    try:
-        obj = obj_class.__new__(obj_class)
-    except Exception:
-        raise SerializationError(f"Cannot create instance of {class_path}")
-    
-    # æ¢å¤å±æ€§
-    attributes = data.get('__attributes__', {})
-    for attr_name, attr_value in attributes.items():
-        try:
-            # é€’å½’ååºåˆ—åŒ–å±æ€§å€¼
-            restored_value = _restore_value(attr_value)
-            setattr(obj, attr_name, restored_value)
-        except Exception as e:
-            # å¿½ç•¥è®¾ç½®å¤±è´¥çš„å±æ€§ï¼Œä½†è®°å½•æ—¥å¿—
-            pass
-    
-    return obj
-
-
-class UniversalSerializer:
-    """é€šç”¨åºåˆ—åŒ–å™¨ï¼ŒåŸºäºåé€‰æœºåˆ¶è‡ªåŠ¨å¤„ç†æ‰€æœ‰å¯åºåˆ—åŒ–å¯¹è±¡"""
-    
-    @staticmethod
-    def serialize_object(obj: Any, 
-                        include: Optional[List[str]] = None,
-                        exclude: Optional[List[str]] = None) -> Dict[str, Any]:
-        """åºåˆ—åŒ–ä»»æ„å¯¹è±¡"""
-        try:
-            # ä½¿ç”¨_serialize_complex_objectæ¥å¤„ç†é¡¶å±‚å¯¹è±¡
-            return _serialize_complex_object(obj)
-            
-        except Exception as e:
-            raise SerializationError(f"Object serialization failed: {e}")
-    
-    @staticmethod
-    def deserialize_object(data: Dict[str, Any]) -> Any:
-        """ååºåˆ—åŒ–å¯¹è±¡"""
-        try:
-            return _deserialize_complex_object(data)
-            
-        except Exception as e:
-            raise SerializationError(f"Object deserialization failed: {e}")
-    
-    @staticmethod
-    def pack_object(obj: Any, 
-                   include: Optional[List[str]] = None,
-                   exclude: Optional[List[str]] = None) -> bytes:
-        """å°†å¯¹è±¡æ‰“åŒ…ä¸ºäºŒè¿›åˆ¶æ•°æ®"""
-        serialized_data = UniversalSerializer.serialize_object(obj, include, exclude)
-        return pickle.dumps(serialized_data)
-    
-    @staticmethod
-    def unpack_object(data: bytes, 
-                     constructor_args: Optional[tuple] = None,
-                     constructor_kwargs: Optional[dict] = None) -> Any:
-        """ä»äºŒè¿›åˆ¶æ•°æ®è§£åŒ…å¯¹è±¡"""
-        serialized_data = pickle.loads(data)
-        return UniversalSerializer.deserialize_object(serialized_data, constructor_args, constructor_kwargs)
-    
-    @staticmethod
-    def save_object_state(obj: Any, path: str,
-                         include: Optional[List[str]] = None,
-                         exclude: Optional[List[str]] = None):
-        """å°†å¯¹è±¡çŠ¶æ€ä¿å­˜åˆ°æ–‡ä»¶"""
-        serialized_data = UniversalSerializer.serialize_object(obj, include, exclude)
-        
-        os.makedirs(os.path.dirname(path), exist_ok=True)
-        with open(path, 'wb') as f:
-            pickle.dump(serialized_data, f)
-    
-    @staticmethod
-    def load_object_state(obj: Any, path: str) -> bool:
-        """ä»æ–‡ä»¶åŠ è½½å¯¹è±¡çŠ¶æ€"""
-        if not os.path.isfile(path):
-            return False
-        
-        try:
-            with open(path, 'rb') as f:
-                serialized_data = pickle.load(f)
-            
-            # åªæ¢å¤å±æ€§ï¼Œä¸åˆ›å»ºæ–°å¯¹è±¡
-            attributes = serialized_data.get('__attributes__', {})
-            
-            # æ£€æŸ¥å¯¹è±¡çš„include/excludeé…ç½®
-            include = getattr(obj, "__state_include__", [])
-            exclude = getattr(obj, "__state_exclude__", [])
-            
-            for attr_name, attr_value in attributes.items():
-                # åº”ç”¨include/excludeè¿‡æ»¤
-                if include and attr_name not in include:
-                    continue
-                if attr_name in (exclude or []):
-                    continue
-                
-                try:
-                    restored_value = _restore_value(attr_value)
-                    setattr(obj, attr_name, restored_value)
-                except Exception:
-                    # å¿½ç•¥è®¾ç½®å¤±è´¥çš„å±æ€§
-                    pass
-            
-            return True
-            
-        except Exception as e:
-            return False
-
-
-# ä¾¿æ·å‡½æ•°
-def serialize_object(obj: Any, 
-                    include: Optional[List[str]] = None,
-                    exclude: Optional[List[str]] = None) -> Dict[str, Any]:
-    """åºåˆ—åŒ–å¯¹è±¡çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.serialize_object(obj, include, exclude)
-
-
-def deserialize_object(data: Dict[str, Any]) -> Any:
-    """ååºåˆ—åŒ–å¯¹è±¡çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.deserialize_object(data)
-
-
-def pack_object(obj: Any, 
-               include: Optional[List[str]] = None,
-               exclude: Optional[List[str]] = None) -> bytes:
-    """æ‰“åŒ…å¯¹è±¡çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.pack_object(obj, include, exclude)
-
-
-def unpack_object(data: bytes, 
-                 constructor_args: Optional[tuple] = None,
-                 constructor_kwargs: Optional[dict] = None) -> Any:
-    """è§£åŒ…å¯¹è±¡çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.unpack_object(data, constructor_args, constructor_kwargs)
-
-
-def save_object_state(obj: Any, path: str,
-                     include: Optional[List[str]] = None,
-                     exclude: Optional[List[str]] = None):
-    """ä¿å­˜å¯¹è±¡çŠ¶æ€çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.save_object_state(obj, path, include, exclude)
-
-
-def load_object_state(obj: Any, path: str) -> bool:
-    """åŠ è½½å¯¹è±¡çŠ¶æ€çš„ä¾¿æ·å‡½æ•°"""
-    return UniversalSerializer.load_object_state(obj, path)
\ No newline at end of file
diff --git a/sage_tests/function_tests/io_tests/test_print_functionality.py b/tests/test_print_functionality.py
similarity index 98%
rename from sage_tests/function_tests/io_tests/test_print_functionality.py
rename to tests/test_print_functionality.py
index f2b49e3..098abe5 100644
--- a/sage_tests/function_tests/io_tests/test_print_functionality.py
+++ b/tests/test_print_functionality.py
@@ -8,11 +8,11 @@ import os
 from io import StringIO
 from unittest.mock import patch
 
-from sage_libs.io.sink import PrintSink
-
 # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
 sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 
+from sage_common_funs.io.sink import PrintSink
+
 
 class TestPrintSink(unittest.TestCase):
     """æµ‹è¯• PrintSink ç±»çš„åŠŸèƒ½"""
diff --git a/third-party/kafka/sage-kafka-stop.sh b/third-party/kafka/sage-kafka-stop.sh
new file mode 100644
index 0000000..4591d54
--- /dev/null
+++ b/third-party/kafka/sage-kafka-stop.sh
@@ -0,0 +1,22 @@
+#!/bin/bash
+# SAGE Kafka Stop Script
+
+KAFKA_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
+
+echo "Stopping SAGE Kafka services..."
+
+# Stop Kafka
+if pgrep -f "sage-server.properties" > /dev/null; then
+    echo "Stopping Kafka..."
+    "$KAFKA_DIR/bin/kafka-server-stop.sh"
+    sleep 2
+fi
+
+# Stop Zookeeper
+if pgrep -f "sage-zookeeper.properties" > /dev/null; then
+    echo "Stopping Zookeeper..."
+    "$KAFKA_DIR/bin/zookeeper-server-stop.sh"
+    sleep 2
+fi
+
+echo "SAGE Kafka services stopped"
\ No newline at end of file
