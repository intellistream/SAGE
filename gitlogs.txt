diff --git a/README.md b/README.md
index 307c678..51e2d85 100644
--- a/README.md
+++ b/README.md
@@ -255,7 +255,7 @@ env = RemoteEnvironment()
 #### 📘 About Long Running
 If your pipeline is meant to run as a long-lived service, use:
 ```python
- # deprecated
+env.run_streaming() 
 ```
 
 See more examples under [sage_examples](sage_examples)
@@ -298,7 +298,7 @@ Sage Engine is the core execution component that orchestrates the compilation an
 The Engine operates in four main phases:
 
 1. **Pipeline Collection**: Gathers user-defined logical pipelines built through DataStream API and validates pipeline integrity
-2. **Compilation & Optimization**: Uses ExecutionGraph to transform logical pipelines into optimized physical execution graphs with parallelism expansion
+2. **Compilation & Optimization**: Uses Compiler to transform logical pipelines into optimized physical execution graphs with parallelism expansion
 3. **Runtime Scheduling**: Selects appropriate Runtime (local/distributed) and converts execution graphs into concrete DAG nodes
 4. **Execution Monitoring**: Monitors pipeline execution status, collects performance metrics, and handles fault recovery
 
diff --git a/config/config_ray.yaml b/config/config_ray.yaml
index b5f8457..71eb3e2 100644
--- a/config/config_ray.yaml
+++ b/config/config_ray.yaml
@@ -43,4 +43,4 @@ promptor:
 
 sink:
   platform: "remote"
-  file_path: "/home/tjy/SAGE/output/response.txt"
+
diff --git a/deployment/README.md b/deployment/README.md
deleted file mode 100644
index 1375397..0000000
--- a/deployment/README.md
+++ /dev/null
@@ -1 +0,0 @@
-# 如何架设 SAGE 集群以及启动的流程
\ No newline at end of file
diff --git a/deployment/jobmanager_controller.py b/deployment/jobmanager_controller.py
deleted file mode 100755
index 4a9c34f..0000000
--- a/deployment/jobmanager_controller.py
+++ /dev/null
@@ -1,873 +0,0 @@
-#!/usr/bin/env python3
-"""
-SAGE JobManager Controller
-命令行工具，用于管理和监控 JobManager 作业
-"""
-
-import argparse
-import json
-import sys
-import time
-import os
-from pathlib import Path
-from typing import Dict, List, Any, Optional
-from datetime import datetime
-import signal
-
-# 添加项目路径
-project_root = Path(__file__).parent.parent
-sys.path.append(str(project_root))
-
-try:
-    from sage_core.jobmanager_client import JobManagerClient
-    from sage_utils.actor_wrapper import ActorWrapper
-    import yaml
-    from tabulate import tabulate
-    from colorama import Fore, Back, Style, init
-    import click
-except ImportError as e:
-    print(f"Missing required dependencies: {e}")
-    print("Please install: pip install pyyaml tabulate colorama click")
-    sys.exit(1)
-
-# 初始化colorama
-init(autoreset=True)
-
-class ControllerError(Exception):
-    """Controller基础异常"""
-    pass
-
-class ConnectionError(ControllerError):
-    """连接异常"""
-    pass
-
-class JobNotFoundError(ControllerError):
-    """作业未找到异常"""
-    pass
-
-class JobManagerController:
-    """JobManager控制器"""
-    
-    def __init__(self, daemon_host: str = "127.0.0.1", daemon_port: int = 19001):
-        self.daemon_host = daemon_host
-        self.daemon_port = daemon_port
-        self.client: Optional[JobManagerClient] = None
-        self.jobmanager: Optional[ActorWrapper] = None
-        self.connected = False
-        
-        # 加载配置
-        self.config = self._load_config()
-        
-    def _load_config(self) -> Dict[str, Any]:
-        """加载配置文件"""
-        config_path = Path.home() / ".sage" / "config.yaml"
-        default_config = {
-            "daemon": {
-                "host": "127.0.0.1",
-                "port": 19001
-            },
-            "output": {
-                "format": "table",
-                "colors": True
-            },
-            "monitor": {
-                "refresh_interval": 5
-            }
-        }
-        
-        if config_path.exists():
-            try:
-                with open(config_path) as f:
-                    user_config = yaml.safe_load(f) or {}
-                    # 合并配置
-                    default_config.update(user_config)
-            except Exception as e:
-                self._print_warning(f"Failed to load config: {e}")
-        
-        return default_config
-    
-    def connect(self) -> bool:
-        """连接到JobManager"""
-        try:
-            self._print_info(f"Connecting to JobManager daemon at {self.daemon_host}:{self.daemon_port}...")
-            
-            self.client = JobManagerClient(self.daemon_host, self.daemon_port)
-            
-            # 健康检查
-            health = self.client.health_check()
-            if health.get("status") != "success":
-                raise ConnectionError(f"Daemon health check failed: {health.get('message')}")
-            
-            # 获取JobManager句柄
-            self.jobmanager = self.client.get_actor_handle()
-            self.connected = True
-            
-            self._print_success("Connected to JobManager successfully")
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to connect: {e}")
-            self.connected = False
-            return False
-    
-    def disconnect(self):
-        """断开连接"""
-        self.connected = False
-        self.client = None
-        self.jobmanager = None
-    
-    def ensure_connected(self):
-        """确保已连接"""
-        if not self.connected:
-            if not self.connect():
-                raise ConnectionError("Not connected to JobManager")
-    
-    def _resolve_job_identifier(self, identifier: str) -> Optional[str]:
-        """解析作业标识符（可以是作业编号或UUID）"""
-        try:
-            self.ensure_connected()
-            
-            # 获取作业列表
-            jobs = self.jobmanager.list_jobs()
-            
-            # 如果是数字，当作作业编号处理
-            if identifier.isdigit():
-                job_index = int(identifier) - 1  # 转换为0基索引
-                if 0 <= job_index < len(jobs):
-                    return jobs[job_index].get('uuid')
-                else:
-                    self._print_error(f"Job number {identifier} is out of range (1-{len(jobs)})")
-                    return None
-            
-            # 如果是UUID（完整或部分）
-            # 首先尝试精确匹配
-            for job in jobs:
-                if job.get('uuid') == identifier:
-                    return identifier
-            
-            # 然后尝试前缀匹配
-            matching_jobs = [job for job in jobs if job.get('uuid', '').startswith(identifier)]
-            
-            if len(matching_jobs) == 1:
-                return matching_jobs[0].get('uuid')
-            elif len(matching_jobs) > 1:
-                self._print_error(f"Ambiguous job identifier '{identifier}'. Matches:")
-                for i, job in enumerate(matching_jobs, 1):
-                    print(f"  {i}. {job.get('uuid')} ({job.get('name', 'unknown')})")
-                return None
-            else:
-                self._print_error(f"No job found matching '{identifier}'")
-                return None
-                
-        except Exception as e:
-            self._print_error(f"Failed to resolve job identifier: {e}")
-            return None
-    
-    # ==================== 命令实现 ====================
-    
-    def cmd_list(self, status_filter: Optional[str] = None, format_type: str = "table", full_uuid: bool = False) -> bool:
-        """列出所有作业"""
-        try:
-            self.ensure_connected()
-            
-            # 获取作业列表
-            jobs = self.jobmanager.list_jobs()
-            
-            # 状态过滤
-            if status_filter:
-                jobs = [job for job in jobs if job.get("status") == status_filter]
-            
-            # 格式化输出
-            if format_type == "json":
-                print(json.dumps({"jobs": jobs}, indent=2))
-            else:
-                self._format_job_table(jobs, short_uuid=not full_uuid)
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to list jobs: {e}")
-            return False
-    
-    def cmd_show(self, job_identifier: str, verbose: bool = False) -> bool:
-        """显示作业详情"""
-        try:
-            # 解析作业标识符
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-            
-            self.ensure_connected()
-            
-            # 获取作业状态
-            job_info = self.jobmanager.get_job_status(job_uuid)
-            
-            if not job_info:
-                raise JobNotFoundError(f"Job {job_uuid} not found")
-            
-            self._format_job_details(job_info, verbose)
-            return True
-            
-        except JobNotFoundError as e:
-            self._print_error(str(e))
-            return False
-        except Exception as e:
-            self._print_error(f"Failed to show job: {e}")
-            return False
-    
-    def cmd_stop(self, job_identifier: str, force: bool = False) -> bool:
-        """停止作业"""
-        try:
-            # 解析作业标识符
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            # 确认操作
-            if not force:
-                # 显示作业信息用于确认
-                job_info = self.jobmanager.get_job_status(job_uuid)
-                if job_info:
-                    job_name = job_info.get('name', 'unknown')
-                    print(f"Job to stop: {job_name} ({job_uuid})")
-                
-                if not click.confirm(f"Are you sure you want to stop this job?"):
-                    self._print_info("Operation cancelled")
-                    return True
-            
-            # 停止作业
-            result = self.jobmanager.pause_job(job_uuid)
-            
-            if result.get("status") == "stopped":
-                self._print_success(f"Job {job_uuid[:8]}... stopped successfully")
-            else:
-                self._print_error(f"Failed to stop job: {result.get('message')}")
-                return False
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to stop job: {e}")
-            return False
-    
-    def cmd_status(self, job_identifier: str) -> bool:
-        """获取作业状态"""
-        try:
-            # 解析作业标识符
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            job_info = self.jobmanager.get_job_status(job_uuid)
-            
-            if not job_info:
-                raise JobNotFoundError(f"Job {job_uuid} not found")
-            
-            status = job_info.get("status", "unknown")
-            job_name = job_info.get("name", "unknown")
-            self._print_status_colored(f"Job '{job_name}' ({job_uuid[:8]}...) status: {status}")
-            
-            return True
-            
-        except JobNotFoundError as e:
-            self._print_error(str(e))
-            return False
-        except Exception as e:
-            self._print_error(f"Failed to get job status: {e}")
-            return False
-    
-    def cmd_info(self) -> bool:
-        """显示JobManager系统信息"""
-        try:
-            self.ensure_connected()
-            
-            # 获取系统信息
-            info = self.jobmanager.get_server_info()
-            
-            print(f"\n{Fore.CYAN}=== JobManager System Information ==={Style.RESET_ALL}")
-            print(f"Session ID: {info.get('session_id')}")
-            print(f"Log Directory: {info.get('log_base_dir')}")
-            print(f"Total Jobs: {info.get('environments_count', 0)}")
-            
-            # 统计作业状态
-            jobs = info.get('jobs', [])
-            status_counts = {}
-            for job in jobs:
-                status = job.get('status', 'unknown')
-                status_counts[status] = status_counts.get(status, 0) + 1
-            
-            if status_counts:
-                print(f"\nJob Status Summary:")
-                for status, count in status_counts.items():
-                    print(f"  {status}: {count}")
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to get system info: {e}")
-            return False
-    
-    def cmd_health(self) -> bool:
-        """健康检查"""
-        try:
-            if not self.client:
-                self.client = JobManagerClient(self.daemon_host, self.daemon_port)
-            
-            health = self.client.health_check()
-            
-            if health.get("status") == "success":
-                self._print_success("JobManager is healthy")
-                
-                daemon_status = health.get("daemon_status", {})
-                print(f"Daemon: {daemon_status.get('socket_service')}")
-                print(f"Actor: {daemon_status.get('actor_name')}@{daemon_status.get('namespace')}")
-                
-                return True
-            else:
-                self._print_warning(f"Health check warning: {health.get('message')}")
-                return False
-                
-        except Exception as e:
-            self._print_error(f"Health check failed: {e}")
-            return False
-    
-    def cmd_continue(self, job_identifier: str, force: bool = False) -> bool:
-        """继续作业"""
-        try:
-            # 解析作业标识符
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            # 确认操作
-            if not force:
-                job_info = self.jobmanager.get_job_status(job_uuid)
-                if job_info:
-                    job_name = job_info.get('name', 'unknown')
-                    print(f"Job to continue: {job_name} ({job_uuid})")
-                
-                if not click.confirm(f"Are you sure you want to continue this job?"):
-                    self._print_info("Operation cancelled")
-                    return True
-            
-            # 重启作业
-            result = self.jobmanager.continue_job(job_uuid)
-            
-            if result.get("status") == "running":
-                self._print_success(f"Job {job_uuid[:8]}... continued successfully")
-            else:
-                self._print_error(f"Failed to continue job: {result.get('message')}")
-                return False
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to continue job: {e}")
-            return False
-
-    def cmd_delete(self, job_identifier: str, force: bool = False) -> bool:
-        """删除作业"""
-        try:
-            # 解析作业标识符
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            # 确认操作
-            if not force:
-                job_info = self.jobmanager.get_job_status(job_uuid)
-                if job_info:
-                    job_name = job_info.get('name', 'unknown')
-                    job_status = job_info.get('status', 'unknown')
-                    print(f"Job to delete: {job_name} ({job_uuid})")
-                    print(f"Current status: {job_status}")
-                
-                if not click.confirm(f"Are you sure you want to delete this job? This action cannot be undone."):
-                    self._print_info("Operation cancelled")
-                    return True
-            
-            # 删除作业
-            result = self.jobmanager.delete_job(job_uuid, force=force)
-            
-            if result.get("status") == "deleted":
-                self._print_success(f"Job {job_uuid[:8]}... deleted successfully")
-            else:
-                self._print_error(f"Failed to delete job: {result.get('message')}")
-                return False
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to delete job: {e}")
-            return False
-
-
-    def cmd_cleanup(self, force: bool = False) -> bool:
-        """清理所有作业"""
-        try:
-            self.ensure_connected()
-            
-            # 确认操作
-            if not force:
-                jobs = self.jobmanager.list_jobs()
-                if not jobs:
-                    self._print_info("No jobs to cleanup")
-                    return True
-                
-                print(f"Found {len(jobs)} jobs to cleanup:")
-                for job in jobs:
-                    print(f"  - {job.get('name')} ({job.get('uuid')[:8]}...) [{job.get('status')}]")
-                
-                if not click.confirm(f"Are you sure you want to cleanup all {len(jobs)} jobs?"):
-                    self._print_info("Operation cancelled")
-                    return True
-            
-            # 清理所有作业
-            result = self.jobmanager.cleanup_all_jobs()
-            
-            if result.get("status") == "success":
-                self._print_success(result.get("message"))
-            else:
-                self._print_error(f"Failed to cleanup jobs: {result.get('message')}")
-                return False
-            
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Failed to cleanup jobs: {e}")
-            return False
-
-    def cmd_monitor(self, refresh_interval: int = 5) -> bool:
-        """实时监控所有作业"""
-        try:
-            self.ensure_connected()
-            
-            self._print_info(f"Monitoring jobs (refresh every {refresh_interval}s, press Ctrl+C to stop)")
-            
-            # 设置信号处理
-            def signal_handler(signum, frame):
-                print("\nMonitoring stopped")
-                sys.exit(0)
-            
-            signal.signal(signal.SIGINT, signal_handler)
-            
-            while True:
-                # 清屏
-                os.system('clear' if os.name == 'posix' else 'cls')
-                
-                # 显示标题
-                print(f"{Fore.CYAN}=== SAGE JobManager Monitor ==={Style.RESET_ALL}")
-                print(f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
-                print()
-                
-                # 获取并显示作业列表
-                jobs = self.jobmanager.list_jobs()
-                self._format_job_table(jobs)
-                
-                # 等待
-                time.sleep(refresh_interval)
-                
-        except KeyboardInterrupt:
-            print("\nMonitoring stopped")
-            return True
-        except Exception as e:
-            self._print_error(f"Monitor failed: {e}")
-            return False
-    
-    def cmd_watch(self, job_identifier: str, refresh_interval: int = 2) -> bool:
-        """监控特定作业"""
-        try:
-            # 解析作业标识符
-            job_uuid = self._resolve_job_identifier(job_identifier)
-            if not job_uuid:
-                return False
-                
-            self.ensure_connected()
-            
-            self._print_info(f"Watching job {job_uuid[:8]}... (refresh every {refresh_interval}s)")
-            
-            def signal_handler(signum, frame):
-                print("\nWatching stopped")
-                sys.exit(0)
-            
-            signal.signal(signal.SIGINT, signal_handler)
-            
-            while True:
-                # 清屏
-                os.system('clear' if os.name == 'posix' else 'cls')
-                
-                # 显示作业详情
-                job_info = self.jobmanager.get_job_status(job_uuid)
-                
-                if not job_info:
-                    self._print_error(f"Job {job_uuid} not found")
-                    return False
-                
-                print(f"{Fore.CYAN}=== Watching Job {job_uuid[:8]}... ==={Style.RESET_ALL}")
-                print(f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
-                print()
-                
-                self._format_job_details(job_info, verbose=True)
-                
-                time.sleep(refresh_interval)
-                
-        except KeyboardInterrupt:
-            print("\nWatching stopped")
-            return True
-        except Exception as e:
-            self._print_error(f"Watch failed: {e}")
-            return False
-    
-    # ==================== 交互式Shell ====================
-    
-    def cmd_shell(self) -> bool:
-        """进入交互式shell"""
-        try:
-            self.ensure_connected()
-            
-            print(f"{Fore.GREEN}SAGE JobManager Interactive Shell{Style.RESET_ALL}")
-            print("Type 'help' for available commands, 'exit' to quit")
-            
-            while True:
-                try:
-                    cmd_input = input(f"{Fore.BLUE}sage-jm> {Style.RESET_ALL}").strip()
-                    
-                    if not cmd_input:
-                        continue
-                    
-                    if cmd_input in ['exit', 'quit']:
-                        break
-                    
-                    if cmd_input == 'help':
-                        self._show_shell_help()
-                        continue
-                    
-                    # 解析命令
-                    parts = cmd_input.split()
-                    cmd = parts[0]
-                    args = parts[1:]
-                    
-                    # 执行命令
-                    if cmd == 'list':
-                        status_filter = None
-                        full_uuid = False
-                        if '--status' in args:
-                            idx = args.index('--status')
-                            if idx + 1 < len(args):
-                                status_filter = args[idx + 1]
-                        if '--full-uuid' in args:
-                            full_uuid = True
-                        self.cmd_list(status_filter, format_type="table", full_uuid=full_uuid)
-                        
-                    elif cmd == 'show':
-                        if args:
-                            verbose = '--verbose' in args or '-v' in args
-                            job_identifier = args[0]
-                            self.cmd_show(job_identifier, verbose)
-                        else:
-                            self._print_error("Usage: show <job_number_or_uuid> [--verbose]")
-                            
-                    elif cmd == 'stop':
-                        if args:
-                            self.cmd_stop(args[0])
-                        else:
-                            self._print_error("Usage: stop <job_number_or_uuid>")
-                            
-                    elif cmd == 'status':
-                        if args:
-                            self.cmd_status(args[0])
-                        else:
-                            self._print_error("Usage: status <job_number_or_uuid>")
-                            
-                    elif cmd == 'info':
-                        self.cmd_info()
-                        
-                    elif cmd == 'health':
-                        self.cmd_health()
-                        
-                    else:
-                        self._print_error(f"Unknown command: {cmd}")
-                        
-                except KeyboardInterrupt:
-                    print()
-                    continue
-                except EOFError:
-                    break
-            
-            print("Goodbye!")
-            return True
-            
-        except Exception as e:
-            self._print_error(f"Shell failed: {e}")
-            return False
-    
-    def _show_shell_help(self):
-        """显示shell帮助"""
-        help_text = """
-Available Commands:
-  list [--status STATUS] [--full-uuid]    List all jobs
-  show <job_number_or_uuid> [--verbose]   Show job details  
-  stop <job_number_or_uuid>               Stop a job
-  status <job_number_or_uuid>             Get job status
-  info                                    Show system information
-  health                                  Health check
-  help                                    Show this help
-  exit/quit                               Exit shell
-
-Job Identifiers:
-  You can use either job numbers (1, 2, 3...) or UUIDs
-  Examples: show 1, show abc123, stop 2, status def456
-"""
-        print(help_text)
-    
-    # ==================== 输出格式化 ====================
-    
-    def _format_job_table(self, jobs: List[Dict[str, Any]], short_uuid: bool = False):
-        """格式化作业表格"""
-        if not jobs:
-            self._print_info("No jobs found")
-            return
-        
-        # 根据终端宽度决定是否显示完整UUID
-        import shutil
-        terminal_width = shutil.get_terminal_size().columns
-        
-        if short_uuid or terminal_width < 120:
-            headers = ['#', 'UUID (Short)', 'Name', 'Status', 'Started', 'Runtime']
-        else:
-            headers = ['#', 'UUID', 'Name', 'Status', 'Started', 'Runtime']
-        
-        rows = []
-        
-        for i, job in enumerate(jobs, 1):
-            full_uuid = job.get('uuid', 'unknown')
-            
-            if short_uuid or terminal_width < 120:
-                uuid_display = full_uuid[:8] + '...' if len(full_uuid) > 8 else full_uuid
-            else:
-                uuid_display = full_uuid
-                
-            name = job.get('name', 'unknown')
-            status = job.get('status', 'unknown')
-            start_time = job.get('start_time', 'unknown')
-            runtime = job.get('runtime', 'unknown')
-            
-            # 状态着色
-            if status == 'running':
-                status = f"{Fore.GREEN}{status}{Style.RESET_ALL}"
-            elif status == 'stopped':
-                status = f"{Fore.YELLOW}{status}{Style.RESET_ALL}"
-            elif status == 'failed':
-                status = f"{Fore.RED}{status}{Style.RESET_ALL}"
-            
-            rows.append([i, uuid_display, name, status, start_time, runtime])
-        
-        print(tabulate(rows, headers=headers, tablefmt='grid'))
-        
-        # 如果使用短UUID，显示提示信息
-        if short_uuid or terminal_width < 120:
-            print(f"\n{Fore.BLUE}💡 Tip:{Style.RESET_ALL} Use job number (#) or full UUID for commands")
-            print(f"   Example: sage-jm show 1  or  sage-jm show {jobs[0].get('uuid', '')}")
-            print(f"   Use --full-uuid to see complete UUIDs")
-    
-    def _format_job_details(self, job_info: Dict[str, Any], verbose: bool = False):
-        """格式化作业详情"""
-        print(f"{Fore.CYAN}=== Job Details ==={Style.RESET_ALL}")
-        
-        uuid = job_info.get('uuid', 'unknown')
-        name = job_info.get('name', 'unknown')
-        status = job_info.get('status', 'unknown')
-        
-        print(f"UUID: {uuid}")
-        print(f"Name: {name}")
-        
-        # 状态着色
-        if status == 'running':
-            status_colored = f"{Fore.GREEN}{status}{Style.RESET_ALL}"
-        elif status == 'stopped':
-            status_colored = f"{Fore.YELLOW}{status}{Style.RESET_ALL}"
-        elif status == 'failed':
-            status_colored = f"{Fore.RED}{status}{Style.RESET_ALL}"
-        else:
-            status_colored = status
-        
-        print(f"Status: {status_colored}")
-        print(f"Start Time: {job_info.get('start_time', 'unknown')}")
-        print(f"Runtime: {job_info.get('runtime', 'unknown')}")
-        
-        if verbose:
-            if 'error' in job_info:
-                print(f"Error: {job_info['error']}")
-            
-            # 显示更多详细信息
-            print(f"\nEnvironment Details:")
-            env_info = job_info.get('environment', {})
-            for key, value in env_info.items():
-                print(f"  {key}: {value}")
-    
-    def _print_success(self, message: str):
-        """打印成功消息"""
-        print(f"{Fore.GREEN}✓{Style.RESET_ALL} {message}")
-    
-    def _print_error(self, message: str):
-        """打印错误消息"""
-        print(f"{Fore.RED}✗{Style.RESET_ALL} {message}")
-    
-    def _print_warning(self, message: str):
-        """打印警告消息"""
-        print(f"{Fore.YELLOW}⚠{Style.RESET_ALL} {message}")
-    
-    def _print_info(self, message: str):
-        """打印信息消息"""
-        print(f"{Fore.BLUE}ℹ{Style.RESET_ALL} {message}")
-    
-    def _print_status_colored(self, message: str):
-        """打印带颜色的状态消息"""
-        if 'running' in message:
-            print(message.replace('running', f"{Fore.GREEN}running{Style.RESET_ALL}"))
-        elif 'stopped' in message:
-            print(message.replace('stopped', f"{Fore.YELLOW}stopped{Style.RESET_ALL}"))
-        elif 'failed' in message:
-            print(message.replace('failed', f"{Fore.RED}failed{Style.RESET_ALL}"))
-        else:
-            print(message)
-
-def create_parser():
-    """创建命令行参数解析器"""
-    parser = argparse.ArgumentParser(
-        prog='sage-jm',
-        description='SAGE JobManager Controller'
-    )
-    
-    # 全局参数
-    parser.add_argument('--host', default='127.0.0.1', help='Daemon host')
-    parser.add_argument('--port', type=int, default=19001, help='Daemon port')
-    parser.add_argument('--no-color', action='store_true', help='Disable colored output')
-    
-    # 子命令
-    subparsers = parser.add_subparsers(dest='command', help='Available commands')
-    
-    # list 命令
-    list_parser = subparsers.add_parser('list', help='List all jobs')
-    list_parser.add_argument('--status', choices=['running', 'stopped', 'failed'], help='Filter by status')
-    list_parser.add_argument('--format', choices=['table', 'json'], default='table', help='Output format')
-    list_parser.add_argument('--full-uuid', action='store_true', help='Show full UUIDs instead of short versions')
-    
-    # show 命令
-    show_parser = subparsers.add_parser('show', help='Show job details')
-    show_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    show_parser.add_argument('--verbose', '-v', action='store_true', help='Show verbose details')
-    
-    # stop 命令
-    stop_parser = subparsers.add_parser('stop', help='Stop a job')
-    stop_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    stop_parser.add_argument('--force', '-f', action='store_true', help='Force stop without confirmation')
-    
-    # status 命令
-    status_parser = subparsers.add_parser('status', help='Get job status')
-    status_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    
-    # info 命令
-    subparsers.add_parser('info', help='Show system information')
-    
-    # health 命令
-    subparsers.add_parser('health', help='Health check')
-    
-    # monitor 命令
-    monitor_parser = subparsers.add_parser('monitor', help='Monitor all jobs')
-    monitor_parser.add_argument('--refresh', type=int, default=5, help='Refresh interval in seconds')
-    
-    # watch 命令
-    watch_parser = subparsers.add_parser('watch', help='Watch specific job')
-    watch_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    watch_parser.add_argument('--refresh', type=int, default=2, help='Refresh interval in seconds')
-
-    # continue 命令
-    continue_parser = subparsers.add_parser('continue', help='continue a job')
-    continue_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    continue_parser.add_argument('--force', '-f', action='store_true', help='Force continue without confirmation')
-    
-    # delete 命令
-    delete_parser = subparsers.add_parser('delete', help='Delete a job')
-    delete_parser.add_argument('job_identifier', help='Job number (1,2,3...) or UUID')
-    delete_parser.add_argument('--force', '-f', action='store_true', help='Force delete without confirmation')
-    
-    # cleanup 命令
-    cleanup_parser = subparsers.add_parser('cleanup', help='Cleanup all jobs')
-    cleanup_parser.add_argument('--force', '-f', action='store_true', help='Force cleanup without confirmation')
-
-    # shell 命令
-    subparsers.add_parser('shell', help='Enter interactive shell')
-    
-    return parser
-
-def main():
-    """主函数"""
-    parser = create_parser()
-    args = parser.parse_args()
-    
-    # 禁用颜色输出
-    if args.no_color:
-        import colorama
-        colorama.deinit()
-    
-    # 创建控制器
-    controller = JobManagerController(args.host, args.port)
-    
-    # 执行命令
-    if not args.command:
-        parser.print_help()
-        return
-    
-    success = True
-    
-    try:
-        if args.command == 'list':
-            success = controller.cmd_list(args.status, args.format, args.full_uuid)
-        elif args.command == 'show':
-            success = controller.cmd_show(args.job_identifier, args.verbose)
-        elif args.command == 'stop':
-            success = controller.cmd_stop(args.job_identifier, args.force)
-        elif args.command == 'continue':
-            success = controller.cmd_continue(args.job_identifier, args.force)
-        elif args.command == 'delete':
-            success = controller.cmd_delete(args.job_identifier, args.force)
-        elif args.command == 'status':
-            success = controller.cmd_status(args.job_identifier)
-        elif args.command == 'info':
-            success = controller.cmd_info()
-        elif args.command == 'health':
-            success = controller.cmd_health()
-        elif args.command == 'monitor':
-            success = controller.cmd_monitor(args.refresh)
-        elif args.command == 'watch':
-            success = controller.cmd_watch(args.job_identifier, args.refresh)
-        elif args.command == 'shell':
-            success = controller.cmd_shell()
-        elif args.command == 'cleanup':
-            success = controller.cmd_cleanup(args.force)
-        else:
-            print(f"Unknown command: {args.command}")
-            success = False
-            
-    except KeyboardInterrupt:
-        print("\nOperation cancelled")
-        success = True
-    except Exception as e:
-        print(f"Unexpected error: {e}")
-        success = False
-    finally:
-        controller.disconnect()
-    
-    sys.exit(0 if success else 1)
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/deployment/jobmanager_daemon.py b/deployment/jobmanager_daemon.py
deleted file mode 100644
index f3af9dc..0000000
--- a/deployment/jobmanager_daemon.py
+++ /dev/null
@@ -1,497 +0,0 @@
-
-import socket, ray
-import threading
-import time
-import json
-import pickle
-import signal
-import sys
-import os
-from pathlib import Path
-from typing import Optional, Dict, Any, TYPE_CHECKING
-from sage_utils.custom_logger import CustomLogger
-from sage_jobmanager.remote_job_manager import RemoteJobManager
-from ray.actor import ActorHandle
-    
-
-
-
-class RayJobManagerDaemon:
-    """
-    Ray JobManager守护服务
-    负责启动detached JobManager Actor并通过socket暴露句柄
-    """
-    
-    def __init__(self, 
-                 host: str = "127.0.0.1", 
-                 port: int = 19001,
-                 actor_name: str = "sage_global_jobmanager",
-                 namespace: str = "sage_system"):
-        """
-        初始化守护服务
-        
-        Args:
-            host: Socket服务监听地址
-            port: Socket服务端口  
-            actor_name: JobManager Actor名称
-            namespace: Ray命名空间
-        """
-        self.host = host
-        self.port = port
-        self.actor_name = actor_name
-        self.namespace = namespace
-        
-        # Actor句柄
-        self._actor_handle: Optional[ActorHandle] = None
-        
-        # Socket服务
-        self._server_socket: Optional[socket.socket] = None
-        self._server_thread: Optional[threading.Thread] = None
-        self._running = False
-        
-        # 日志
-        self._setup_logging()
-        
-        # 信号处理
-        self._setup_signal_handlers()
-        
-    def _setup_logging(self):
-        """设置日志"""
-        # 获取项目根目录：当前文件在deployment目录下，需要回到项目根目录
-        project_root = Path(__file__).parent.parent
-        log_dir = project_root / "logs" / "daemon"
-        log_dir.mkdir(parents=True, exist_ok=True)
-        
-        timestamp = time.strftime("%Y%m%d_%H%M%S")
-        self.logger = CustomLogger([
-            ("console", "INFO"),
-            (log_dir / f"jobmanager_daemon_{timestamp}.log", "DEBUG"),
-            (log_dir / "daemon_error.log", "ERROR")
-        ], name="JobManagerDaemon")
-        
-    def _setup_signal_handlers(self):
-        """设置信号处理"""
-        def signal_handler(signum, frame):
-            self.logger.info(f"Received signal {signum}, shutting down daemon...")
-            self.shutdown()
-            sys.exit(0)
-            
-        signal.signal(signal.SIGINT, signal_handler)
-        signal.signal(signal.SIGTERM, signal_handler)
-    
-    def start_daemon(self):
-        """启动守护服务"""
-        try:
-            self.logger.info("Starting Ray JobManager Daemon...")
-            
-            # 1. 确保进程内Ray客户端已初始化
-            if not ray.is_initialized():
-                ray.init(address="auto", _temp_dir="/var/lib/ray_shared")
-                self.logger.info("Ray initialized")
-            
-            # 2. 启动JobManager Actor
-            self._start_jobmanager_actor()
-            
-            # 3. 启动Socket服务
-            self._start_socket_service()
-            
-            self.logger.info(f"Daemon started successfully on {self.host}:{self.port}")
-            return True
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start daemon: {e}")
-            self.shutdown()
-            return False
-    
-    def _start_jobmanager_actor(self):
-        """启动JobManager Actor"""
-        try:
-            # 检查是否已存在Actor
-            try:
-                existing_actor = ray.get_actor(self.actor_name, namespace=self.namespace)
-                # 测试Actor是否健康
-                ray.get(existing_actor.health_check.remote(), timeout=5)
-                self.logger.info(f"Found existing healthy JobManager Actor: {self.actor_name}")
-                self._actor_handle = existing_actor
-                return
-            except (ValueError, ray.exceptions.RayActorError):
-                self.logger.info("No existing healthy Actor found, creating new one")
-            
-            # 创建新的detached Actor
-            self.logger.info(f"Creating detached JobManager Actor: {self.actor_name}")
-            
-            self._actor_handle = RemoteJobManager.options(
-                name=self.actor_name,
-                namespace=self.namespace,
-                lifetime="detached",  # 关键：使用detached生命周期
-                max_restarts=3,
-                max_task_retries=2,
-                resources={"jobmanager": 1.0}  # 资源标记防止回收
-            ).remote()
-            
-            # 验证Actor创建成功 - 使用Ray内置的ready检查
-            self.logger.info("Waiting for Actor to be ready...")
-            ray.get(self._actor_handle.get_actor_info.remote(), timeout=30)
-            
-            # 获取Actor基本信息
-            try:
-                actor_id = self._actor_handle._actor_id.hex()
-                self.logger.info(f"JobManager Actor created successfully: {actor_id}")
-            except Exception as e:
-                self.logger.info("JobManager Actor created (could not get ID)")
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start JobManager Actor: {e}")
-            raise
-    
-    def _start_socket_service(self):
-        """启动Socket服务"""
-        try:
-            self._server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-            self._server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
-            self._server_socket.bind((self.host, self.port))
-            self._server_socket.listen(10)
-            
-            self._running = True
-            self._server_thread = threading.Thread(
-                target=self._socket_server_loop,
-                name="SocketServer",
-                daemon=True
-            )
-            self._server_thread.start()
-            
-            self.logger.info(f"Socket service started on {self.host}:{self.port}")
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start socket service: {e}")
-            raise
-    
-    def _socket_server_loop(self):
-        """Socket服务主循环"""
-        self.logger.debug("Socket server loop started")
-        
-        while self._running:
-            try:
-                if not self._server_socket:
-                    break
-                    
-                self._server_socket.settimeout(1.0)  # 1秒超时，允许检查_running状态
-                
-                try:
-                    client_socket, address = self._server_socket.accept()
-                    self.logger.debug(f"New client connected: {address}")
-                    
-                    # 在新线程中处理客户端
-                    client_thread = threading.Thread(
-                        target=self._handle_client,
-                        args=(client_socket, address),
-                        name=f"Client-{address[0]}:{address[1]}",
-                        daemon=True
-                    )
-                    client_thread.start()
-                    
-                except socket.timeout:
-                    continue
-                    
-            except Exception as e:
-                if self._running:
-                    self.logger.error(f"Error in socket server loop: {e}")
-                break
-        
-        self.logger.debug("Socket server loop stopped")
-    
-    def _handle_client(self, client_socket: socket.socket, address: tuple):
-        """处理客户端请求"""
-        try:
-            with client_socket:
-                # 接收请求
-                request_data = self._receive_message(client_socket)
-                if not request_data:
-                    return
-                
-                request = json.loads(request_data.decode('utf-8'))
-                self.logger.debug(f"Received request from {address}: {request}")
-                
-                # 处理请求
-                response = self._process_request(request)
-                
-                # 发送响应
-                self._send_message(client_socket, json.dumps(response).encode('utf-8'))
-                
-        except Exception as e:
-            self.logger.error(f"Error handling client {address}: {e}")
-    
-    def _receive_message(self, client_socket: socket.socket) -> Optional[bytes]:
-        """接收消息"""
-        try:
-            # 接收消息长度
-            length_data = client_socket.recv(4)
-            if len(length_data) != 4:
-                return None
-            
-            message_length = int.from_bytes(length_data, byteorder='big')
-            if message_length <= 0 or message_length > 10 * 1024 * 1024:  # 10MB限制
-                return None
-            
-            # 接收消息内容
-            message_data = b''
-            while len(message_data) < message_length:
-                chunk = client_socket.recv(min(message_length - len(message_data), 8192))
-                if not chunk:
-                    return None
-                message_data += chunk
-            
-            return message_data
-            
-        except Exception as e:
-            self.logger.debug(f"Error receiving message: {e}")
-            return None
-    
-    def _send_message(self, client_socket: socket.socket, message: bytes):
-        """发送消息"""
-        try:
-            # 发送消息长度
-            length_data = len(message).to_bytes(4, byteorder='big')
-            client_socket.sendall(length_data)
-            
-            # 发送消息内容
-            client_socket.sendall(message)
-            
-        except Exception as e:
-            self.logger.debug(f"Error sending message: {e}")
-    
-    def _process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """处理客户端请求"""
-        try:
-            action = request.get("action", "")
-            
-            if action == "get_actor_handle":
-                return self._handle_get_actor_handle(request)
-            elif action == "get_actor_info":
-                return self._handle_get_actor_info(request)
-            elif action == "health_check":
-                return self._handle_health_check(request)
-            elif action == "restart_actor":
-                return self._handle_restart_actor(request)
-            else:
-                return {
-                    "status": "error",
-                    "message": f"Unknown action: {action}",
-                    "request_id": request.get("request_id")
-                }
-                
-        except Exception as e:
-            self.logger.error(f"Error processing request: {e}")
-            return {
-                "status": "error", 
-                "message": str(e),
-                "request_id": request.get("request_id")
-            }
-    
-    def _handle_get_actor_handle(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """处理获取Actor句柄请求"""
-        if self._actor_handle is None:
-            return {
-                "status": "error",
-                "message": "No JobManager Actor available",
-                "request_id": request.get("request_id")
-            }
-        
-        try:
-            # 序列化Actor句柄
-            # 注意：Ray ActorHandle需要特殊处理
-            actor_serialized = pickle.dumps(self._actor_handle)
-            
-            return {
-                "status": "success",
-                "actor_handle": actor_serialized.hex(),  # 转换为hex字符串传输
-                "actor_name": self.actor_name,
-                "namespace": self.namespace,
-                "request_id": request.get("request_id")
-            }
-            
-        except Exception as e:
-            self.logger.error(f"Failed to serialize actor handle: {e}")
-            return {
-                "status": "error",
-                "message": f"Failed to serialize actor handle: {e}",
-                "request_id": request.get("request_id")
-            }
-    
-    def _handle_get_actor_info(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """处理获取Actor信息请求"""
-        if self._actor_handle is None:
-            return {
-                "status": "error",
-                "message": "No JobManager Actor available",
-                "request_id": request.get("request_id")
-            }
-        
-        try:
-            # 构建基本的Actor信息
-            actor_info = {
-                "actor_name": self.actor_name,
-                "namespace": self.namespace,
-            }
-            
-            # 尝试获取Actor ID
-            try:
-                actor_info["actor_id"] = self._actor_handle._actor_id.hex()
-            except:
-                actor_info["actor_id"] = "unknown"
-            
-            # 检查Actor是否ready
-            try:
-                ray.get(self._actor_handle.__ray_ready__.remote(), timeout=3)
-                actor_info["status"] = "ready"
-            except:
-                actor_info["status"] = "not_ready"
-            
-            return {
-                "status": "success",
-                "actor_info": actor_info,
-                "request_id": request.get("request_id")
-            }
-        except Exception as e:
-            return {
-                "status": "error",
-                "message": f"Failed to get actor info: {e}",
-                "request_id": request.get("request_id")
-            }
-    
-    def _handle_health_check(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """处理健康检查请求"""
-        daemon_status = {
-            "daemon_running": True,
-            "socket_service": f"{self.host}:{self.port}",
-            "actor_name": self.actor_name,
-            "namespace": self.namespace
-        }
-        
-        if self._actor_handle is None:
-            daemon_status["actor_available"] = False
-            return {
-                "status": "warning",
-                "message": "Daemon running but no Actor available",
-                "daemon_status": daemon_status,
-                "request_id": request.get("request_id")
-            }
-        
-        try:
-            # 使用Ray内置的健康检查
-            ray.get(self._actor_handle.__ray_ready__.remote(), timeout=5)
-            daemon_status["actor_available"] = True
-            daemon_status["actor_ready"] = True
-            
-            return {
-                "status": "success",
-                "message": "Daemon and Actor are healthy",
-                "daemon_status": daemon_status,
-                "request_id": request.get("request_id")
-            }
-        except Exception as e:
-            daemon_status["actor_available"] = False
-            daemon_status["actor_error"] = str(e)
-            
-            return {
-                "status": "error",
-                "message": f"Actor health check failed: {e}",
-                "daemon_status": daemon_status,
-                "request_id": request.get("request_id")
-            }
-    
-    def _handle_restart_actor(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """处理重启Actor请求"""
-        try:
-            # 停止现有Actor
-            if self._actor_handle:
-                try:
-                    ray.kill(self._actor_handle)
-                    self.logger.info("Old Actor killed")
-                except:
-                    pass
-            
-            # 启动新Actor
-            self._start_jobmanager_actor()
-            
-            return {
-                "status": "success",
-                "message": "Actor restarted successfully",
-                "request_id": request.get("request_id")
-            }
-        except Exception as e:
-            return {
-                "status": "error",
-                "message": f"Failed to restart Actor: {e}",
-                "request_id": request.get("request_id")
-            }
-    
-    def shutdown(self):
-        """关闭守护服务"""
-        self.logger.info("Shutting down daemon...")
-        
-        # 停止socket服务
-        self._running = False
-        if self._server_socket:
-            try:
-                self._server_socket.close()
-            except:
-                pass
-        
-        # 等待服务线程结束
-        if self._server_thread and self._server_thread.is_alive():
-            self._server_thread.join(timeout=5)
-        
-        # 注意：不要杀死detached Actor，它应该继续运行
-        # 只是释放句柄
-        self._actor_handle = None
-        
-        self.logger.info("Daemon shutdown complete")
-    
-    def run_forever(self):
-        """运行守护服务直到收到停止信号"""
-        if not self.start_daemon():
-            return False
-        
-        try:
-            while self._running:
-                time.sleep(1)
-        except KeyboardInterrupt:
-            pass
-        finally:
-            self.shutdown()
-        
-        return True
-
-
-
-# ==================== 命令行工具 ====================
-
-def main():
-    """命令行入口"""
-    import argparse
-    
-    parser = argparse.ArgumentParser(description="Ray JobManager Daemon")
-    parser.add_argument("--host", default="127.0.0.1", help="Daemon host")
-    parser.add_argument("--port", type=int, default=19001, help="Daemon port")
-    parser.add_argument("--actor-name", default="sage_global_jobmanager", help="Actor name")
-    parser.add_argument("--namespace", default="sage_system", help="Ray namespace")
-    
-    args = parser.parse_args()
-    
-    daemon = RayJobManagerDaemon(
-        host=args.host,
-        port=args.port,
-        actor_name=args.actor_name,
-        namespace=args.namespace
-    )
-    
-    print(f"Starting JobManager Daemon on {args.host}:{args.port}")
-    print(f"Actor: {args.actor_name}@{args.namespace}")
-    print("Press Ctrl+C to stop...")
-    
-    daemon.run_forever()
-
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/deployment/sage_deployment.sh b/deployment/sage_deployment.sh
deleted file mode 100755
index 683285a..0000000
--- a/deployment/sage_deployment.sh
+++ /dev/null
@@ -1,606 +0,0 @@
-#!/bin/bash
-
-# SAGE System Startup Script
-# 功能：
-# 1. 检查并启动 Ray head 节点
-# 2. 设置 Ray session 权限
-# 3. 检查并启动 JobManager Daemon
-# 4. 验证系统状态
-# 5. 设置命令行工具
-
-set -e  # 遇到错误立即退出
-
-# 配置参数
-RAY_HEAD_PORT=${RAY_HEAD_PORT:-10001}           # GCS server port
-RAY_CLIENT_PORT=${RAY_CLIENT_PORT:-10002}       # Client server port (新增)
-RAY_DASHBOARD_PORT=${RAY_DASHBOARD_PORT:-8265}
-RAY_TEMP_DIR=${RAY_TEMP_DIR:-/var/lib/ray_shared}
-DAEMON_HOST=${DAEMON_HOST:-127.0.0.1}
-DAEMON_PORT=${DAEMON_PORT:-19001}
-ACTOR_NAME=${ACTOR_NAME:-sage_global_jobmanager}
-NAMESPACE=${NAMESPACE:-sage_system}
-
-# 颜色输出
-RED='\033[0;31m'
-GREEN='\033[0;32m'
-YELLOW='\033[1;33m'
-BLUE='\033[0;34m'
-NC='\033[0m' # No Color
-
-# 日志函数
-log_info() {
-    echo -e "${BLUE}[INFO]${NC} $1"
-}
-
-log_success() {
-    echo -e "${GREEN}[SUCCESS]${NC} $1"
-}
-
-log_warning() {
-    echo -e "${YELLOW}[WARNING]${NC} $1"
-}
-
-log_error() {
-    echo -e "${RED}[ERROR]${NC} $1"
-}
-
-# 检查命令是否存在
-check_command() {
-    if ! command -v "$1" &> /dev/null; then
-        log_error "Command '$1' not found. Please install it first."
-        exit 1
-    fi
-}
-
-# 检查端口是否被占用
-check_port() {
-    local port=$1
-    if lsof -Pi :$port -sTCP:LISTEN -t >/dev/null 2>&1; then
-        return 0  # 端口被占用
-    else
-        return 1  # 端口未被占用
-    fi
-}
-
-# 检查端口范围是否可用
-check_port_range() {
-    local start_port=$1
-    local end_port=$2
-    local conflicts=()
-    
-    for port in $(seq $start_port $end_port); do
-        if check_port $port; then
-            conflicts+=($port)
-        fi
-    done
-    
-    if [ ${#conflicts[@]} -gt 0 ]; then
-        log_warning "Ports in use: ${conflicts[*]}"
-        return 1
-    else
-        return 0
-    fi
-}
-
-# 等待端口启动
-wait_for_port() {
-    local port=$1
-    local timeout=${2:-30}
-    local count=0
-    
-    log_info "Waiting for port $port to be ready..."
-    
-    while [ $count -lt $timeout ]; do
-        if check_port $port; then
-            log_success "Port $port is ready"
-            return 0
-        fi
-        sleep 1
-        count=$((count + 1))
-        echo -n "."
-    done
-    
-    echo
-    log_error "Timeout waiting for port $port"
-    return 1
-}
-
-# 设置 Ray temp 目录权限
-setup_ray_temp_dir() {
-    log_info "Setting up Ray temp directory: $RAY_TEMP_DIR"
-    
-    if [ ! -d "$RAY_TEMP_DIR" ]; then
-        sudo mkdir -p "$RAY_TEMP_DIR"
-        log_info "Created Ray temp directory: $RAY_TEMP_DIR"
-    fi
-    
-    sudo chmod 1777 "$RAY_TEMP_DIR"
-    log_success "Ray temp directory permissions set"
-}
-
-# 设置 Ray session 权限
-setup_ray_session_permissions() {
-    log_info "Setting Ray session permissions..."
-    
-    # 查找最新的 Ray session
-    local ray_session_dir="/tmp/ray"
-    if [ -d "$ray_session_dir" ]; then
-        # 给 session_latest 符号链接设置权限
-        local session_latest="$ray_session_dir/session_latest"
-        if [ -L "$session_latest" ]; then
-            local actual_session=$(readlink -f "$session_latest")
-            if [ -d "$actual_session" ]; then
-                log_info "Setting permissions for Ray session: $actual_session"
-                sudo chmod -R 755 "$actual_session" 2>/dev/null || true
-                
-                # 特别设置 session.json 权限
-                local session_json="$actual_session/session.json"
-                if [ -f "$session_json" ]; then
-                    sudo chmod 644 "$session_json"
-                    log_success "Ray session permissions updated"
-                else
-                    log_warning "session.json not found at $session_json"
-                fi
-            fi
-        fi
-    else
-        log_warning "Ray session directory not found at $ray_session_dir"
-    fi
-}
-
-# 设置命令行工具
-setup_cli_tools() {
-    log_info "Setting up SAGE command line tools..."
-    
-    local script_dir=$(dirname $(realpath $0))
-    local controller_script="$script_dir/jobmanager_controller.py"
-    local symlink_path="/usr/local/bin/sage-jm"
-    
-    # 检查 controller 脚本是否存在
-    if [ ! -f "$controller_script" ]; then
-        log_warning "JobManager controller script not found at $controller_script"
-        return 1
-    fi
-    
-    # 使脚本可执行
-    chmod +x "$controller_script" 2>/dev/null || {
-        log_warning "Failed to make controller script executable (permission denied)"
-        log_info "You may need to run: chmod +x $controller_script"
-    }
-    
-    # 检查是否已存在符号链接
-    if [ -L "$symlink_path" ]; then
-        local current_target=$(readlink "$symlink_path")
-        if [ "$current_target" = "$controller_script" ]; then
-            log_success "Command line tool 'sage-jm' is already set up"
-            return 0
-        else
-            log_info "Updating existing sage-jm symlink"
-            sudo rm -f "$symlink_path"
-        fi
-    elif [ -f "$symlink_path" ]; then
-        log_warning "File exists at $symlink_path (not a symlink)"
-        log_info "Please remove it manually to install sage-jm command"
-        return 1
-    fi
-    
-    # 创建符号链接
-    if sudo ln -s "$controller_script" "$symlink_path" 2>/dev/null; then
-        log_success "Command line tool 'sage-jm' installed successfully"
-        
-        # 验证安装
-        if command -v sage-jm >/dev/null 2>&1; then
-            log_success "sage-jm command is ready to use"
-        else
-            log_warning "sage-jm command not found in PATH"
-            log_info "You may need to restart your terminal or add /usr/local/bin to PATH"
-        fi
-    else
-        log_warning "Failed to create sage-jm symlink (sudo required)"
-        log_info "To manually install the command line tool, run:"
-        log_info "  sudo ln -s $controller_script /usr/local/bin/sage-jm"
-        return 1
-    fi
-}
-
-# 检查 Ray 状态
-check_ray_status() {
-    if ray status >/dev/null 2>&1; then
-        log_success "Ray cluster is running"
-        ray status
-        return 0
-    else
-        log_info "Ray cluster is not running"
-        return 1
-    fi
-}
-
-# 启动 Ray head 节点
-start_ray_head() {
-    log_info "Starting Ray head node..."
-    
-    # 检查端口冲突
-    log_info "Checking port availability..."
-    local ports_to_check=($RAY_HEAD_PORT $RAY_CLIENT_PORT $RAY_DASHBOARD_PORT)
-    local conflicts=()
-    
-    for port in "${ports_to_check[@]}"; do
-        if check_port $port; then
-            conflicts+=($port)
-        fi
-    done
-    
-    if [ ${#conflicts[@]} -gt 0 ]; then
-        log_error "Port conflicts detected: ${conflicts[*]}"
-        log_error "Please stop conflicting services or use different ports"
-        exit 1
-    fi
-    
-    # 设置 Ray temp 目录
-    setup_ray_temp_dir
-    
-    # 启动 Ray head，明确指定端口避免冲突
-    log_info "Starting Ray with ports: GCS=$RAY_HEAD_PORT, Client=$RAY_CLIENT_PORT, Dashboard=$RAY_DASHBOARD_PORT"
-    
-    ray start --head \
-        --temp-dir="$RAY_TEMP_DIR" \
-        --disable-usage-stats \
-        --verbose \
-        --resources='{"jobmanager": 1.0}'  # 添加自定义资源
-        # --port=$RAY_HEAD_PORT \
-        # --dashboard-port=$RAY_DASHBOARD_PORT \
-        #--ray-client-server-port=$RAY_CLIENT_PORT \
-
-    
-    # 等待 Ray 启动完成
-    sleep 5
-    
-    # 设置 session 权限
-    setup_ray_session_permissions
-    
-    # 验证 Ray 状态
-    if check_ray_status; then
-        log_success "Ray head node started successfully"
-        log_info "Ray ports: GCS=$RAY_HEAD_PORT, Client=$RAY_CLIENT_PORT, Dashboard=$RAY_DASHBOARD_PORT"
-    else
-        log_error "Failed to start Ray head node"
-        exit 1
-    fi
-}
-
-# 检查 JobManager Daemon 状态
-check_daemon_status() {
-    if check_port $DAEMON_PORT; then
-        # 尝试健康检查
-        local health_check=$(python3 -c "
-import sys
-sys.path.append('$(dirname $(realpath $0))/..')
-from sage_core.jobmanager_client import JobManagerClient
-try:
-    client = JobManagerClient('$DAEMON_HOST', $DAEMON_PORT)
-    response = client.health_check()
-    if response.get('status') == 'success':
-        print('healthy')
-    else:
-        print('unhealthy')
-except:
-    print('error')
-" 2>/dev/null)
-        
-        if [ "$health_check" = "healthy" ]; then
-            log_success "JobManager Daemon is running and healthy"
-            return 0
-        else
-            log_warning "JobManager Daemon is running but not healthy"
-            return 1
-        fi
-    else
-        log_info "JobManager Daemon is not running"
-        return 1
-    fi
-}
-
-# 启动 JobManager Daemon
-start_daemon() {
-    log_info "Starting JobManager Daemon..."
-    
-    # 检查守护进程端口
-    if check_port $DAEMON_PORT; then
-        log_error "Port $DAEMON_PORT is already in use"
-        log_error "Please stop the conflicting service or use a different port"
-        return 1
-    fi
-    
-    # 获取脚本目录
-    local script_dir=$(dirname $(realpath $0))
-    local project_root=$(dirname "$script_dir")
-    
-    # 设置 Python 路径
-    export PYTHONPATH="$project_root:$PYTHONPATH"
-    
-    # 后台启动守护进程
-    nohup python3 "$script_dir/jobmanager_daemon.py" \
-        --host "$DAEMON_HOST" \
-        --port "$DAEMON_PORT" \
-        --actor-name "$ACTOR_NAME" \
-        --namespace "$NAMESPACE" \
-        > /tmp/sage_daemon.log 2>&1 &
-    
-    local daemon_pid=$!
-    log_info "JobManager Daemon started with PID: $daemon_pid"
-    
-    # 等待守护进程启动
-    if wait_for_port $DAEMON_PORT 30; then
-        # 再次检查健康状态
-        sleep 2
-        if check_daemon_status; then
-            log_success "JobManager Daemon started successfully"
-            echo $daemon_pid > /tmp/sage_daemon.pid
-        else
-            log_error "JobManager Daemon started but health check failed"
-            # 显示日志以帮助调试
-            log_info "Daemon log output:"
-            tail -20 /tmp/sage_daemon.log
-            return 1
-        fi
-    else
-        log_error "JobManager Daemon failed to start"
-        # 显示日志以帮助调试
-        log_info "Daemon log output:"
-        tail -20 /tmp/sage_daemon.log
-        return 1
-    fi
-}
-
-# 显示系统状态
-show_status() {
-    echo
-    log_info "=== SAGE System Status ==="
-    
-    # Ray 状态
-    echo -e "\n${BLUE}Ray Cluster:${NC}"
-    if check_ray_status >/dev/null 2>&1; then
-        echo "  ✓ Running"
-        echo "  GCS Port: $RAY_HEAD_PORT"
-        echo "  Client Port: $RAY_CLIENT_PORT" 
-        echo "  Dashboard Port: $RAY_DASHBOARD_PORT"
-    else
-        echo "  ✗ Not running"
-    fi
-    
-    # JobManager Daemon 状态
-    echo -e "\n${BLUE}JobManager Daemon:${NC}"
-    if check_daemon_status; then
-        echo "  ✓ Running and healthy at $DAEMON_HOST:$DAEMON_PORT"
-        
-        # 显示 Actor 信息
-        local actor_info=$(python3 -c "
-import sys
-sys.path.append('$(dirname $(realpath $0))/..')
-from sage_core.jobmanager_client import JobManagerClient
-try:
-    client = JobManagerClient('$DAEMON_HOST', $DAEMON_PORT)
-    response = client.get_actor_info()
-    if response.get('status') == 'success':
-        info = response.get('actor_info', {})
-        print(f\"  Actor: {info.get('actor_name', 'N/A')}@{info.get('namespace', 'N/A')}\")
-        print(f\"  Actor ID: {info.get('actor_id', 'N/A')}\")
-    else:
-        print('  Actor info unavailable')
-except:
-    print('  Actor info error')
-" 2>/dev/null)
-        echo "$actor_info"
-    else
-        echo "  ✗ Not running or unhealthy"
-    fi
-    
-    # 命令行工具状态
-    echo -e "\n${BLUE}Command Line Tools:${NC}"
-    if command -v sage-jm >/dev/null 2>&1; then
-        echo "  ✓ sage-jm command available"
-    else
-        echo "  ✗ sage-jm command not available"
-    fi
-    
-    # 访问信息
-    echo -e "\n${BLUE}Access Information:${NC}"
-    echo "  Ray Dashboard: http://localhost:$RAY_DASHBOARD_PORT"
-    echo "  JobManager API: tcp://$DAEMON_HOST:$DAEMON_PORT"
-    echo "  Logs: /tmp/sage/logs/"
-    echo "  Daemon Log: /tmp/sage_daemon.log"
-}
-
-# 显示使用指南
-show_usage_guide() {
-    echo
-    log_info "=== SAGE System Ready ==="
-    echo
-    echo -e "${GREEN}🎉 SAGE system started successfully!${NC}"
-    echo
-    echo -e "${BLUE}Quick Start Guide:${NC}"
-    echo
-    
-    # 检查命令行工具是否可用
-    if command -v sage-jm >/dev/null 2>&1; then
-        echo -e "${GREEN}📋 Job Management Commands:${NC}"
-        echo "  sage-jm list                    # List all jobs"
-        echo "  sage-jm show <job_uuid>         # Show job details"
-        echo "  sage-jm stop <job_uuid>         # Stop a job"
-        echo "  sage-jm health                  # Check system health"
-        echo "  sage-jm monitor                 # Real-time monitoring"
-        echo "  sage-jm shell                   # Interactive shell"
-        echo
-        echo -e "${GREEN}🔍 Monitoring:${NC}"
-        echo "  sage-jm monitor --refresh 3     # Monitor with 3s refresh"
-        echo "  sage-jm watch <job_uuid>        # Watch specific job"
-        echo
-        echo -e "${GREEN}ℹ️  Getting Help:${NC}"
-        echo "  sage-jm --help                  # Show all commands"
-        echo "  sage-jm <command> --help        # Command-specific help"
-        echo
-    else
-        echo -e "${YELLOW}⚠️  Command line tool setup:${NC}"
-        echo "  The 'sage-jm' command is not available in your PATH."
-        echo "  To set it up manually, run:"
-        echo "    sudo ln -s $(dirname $(realpath $0))/jobmanager_controller.py /usr/local/bin/sage-jm"
-        echo
-    fi
-    
-    echo -e "${GREEN}🌐 Web Interfaces:${NC}"
-    echo "  Ray Dashboard: http://localhost:$RAY_DASHBOARD_PORT"
-    echo
-    echo -e "${GREEN}📁 Important Paths:${NC}"
-    echo "  Logs: {project folder}/logs/jobmanager_{timestamp}.log"
-    echo "  Daemon Log: {project folder}/logs/daemon"
-    echo "  Ray Temp: $RAY_TEMP_DIR"
-    echo
-    echo -e "${GREEN}🔄 System Management:${NC}"
-    echo "  ./sage_deployment.sh status      # Check system status"
-    echo "  ./sage_deployment.sh restart     # Restart system"
-    echo "  ./sage_deployment.sh stop        # Stop system"
-    echo
-    echo -e "${BLUE}Happy coding with SAGE! 🚀${NC}"
-}
-
-# 停止系统
-stop_system() {
-    log_info "Stopping SAGE system..."
-    
-    # 停止 JobManager Daemon
-    if [ -f /tmp/sage_daemon.pid ]; then
-        local daemon_pid=$(cat /tmp/sage_daemon.pid)
-        if kill -0 $daemon_pid 2>/dev/null; then
-            log_info "Stopping JobManager Daemon (PID: $daemon_pid)..."
-            kill $daemon_pid
-            sleep 2
-            # 强制杀死
-            kill -9 $daemon_pid 2>/dev/null || true
-        fi
-        rm -f /tmp/sage_daemon.pid
-    fi
-    
-    # 停止 Ray
-    if ray status >/dev/null 2>&1; then
-        log_info "Stopping Ray cluster..."
-        ray stop
-    fi
-    
-    log_success "SAGE system stopped"
-}
-
-# 安装命令行工具
-install_cli() {
-    log_info "Installing SAGE command line tools..."
-    
-    # 设置命令行工具
-    if setup_cli_tools; then
-        echo
-        log_success "✅ Command line tools installed successfully!"
-        echo
-        echo -e "${GREEN}You can now use:${NC}"
-        echo "  sage-jm --help          # Show help"
-        echo "  sage-jm list             # List jobs"
-        echo "  sage-jm health           # Check health"
-        echo "  sage-jm monitor          # Monitor jobs"
-        echo
-    else
-        echo
-        log_error "❌ Failed to install command line tools"
-        echo
-        echo -e "${YELLOW}Manual installation:${NC}"
-        echo "  chmod +x $(dirname $(realpath $0))/jobmanager_controller.py"
-        echo "  sudo ln -s $(dirname $(realpath $0))/jobmanager_controller.py /usr/local/bin/sage-jm"
-    fi
-}
-
-# 主函数
-main() {
-    case "${1:-start}" in
-        start)
-            log_info "Starting SAGE System..."
-            log_info "Configuration:"
-            log_info "  Ray GCS Port: $RAY_HEAD_PORT"
-            log_info "  Ray Client Port: $RAY_CLIENT_PORT"
-            log_info "  Ray Dashboard Port: $RAY_DASHBOARD_PORT"
-            log_info "  Daemon Port: $DAEMON_PORT"
-            
-            # 检查依赖
-            check_command "ray"
-            check_command "python3"
-            check_command "lsof"
-            
-            # 1. 检查并启动 Ray
-            if ! check_ray_status >/dev/null 2>&1; then
-                start_ray_head
-            else
-                log_success "Ray cluster is already running"
-                # 仍然需要设置权限
-                setup_ray_session_permissions
-            fi
-            
-            # 2. 检查并启动 JobManager Daemon
-            if ! check_daemon_status; then
-                start_daemon
-            else
-                log_success "JobManager Daemon is already running"
-            fi
-            
-            # 3. 设置命令行工具
-            setup_cli_tools
-            
-            # 4. 显示状态和使用指南
-            show_status
-            show_usage_guide
-            ;;
-            
-        stop)
-            stop_system
-            ;;
-            
-        restart)
-            stop_system
-            sleep 2
-            main start
-            ;;
-            
-        status)
-            show_status
-            ;;
-            
-        install-cli)
-            install_cli
-            ;;
-            
-        *)
-            echo "Usage: $0 {start|stop|restart|status|install-cli}"
-            echo
-            echo "Commands:"
-            echo "  start       - Start Ray cluster and JobManager Daemon"
-            echo "  stop        - Stop the entire SAGE system"
-            echo "  restart     - Restart the system"
-            echo "  status      - Show system status"
-            echo "  install-cli - Install command line tools only"
-            echo
-            echo "Environment Variables:"
-            echo "  RAY_HEAD_PORT=$RAY_HEAD_PORT         (GCS server port)"
-            echo "  RAY_CLIENT_PORT=$RAY_CLIENT_PORT       (Client server port)"
-            echo "  RAY_DASHBOARD_PORT=$RAY_DASHBOARD_PORT"
-            echo "  DAEMON_HOST=$DAEMON_HOST"
-            echo "  DAEMON_PORT=$DAEMON_PORT"
-            echo
-            echo "After starting, use 'sage-jm' command to manage jobs:"
-            echo "  sage-jm list           # List all jobs"
-            echo "  sage-jm health         # Check system health"
-            echo "  sage-jm monitor        # Real-time monitoring"
-            echo "  sage-jm --help         # Show all commands"
-            exit 1
-            ;;
-    esac
-}
-
-# 捕获信号
-trap 'log_info "Interrupted, stopping system..."; stop_system; exit 1' INT TERM
-
-# 执行主函数
-main "$@"
\ No newline at end of file
diff --git a/deployment/start_jobmanager.sh b/deployment/start_jobmanager.sh
deleted file mode 100644
index e590c08..0000000
--- a/deployment/start_jobmanager.sh
+++ /dev/null
@@ -1,52 +0,0 @@
-#!/bin/bash
-# filepath: /home/tjy/SAGE/scripts/start_jobmanager.sh
-
-SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
-DAEMON_SCRIPT="$SCRIPT_DIR/jobmanager_daemon.py"
-
-# 检查并启动JobManager
-echo "🚀 Checking JobManager service..."
-
-python3 "$DAEMON_SCRIPT" ensure
-
-if [ $? -eq 0 ]; then
-    echo "✅ JobManager is running on 127.0.0.1:19000"
-else
-    echo "❌ Failed to start JobManager"
-    exit 1
-fi
-
-# usage:
-# # 给脚本执行权限
-# chmod +x scripts/jobmanager_daemon.py
-# chmod +x scripts/start_jobmanager.sh
-
-# # 1. 检查并启动JobManager（推荐）
-# python3 scripts/jobmanager_daemon.py ensure
-
-# # 2. 使用bash脚本
-# ./scripts/start_jobmanager.sh
-
-# # 3. 其他命令
-# python3 scripts/jobmanager_daemon.py status    # 查看状态
-# python3 scripts/jobmanager_daemon.py stop      # 停止服务
-# python3 scripts/jobmanager_daemon.py restart   # 重启服务
-# python3 scripts/jobmanager_daemon.py start     # 启动服务
-
-# systemd service:
-
-# [Unit]
-# Description=SAGE JobManager Service
-# After=network.target
-
-# [Service]
-# Type=simple
-# User=tjy
-# WorkingDirectory=/home/tjy/SAGE
-# ExecStart=/usr/bin/python3 /home/tjy/SAGE/scripts/jobmanager_daemon.py start
-# ExecStop=/usr/bin/python3 /home/tjy/SAGE/scripts/jobmanager_daemon.py stop
-# Restart=always
-# RestartSec=10
-
-# [Install]
-# WantedBy=multi-user.target
\ No newline at end of file
diff --git a/deployment/stop_ray.sh b/deployment/stop_ray.sh
deleted file mode 100755
index d52941a..0000000
--- a/deployment/stop_ray.sh
+++ /dev/null
@@ -1,5 +0,0 @@
-# 第一步：停止老的 Ray 实例
-ray stop --force
-
-# 第二步：清理 tmp 下残留的 Ray 文件
-rm -rf /tmp/ray
\ No newline at end of file
diff --git a/playground/AbstractFileSource.java b/playground/AbstractFileSource.java
deleted file mode 100644
index 8b0e197..0000000
--- a/playground/AbstractFileSource.java
+++ /dev/null
@@ -1,349 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.file.src;
-
-import org.apache.flink.annotation.PublicEvolving;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.connector.source.Boundedness;
-import org.apache.flink.api.connector.source.Source;
-import org.apache.flink.api.connector.source.SourceReader;
-import org.apache.flink.api.connector.source.SourceReaderContext;
-import org.apache.flink.api.connector.source.SplitEnumerator;
-import org.apache.flink.api.connector.source.SplitEnumeratorContext;
-import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
-import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;
-import org.apache.flink.connector.file.src.enumerate.FileEnumerator;
-import org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumerator;
-import org.apache.flink.connector.file.src.impl.FileSourceReader;
-import org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator;
-import org.apache.flink.connector.file.src.reader.BulkFormat;
-import org.apache.flink.connector.file.src.reader.StreamFormat;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-import org.apache.flink.util.FlinkRuntimeException;
-
-import javax.annotation.Nullable;
-
-import java.io.IOException;
-import java.time.Duration;
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.stream.Collectors;
-
-import static org.apache.flink.util.Preconditions.checkArgument;
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * The base class for File Sources. The main implementation to use is the {@link FileSource}, which
- * also has the majority of the documentation.
- *
- * <p>To read new formats, one commonly does NOT need to extend this class, but should implement a
- * new Format Reader (like {@link StreamFormat}, {@link BulkFormat} and use it with the {@code
- * FileSource}.
- *
- * <p>The only reason to extend this class is when a source needs a different type of <i>split</i>,
- * meaning an extension of the {@link FileSourceSplit} to carry additional information.
- *
- * @param <T> The type of the events/records produced by this source.
- * @param <SplitT> The subclass type of the FileSourceSplit used by the source implementation.
- */
-@PublicEvolving
-public abstract class AbstractFileSource<T, SplitT extends FileSourceSplit>
-        implements Source<T, SplitT, PendingSplitsCheckpoint<SplitT>>, ResultTypeQueryable<T> {
-
-    private static final long serialVersionUID = 1L;
-
-    final Path[] inputPaths;
-
-    private final FileEnumerator.Provider enumeratorFactory;
-
-    private final FileSplitAssigner.Provider assignerFactory;
-
-    private final BulkFormat<T, SplitT> readerFormat;
-
-    @Nullable private final ContinuousEnumerationSettings continuousEnumerationSettings;
-
-    // ------------------------------------------------------------------------
-
-    protected AbstractFileSource(
-            final Path[] inputPaths,
-            final FileEnumerator.Provider fileEnumerator,
-            final FileSplitAssigner.Provider splitAssigner,
-            final BulkFormat<T, SplitT> readerFormat,
-            @Nullable final ContinuousEnumerationSettings continuousEnumerationSettings) {
-
-        checkArgument(inputPaths.length > 0);
-        this.inputPaths = inputPaths;
-        this.enumeratorFactory = checkNotNull(fileEnumerator);
-        this.assignerFactory = checkNotNull(splitAssigner);
-        this.readerFormat = checkNotNull(readerFormat);
-        this.continuousEnumerationSettings = continuousEnumerationSettings;
-    }
-
-    // ------------------------------------------------------------------------
-    //  Getters
-    // ------------------------------------------------------------------------
-
-    protected FileEnumerator.Provider getEnumeratorFactory() {
-        return enumeratorFactory;
-    }
-
-    public FileSplitAssigner.Provider getAssignerFactory() {
-        return assignerFactory;
-    }
-
-    @Nullable
-    public ContinuousEnumerationSettings getContinuousEnumerationSettings() {
-        return continuousEnumerationSettings;
-    }
-
-    // ------------------------------------------------------------------------
-    //  Source API Methods
-    // ------------------------------------------------------------------------
-
-    @Override
-    public Boundedness getBoundedness() {
-        return continuousEnumerationSettings == null
-                ? Boundedness.BOUNDED
-                : Boundedness.CONTINUOUS_UNBOUNDED;
-    }
-
-    @Override
-    public SourceReader<T, SplitT> createReader(SourceReaderContext readerContext) {
-        return new FileSourceReader<>(
-                readerContext, readerFormat, readerContext.getConfiguration());
-    }
-
-    @Override
-    public SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>> createEnumerator(
-            SplitEnumeratorContext<SplitT> enumContext) {
-
-        final FileEnumerator enumerator = enumeratorFactory.create();
-
-        // read the initial set of splits (which is also the total set of splits for bounded
-        // sources)
-        final Collection<FileSourceSplit> splits;
-        try {
-            // TODO - in the next cleanup pass, we should try to remove the need to "wrap unchecked"
-            // here
-            splits = enumerator.enumerateSplits(inputPaths, enumContext.currentParallelism());
-        } catch (IOException e) {
-            throw new FlinkRuntimeException("Could not enumerate file splits", e);
-        }
-
-        return createSplitEnumerator(enumContext, enumerator, splits, null);
-    }
-
-    @Override
-    public SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>> restoreEnumerator(
-            SplitEnumeratorContext<SplitT> enumContext,
-            PendingSplitsCheckpoint<SplitT> checkpoint) {
-
-        final FileEnumerator enumerator = enumeratorFactory.create();
-
-        // cast this to a collection of FileSourceSplit because the enumerator code work
-        // non-generically just on that base split type
-        @SuppressWarnings("unchecked")
-        final Collection<FileSourceSplit> splits =
-                (Collection<FileSourceSplit>) checkpoint.getSplits();
-
-        return createSplitEnumerator(
-                enumContext, enumerator, splits, checkpoint.getAlreadyProcessedPaths());
-    }
-
-    @Override
-    public abstract SimpleVersionedSerializer<SplitT> getSplitSerializer();
-
-    @Override
-    public SimpleVersionedSerializer<PendingSplitsCheckpoint<SplitT>>
-            getEnumeratorCheckpointSerializer() {
-        return new PendingSplitsCheckpointSerializer<>(getSplitSerializer());
-    }
-
-    @Override
-    public TypeInformation<T> getProducedType() {
-        return readerFormat.getProducedType();
-    }
-
-    // ------------------------------------------------------------------------
-    //  helpers
-    // ------------------------------------------------------------------------
-
-    private SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>> createSplitEnumerator(
-            SplitEnumeratorContext<SplitT> context,
-            FileEnumerator enumerator,
-            Collection<FileSourceSplit> splits,
-            @Nullable Collection<Path> alreadyProcessedPaths) {
-
-        // cast this to a collection of FileSourceSplit because the enumerator code work
-        // non-generically just on that base split type
-        @SuppressWarnings("unchecked")
-        final SplitEnumeratorContext<FileSourceSplit> fileSplitContext =
-                (SplitEnumeratorContext<FileSourceSplit>) context;
-
-        final FileSplitAssigner splitAssigner = assignerFactory.create(splits);
-
-        if (continuousEnumerationSettings == null) {
-            // bounded case
-            return castGeneric(new StaticFileSplitEnumerator(fileSplitContext, splitAssigner));
-        } else {
-            // unbounded case
-            if (alreadyProcessedPaths == null) {
-                alreadyProcessedPaths = splitsToPaths(splits);
-            }
-
-            return castGeneric(
-                    new ContinuousFileSplitEnumerator(
-                            fileSplitContext,
-                            enumerator,
-                            splitAssigner,
-                            inputPaths,
-                            alreadyProcessedPaths,
-                            continuousEnumerationSettings.getDiscoveryInterval().toMillis()));
-        }
-    }
-
-    @SuppressWarnings("unchecked")
-    private SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>> castGeneric(
-            final SplitEnumerator<FileSourceSplit, PendingSplitsCheckpoint<FileSourceSplit>>
-                    enumerator) {
-
-        // cast arguments away then cast them back. Java Generics Hell :-/
-        return (SplitEnumerator<SplitT, PendingSplitsCheckpoint<SplitT>>)
-                (SplitEnumerator<?, ?>) enumerator;
-    }
-
-    private static Collection<Path> splitsToPaths(Collection<FileSourceSplit> splits) {
-        return splits.stream()
-                .map(FileSourceSplit::path)
-                .collect(Collectors.toCollection(HashSet::new));
-    }
-
-    // ------------------------------------------------------------------------
-    //  Builder
-    // ------------------------------------------------------------------------
-
-    /**
-     * The generic base builder. This builder carries a <i>SELF</i> type to make it convenient to
-     * extend this for subclasses, using the following pattern.
-     *
-     * <pre>{@code
-     * public class SubBuilder<T> extends AbstractFileSourceBuilder<T, SubBuilder<T>> {
-     *     ...
-     * }
-     * }</pre>
-     *
-     * <p>That way, all return values from builder method defined here are typed to the sub-class
-     * type and support fluent chaining.
-     *
-     * <p>We don't make the publicly visible builder generic with a SELF type, because it leads to
-     * generic signatures that can look complicated and confusing.
-     */
-    protected abstract static class AbstractFileSourceBuilder<
-            T,
-            SplitT extends FileSourceSplit,
-            SELF extends AbstractFileSourceBuilder<T, SplitT, SELF>> {
-
-        // mandatory - have no defaults
-        protected final Path[] inputPaths;
-        protected final BulkFormat<T, SplitT> readerFormat;
-
-        // optional - have defaults
-        protected FileEnumerator.Provider fileEnumerator;
-        protected FileSplitAssigner.Provider splitAssigner;
-        @Nullable protected ContinuousEnumerationSettings continuousSourceSettings;
-
-        protected AbstractFileSourceBuilder(
-                final Path[] inputPaths,
-                final BulkFormat<T, SplitT> readerFormat,
-                final FileEnumerator.Provider defaultFileEnumerator,
-                final FileSplitAssigner.Provider defaultSplitAssigner) {
-
-            this.inputPaths = checkNotNull(inputPaths);
-            this.readerFormat = checkNotNull(readerFormat);
-            this.fileEnumerator = defaultFileEnumerator;
-            this.splitAssigner = defaultSplitAssigner;
-        }
-
-        /** Creates the file source with the settings applied to this builder. */
-        public abstract AbstractFileSource<T, SplitT> build();
-
-        /**
-         * Sets this source to streaming ("continuous monitoring") mode.
-         *
-         * <p>This makes the source a "continuous streaming" source that keeps running, monitoring
-         * for new files, and reads these files when they appear and are discovered by the
-         * monitoring.
-         *
-         * <p>The interval in which the source checks for new files is the {@code
-         * discoveryInterval}. Shorter intervals mean that files are discovered more quickly, but
-         * also imply more frequent listing or directory traversal of the file system / object
-         * store.
-         */
-        public SELF monitorContinuously(Duration discoveryInterval) {
-            checkNotNull(discoveryInterval, "discoveryInterval");
-            checkArgument(
-                    !(discoveryInterval.isNegative() || discoveryInterval.isZero()),
-                    "discoveryInterval must be > 0");
-
-            this.continuousSourceSettings = new ContinuousEnumerationSettings(discoveryInterval);
-            return self();
-        }
-
-        /**
-         * Sets this source to bounded (batch) mode.
-         *
-         * <p>In this mode, the source processes the files that are under the given paths when the
-         * application is started. Once all files are processed, the source will finish.
-         *
-         * <p>This setting is also the default behavior. This method is mainly here to "switch back"
-         * to bounded (batch) mode, or to make it explicit in the source construction.
-         */
-        public SELF processStaticFileSet() {
-            this.continuousSourceSettings = null;
-            return self();
-        }
-
-        /**
-         * Configures the {@link FileEnumerator} for the source. The File Enumerator is responsible
-         * for selecting from the input path the set of files that should be processed (and which to
-         * filter out). Furthermore, the File Enumerator may split the files further into
-         * sub-regions, to enable parallelization beyond the number of files.
-         */
-        public SELF setFileEnumerator(FileEnumerator.Provider fileEnumerator) {
-            this.fileEnumerator = checkNotNull(fileEnumerator);
-            return self();
-        }
-
-        /**
-         * Configures the {@link FileSplitAssigner} for the source. The File Split Assigner
-         * determines which parallel reader instance gets which {@link FileSourceSplit}, and in
-         * which order these splits are assigned.
-         */
-        public SELF setSplitAssigner(FileSplitAssigner.Provider splitAssigner) {
-            this.splitAssigner = checkNotNull(splitAssigner);
-            return self();
-        }
-
-        @SuppressWarnings("unchecked")
-        private SELF self() {
-            return (SELF) this;
-        }
-    }
-}
\ No newline at end of file
diff --git a/playground/BatchWordCount.java b/playground/BatchWordCount.java
deleted file mode 100644
index 968cd80..0000000
--- a/playground/BatchWordCount.java
+++ /dev/null
@@ -1,29 +0,0 @@
-ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
-
-DataSet<String> text = env.readTextFile("/path/to/file");
-
-DataSet<Tuple2<String, Integer>> counts =
-        // split up the lines in pairs (2-tuples) containing: (word,1)
-        text.flatMap(new Tokenizer())
-        // group by the tuple field "0" and sum up tuple field "1"
-        .groupBy(0)
-        .sum(1);
-
-counts.writeAsCsv(outputPath, "\n", " ");
-
-// User-defined functions
-public static class Tokenizer implements FlatMapFunction<String, Tuple2<String, Integer>> {
-
-    @Override
-    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
-        // normalize and split the line
-        String[] tokens = value.toLowerCase().split("\\W+");
-
-        // emit the pairs
-        for (String token : tokens) {
-            if (token.length() > 0) {
-                out.collect(new Tuple2<String, Integer>(token, 1));
-            }
-        }
-    }
-}
\ No newline at end of file
diff --git a/playground/ExecutionEnvironmentImpl.java b/playground/ExecutionEnvironmentImpl.java
deleted file mode 100644
index 81c984b..0000000
--- a/playground/ExecutionEnvironmentImpl.java
+++ /dev/null
@@ -1,363 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.datastream.impl;
-
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.RuntimeExecutionMode;
-import org.apache.flink.api.common.eventtime.WatermarkStrategy;
-import org.apache.flink.api.common.functions.InvalidTypesException;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.connector.dsv2.FromDataSource;
-import org.apache.flink.api.connector.dsv2.Source;
-import org.apache.flink.api.connector.dsv2.WrappedSource;
-import org.apache.flink.api.dag.Transformation;
-import org.apache.flink.api.java.typeutils.MissingTypeInfo;
-import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
-import org.apache.flink.api.java.typeutils.TypeExtractor;
-import org.apache.flink.configuration.Configuration;
-import org.apache.flink.configuration.DeploymentOptions;
-import org.apache.flink.configuration.ExecutionOptions;
-import org.apache.flink.configuration.ReadableConfig;
-import org.apache.flink.connector.datagen.functions.FromElementsGeneratorFunction;
-import org.apache.flink.connector.datagen.source.DataGeneratorSource;
-import org.apache.flink.core.execution.DefaultExecutorServiceLoader;
-import org.apache.flink.core.execution.JobClient;
-import org.apache.flink.core.execution.PipelineExecutor;
-import org.apache.flink.core.execution.PipelineExecutorFactory;
-import org.apache.flink.core.execution.PipelineExecutorServiceLoader;
-import org.apache.flink.datastream.api.ExecutionEnvironment;
-import org.apache.flink.datastream.api.stream.NonKeyedPartitionStream.ProcessConfigurableAndNonKeyedPartitionStream;
-import org.apache.flink.datastream.impl.stream.NonKeyedPartitionStreamImpl;
-import org.apache.flink.datastream.impl.utils.StreamUtils;
-import org.apache.flink.streaming.api.environment.CheckpointConfig;
-import org.apache.flink.streaming.api.graph.StreamGraph;
-import org.apache.flink.streaming.api.graph.StreamGraphGenerator;
-import org.apache.flink.streaming.api.transformations.SourceTransformation;
-import org.apache.flink.streaming.runtime.translators.DataStreamV2SinkTransformationTranslator;
-import org.apache.flink.util.ExceptionUtils;
-import org.apache.flink.util.FlinkException;
-import org.apache.flink.util.Preconditions;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutionException;
-
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * The implementation of {@link ExecutionEnvironment}.
- *
- * <p>IMPORTANT: Even though this is not part of public API, {@link ExecutionEnvironment} will get
- * this class instance through reflection, so we must ensure that the package path, class name and
- * the signature of {@link #newInstance()} does not change.
- */
-public class ExecutionEnvironmentImpl implements ExecutionEnvironment {
-    private final List<Transformation<?>> transformations = new ArrayList<>();
-
-    private final ExecutionConfig executionConfig;
-
-    /** Settings that control the checkpointing behavior. */
-    private final CheckpointConfig checkpointCfg;
-
-    private final Configuration configuration;
-
-    private final ClassLoader userClassloader;
-
-    private final PipelineExecutorServiceLoader executorServiceLoader;
-
-    /**
-     * The environment of the context (local by default, cluster if invoked through command line).
-     */
-    private static ExecutionEnvironmentFactory contextEnvironmentFactory = null;
-
-    static {
-        try {
-            // All transformation translator must be put to a map in StreamGraphGenerator, but
-            // streaming-java is not depend on process-function module, using reflect to handle
-            // this.
-            DataStreamV2SinkTransformationTranslator.registerSinkTransformationTranslator();
-        } catch (Exception e) {
-            throw new RuntimeException(
-                    "Can not register process function transformation translator.", e);
-        }
-    }
-
-    /**
-     * Create and return an instance of {@link ExecutionEnvironment}.
-     *
-     * <p>IMPORTANT: The method is only expected to be called by {@link ExecutionEnvironment} via
-     * reflection, so we must ensure that the package path, class name and the signature of this
-     * method does not change.
-     */
-    public static ExecutionEnvironment newInstance() {
-        if (contextEnvironmentFactory != null) {
-            return contextEnvironmentFactory.createExecutionEnvironment(new Configuration());
-        } else {
-            final Configuration configuration = new Configuration();
-            configuration.set(DeploymentOptions.TARGET, "local");
-            configuration.set(DeploymentOptions.ATTACHED, true);
-            return new ExecutionEnvironmentImpl(
-                    new DefaultExecutorServiceLoader(), configuration, null);
-        }
-    }
-
-    ExecutionEnvironmentImpl(
-            PipelineExecutorServiceLoader executorServiceLoader,
-            Configuration configuration,
-            ClassLoader classLoader) {
-        this.executorServiceLoader = checkNotNull(executorServiceLoader);
-        this.configuration = configuration;
-        this.executionConfig = new ExecutionConfig(this.configuration);
-        this.checkpointCfg = new CheckpointConfig(this.configuration);
-        this.userClassloader = classLoader == null ? getClass().getClassLoader() : classLoader;
-        configure(configuration, userClassloader);
-    }
-
-    @Override
-    public void execute(String jobName) throws Exception {
-        StreamGraph streamGraph = getStreamGraph();
-        if (jobName != null) {
-            streamGraph.setJobName(jobName);
-        }
-
-        execute(streamGraph);
-    }
-
-    @Override
-    public RuntimeExecutionMode getExecutionMode() {
-        return configuration.get(ExecutionOptions.RUNTIME_MODE);
-    }
-
-    @Override
-    public ExecutionEnvironment setExecutionMode(RuntimeExecutionMode runtimeMode) {
-        checkNotNull(runtimeMode);
-        configuration.set(ExecutionOptions.RUNTIME_MODE, runtimeMode);
-        return this;
-    }
-
-    protected static void initializeContextEnvironment(ExecutionEnvironmentFactory ctx) {
-        contextEnvironmentFactory = ctx;
-    }
-
-    protected static void resetContextEnvironment() {
-        contextEnvironmentFactory = null;
-    }
-    /******
-     * // <OUT>: 方法泛型
-     * 
-     * 
-
-     */
-    
-    @Override
-    public <OUT> ProcessConfigurableAndNonKeyedPartitionStream<OUT> fromSource(
-            Source<OUT> source, String sourceName) {
-        if (source instanceof WrappedSource) {
-            org.apache.flink.api.connector.source.Source<OUT, ?, ?> innerSource =
-                    ((WrappedSource<OUT>) source).getWrappedSource();
-            final TypeInformation<OUT> resolvedTypeInfo =
-                    getSourceTypeInfo(innerSource, sourceName);
-
-            SourceTransformation<OUT, ?, ?> sourceTransformation =
-                    new SourceTransformation<>(
-                            sourceName,
-                            innerSource,
-                            WatermarkStrategy.noWatermarks(),
-                            resolvedTypeInfo,
-                            getParallelism(),
-                            false);
-            return StreamUtils.wrapWithConfigureHandle(
-                    new NonKeyedPartitionStreamImpl<>(this, sourceTransformation));
-        } else if (source instanceof FromDataSource) {
-            Collection<OUT> data = ((FromDataSource<OUT>) source).getData();
-            TypeInformation<OUT> outType = extractTypeInfoFromCollection(data);
-
-            FromElementsGeneratorFunction<OUT> generatorFunction =
-                    new FromElementsGeneratorFunction<>(outType, executionConfig, data);
-
-            DataGeneratorSource<OUT> generatorSource =
-                    new DataGeneratorSource<>(generatorFunction, data.size(), outType);
-
-            return fromSource(new WrappedSource<>(generatorSource), "Collection Source");
-        } else {
-            throw new UnsupportedOperationException(
-                    "Unsupported type of source, you could use DataStreamV2SourceUtils to wrap a FLIP-27 based source.");
-        }
-    }
-
-    public Configuration getConfiguration() {
-        return this.configuration;
-    }
-
-    public ExecutionConfig getExecutionConfig() {
-        return executionConfig;
-    }
-
-    public int getParallelism() {
-        return executionConfig.getParallelism();
-    }
-
-    public List<Transformation<?>> getTransformations() {
-        return transformations;
-    }
-
-    public void setParallelism(int parallelism) {
-        executionConfig.setParallelism(parallelism);
-    }
-
-    public CheckpointConfig getCheckpointCfg() {
-        return checkpointCfg;
-    }
-
-    // -----------------------------------------------
-    //              Internal Methods
-    // -----------------------------------------------
-
-    private static <OUT> TypeInformation<OUT> extractTypeInfoFromCollection(Collection<OUT> data) {
-        Preconditions.checkNotNull(data, "Collection must not be null");
-        if (data.isEmpty()) {
-            throw new IllegalArgumentException("Collection must not be empty");
-        }
-
-        OUT first = data.iterator().next();
-        if (first == null) {
-            throw new IllegalArgumentException("Collection must not contain null elements");
-        }
-
-        TypeInformation<OUT> typeInfo;
-        try {
-            typeInfo = TypeExtractor.getForObject(first);
-        } catch (Exception e) {
-            throw new RuntimeException(
-                    "Could not create TypeInformation for type "
-                            + first.getClass()
-                            + "; please specify the TypeInformation manually via the version of the "
-                            + "method that explicitly accepts it as an argument.",
-                    e);
-        }
-        return typeInfo;
-    }
-
-    @SuppressWarnings("unchecked")
-    private static <OUT, T extends TypeInformation<OUT>> T getSourceTypeInfo(
-            org.apache.flink.api.connector.source.Source<OUT, ?, ?> source, String sourceName) {
-        TypeInformation<OUT> resolvedTypeInfo = null;
-        if (source instanceof ResultTypeQueryable) {
-            resolvedTypeInfo = ((ResultTypeQueryable<OUT>) source).getProducedType();
-        }
-        if (resolvedTypeInfo == null) {
-            try {
-                resolvedTypeInfo =
-                        TypeExtractor.createTypeInfo(
-                                org.apache.flink.api.connector.source.Source.class,
-                                source.getClass(),
-                                0,
-                                null,
-                                null);
-            } catch (final InvalidTypesException e) {
-                resolvedTypeInfo = (TypeInformation<OUT>) new MissingTypeInfo(sourceName, e);
-            }
-        }
-        return (T) resolvedTypeInfo;
-    }
-
-    public void addOperator(Transformation<?> transformation) {
-        checkNotNull(transformation, "transformation must not be null.");
-        this.transformations.add(transformation);
-    }
-
-    private void execute(StreamGraph streamGraph) throws Exception {
-        final JobClient jobClient = executeAsync(streamGraph);
-
-        try {
-            if (configuration.get(DeploymentOptions.ATTACHED)) {
-                jobClient.getJobExecutionResult().get();
-            }
-            // TODO Supports accumulator.
-        } catch (Throwable t) {
-            // get() on the JobExecutionResult Future will throw an ExecutionException. This
-            // behaviour was largely not there in Flink versions before the PipelineExecutor
-            // refactoring so we should strip that exception.
-            Throwable strippedException = ExceptionUtils.stripExecutionException(t);
-            ExceptionUtils.rethrowException(strippedException);
-        }
-    }
-
-    private JobClient executeAsync(StreamGraph streamGraph) throws Exception {
-        checkNotNull(streamGraph, "StreamGraph cannot be null.");
-        final PipelineExecutor executor = getPipelineExecutor();
-
-        CompletableFuture<JobClient> jobClientFuture =
-                executor.execute(streamGraph, configuration, getClass().getClassLoader());
-
-        try {
-            // TODO Supports job listeners.
-            return jobClientFuture.get();
-        } catch (ExecutionException executionException) {
-            final Throwable strippedException =
-                    ExceptionUtils.stripExecutionException(executionException);
-            throw new FlinkException(
-                    String.format("Failed to execute job '%s'.", streamGraph.getJobName()),
-                    strippedException);
-        }
-    }
-
-    /** Get {@link StreamGraph} and clear all transformations. */
-    public StreamGraph getStreamGraph() {
-        final StreamGraph streamGraph = getStreamGraphGenerator(transformations).generate();
-        transformations.clear();
-        return streamGraph;
-    }
-
-    private StreamGraphGenerator getStreamGraphGenerator(List<Transformation<?>> transformations) {
-        if (transformations.size() <= 0) {
-            throw new IllegalStateException(
-                    "No operators defined in streaming topology. Cannot execute.");
-        }
-
-        // We copy the transformation so that newly added transformations cannot intervene with the
-        // stream graph generation.
-        return new StreamGraphGenerator(
-                new ArrayList<>(transformations), executionConfig, checkpointCfg, configuration);
-    }
-
-    private PipelineExecutor getPipelineExecutor() throws Exception {
-        checkNotNull(
-                configuration.get(DeploymentOptions.TARGET),
-                "No execution.target specified in your configuration file.");
-
-        final PipelineExecutorFactory executorFactory =
-                executorServiceLoader.getExecutorFactory(configuration);
-
-        checkNotNull(
-                executorFactory,
-                "Cannot find compatible factory for specified execution.target (=%s)",
-                configuration.get(DeploymentOptions.TARGET));
-
-        return executorFactory.getExecutor(configuration);
-    }
-
-    private void configure(ReadableConfig configuration, ClassLoader classLoader) {
-        this.configuration.addAll(Configuration.fromMap(configuration.toMap()));
-        executionConfig.configure(configuration, classLoader);
-        checkpointCfg.configure(configuration);
-    }
-}
\ No newline at end of file
diff --git a/playground/FileSource.java b/playground/FileSource.java
deleted file mode 100644
index d06799b..0000000
--- a/playground/FileSource.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.file.src;
-
-import org.apache.flink.annotation.PublicEvolving;
-import org.apache.flink.api.connector.source.DynamicParallelismInference;
-import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;
-import org.apache.flink.connector.file.src.assigners.LocalityAwareSplitAssigner;
-import org.apache.flink.connector.file.src.enumerate.BlockSplittingRecursiveEnumerator;
-import org.apache.flink.connector.file.src.enumerate.FileEnumerator;
-import org.apache.flink.connector.file.src.enumerate.NonSplittingRecursiveEnumerator;
-import org.apache.flink.connector.file.src.impl.StreamFormatAdapter;
-import org.apache.flink.connector.file.src.reader.BulkFormat;
-import org.apache.flink.connector.file.src.reader.StreamFormat;
-import org.apache.flink.core.fs.FileSystem;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-import org.apache.flink.util.FlinkRuntimeException;
-
-import javax.annotation.Nullable;
-
-import java.io.IOException;
-import java.time.Duration;
-import java.util.Collection;
-
-import static org.apache.flink.util.Preconditions.checkArgument;
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * A unified data source that reads files - both in batch and in streaming mode.
- *
- * <p>This source supports all (distributed) file systems and object stores that can be accessed via
- * the Flink's {@link FileSystem} class.
- *
- * <p>Start building a file source via one of the following calls:
- *
- * <ul>
- *   <li>{@link FileSource#forRecordStreamFormat(StreamFormat, Path...)}
- *   <li>{@link FileSource#forBulkFileFormat(BulkFormat, Path...)}
- * </ul>
- *
- * <p>This creates a {@link FileSource.FileSourceBuilder} on which you can configure all the
- * properties of the file source.
- *
- * <h2>Batch and Streaming</h2>
- *
- * <p>This source supports both bounded/batch and continuous/streaming data inputs. For the
- * bounded/batch case, the file source processes all files under the given path(s). In the
- * continuous/streaming case, the source periodically checks the paths for new files and will start
- * reading those.
- *
- * <p>When you start creating a file source (via the {@link FileSource.FileSourceBuilder} created
- * through one of the above-mentioned methods) the source is by default in bounded/batch mode. Call
- * {@link FileSource.FileSourceBuilder#monitorContinuously(Duration)} to put the source into
- * continuous streaming mode.
- *
- * <h2>Format Types</h2>
- *
- * <p>The reading of each file happens through file readers defined by <i>file formats</i>. These
- * define the parsing logic for the contents of the file. There are multiple classes that the source
- * supports. Their interfaces trade of simplicity of implementation and flexibility/efficiency.
- *
- * <ul>
- *   <li>A {@link StreamFormat} reads the contents of a file from a file stream. It is the simplest
- *       format to implement, and provides many features out-of-the-box (like checkpointing logic)
- *       but is limited in the optimizations it can apply (such as object reuse, batching, etc.).
- *   <li>A {@link BulkFormat} reads batches of records from a file at a time. It is the most "low
- *       level" format to implement, but offers the greatest flexibility to optimize the
- *       implementation.
- * </ul>
- *
- * <h2>Discovering / Enumerating Files</h2>
- *
- * <p>The way that the source lists the files to be processes is defined by the {@link
- * FileEnumerator}. The {@code FileEnumerator} is responsible to select the relevant files (for
- * example filter out hidden files) and to optionally splits files into multiple regions (= file
- * source splits) that can be read in parallel).
- *
- * @param <T> The type of the events/records produced by this source.
- */
-@PublicEvolving
-public final class FileSource<T> extends AbstractFileSource<T, FileSourceSplit>
-        implements DynamicParallelismInference {
-
-    private static final long serialVersionUID = 1L;
-
-    /** The default split assigner, a lazy locality-aware assigner. */
-    public static final FileSplitAssigner.Provider DEFAULT_SPLIT_ASSIGNER =
-            LocalityAwareSplitAssigner::new;
-
-    /**
-     * The default file enumerator used for splittable formats. The enumerator recursively
-     * enumerates files, split files that consist of multiple distributed storage blocks into
-     * multiple splits, and filters hidden files (files starting with '.' or '_'). Files with
-     * suffixes of common compression formats (for example '.gzip', '.bz2', '.xy', '.zip', ...) will
-     * not be split.
-     */
-    public static final FileEnumerator.Provider DEFAULT_SPLITTABLE_FILE_ENUMERATOR =
-            BlockSplittingRecursiveEnumerator::new;
-
-    /**
-     * The default file enumerator used for non-splittable formats. The enumerator recursively
-     * enumerates files, creates one split for the file, and filters hidden files (files starting
-     * with '.' or '_').
-     */
-    public static final FileEnumerator.Provider DEFAULT_NON_SPLITTABLE_FILE_ENUMERATOR =
-            NonSplittingRecursiveEnumerator::new;
-
-    // ------------------------------------------------------------------------
-
-    private FileSource(
-            final Path[] inputPaths,
-            final FileEnumerator.Provider fileEnumerator,
-            final FileSplitAssigner.Provider splitAssigner,
-            final BulkFormat<T, FileSourceSplit> readerFormat,
-            @Nullable final ContinuousEnumerationSettings continuousEnumerationSettings) {
-
-        super(
-                inputPaths,
-                fileEnumerator,
-                splitAssigner,
-                readerFormat,
-                continuousEnumerationSettings);
-    }
-
-    @Override
-    public SimpleVersionedSerializer<FileSourceSplit> getSplitSerializer() {
-        return FileSourceSplitSerializer.INSTANCE;
-    }
-
-    @Override
-    public int inferParallelism(Context dynamicParallelismContext) {
-        FileEnumerator fileEnumerator = getEnumeratorFactory().create();
-
-        Collection<FileSourceSplit> splits;
-        try {
-            splits =
-                    fileEnumerator.enumerateSplits(
-                            inputPaths,
-                            dynamicParallelismContext.getParallelismInferenceUpperBound());
-        } catch (IOException e) {
-            throw new FlinkRuntimeException("Could not enumerate file splits", e);
-        }
-
-        return Math.min(
-                splits.size(), dynamicParallelismContext.getParallelismInferenceUpperBound());
-    }
-
-    // ------------------------------------------------------------------------
-    //  Entry-point Factory Methods
-    // ------------------------------------------------------------------------
-
-    /**
-     * Builds a new {@code FileSource} using a {@link StreamFormat} to read record-by-record from a
-     * file stream.
-     *
-     * <p>When possible, stream-based formats are generally easier (preferable) to file-based
-     * formats, because they support better default behavior around I/O batching or progress
-     * tracking (checkpoints).
-     *
-     * <p>Stream formats also automatically de-compress files based on the file extension. This
-     * supports files ending in ".deflate" (Deflate), ".xz" (XZ), ".bz2" (BZip2), ".gz", ".gzip"
-     * (GZip).
-     */
-    public static <T> FileSourceBuilder<T> forRecordStreamFormat(
-            final StreamFormat<T> streamFormat, final Path... paths) {
-        return forBulkFileFormat(new StreamFormatAdapter<>(streamFormat), paths);
-    }
-
-    /**
-     * Builds a new {@code FileSource} using a {@link BulkFormat} to read batches of records from
-     * files.
-     *
-     * <p>Examples for bulk readers are compressed and vectorized formats such as ORC or Parquet.
-     */
-    public static <T> FileSourceBuilder<T> forBulkFileFormat(
-            final BulkFormat<T, FileSourceSplit> bulkFormat, final Path... paths) {
-        checkNotNull(bulkFormat, "reader");
-        checkNotNull(paths, "paths");
-        checkArgument(paths.length > 0, "paths must not be empty");
-
-        return new FileSourceBuilder<>(paths, bulkFormat);
-    }
-
-    // ------------------------------------------------------------------------
-    //  Builder
-    // ------------------------------------------------------------------------
-
-    /**
-     * The builder for the {@code FileSource}, to configure the various behaviors.
-     *
-     * <p>Start building the source via one of the following methods:
-     *
-     * <ul>
-     *   <li>{@link FileSource#forRecordStreamFormat(StreamFormat, Path...)}
-     *   <li>{@link FileSource#forBulkFileFormat(BulkFormat, Path...)}
-     * </ul>
-     */
-    public static final class FileSourceBuilder<T>
-            extends AbstractFileSourceBuilder<T, FileSourceSplit, FileSourceBuilder<T>> {
-
-        FileSourceBuilder(Path[] inputPaths, BulkFormat<T, FileSourceSplit> readerFormat) {
-            super(
-                    inputPaths,
-                    readerFormat,
-                    readerFormat.isSplittable()
-                            ? DEFAULT_SPLITTABLE_FILE_ENUMERATOR
-                            : DEFAULT_NON_SPLITTABLE_FILE_ENUMERATOR,
-                    DEFAULT_SPLIT_ASSIGNER);
-        }
-
-        @Override
-        public FileSource<T> build() {
-            return new FileSource<>(
-                    inputPaths,
-                    fileEnumerator,
-                    splitAssigner,
-                    readerFormat,
-                    continuousSourceSettings);
-        }
-    }
-}
\ No newline at end of file
diff --git a/playground/Source.java b/playground/Source.java
deleted file mode 100644
index e7f2f77..0000000
--- a/playground/Source.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.api.connector.source;
-
-import org.apache.flink.annotation.Public;
-import org.apache.flink.api.common.watermark.WatermarkDeclaration;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-
-import java.util.Collections;
-import java.util.Set;
-
-/**
- * The interface for Source. It acts like a factory class that helps construct the {@link
- * SplitEnumerator} and {@link SourceReader} and corresponding serializers.
- *
- * @param <T> The type of records produced by the source.
- * @param <SplitT> The type of splits handled by the source.
- * @param <EnumChkT> The type of the enumerator checkpoints.
- */
-@Public
-public interface Source<T, SplitT extends SourceSplit, EnumChkT>
-        extends SourceReaderFactory<T, SplitT> {
-
-    /**
-     * Get the boundedness of this source.
-     *
-     * @return the boundedness of this source.
-     */
-    Boundedness getBoundedness();
-
-    /**
-     * Creates a new SplitEnumerator for this source, starting a new input.
-     *
-     * @param enumContext The {@link SplitEnumeratorContext context} for the split enumerator.
-     * @return A new SplitEnumerator.
-     * @throws Exception The implementor is free to forward all exceptions directly. Exceptions
-     *     thrown from this method cause JobManager failure/recovery.
-     */
-    SplitEnumerator<SplitT, EnumChkT> createEnumerator(SplitEnumeratorContext<SplitT> enumContext)
-            throws Exception;
-
-    /**
-     * Restores an enumerator from a checkpoint.
-     *
-     * @param enumContext The {@link SplitEnumeratorContext context} for the restored split
-     *     enumerator.
-     * @param checkpoint The checkpoint to restore the SplitEnumerator from.
-     * @return A SplitEnumerator restored from the given checkpoint.
-     * @throws Exception The implementor is free to forward all exceptions directly. Exceptions
-     *     thrown from this method cause JobManager failure/recovery.
-     */
-    SplitEnumerator<SplitT, EnumChkT> restoreEnumerator(
-            SplitEnumeratorContext<SplitT> enumContext, EnumChkT checkpoint) throws Exception;
-
-    // ------------------------------------------------------------------------
-    //  serializers for the metadata
-    // ------------------------------------------------------------------------
-
-    /**
-     * Creates a serializer for the source splits. Splits are serialized when sending them from
-     * enumerator to reader, and when checkpointing the reader's current state.
-     *
-     * @return The serializer for the split type.
-     */
-    SimpleVersionedSerializer<SplitT> getSplitSerializer();
-
-    /**
-     * Creates the serializer for the {@link SplitEnumerator} checkpoint. The serializer is used for
-     * the result of the {@link SplitEnumerator#snapshotState(long)} method.
-     *
-     * @return The serializer for the SplitEnumerator checkpoint.
-     */
-    SimpleVersionedSerializer<EnumChkT> getEnumeratorCheckpointSerializer();
-
-    /**
-     * Explicitly declare watermarks upfront. Each specific watermark must be declared in this
-     * method before it can be used.
-     *
-     * @return all watermark declarations used by this application.
-     */
-    default Set<? extends WatermarkDeclaration> declareWatermarks() {
-        return Collections.emptySet();
-    }
-}
\ No newline at end of file
diff --git a/playground/SourceOperatorFactory.java b/playground/SourceOperatorFactory.java
deleted file mode 100644
index 4624854..0000000
--- a/playground/SourceOperatorFactory.java
+++ /dev/null
@@ -1,236 +0,0 @@
-/*
-Licensed to the Apache Software Foundation (ASF) under one
-or more contributor license agreements.  See the NOTICE file
-distributed with this work for additional information
-regarding copyright ownership.  The ASF licenses this file
-to you under the Apache License, Version 2.0 (the
-"License"); you may not use this file except in compliance
-with the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package org.apache.flink.streaming.api.operators;
-
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.eventtime.WatermarkStrategy;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.watermark.WatermarkDeclaration;
-import org.apache.flink.api.connector.source.Boundedness;
-import org.apache.flink.api.connector.source.Source;
-import org.apache.flink.api.connector.source.SourceReader;
-import org.apache.flink.api.connector.source.SourceReaderContext;
-import org.apache.flink.api.connector.source.SourceSplit;
-import org.apache.flink.configuration.Configuration;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-import org.apache.flink.runtime.jobgraph.OperatorID;
-import org.apache.flink.runtime.operators.coordination.OperatorCoordinator;
-import org.apache.flink.runtime.operators.coordination.OperatorEventGateway;
-import org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider;
-import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;
-import org.apache.flink.streaming.runtime.tasks.ProcessingTimeServiceAware;
-import org.apache.flink.streaming.runtime.tasks.StreamTask.CanEmitBatchOfRecordsChecker;
-import org.apache.flink.streaming.runtime.watermark.AbstractInternalWatermarkDeclaration;
-import org.apache.flink.streaming.util.watermark.WatermarkUtils;
-import org.apache.flink.util.function.FunctionWithException;
-
-import javax.annotation.Nullable;
-
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
-import java.util.stream.Collectors;
-
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/** The Factory class for {@link SourceOperator}. */
-public class SourceOperatorFactory<OUT> extends AbstractStreamOperatorFactory<OUT>
-        implements CoordinatedOperatorFactory<OUT>, ProcessingTimeServiceAware {
-
-    private static final long serialVersionUID = 1L;
-
-    /** The {@link Source} to create the {@link SourceOperator}. */
-    private final Source<OUT, ?, ?> source;
-
-    /** The event time setup (timestamp assigners, watermark generators, etc.). */
-    private final WatermarkStrategy<OUT> watermarkStrategy;
-
-    /** Whether to emit intermediate watermarks or only one final watermark at the end of input. */
-    private final boolean emitProgressiveWatermarks;
-
-    /** The number of worker thread for the source coordinator. */
-    private final int numCoordinatorWorkerThread;
-
-    private @Nullable String coordinatorListeningID;
-
-    public SourceOperatorFactory(
-            Source<OUT, ?, ?> source, WatermarkStrategy<OUT> watermarkStrategy) {
-        this(source, watermarkStrategy, true /* emit progressive watermarks */, 1);
-    }
-
-    public SourceOperatorFactory(
-            Source<OUT, ?, ?> source,
-            WatermarkStrategy<OUT> watermarkStrategy,
-            boolean emitProgressiveWatermarks) {
-        this(source, watermarkStrategy, emitProgressiveWatermarks, 1);
-    }
-
-    public SourceOperatorFactory(
-            Source<OUT, ?, ?> source,
-            WatermarkStrategy<OUT> watermarkStrategy,
-            boolean emitProgressiveWatermarks,
-            int numCoordinatorWorkerThread) {
-        this.source = checkNotNull(source);
-        this.watermarkStrategy = checkNotNull(watermarkStrategy);
-        this.emitProgressiveWatermarks = emitProgressiveWatermarks;
-        this.numCoordinatorWorkerThread = numCoordinatorWorkerThread;
-    }
-
-    public Boundedness getBoundedness() {
-        return source.getBoundedness();
-    }
-
-    public void setCoordinatorListeningID(@Nullable String coordinatorListeningID) {
-        this.coordinatorListeningID = coordinatorListeningID;
-    }
-
-    @Override
-    public <T extends StreamOperator<OUT>> T createStreamOperator(
-            StreamOperatorParameters<OUT> parameters) {
-        final OperatorID operatorId = parameters.getStreamConfig().getOperatorID();
-        final OperatorEventGateway gateway =
-                parameters.getOperatorEventDispatcher().getOperatorEventGateway(operatorId);
-
-        final SourceOperator<OUT, ?> sourceOperator =
-                instantiateSourceOperator(
-                        parameters,
-                        source::createReader,
-                        gateway,
-                        source.getSplitSerializer(),
-                        watermarkStrategy,
-                        parameters.getProcessingTimeService(),
-                        parameters
-                                .getContainingTask()
-                                .getEnvironment()
-                                .getTaskManagerInfo()
-                                .getConfiguration(),
-                        parameters
-                                .getContainingTask()
-                                .getEnvironment()
-                                .getTaskManagerInfo()
-                                .getTaskManagerExternalAddress(),
-                        emitProgressiveWatermarks,
-                        parameters.getContainingTask().getCanEmitBatchOfRecords(),
-                        getSourceWatermarkDeclarations());
-
-        parameters.getOperatorEventDispatcher().registerEventHandler(operatorId, sourceOperator);
-
-        // today's lunch is generics spaghetti
-        @SuppressWarnings("unchecked")
-        final T castedOperator = (T) sourceOperator;
-
-        return castedOperator;
-    }
-
-    @Override
-    public OperatorCoordinator.Provider getCoordinatorProvider(
-            String operatorName, OperatorID operatorID) {
-        return new SourceCoordinatorProvider<>(
-                operatorName,
-                operatorID,
-                source,
-                numCoordinatorWorkerThread,
-                watermarkStrategy.getAlignmentParameters(),
-                coordinatorListeningID);
-    }
-
-    @SuppressWarnings("rawtypes")
-    @Override
-    public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {
-        return SourceOperator.class;
-    }
-
-    @Override
-    public boolean isStreamSource() {
-        return true;
-    }
-
-    @Override
-    public boolean isOutputTypeConfigurable() {
-        return source instanceof OutputTypeConfigurable;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public void setOutputType(TypeInformation<OUT> type, ExecutionConfig executionConfig) {
-        if (source instanceof OutputTypeConfigurable) {
-            ((OutputTypeConfigurable<OUT>) source).setOutputType(type, executionConfig);
-        }
-    }
-
-    public Set<? extends WatermarkDeclaration> getSourceWatermarkDeclarations() {
-        return source.declareWatermarks();
-    }
-
-    /**
-     * This is a utility method to conjure up a "SplitT" generics variable binding so that we can
-     * construct the SourceOperator without resorting to "all raw types". That way, this methods
-     * puts all "type non-safety" in one place and allows to maintain as much generics safety in the
-     * main code as possible.
-     */
-    @SuppressWarnings("unchecked")
-    private static <T, SplitT extends SourceSplit>
-            SourceOperator<T, SplitT> instantiateSourceOperator(
-                    StreamOperatorParameters<T> parameters,
-                    FunctionWithException<SourceReaderContext, SourceReader<T, ?>, Exception>
-                            readerFactory,
-                    OperatorEventGateway eventGateway,
-                    SimpleVersionedSerializer<?> splitSerializer,
-                    WatermarkStrategy<T> watermarkStrategy,
-                    ProcessingTimeService timeService,
-                    Configuration config,
-                    String localHostName,
-                    boolean emitProgressiveWatermarks,
-                    CanEmitBatchOfRecordsChecker canEmitBatchOfRecords,
-                    Collection<? extends WatermarkDeclaration> watermarkDeclarations) {
-
-        // jumping through generics hoops: cast the generics away to then cast them back more
-        // strictly typed
-        final FunctionWithException<SourceReaderContext, SourceReader<T, SplitT>, Exception>
-                typedReaderFactory =
-                        (FunctionWithException<
-                                        SourceReaderContext, SourceReader<T, SplitT>, Exception>)
-                                (FunctionWithException<?, ?, ?>) readerFactory;
-
-        final SimpleVersionedSerializer<SplitT> typedSplitSerializer =
-                (SimpleVersionedSerializer<SplitT>) splitSerializer;
-
-        Map<String, Boolean> watermarkIsAlignedMap =
-                WatermarkUtils.convertToInternalWatermarkDeclarations(
-                                new HashSet<>(watermarkDeclarations))
-                        .stream()
-                        .collect(
-                                Collectors.toMap(
-                                        AbstractInternalWatermarkDeclaration::getIdentifier,
-                                        AbstractInternalWatermarkDeclaration::isAligned));
-        return new SourceOperator<>(
-                parameters,
-                typedReaderFactory,
-                eventGateway,
-                typedSplitSerializer,
-                watermarkStrategy,
-                timeService,
-                config,
-                localHostName,
-                emitProgressiveWatermarks,
-                canEmitBatchOfRecords,
-                watermarkIsAlignedMap);
-    }
-}
\ No newline at end of file
diff --git a/playground/StreamingWordCount b/playground/StreamingWordCount
deleted file mode 100644
index 62b86e4..0000000
--- a/playground/StreamingWordCount
+++ /dev/null
@@ -1,127 +0,0 @@
-package com.my.examples.redpandaFlink;
-
-import org.apache.flink.api.common.functions.FlatMapFunction;
-import org.apache.flink.streaming.api.datastream.DataStream;
-import org.apache.flink.api.java.tuple.Tuple2;
-import org.apache.flink.util.Collector;
-import org.apache.flink.connector.kafka.source.KafkaSource;
-import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
-import org.apache.flink.connector.kafka.sink.KafkaSink;
-import org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchema;
-import org.apache.flink.api.common.serialization.SimpleStringSchema;
-import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-import org.apache.flink.api.common.eventtime.WatermarkStrategy;
-
-// this code was mostly cobbled together from an original demo found at:
-// https://github.com/redpanda-data/flink-kafka-examples/blob/main/src/main/java/io/redpanda/examples/WordCount.java
-
-public class StreamingJob {
-    final static String inputTopic = "words";
-    final static String outputTopic = "words-count";
-    final static String jobTitle = "WordCount";
-
-    public static void main(String[] args) throws Exception {
-        // Redpanda is listening on localhost. Remember to use the container name for the address
-        final String bootstrapServers = args.length > 0 ? args[0] : "redpanda-1:9092";
-
-        // set up the streaming execution environment
-        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
-
-        KafkaSource<String> source = KafkaSource.<String>builder()
-                .setBootstrapServers(bootstrapServers)
-                .setTopics(inputTopic)
-                .setStartingOffsets(OffsetsInitializer.earliest())
-                .setValueOnlyDeserializer(new SimpleStringSchema())
-                .build();
-
-        KafkaRecordSerializationSchema<String> serializer = KafkaRecordSerializationSchema.builder()
-                .setValueSerializationSchema(new SimpleStringSchema())
-                .setTopic(outputTopic)
-                .build();
-
-        KafkaSink<String> sink = KafkaSink.<String>builder()
-                .setBootstrapServers(bootstrapServers)
-                .setRecordSerializer(serializer)
-                .build();
-
-
-        /**
-         * Adds a data {@link Source} to the environment to get a {@link DataStream}.
-         *
-         * <p>The result will be either a bounded data stream (that can be processed in a batch way) or
-         * an unbounded data stream (that must be processed in a streaming way), based on the
-         * boundedness property of the source, as defined by {@link Source#getBoundedness()}.
-         *
-         * <p>The result type (that is used to create serializers for the produced data events) will be
-         * automatically extracted. This is useful for sources that describe the produced types already
-         * in their configuration, to avoid having to declare the type multiple times. For example the
-         * file sources and Kafka sources already define the produced byte their
-         * parsers/serializers/formats, and can forward that information.
-         *
-         * @param source the user defined source
-         * @param sourceName Name of the data source
-         * @param <OUT> type of the returned stream
-         * @return the data stream constructed
-         */
-        @PublicEvolving
-        public <OUT> DataStreamSource<OUT> fromSource(
-                Source<OUT, ?, ?> source,
-                WatermarkStrategy<OUT> timestampsAndWatermarks,
-                String sourceName) {
-            return fromSource(source, timestampsAndWatermarks, sourceName, null);
-        }
-
-        DataStream<String> text = env.fromSource(source, WatermarkStrategy.noWatermarks(), "Redpanda Source");
-
-        // Split up the lines in pairs (2-tuples) containing: (word,1)
-        DataStream<String> counts = text.flatMap(new Tokenizer())
-        // Group by the tuple field "0" and sum up tuple field "1"
-        .keyBy(value -> value.f0)
-        .sum(1)
-        .flatMap(new Reducer());
-
-        // Add the sinkTo so results
-        // are written to the outputTopic
-        counts.sinkTo(sink);
-
-        // Execute program
-        env.execute(jobTitle);
-    }
-
-    /**
-     * Implements the string tokenizer that splits sentences into words as a user-defined
-     * FlatMapFunction. The function takes a line (String) and splits it into multiple pairs in the
-     * form of "(word,1)" ({@code Tuple2<String, Integer>}).
-     */
-    public static final class Tokenizer
-            implements FlatMapFunction<String, Tuple2<String, Integer>> {
-
-        @Override
-        public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
-            // Normalize and split the line
-            String[] tokens = value.toLowerCase().split("\\W+");
-
-            // Emit the pairs
-            for (String token : tokens) {
-                if (token.length() > 0) {
-                    out.collect(new Tuple2<>(token, 1));
-                }
-            }
-        }
-    }
-
-    // Implements a simple reducer using FlatMap to
-    // reduce the Tuple2 into a single string for
-    // writing to kafka topics
-    public static final class Reducer
-            implements FlatMapFunction<Tuple2<String, Integer>, String> {
-
-        @Override
-        public void flatMap(Tuple2<String, Integer> value, Collector<String> out) {
-            // Convert the pairs to a string
-            // for easy writing to Kafka Topic
-            String count = value.f0 + " " + value.f1;
-            out.collect(count);
-        }
-    }
-}
\ No newline at end of file
diff --git a/playground/WrappedSource.java b/playground/WrappedSource.java
deleted file mode 100644
index f766e81..0000000
--- a/playground/WrappedSource.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.api.connector.dsv2;
-
-import org.apache.flink.annotation.Internal;
-
-/** A simple {@link Source} implementation that wrap a FLIP-27 source. */
-@Internal
-public class WrappedSource<T> implements Source<T> {
-    org.apache.flink.api.connector.source.Source<T, ?, ?> wrappedSource;
-
-    public WrappedSource(org.apache.flink.api.connector.source.Source<T, ?, ?> wrappedSource) {
-        this.wrappedSource = wrappedSource;
-    }
-
-    public org.apache.flink.api.connector.source.Source<T, ?, ?> getWrappedSource() {
-        return wrappedSource;
-    }
-}
\ No newline at end of file
diff --git a/pytest.ini b/pytest.ini
deleted file mode 100644
index aa7f5b7..0000000
--- a/pytest.ini
+++ /dev/null
@@ -1,2 +0,0 @@
-[pytest]
-norecursedirs = build dist .eggs
diff --git a/requirements.txt b/requirements.txt
index 4bb9099..614e19a 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -43,6 +43,3 @@ ray[default]==2.37.0
 python-multipart>=0.0.6
 huggingface_hub>=0.22.2
 kafka-python>=2.2.0
-dill>=0.3.8
-tabulate>=0.9.0
-colorama>=0.4.6
\ No newline at end of file
diff --git a/sage_core/environment/__init__.py b/sage_common_funs/__init__.py
similarity index 100%
rename from sage_core/environment/__init__.py
rename to sage_common_funs/__init__.py
diff --git a/sage_jobmanager/utils/__init__.py b/sage_common_funs/agent/__init__.py
similarity index 100%
rename from sage_jobmanager/utils/__init__.py
rename to sage_common_funs/agent/__init__.py
diff --git a/sage_common_funs/agent/agent.py b/sage_common_funs/agent/agent.py
new file mode 100644
index 0000000..2e1c611
--- /dev/null
+++ b/sage_common_funs/agent/agent.py
@@ -0,0 +1,165 @@
+from calendar import c
+from sage_common_funs.utils.generator_model import apply_generator_model
+from sage_core.function.map_function import MapFunction
+from jinja2 import Template
+
+from sage_utils.custom_logger import CustomLogger
+from typing import Any,Tuple
+import requests
+import json
+import re, time
+
+
+class Tool:
+    def __init__(self, name, func, description):
+        self.name = name
+        self.func = func
+        self.description = description
+
+    def run(self, *args, **kwargs):
+        return self.func(*args, **kwargs)
+    
+class BochaSearch:
+    def __init__(self,api_key):
+        self.url = "https://api.bochaai.com/v1/web-search"
+        self.api_key = api_key
+        self.headers = {
+            'Authorization': api_key,
+            'Content-Type': 'application/json'
+        }
+
+    def run(self, query):
+        payload = json.dumps({
+            "query": query,
+            "summary": True,
+            "count": 10,
+            "page": 1
+        })
+        response = requests.request("POST", self.url, headers=self.headers, data=payload)
+        return response.json()
+
+PREFIX = """Answer the following questions as best you can. You have access to the following tools:{tool_names}"""
+FORMAT_INSTRUCTIONS = """Always respond in the following JSON format:
+
+```json
+{{
+  "thought": "your thought process",
+  "action": "the action to take, should be one of [{tool_names}]",
+  "action_input": "the input to the action",
+  "observation": "Result from tool after execution",
+  "final_answer": "Final answer to the original question"
+}}
+```
+Notes:
+If you are taking an action, set 'final_answer' to "" and 'observation' to "".
+If you have enough information to answer, set 'action' to "", and fill in 'final_answer' directly. 
+"""
+
+SUFFIX = """Begin!
+Question: {input}
+Thought:{agent_scratchpad}
+"""
+
+
+class BaseAgent(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.logger.set_console_level("DEBUG")
+        self.config = config
+        search = BochaSearch(api_key=self.config["search_api_key"])
+
+        self.tools = [
+            Tool(
+                name = "Search",
+                func=search.run,
+                description="useful for when you need to search to answer questions about current events"
+            )
+        ]
+        self.tools = {tool.name: tool for tool in self.tools}
+        self.tool_names = ", ".join(self.tools.keys())  # 修复点
+        self.format_instructions = FORMAT_INSTRUCTIONS.format(tool_names=self.tool_names)
+        self.prefix= PREFIX.format(tool_names=self.tool_names)
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"],
+            base_url=self.config["base_url"],
+            api_key=self.config["api_key"],
+            seed=42 
+        )
+        self.max_steps=self.config.get("max_steps", 5)
+
+    def get_prompt(self, input, agent_scratchpad):
+        return self.prefix + self.format_instructions + SUFFIX.format(
+            input=input, agent_scratchpad=agent_scratchpad
+        )
+    
+    def parse_json_output(self, output: str) -> dict:
+        # 尝试直接加载
+        try:
+            return json.loads(output)
+        except json.JSONDecodeError:
+            pass
+
+        # 如果不是纯 JSON，再试图从 Markdown 中提取
+        match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", output, re.DOTALL)
+        if match:
+            json_str = match.group(1).strip()
+            try:
+                return json.loads(json_str)
+            except json.JSONDecodeError as e:
+                raise ValueError(f"Malformed JSON inside Markdown: {str(e)}") from e
+
+        # 兜底报错
+        raise ValueError("Invalid JSON format: No valid JSON found (either plain or wrapped in Markdown)")
+        
+    def execute(self, data: str) -> Tuple[str, str]:
+        query = data
+        agent_scratchpad = ""
+        count = 0
+        while True:
+            count += 1
+            self.logger.debug(f"Step {count}: Processing query: {query}")
+            if count > self.max_steps:
+                # raise ValueError("Max steps exceeded.")
+                return (query,"")
+            
+            prompt = self.get_prompt(query, agent_scratchpad)
+            self.logger.debug(f"Prompt: {prompt}")
+            prompt=[{"role":"user","content":prompt}]
+            output = self.model.generate(prompt)
+            self.logger.debug(output)
+            output=self.parse_json_output(output)
+            # self.logger.debug(output)
+            if output.get("final_answer") is not "":
+                final_answer = output["final_answer"]
+                
+                self.logger.debug(f"Final Answer: {final_answer}")
+                return (query,final_answer)
+
+            action, action_input = output.get("action"), output.get("action_input")
+
+            if action is None:
+                # raise ValueError("Could not parse action.")
+                return (query,"")
+
+            if action not in self.tools:
+                # raise ValueError(f"Unknown tool requested: {action}")
+                return (query,"")
+
+            tool = self.tools[action]
+            tool_result = tool.run(action_input)
+            self.logger.debug(f"Tool {action} result: {tool_result}")
+            snippets =[item["snippet"] for item in tool_result["data"]["webPages"]["value"]]
+            observation = "\n".join(snippets)
+            self.logger.debug(f"Observation: {observation}")
+            agent_scratchpad += str(output) + f"\nObservation: {observation}\nThought: "
+            time.sleep(5)
+
+# import yaml
+# def load_config(path: str) -> dict:
+#     with open(path, 'r') as f:
+#         return yaml.safe_load(f)
+
+# config=load_config("/home/zsl/workspace/sage/api/operator/operator_impl/config.yaml")
+# agent=BaseAgent(config)
+# agent.run("你是谁")
\ No newline at end of file
diff --git a/sage_libs/io/__init__.py b/sage_common_funs/io/__init__.py
similarity index 100%
rename from sage_libs/io/__init__.py
rename to sage_common_funs/io/__init__.py
diff --git a/sage_libs/io/sink.py b/sage_common_funs/io/sink.py
similarity index 90%
rename from sage_libs/io/sink.py
rename to sage_common_funs/io/sink.py
index f7f9e24..cad3c4c 100644
--- a/sage_libs/io/sink.py
+++ b/sage_common_funs/io/sink.py
@@ -1,4 +1,5 @@
 from sage_core.function.sink_function import SinkFunction
+from sage_utils.custom_logger import CustomLogger
 from typing import Tuple, List, Union, Type, Any
 import os
 
@@ -33,22 +34,11 @@ class RetriveSink(SinkFunction):
 
 
 class FileSink(SinkFunction):
-    def __init__(self, config: dict = None, **kwargs):
+    def __init__(self, config: dict = None,  **kwargs):
         super().__init__(**kwargs)
-        
-        file_path = config.get("file_path", "qa_output.txt") if config else "qa_output.txt"
+        os.makedirs("output", exist_ok=True)
+        self.file_path = os.path.join("output", config.get("file_path", "qa_output.txt"))
         self.config = config
-        
-        # 判断路径类型并处理
-        if os.path.isabs(file_path):
-            # 绝对路径：直接使用
-            self.file_path = file_path
-            # 确保目录存在
-            os.makedirs(os.path.dirname(file_path), exist_ok=True)
-        else:
-            # 相对路径：添加output前缀
-            os.makedirs("output", exist_ok=True)
-            self.file_path = os.path.join("output", file_path)
 
         # 创建或清空文件
         with open(self.file_path, "w", encoding="utf-8") as f:
diff --git a/sage_libs/io/source.py b/sage_common_funs/io/source.py
similarity index 90%
rename from sage_libs/io/source.py
rename to sage_common_funs/io/source.py
index 9528ec9..5aef831 100644
--- a/sage_libs/io/source.py
+++ b/sage_common_funs/io/source.py
@@ -1,6 +1,8 @@
 
 from sage_core.function.source_function import SourceFunction
-from sage_libs.io.utils.data_loader import resolve_data_path
+from sage_utils.custom_logger import CustomLogger
+from sage_utils.data_loader import resolve_data_path
+from typing import List
 
 
 class FileSource(SourceFunction):
@@ -44,8 +46,9 @@ class FileSource(SourceFunction):
                         self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Read query: {line.strip()}\033[0m ")
                         return line.strip()  # Return non-empty lines
                     else:
-                        self.logger.info(f"\033[33m[ {self.__class__.__name__}]: Reached end of file, maintaining position.\033[0m ")
+                        self.logger.info(f"\033[33m[ {self.__class__.__name__}]: Reached end of file, resetting position.\033[0m ")
                         # Reset position if end of file is reached (optional)
+                        self.file_pos = 0
                         continue
         except FileNotFoundError:
             self.logger.error(f"File not found: {self.data_path}")
diff --git a/sage_libs/io/utils/__init__.py b/sage_common_funs/rag/__init__.py
similarity index 100%
rename from sage_libs/io/utils/__init__.py
rename to sage_common_funs/rag/__init__.py
diff --git a/sage_common_funs/rag/arxiv.py b/sage_common_funs/rag/arxiv.py
new file mode 100644
index 0000000..507586f
--- /dev/null
+++ b/sage_common_funs/rag/arxiv.py
@@ -0,0 +1,276 @@
+from typing import Any, List, Literal, Optional, Union
+from typing import Any, List, Optional
+from urllib.parse import quote
+import feedparser
+import os
+import requests
+import time
+import json
+import fitz
+from PIL import Image
+from collections import Counter
+import json
+import re
+
+from sage_core.function.map_function import MapFunction
+
+
+
+class Paper:
+    def __init__(self, path, title='', url='', abs='', authors=[],**kwargs):
+        super().__init__(**kwargs)
+        # 初始化函数，根据pdf路径初始化Paper对象                
+        self.url = url  # 文章链接
+        self.path = path  # pdf路径
+        self.section_names = []  # 段落标题
+        self.section_texts = {}  # 段落内容
+        self.abs = abs
+        self.title_page = 0
+        if title == '':
+            self.pdf = fitz.open(self.path)  # pdf文档
+            self.title = self.get_title()
+            self.parse_pdf()
+        else:
+            self.title = title
+        self.authors = authors
+        self.roman_num = ["I", "II", 'III', "IV", "V", "VI", "VII", "VIII", "IIX", "IX", "X"]
+        self.digit_num = [str(d + 1) for d in range(10)]
+        self.first_image = ''
+
+    def parse_pdf(self):
+        self.pdf = fitz.open(self.path)  # pdf文档
+        self.text_list = [page.get_text() for page in self.pdf]
+        self.all_text = ' '.join(self.text_list)
+        self.extract_section_infomation()
+        self.section_texts.update({"title": self.title})
+        self.pdf.close()
+
+    # 定义一个函数，根据字体的大小，识别每个章节名称，并返回一个列表
+    def get_chapter_names(self, ):
+        # # 打开一个pdf文件
+        doc = fitz.open(self.path)  # pdf文档
+        text_list = [page.get_text() for page in doc]
+        all_text = ''
+        for text in text_list:
+            all_text += text
+        # # 创建一个空列表，用于存储章节名称
+        chapter_names = []
+        for line in all_text.split('\n'):
+            line_list = line.split(' ')
+            if '.' in line:
+                point_split_list = line.split('.')
+                space_split_list = line.split(' ')
+                if 1 < len(space_split_list) < 5:
+                    if 1 < len(point_split_list) < 5 and (
+                            point_split_list[0] in self.roman_num or point_split_list[0] in self.digit_num):
+                        # print("line:", line)
+                        chapter_names.append(line)
+
+        return chapter_names
+
+    def get_title(self):
+        doc = self.pdf  # 打开pdf文件
+        max_font_size = 0  # 初始化最大字体大小为0
+        max_string = ""  # 初始化最大字体大小对应的字符串为空
+        max_font_sizes = [0]
+        for page_index, page in enumerate(doc):  # 遍历每一页
+            text = page.get_text("dict")  # 获取页面上的文本信息
+            blocks = text["blocks"]  # 获取文本块列表
+            for block in blocks:  # 遍历每个文本块
+                if block["type"] == 0 and len(block['lines']):  # 如果是文字类型
+                    if len(block["lines"][0]["spans"]):
+                        font_size = block["lines"][0]["spans"][0]["size"]  # 获取第一行第一段文字的字体大小
+                        max_font_sizes.append(font_size)
+                        if font_size > max_font_size:  # 如果字体大小大于当前最大值
+                            max_font_size = font_size  # 更新最大值
+                            max_string = block["lines"][0]["spans"][0]["text"]  # 更新最大值对应的字符串
+        max_font_sizes.sort()
+        # print("max_font_sizes", max_font_sizes[-10:])
+        cur_title = ''
+        for page_index, page in enumerate(doc):  # 遍历每一页
+            text = page.get_text("dict")  # 获取页面上的文本信息
+            blocks = text["blocks"]  # 获取文本块列表
+            for block in blocks:  # 遍历每个文本块
+                if block["type"] == 0 and len(block['lines']):  # 如果是文字类型
+                    if len(block["lines"][0]["spans"]):
+                        cur_string = block["lines"][0]["spans"][0]["text"]  # 更新最大值对应的字符串
+                        font_flags = block["lines"][0]["spans"][0]["flags"]  # 获取第一行第一段文字的字体特征
+                        font_size = block["lines"][0]["spans"][0]["size"]  # 获取第一行第一段文字的字体大小
+                        # print(font_size)
+                        if abs(font_size - max_font_sizes[-1]) < 0.3 or abs(font_size - max_font_sizes[-2]) < 0.3:
+                            # print("The string is bold.", max_string, "font_size:", font_size, "font_flags:", font_flags)                            
+                            if len(cur_string) > 4 and "arXiv" not in cur_string:
+                                # print("The string is bold.", max_string, "font_size:", font_size, "font_flags:", font_flags) 
+                                if cur_title == '':
+                                    cur_title += cur_string
+                                else:
+                                    cur_title += ' ' + cur_string
+                                self.title_page = page_index
+                                # break
+        title = cur_title.replace('\n', ' ')
+        return title
+
+    def extract_section_infomation(self):
+        doc = fitz.open(self.path)
+
+        # 获取文档中所有字体大小
+        font_sizes = []
+        for page in doc:
+            blocks = page.get_text("dict")["blocks"]
+            for block in blocks:
+                if 'lines' not in block:
+                    continue
+                lines = block["lines"]
+                for line in lines:
+                    for span in line["spans"]:
+                        font_sizes.append(span["size"])
+        most_common_size, _ = Counter(font_sizes).most_common(1)[0]
+
+        # 按照最频繁的字体大小确定标题字体大小的阈值
+        threshold = most_common_size * 1
+        section_dict = {}
+        section_dict["Abstract"] = ""
+        last_heading = None
+        subheadings = []
+        heading_font = -1
+        # 遍历每一页并查找子标题
+        found_abstract = False
+        upper_heading = False
+        font_heading = False
+        for page in doc:
+            blocks = page.get_text("dict")["blocks"]
+            for block in blocks:
+                if not found_abstract:
+                    try:
+                        text = json.dumps(block)
+                    except:
+                        continue
+                    if re.search(r"\bAbstract\b", text, re.IGNORECASE):
+                        found_abstract = True
+                        last_heading = "Abstract"
+                if found_abstract:
+                    if 'lines' not in block:
+                        continue
+                    lines = block["lines"]
+                    for line in lines:
+                        for span in line["spans"]:
+                            # 如果当前文本是子标题
+                            if not font_heading and span["text"].isupper() and sum(1 for c in span["text"] if c.isupper() and ('A' <= c <='Z')) > 4:  # 针对一些标题大小一样,但是全大写的论文
+                                upper_heading = True
+                                heading = span["text"].strip()
+                                if "References" in heading:  # reference 以后的内容不考虑
+                                    self.section_names = subheadings
+                                    self.section_texts = section_dict
+                                    return
+                                subheadings.append(heading)
+                                if last_heading is not None:
+                                    section_dict[last_heading] = section_dict[last_heading].strip()
+                                section_dict[heading] = ""
+                                last_heading = heading
+                            if not upper_heading and span["size"] > threshold and re.match(  # 正常情况下,通过字体大小判断
+                                    r"[A-Z][a-z]+(?:\s[A-Z][a-z]+)*",
+                                    span["text"].strip()):
+                                font_heading = True
+                                if heading_font == -1:
+                                    heading_font = span["size"]
+                                elif heading_font != span["size"]:
+                                    continue
+                                heading = span["text"].strip()
+                                if "References" in heading:  # reference 以后的内容不考虑
+                                    self.section_names = subheadings
+                                    self.section_texts = section_dict
+                                    return
+                                subheadings.append(heading)
+                                if last_heading is not None:
+                                    section_dict[last_heading] = section_dict[last_heading].strip()
+                                section_dict[heading] = ""
+                                last_heading = heading
+                            # 否则将当前文本添加到上一个子标题的文本中
+                            elif last_heading is not None:
+                                section_dict[last_heading] += " " + span["text"].strip()
+        self.section_names = subheadings
+        self.section_texts = section_dict
+
+class ArxivPDFDownloader(MapFunction):
+    def __init__(self, config):
+        super().__init__()
+        config = config["ArxivPDFDownloader"]
+        self.max_results = config.get("max_results", 5)
+        self.save_dir = config.get("save_dir", "arxiv_pdfs")
+        os.makedirs(self.save_dir, exist_ok=True)
+
+    def execute(self, data: str) -> List[str]:
+        self.query = data
+        base_url = 'http://export.arxiv.org/api/query?'
+        encoded_query = quote(self.query)
+        query = f'search_query={encoded_query}&start=0&max_results={self.max_results}&sortBy=submittedDate&sortOrder=descending'
+        url = base_url + query
+        feed = feedparser.parse(url)
+
+        pdf_paths = []
+
+        print(feed)
+        for entry in feed.entries:
+            arxiv_id = entry.id.split('/abs/')[-1]
+            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
+            pdf_path = os.path.join(self.save_dir, f"{arxiv_id}.pdf")
+
+            if not os.path.exists(pdf_path):
+                try:
+                    resp = requests.get(pdf_url, timeout=15)
+                    if resp.status_code == 200:
+                        with open(pdf_path, 'wb') as f:
+                            f.write(resp.content)
+                        pdf_paths.append(pdf_path)
+                        self.logger.info(f"Downloaded: {pdf_path}")
+                    else:
+                        self.logger.error(f"HTTP {resp.status_code} for {pdf_url}")
+                except Exception as e:
+                    self.logger.error(f"Failed to download {pdf_url}: {e}")
+            else:
+                self.logger.info(f"File already exists: {pdf_path}")
+                pdf_paths.append(pdf_path)
+
+            time.sleep(1)  # 防止请求过快
+
+        return pdf_paths
+
+
+class ArxivPDFParser(MapFunction):
+    def __init__(self, config):
+        super().__init__()
+        config = config["ArxivPDFParser"]
+        print(config)
+        self.output_dir = config.get("output_dir", "arxiv_structured_json")
+        os.makedirs(self.output_dir, exist_ok=True)
+
+    def execute(self, data: str) -> List[str]:
+
+        pdf_paths = data
+        output_paths = []
+
+        for pdf_path in pdf_paths:
+
+            filename = os.path.basename(pdf_path).replace('.pdf', '.json')
+            json_path = os.path.join(self.output_dir, filename)
+
+            if not os.path.exists(json_path):
+                try:
+                    paper = Paper(pdf_path)
+                    paper.parse_pdf()
+                    with open(json_path, 'w', encoding='utf-8') as f:
+                        json.dump({
+                            "title": paper.title,
+                            "authors": paper.authors,
+                            "abs": paper.abs,
+                            "sections": paper.section_texts
+                        }, f, ensure_ascii=False, indent=4)
+                    output_paths.append(json_path)
+                    self.logger.info(f"Parsed and saved: {json_path}")
+                except Exception as e:
+                    self.logger.error(f"Failed to parse {pdf_path}: {e}")
+            else:
+                self.logger.info(f"JSON already exists: {json_path}")
+                output_paths.append(json_path)
+
+        return output_paths
\ No newline at end of file
diff --git a/sage_common_funs/rag/chunk.py b/sage_common_funs/rag/chunk.py
new file mode 100644
index 0000000..76b2c4c
--- /dev/null
+++ b/sage_common_funs/rag/chunk.py
@@ -0,0 +1,157 @@
+from typing import Any, List, Literal, Optional, Union
+from sage_core.function.map_function import MapFunction
+
+from typing import Any, List, Optional
+from sentence_transformers import SentenceTransformer
+from transformers import AutoTokenizer
+
+class CharacterSplitter(MapFunction):
+    """
+    A source rag that reads a file and splits its contents into overlapping chunks.
+
+    Input: None (reads directly from a file at the configured path).
+    Output: A Data object containing a list of text chunks.
+
+    Config:
+        - data_path: Path to the input text file.
+        - chunk_size: Number of tokens per chunk (default: 512).
+        - overlap: Number of overlapping tokens (default: 128).
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.config = config
+        self.chunk_size = self.config.get("chunk_size", 512)
+        self.overlap = self.config.get("overlap", 128)
+
+    def _split_text(self, text: str) -> List[str]:
+        """
+        Splits text into chunks of length `chunk_size` with `overlap` between chunks.
+
+        :param text: The full text string to be split.
+        :return: A list of chunked text strings.
+        """
+        tokens = list(text)  # character-level split
+        chunks = []
+        start = 0
+        while start < len(tokens):
+            end = start + self.chunk_size
+            chunk = tokens[start:end]
+            chunks.append("".join(chunk))
+            start += self.chunk_size - self.overlap  # move forward with overlap
+        return chunks
+
+    def execute(self,data:str) -> List[str]:
+        """
+        Reads and splits the file into overlapping text chunks.
+
+        :return: A Data object containing a list of text chunks.
+        """
+        content=data
+        try:
+            chunks = self._split_text(content)
+            return chunks
+        except Exception as e:
+            self.logger.error(f"CharacterSplitter error: {e}", exc_info=True)
+
+
+class SentenceTransformersTokenTextSplitter(MapFunction):
+    """
+    A source rag that splits text into tokens using SentenceTransformer's tokenizer.
+
+    Input: A Data object containing the text to be split.
+    Output: A Data object containing a list of token-based text chunks.
+
+    Config:
+        - chunk_overlap: Number of overlapping tokens between chunks (default: 50).
+        - model_name: The model name for SentenceTransformer (default: "sentence-transformers/all-mpnet-base-v2").
+        - chunk_size: Optional number of tokens per chunk.
+    """
+
+    def __init__(self, config: dict) -> None:
+        super().__init__()
+        self.config = config.get("chunk", {})
+        self.model_name = self.config.get("model_name", "sentence-transformers/all-mpnet-base-v2")
+        self.chunk_size = self.config.get("chunk_size", 512)
+        self.chunk_overlap = self.config.get("chunk_overlap", 50)
+
+        try:
+            # Load the SentenceTransformer model
+            self._model = SentenceTransformer(self.model_name)
+            # Use AutoTokenizer for transformer-based tokenization
+            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
+        except ImportError:
+            raise ImportError(
+                "Could not import sentence_transformers or transformers python packages. "
+                "Please install them with `pip install sentence-transformers transformers`."
+            )
+        except Exception as e:
+            self.logger.error(f"Error while loading model or tokenizer: {e}")
+            raise e
+        
+        if self.chunk_overlap >= self.chunk_size:
+            raise ValueError("Chunk overlap must be less than chunk size.")
+        if self.chunk_size <= 0:
+            raise ValueError("Chunk size must be greater than 0.")
+
+    def split_text_on_tokens(self, text: str) -> List[str]:
+        """
+        Splits incoming text into smaller chunks using the tokenizer.
+
+        :param text: The full text string to be split.
+        :return: A list of token-based text chunks.
+        """
+        print(text)
+        _max_length_equal_32_bit_integer: int = 2**32
+        splits: List[str] = []
+        input_ids = self.tokenizer.encode(text, truncation=True, padding=False)
+        start_idx = 0
+
+        print(f"Input IDs: {input_ids}")
+
+        # Iterate through the text and split it into chunks
+        while start_idx < len(input_ids):
+            print(f"Start Index: {start_idx}")
+            # Define the end of the current chunk
+            cur_idx = min(start_idx + self.chunk_size, len(input_ids))
+            chunk_ids = input_ids[start_idx:cur_idx]
+
+            # Decode the chunk and add it to the list of splits
+            splits.append(self.tokenizer.decode(chunk_ids, skip_special_tokens=True))
+            
+            # Move the starting index forward with the overlap
+            start_idx = cur_idx - self.chunk_overlap
+
+            # Break the loop when we've processed all tokens
+            if cur_idx == len(input_ids):
+                break
+
+        return splits
+
+    def execute(self, data: str) -> List[str]:
+        """
+        Splits the input text data into smaller token-based chunks.
+
+        :param data: The input Data object containing the text to be split.
+        :return: A Data object containing a list of token-based text chunks.
+        """
+        content = data
+        # print(f"Content: {content}")
+        try:
+            chunks = self.split_text_on_tokens(content)
+            return chunks
+        except Exception as e:
+            self.logger.error(f"SentenceTransformersTokenTextSplitter error: {e}", exc_info=True)
+
+# config={
+#     "chunk": {
+#         "chunk_size": 8,
+#         "chunk_overlap": 2,
+#         "model_name": "sentence-transformers/all-mpnet-base-v2",
+#     }
+# }
+
+# split=SentenceTransformersTokenTextSplitter(config)
+# print(split.execute(Data("This is a operator_test sentence to be split into smaller chunks.This is a operator_test sentence to be split into smaller chunks.This is a operator_test sentence to be split into smaller chunks.This is a operator_test sentence to be split into smaller chunks.")))
+
+
diff --git a/sage_common_funs/rag/evaluate.py b/sage_common_funs/rag/evaluate.py
new file mode 100644
index 0000000..9794479
--- /dev/null
+++ b/sage_common_funs/rag/evaluate.py
@@ -0,0 +1,138 @@
+from collections import Counter
+import numpy as np
+import torch
+from transformers import AutoTokenizer, AutoModel
+from sklearn.metrics.pairwise import cosine_similarity
+from rouge import Rouge
+
+from sage_core.function.map_function import MapFunction
+
+
+
+class F1Evaluate(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+    
+    def _get_tokens(self, text):
+        return text.lower().split()
+    
+    def _f1_score(self, prediction, reference):
+        reference_tokens = self._get_tokens(reference)
+        prediction_tokens = self._get_tokens(prediction)
+
+        common_tokens = Counter(reference_tokens) & Counter(prediction_tokens)
+        num_common = sum(common_tokens.values())
+
+        if len(reference_tokens) == 0 or len(prediction_tokens) == 0:
+            return int(reference_tokens == prediction_tokens)
+
+        if num_common == 0:
+            return 0
+
+        precision = num_common / len(prediction_tokens)
+        recall = num_common / len(reference_tokens)
+        f1 = (2 * precision * recall) / (precision + recall)
+        return f1
+
+    def execute(self, data: tuple[str, str]):
+        reference, prediction = data
+        score = self._f1_score(prediction, reference)
+
+        print(f"\033[93m[F1 Score] : {score:.4f}\033[0m")
+
+
+class BertRecallEvaluate(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.model = AutoModel.from_pretrained("bert-base-uncased")
+        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
+        
+    def bert_recall(self, reference, generated):
+        if not generated.strip():
+            return 0.0
+        inputs_ref = self.tokenizer(reference, return_tensors="pt", padding=True, truncation=True)
+        inputs_gen = self.tokenizer(generated, return_tensors="pt", padding=True, truncation=True)
+
+        with torch.no_grad():
+            outputs_ref = self.model(**inputs_ref).last_hidden_state.mean(dim=1)
+            outputs_gen = self.model(**inputs_gen).last_hidden_state.mean(dim=1)
+
+        similarity = cosine_similarity(outputs_ref.numpy(), outputs_gen.numpy())
+        return similarity[0][0]
+
+    def execute(self, data: tuple[str, str]):
+        reference, generated = data
+        score = self.bert_recall(reference, generated)
+
+        print(f"\033[95m[BERT Recall] : {score:.4f}\033[0m")
+
+
+class RougeLEvaluate(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.rouge = Rouge()
+
+    def rouge_l(self, reference, generated):
+        if not generated.strip():
+            return 0.0
+        scores = self.rouge.get_scores(generated, reference)
+        return scores[0]['rouge-l']['f']
+
+    def execute(self, data: tuple[str, str]):
+        reference, generated = data
+        score = self.rouge_l(reference, generated)
+
+        print(f"\033[94m[ROUGE-L] : {score:.4f}\033[0m")
+
+
+class BRSEvaluate(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.bert_recall_evaluate = BertRecallEvaluate(config)
+        self.rouge_l_evaluate = RougeLEvaluate(config)
+
+    def BRS(self, reference, generated):
+        bert_rec = self.bert_recall_evaluate.bert_recall(reference, generated)
+        rouge_l_score = self.rouge_l_evaluate.rouge_l(reference, generated)
+
+        if bert_rec == 0 or rouge_l_score == 0:
+            return 0
+        return (2 * bert_rec * rouge_l_score) / (bert_rec + rouge_l_score)
+
+    def execute(self, data: tuple[str, str]):
+        reference, generated = data
+        score = self.BRS(reference, generated)
+
+        print(f"\033[92m[BRS Score] : {score:.4f}\033[0m")
+
+
+
+# def test_evaluate_functions():
+#     # 模拟一条数据：reference 和 generated
+#     reference = "The cat sits on the mat."
+#     generated = "A cat is sitting on a mat."
+
+#     data = Data((reference, generated))
+
+#     config = {}  # 测试时config可以是空的
+
+#     # 初始化所有评估器
+#     f1_eval = F1Evaluate(config)
+#     bert_recall_eval = BertRecallEvaluate(config)
+#     rouge_l_eval = RougeLEvaluate(config)
+#     brs_eval = BRSEvaluate(config)
+
+#     # 分别执行
+#     print("\n=== F1 Evaluate ===")
+#     f1_eval.execute(data)
+
+#     print("\n=== BERT Recall Evaluate ===")
+#     bert_recall_eval.execute(data)
+
+#     print("\n=== ROUGE-L Evaluate ===")
+#     rouge_l_eval.execute(data)
+
+#     print("\n=== BRS Evaluate ===")
+#     brs_eval.execute(data)
+
+# test_evaluate_functions()
\ No newline at end of file
diff --git a/sage_common_funs/rag/generator.py b/sage_common_funs/rag/generator.py
new file mode 100644
index 0000000..b14dcce
--- /dev/null
+++ b/sage_common_funs/rag/generator.py
@@ -0,0 +1,153 @@
+import os
+from typing import Tuple,List
+from sage_common_funs.utils.generator_model import apply_generator_model
+from sage_core.function.map_function import MapFunction 
+from sage_core.function.base_function import StatefulFunction
+from sage_utils.custom_logger import CustomLogger
+
+class OpenAIGenerator(MapFunction):
+    """
+    OpenAIGenerator is a generator rag that interfaces with a specified OpenAI model
+    to generate responses based on input data.
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+
+        """
+        Initializes the OpenAIGenerator instance with configuration parameters.
+
+        :param config: Dictionary containing configuration for the generator, including 
+                       the method, model name, base URL, API key, etc.
+        """
+        self.config = config
+        # Apply the generator model with the provided configuration
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"],
+            base_url=self.config["base_url"],
+            api_key=self.config["api_key"] or os.getenv("ALIBABA_API_KEY"),
+            seed=42  # Hardcoded seed for reproducibility
+        )
+        self.num = 1
+
+
+
+    # 其中原有的**kwargs应该由函数内部或者data内部提供
+    def execute(self, data: list) -> Tuple[str, str]:
+        """
+        Executes the response generation using the configured model based on the input data.
+
+        :param data: Data object containing a list of input data.
+                     The second item in the list is expected to be a dictionary with a "content" key that contains the user's query.
+        :param kwargs: Additional parameters for the model generation (e.g., temperature, max_tokens, etc.).
+
+        :return: A Data object containing a tuple (user_query, response), where user_query is the original input query,
+                and response is the generated response from the model.
+        """
+        # Extract the user query from the input data
+        user_query = data[0] if len(data) > 1  else None
+ 
+        prompt = data[1] if len(data) > 1 else data
+
+        response = self.model.generate(prompt)
+
+        self.num += 1
+
+        self.logger.info(f"[ {self.__class__.__name__}]: Response: {response}")
+
+        # Return the generated response along with the original user query as a tuple
+        return (user_query, response)
+
+class OpenAIGeneratorWithHistory(StatefulFunction):
+    """
+    OpenAIGenerator with global dialogue memory.
+    Maintains a rolling history of past user and assistant turns (stateful).
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.config = config
+
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"],
+            base_url=self.config["base_url"],
+            api_key=self.config["api_key"] or os.getenv("ALIBABA_API_KEY"),
+            seed=42
+        )
+
+        # 全局历史状态，按对话轮次记录字符串
+        self.dialogue_history: List[str] = []
+        self.history_turns = config.get("max_history_turns", 5)
+        self.num = 1
+
+    def execute(self, data: List, **kwargs) -> Tuple[str, str]:
+        """
+        Expects input data: [user_query, prompt_dict]
+        Where prompt_dict includes {"content": ...}
+        """
+        user_query = data[0] if len(data) > 1 else None
+        prompt_info = data[1] if len(data) > 1 else data
+
+        new_turns = [entry for entry in prompt_info if entry["role"] in ("user", "system")]
+
+        history_to_use = self.dialogue_history[-2 * self.history_turns:]
+        full_prompt = history_to_use + new_turns
+
+        self.logger.debug(f"[Prompt with history]:\n{full_prompt}")
+
+        response = self.model.generate(full_prompt, **kwargs)
+
+        for entry in new_turns:
+            if entry["role"] == "user":
+                self.dialogue_history.append(entry)
+        self.dialogue_history.append({"role": "assistant", "content": response})
+        self.dialogue_history = self.dialogue_history[-2 * self.history_turns:]  # 保留最近 N 轮
+
+        self.logger.info(f"\033[32m[{self.__class__.__name__}] Response: {response}\033[0m")
+
+        return (user_query, response)
+
+class HFGenerator(MapFunction):
+    """
+    HFGenerator is a generator rag that interfaces with a Hugging Face model
+    to generate responses based on input data.
+    """
+
+    def __init__(self, config, **kwargs):
+        """
+        Initializes the HFGenerator instance with configuration parameters.
+
+        :param config: Dictionary containing configuration for the generator, including
+                       the method and model name.
+        """
+        super().__init__(**kwargs)
+        self.config = config
+        # Apply the generator model with the provided configuration
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"]
+        )
+
+    def execute(self, data: list, **kwargs) -> Tuple[str, str]:
+        """
+        Executes the response generation using the configured Hugging Face model based on the input data.
+
+        :param data: Data object containing a list of input data.
+                     The expected format and the content of the data depend on the model's requirements.
+        :param kwargs: Additional parameters for the model generation (e.g., temperature, max_tokens, etc.).
+
+        :return: A Data object containing the generated response as a string.
+        """
+        # Generate the response from the Hugging Face model using the provided data and additional arguments
+        user_query = data[0] if len(data) > 1  else None
+ 
+        prompt = data[1] if len(data) > 1 else data
+        
+        response = self.model.generate(prompt, **kwargs)
+
+        # Return the generated response as a Data object
+        self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Response: {response}\033[0m ")
+
+        return (user_query, response)
diff --git a/sage_common_funs/rag/promptor.py b/sage_common_funs/rag/promptor.py
new file mode 100644
index 0000000..b1f35c1
--- /dev/null
+++ b/sage_common_funs/rag/promptor.py
@@ -0,0 +1,181 @@
+from jinja2 import Template
+from sage_core.function.base_function import StatefulFunction, MemoryFunction
+
+from sage_core.function.map_function import MapFunction
+
+from sage_utils.custom_logger import CustomLogger
+
+QA_prompt_template='''Instruction:
+You are an intelligent assistant with access to a knowledge base. Answer the question below with reference to the provided context.
+Only give me the answer and do not output any other words.
+{%- if external_corpus %}
+Relevant corpus for the current question:
+{{ external_corpus }}
+{%- endif %}
+'''
+
+summarization_prompt_template = '''Instruction:
+You are an intelligent assistant. Summarize the content provided below in a concise and clear manner.
+Only provide the summary and do not include any additional information.
+{%- if external_corpus %}
+Content to summarize:
+{{ external_corpus }}
+{%- endif %}
+'''
+QA_prompt_template = Template(QA_prompt_template)
+summarization_prompt_template = Template(summarization_prompt_template)
+
+
+class QAPromptor(MapFunction):
+    """
+    QAPromptor is a prompt rag that generates a QA-style prompt using
+    an external corpus and a user query. This class is designed to prepare 
+    the necessary prompt structure for a question-answering model.
+
+    Attributes:
+        config: Configuration data for initializing the prompt rag (e.g., model details, etc.).
+        prompt_template: A template used for generating the system prompt, typically includes context or instructions.
+    """
+    
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+
+        """
+        Initializes the QAPromptor instance with configuration and prompt template.
+
+        :param config: Dictionary containing configuration for the prompt rag.
+        """
+        self.config = config  # Store the configuration for later use
+        self.prompt_template = QA_prompt_template  # Load the QA prompt template
+
+    # sage_lib/functions/rag/qapromptor.py
+    def execute(self, data) -> list:
+        """
+        生成 ChatGPT 风格的 prompt（system+user 两条消息）。
+
+        支持两种输入：
+        1. (query, external_corpus_list_or_str))
+        2. query_str)
+        """
+        try:
+            # -------- 解析输入 --------
+            raw = data
+            if isinstance(raw, tuple) and len(raw) == 2:
+                query, external_corpus = raw
+                if isinstance(external_corpus, list):
+                    external_corpus = "\n".join(external_corpus)
+            else:
+                query = raw
+                external_corpus = ""
+
+            external_corpus = external_corpus or ""
+
+            # -------- system prompt --------
+            if external_corpus:
+                system_prompt = {
+                    "role": "system",
+                    "content": self.prompt_template.render(external_corpus=external_corpus),
+                }
+            else:
+                system_prompt = {
+                    "role": "system",
+                    "content": (
+                        "You are a helpful AI assistant. "
+                        "Answer the user's questions accurately."
+                    ),
+                }
+
+            # -------- user prompt --------
+            user_prompt = {
+                "role": "user",
+                "content": f"Question: {query}",
+            }
+
+            prompt = [system_prompt, user_prompt]
+            return [query,prompt]
+
+        except Exception as e:
+            self.logger.error(
+                "QAPromptor error: %s | input=%s", e, getattr(data, "data", "")
+            )
+            fallback = [
+                {"role": "system", "content": "System encountered an error."},
+                {
+                    "role": "user",
+                    "content": (
+                        "Question: Error occurred. Please try again."
+                        f" (Original: {getattr(data, 'data', '')})"
+                    ),
+                },
+            ]
+            return fallback
+
+
+class SummarizationPromptor(MapFunction):
+    """
+    QAPromptor is a prompt rag that generates a QA-style prompt using
+    an external corpus and a user query. This class is designed to prepare
+    the necessary prompt structure for a question-answering model.
+
+    Attributes:
+        config: Configuration data for initializing the prompt rag (e.g., model details, etc.).
+        prompt_template: A template used for generating the system prompt, typically includes context or instructions.
+    """
+
+    def __init__(self, config):
+        """
+        Initializes the QAPromptor instance with configuration and prompt template.
+
+        :param config: Dictionary containing configuration for the prompt rag.
+        """
+        super().__init__()
+        self.config = config  # Store the configuration for later use
+        self.prompt_template = summarization_prompt_template  # Load the summarization prompt template
+
+    def execute(self, data) -> list:
+        """
+        Generates a QA-style prompt for the input question and external corpus.
+
+        This method takes the query and external corpus, processes the corpus
+        into a single string, and creates a system prompt and user prompt based
+        on a predefined template.
+
+        :param data: A Data object containing a tuple. The first element is the query (a string),
+                     and the second is a list of external corpus (contextual information for the model).
+
+        :return: A Data object containing a list with two prompts:
+                 1. system_prompt: A system prompt based on the template with external corpus data.
+                 2. user_prompt: A user prompt containing the question to be answered.
+        """
+        # Unpack the input data into query and external_corpus
+        query, external_corpus = data
+
+        # Combine the external corpus list into a single string (in case it's split into multiple parts)
+        external_corpus = "".join(external_corpus)
+
+        # Prepare the base data for the system prompt, which includes the external corpus
+        base_system_prompt_data = {
+            "external_corpus": external_corpus
+        }
+
+        # query = data
+        # Create the system prompt using the template and the external corpus data
+        system_prompt = {
+            "role": "system",
+            "content": self.prompt_template.render(**base_system_prompt_data)
+        }
+        # system_prompt = {
+        #     "role": "system",
+        #     "content": ""
+        # }
+        # Create the user prompt using the query
+        user_prompt = {
+            "role": "user",
+            "content": f"Question: {query}"
+        }
+
+        # Combine the system and user prompts into one list
+        prompt = [system_prompt, user_prompt]
+
+        # Return the prompt list wrapped in a Data object
+        return prompt
diff --git a/sage_common_funs/rag/refiner.py b/sage_common_funs/rag/refiner.py
new file mode 100644
index 0000000..b8ad16c
--- /dev/null
+++ b/sage_common_funs/rag/refiner.py
@@ -0,0 +1,117 @@
+from sage_core.function.map_function import MapFunction
+
+from sage_common_funs.utils.generator_model import apply_generator_model
+from typing import Tuple,List
+import logging
+
+
+class AbstractiveRecompRefiner(MapFunction):
+    """
+    AbstractiveRecompRefiner is an abstractive refiner using the RECOMP approach. 
+    This class is responsible for refining retrieved documents by generating concise summaries
+    that directly answer a given question. The summary is generated using an external model.
+
+    Attributes:
+        logger: Logger for logging errors and information.
+        config: Configuration dictionary that holds settings for the refiner (e.g., model parameters).
+        model: A model instance used for generating summaries based on the provided input.
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        """
+        Initializes the AbstractiveRecompRefiner instance with configuration and model.
+
+        :param config: Dictionary containing configuration for the refiner, including model details 
+                       (method, model_name, base_url, api_key, etc.).
+        """
+        self.config = config  # Store the refiner configuration
+        # Apply the generator model based on provided configuration
+        self.model = apply_generator_model(
+            method=self.config["method"],
+            model_name=self.config["model_name"],
+            base_url=self.config["base_url"],
+            api_key=self.api_key,
+            seed=42  # Set a seed for reproducibility of results
+        )
+
+    def execute(self, data: Tuple[str, List[str]]) -> Tuple[str, List[str]]:
+        """
+        Executes the refining process by generating concise summaries for retrieved documents.
+
+        :param data: A Data object containing a tuple of (query, doc_set), where:
+                     - query: The user question that needs to be answered.
+                     - doc_set: A list of retrieved documents relevant to the query.
+
+        :return: A Data object containing a tuple (query, emit_docs), where:
+                 - query: The original user question.
+                 - emit_docs: A list of generated summaries corresponding to the documents.
+        """
+        try:
+            # Unpack the input data into the query and document set
+            query, doc_set = data
+            emit_docs = []  # List to hold the summaries of the documents
+            
+            # Process each retrieved document
+            for retrieval_docs in doc_set:
+                # Format the input for the model (question + document content)
+                query, processed_docs = self._format_input(query, retrieval_docs)
+                # Build the prompt for the model
+                input_prompt = self.build_prompt(query, processed_docs)
+                # Generate a summary for the document
+                summary = self.model.generate(input_prompt)
+                summary = [summary]  # Wrap the summary in a list
+                emit_docs.append(summary)  # Add the summary to the list of emitted documents
+
+        except Exception as e:
+            # Log any errors that occur during the refining process
+            self.logger.error(f"{str(e)} when RefinerFuction")
+            raise RuntimeError(f"Refining error: {str(e)}")
+        
+        # Return the refined results as a Data object
+        return (query, emit_docs[0])
+
+    def _format_input(self, question, retrieval_result):
+        """
+        Formats the input for the model by processing the retrieved documents.
+
+        :param question: The user query that needs to be answered.
+        :param retrieval_result: A list of documents retrieved for the question.
+
+        :return: A tuple containing the question and the processed documents, where:
+                 - The question remains unchanged.
+                 - The documents are formatted into a single string.
+        """
+        # Process the documents by joining all the lines in each document
+        processed_docs = [
+            "\n".join(doc.split("\n")[:])  # Here we are just splitting and joining to simulate processing
+            for doc in retrieval_result
+        ]
+        processed_docs = '\n'.join(processed_docs)  # Join all processed documents into one string
+        return question, processed_docs
+
+    def build_prompt(self, question, docs):
+        """
+        Builds the prompt for the model to generate a summary from the provided documents.
+
+        :param question: The user query that needs to be answered.
+        :param docs: The processed documents that will be used to generate the summary.
+
+        :return: A list containing a single dictionary, which represents the user prompt in the required format.
+        """
+        user_content = f"""
+        Generate a concise, factual summary from the document below that specifically answers the question. Follow these requirements:
+        1. Extract ONLY information directly related to the question
+        2. Present results using bullet points or short structured paragraphs
+        3. Include critical elements: 
+        - Numerical data/percentages 
+        - Time periods/dates 
+        - Definitive conclusions
+        - Named entities (people/organizations/locations)
+        4. Exclude speculative statements and irrelevant details
+
+        Question:
+        {question}
+
+        Document:
+        """
diff --git a/sage_common_funs/rag/reranker.py b/sage_common_funs/rag/reranker.py
new file mode 100644
index 0000000..84d8204
--- /dev/null
+++ b/sage_common_funs/rag/reranker.py
@@ -0,0 +1,300 @@
+import torch
+from typing import List, Tuple
+from transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoModelForCausalLM
+import logging
+
+from sage_core.function.map_function import MapFunction
+
+
+
+class BGEReranker(MapFunction):
+    """
+    A reranker that uses the BAAI/bge-reranker-v2-m3 model to reorder a list of retrieved documents.
+    The model assigns relevance scores to the documents and ranks them accordingly.
+
+    Input: A tuple of (query, List[retrieved_documents])
+    Output: A tuple of (query, List[reranked_documents_with_scores])
+
+    Attributes:
+        logger: Logger for logging error and information messages.
+        config: Configuration dictionary containing reranker settings (model name, top_k, etc.).
+        device: Device ('cuda' or 'cpu') where the model will be loaded.
+        tokenizer: Tokenizer used to preprocess input queries and documents.
+        model: The pre-trained reranking model.
+    """
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        """
+        Initializes the BGEReranker with configuration settings and loads the model.
+
+        :param config: Dictionary containing configuration options, including model name and device settings.
+        """
+        self.config = config
+        self.device = "cuda" if torch.cuda.is_available() else "cpu"  # Set device to GPU if available, otherwise CPU
+        
+        # Load tokenizer and model using the provided model name
+        self.tokenizer, self.model = self._load_model(self.config["model_name"])
+        self.model = self.model.to(self.device)
+        self.model.eval()  # Set the model to evaluation mode
+
+    def _load_model(self, model_name: str):
+        """
+        Loads the tokenizer and model for the reranker.
+
+        :param model_name: Name of the pre-trained model to load.
+        :return: Tuple containing the tokenizer and the model.
+        """
+        try:
+            self.logger.info(f"Loading reranker: {model_name}")
+            tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load the tokenizer
+            model = AutoModelForSequenceClassification.from_pretrained(model_name)  # Load the model
+            return tokenizer, model
+        except Exception as e:
+            self.logger.error(f"Failed to load model {model_name}: {str(e)}")
+            raise RuntimeError(f"Model loading failed: {str(e)}")
+        
+    def execute(self, data: Tuple[str, List[str]]):
+        """
+        Executes the reranking process:
+        1. Unpacks the input data (query and list of documents).
+        2. Generates query-document pairs.
+        3. Calculates relevance scores using the model.
+        4. Sorts documents based on their relevance scores.
+
+        :param data: A Data object containing a tuple of (query, doc_set), where:
+                     - query: The user query.
+                     - doc_set: List of retrieved documents.
+        :return: A Data object containing a tuple (query, reranked_documents_with_scores).
+        """
+        try:
+            query, doc_set = data  # Unpack the input data
+            top_k = self.config["topk"]  # Get the top-k parameter for reranking
+
+            # Generate query-document pairs for scoring
+            pairs = [(query, doc) for doc in doc_set]
+
+            # Tokenize the pairs and move inputs to the appropriate device
+            raw_inputs = self.tokenizer(
+                            pairs,
+                            padding=True,
+                            truncation=True,
+                            max_length=512,
+                            return_tensors="pt"
+                        )
+            inputs = {
+                k: v.to(self.device) if isinstance(v, torch.Tensor) else v
+                for k, v in raw_inputs.items()
+            }
+
+
+            
+            # Perform inference and calculate scores
+            scores = self.model(**inputs).logits.view(-1).float()
+
+            # Create a list of scored documents
+            scored_docs = [
+                {"retrieved_docs": doc, "relevance_score": score}
+                for doc, score in zip(doc_set, scores)
+            ]
+            
+            # Sort the documents by relevance score in descending order
+            reranked_docs = sorted(scored_docs, key=lambda x: x["relevance_score"], reverse=True)[:top_k]
+            reranked_docs_list = [doc["retrieved_docs"] for doc in reranked_docs]
+            self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Rerank Results: {reranked_docs_list }\033[0m ")
+            self.logger.debug(f"Top score: {reranked_docs[0]['relevance_score'] if reranked_docs else 'N/A'}")
+
+        except Exception as e:
+            raise RuntimeError(f"BGEReranker error: {str(e)}")
+        
+        return [query, reranked_docs_list]  # Return the reranked documents along with the original query
+
+
+class LLMbased_Reranker(MapFunction):
+    """
+    A reranker that uses the BAAI/bge-reranker-v2-gemma model to determine if a retrieved document contains an answer to a given query.
+    It scores the documents with 'Yes' or 'No' predictions based on whether the document answers the query.
+
+    Input: A tuple of (query, List[retrieved_documents])
+    Output: A tuple of (query, List[reranked_documents_with_scores])
+
+    Attributes:
+        logger: Logger for logging error and information messages.
+        config: Configuration dictionary containing reranker settings (model name, top_k, etc.).
+        device: Device ('cuda' or 'cpu') where the model will be loaded.
+        tokenizer: Tokenizer used to preprocess input queries and documents.
+        model: The pre-trained reranking model.
+        yes_loc: Token ID representing 'Yes' (used for scoring).
+    """
+
+    def __init__(self, config, model_name: str = "BAAI/bge-reranker-v2-gemma"):
+        """
+        Initializes the LLMbased_Reranker with configuration settings and loads the model.
+
+        :param config: Dictionary containing configuration options, including model name and device settings.
+        :param model_name: Name of the pre-trained model to load (default is "BAAI/bge-reranker-v2-gemma").
+        """
+        super().__init__()
+        self.config = config
+        self.logger = logging.getLogger(self.__class__.__name__)
+        self.device = "cuda" if torch.cuda.is_available() else "cpu"  # Set device to GPU if available, otherwise CPU
+
+        # Load tokenizer and model using the provided model name
+        self.tokenizer, self.model = self._load_model(model_name)
+        self.model = self.model.to(self.device)
+        
+        # Get the token ID for the 'Yes' token (used for classification)
+        self.yes_loc = self.tokenizer('Yes', add_special_tokens=False)['input_ids'][0]
+
+    def _load_model(self, model_name: str):
+        """
+        Loads the tokenizer and model for the reranker.
+
+        :param model_name: Name of the pre-trained model to load.
+        :return: Tuple containing the tokenizer and the model.
+        """
+        try:
+            self.logger.info(f"Loading reranker: {model_name}")
+            tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load the tokenizer
+            model = AutoModelForCausalLM.from_pretrained(model_name)  # Load the model
+            return tokenizer, model
+        except Exception as e:
+            self.logger.error(f"Failed to load model {model_name}: {str(e)}")
+            raise RuntimeError(f"Model loading failed: {str(e)}")
+
+    def get_inputs(self, pairs, tokenizer, prompt=None, max_length=1024):
+        """
+        Prepares the input for the model, including the prompt and the query-document pairs.
+
+        :param pairs: List of query-document pairs.
+        :param tokenizer: The tokenizer used to process the input data.
+        :param prompt: Optional prompt to guide the model (defaults to a generic query-passage prompt).
+        :param max_length: Maximum length of the tokenized input sequences.
+        :return: A tensor of tokenized inputs, ready for model inference.
+        """
+        if prompt is None:
+            prompt = "Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'."
+        
+        sep = "\n"
+        prompt_inputs = tokenizer(prompt, return_tensors=None, add_special_tokens=False)['input_ids']
+        sep_inputs = tokenizer(sep, return_tensors=None, add_special_tokens=False)['input_ids']
+        
+        inputs = []
+        for query, passage in pairs:
+            query_inputs = tokenizer(f'A: {query}', return_tensors=None, add_special_tokens=False, max_length=max_length * 3 // 4, truncation=True)
+            passage_inputs = tokenizer(f'B: {passage}', return_tensors=None, add_special_tokens=False, max_length=max_length, truncation=True)
+            
+            item = tokenizer.prepare_for_model(
+                [tokenizer.bos_token_id] + query_inputs['input_ids'],
+                sep_inputs + passage_inputs['input_ids'],
+                truncation='only_second',
+                max_length=max_length,
+                padding=False,
+                return_attention_mask=False,
+                return_token_type_ids=False,
+                add_special_tokens=False
+            )
+            item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs
+            item['attention_mask'] = [1] * len(item['input_ids'])
+            inputs.append(item)
+        
+        return tokenizer.pad(
+            inputs,
+            padding=True,
+            max_length=max_length + len(sep_inputs) + len(prompt_inputs),
+            pad_to_multiple_of=8,
+            return_tensors='pt',
+        )
+
+    # @torch.inference_mode()
+    def execute(self, data: Tuple[str, List[str]]) -> Tuple[str, List[str]]:
+        """
+        Executes the reranking process:
+        1. Unpacks the input data (query and list of documents).
+        2. Generates query-document pairs for classification.
+        3. Calculates relevance scores based on 'Yes'/'No' predictions.
+        4. Sorts documents based on their relevance scores.
+
+        :param data: A Data object containing a tuple of (query, doc_set), where:
+                     - query: The user query.
+                     - doc_set: List of retrieved documents.
+        :return: A Data object containing a tuple (query, reranked_documents_with_scores).
+        """
+        try:
+            query, doc_set = data  # Unpack the input data
+            doc_set = [doc_set]  # Wrap doc_set in a list for processing
+            top_k = self.config["topk"]  # Get the top-k parameter for reranking
+            emit_docs = []  # Initialize the list to store reranked documents
+
+            for retrieved_docs in doc_set:
+                # Generate query-document pairs for classification
+                pairs = [[query, doc] for doc in retrieved_docs]
+                
+                # Tokenize the pairs and move inputs to the appropriate device
+                with torch.no_grad():
+                    raw_inputs = self.get_inputs(pairs, self.tokenizer)
+                    inputs = {k: v.to(self.device) for k, v in raw_inputs.items()}
+
+                    scores = self.model(**inputs, return_dict=True).logits[:, -1, self.yes_loc].view(-1).float()
+
+                # Create a list of scored documents
+                scored_docs = [
+                    {"retrieved_docs": doc, "relevance_score": score}
+                    for doc, score in zip(retrieved_docs, scores)
+                ]
+                
+                # Sort the documents by relevance score in descending order
+                reranked_docs = sorted(scored_docs, key=lambda x: x["relevance_score"], reverse=True)[:top_k]
+                reranked_docs_list = [doc["retrieved_docs"] for doc in reranked_docs]
+                emit_docs.append(reranked_docs_list)
+                self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Rerank Results: {reranked_docs_list }\033[0m ")
+                self.logger.debug(f"Top score: {reranked_docs[0]['relevance_score'] if reranked_docs else 'N/A'}")
+
+        except Exception as e:
+            self.logger.error(f"{str(e)} when RerankerFuncton")
+            raise RuntimeError(f"Reranker error: {str(e)}")
+        
+        emit_docs = emit_docs[0]  # Only return the first set of reranked documents
+        return (query, emit_docs)  # Return the reranked documents along with the original query
+
+
+if __name__ == '__main__':
+
+   # 设置配置
+    config1 = {
+        "reranker": {
+            "model_name":"BAAI/bge-reranker-v2-m3",
+            "top_k": 3
+        }
+    }
+
+    config2 = {
+        "reranker": {
+            "model_name":"BAAI/bge-reranker-v2-gemma",
+            "top_k": 3
+        }
+    }
+
+    # 创建实例
+    # reranker = BGEReranker(config)
+    reranker = LLMbased_Reranker(config2)
+    # 测试数据
+    query = "What is the capital of France?"
+    docs = [
+        "Paris is the capital of France.",
+        "Berlin is a city in Germany.",
+        "The Eiffel Tower is located in Paris.",
+        "France is a country in Western Europe.",
+        "Madrid is the capital of Spain."
+    ]
+
+    # 执行重排
+    input_data = (query, docs)
+    output = reranker.execute(input_data)
+
+    # 输出结果
+    result_query, result_docs = output
+    print("Query:", result_query)
+    print("Top-k Re-ranked Documents:")
+    for i, doc in enumerate(result_docs, 1):
+        print(f"{i}. {doc}")
\ No newline at end of file
diff --git a/sage_common_funs/rag/retriever.py b/sage_common_funs/rag/retriever.py
new file mode 100644
index 0000000..b97f84d
--- /dev/null
+++ b/sage_common_funs/rag/retriever.py
@@ -0,0 +1,76 @@
+from typing import Tuple, List
+import time  # 替换 asyncio 为 time 用于同步延迟
+
+from sage_core.function.map_function import MapFunction
+from sage_core.function.base_function import MemoryFunction, StatefulFunction
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.runtime_context import RuntimeContext
+
+# 更新后的 SimpleRetriever
+class DenseRetriever(MapFunction):
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+
+        self.config = config
+
+        
+        if self.config.get("ltm", False):
+            self.ltm_config = self.config.get("ltm", {})
+        else:
+            self.ltm = None
+
+    def execute(self, data: str) -> Tuple[str, List[str]]:
+
+        input_query = data[0] if isinstance(data, tuple) and len(data) > 0 else data
+        chunks = []
+        self.logger.info(f"[ {self.__class__.__name__}]: Retrieving from LTM")
+        self.logger.info(f"Starting retrieval for query: {input_query}")
+        # LTM 检索
+        if self.config.get("ltm", False):
+            self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Retrieving from LTM \033[0m ")
+            try:
+                # 使用LTM配置和输入查询调用检索
+                ltm_results = self.runtime_context.retrieve(
+                    query=input_query,
+                    collection_config=self.ltm_config
+                )
+                self.logger.info(f"Retrieved {len(ltm_results)} from LTM")
+                self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Retrieval Results: {ltm_results}\033[0m ")
+                chunks.extend(ltm_results)
+
+            except Exception as e:
+                self.logger.error(f"LTM retrieval failed: {str(e)}")
+
+        return (input_query, chunks)
+    
+class BM25sRetriever(MapFunction): # 目前runtime context还只支持ltm
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        self.config = config
+        self.bm25s_collection = self.config.get("bm25s_collection")
+        self.bm25s_config = self.config.get("bm25s_config", {})
+
+
+    def execute(self, data: str) -> Tuple[str, List[str]]:
+        input_query = data
+        chunks = []
+        self.logger.debug(f"Starting BM25s retrieval for query: {input_query}")
+
+        if not self.bm25s_collection:
+            raise ValueError("BM25s collection is not configured.")
+
+        try:
+            # 使用BM25s配置和输入查询调用检索
+            bm25s_results = self.runtime_context.retrieve(
+                # self.bm25s_collection,
+                query=input_query,
+                collection_config=self.bm25s_config
+            )
+            chunks.extend(bm25s_results)
+            self.logger.info(f"\033[32m[ {self.__class__.__name__}]:Query: {input_query} Retrieved {len(bm25s_results)} from BM25s\033[0m ")
+            print(input_query)
+            print(bm25s_results)
+        except Exception as e:
+            self.logger.error(f"BM25s retrieval failed: {str(e)}")
+
+        return (input_query, chunks)
\ No newline at end of file
diff --git a/sage_common_funs/rag/searcher.py b/sage_common_funs/rag/searcher.py
new file mode 100644
index 0000000..50d449c
--- /dev/null
+++ b/sage_common_funs/rag/searcher.py
@@ -0,0 +1,40 @@
+from sage_core.function.map_function import MapFunction
+
+from typing import Dict, Any
+import requests
+import json
+import logging
+
+class BochaWebSearch(MapFunction):
+
+    def __init__(self, config: Dict[str, Any], **kwargs):
+        super().__init__(**kwargs)
+        self.api_key = config.get("api_key")
+        self.count = config.get("count", 10)
+        self.page = config.get("page", 1)
+        self.summary = config.get("summary", True)
+        self.url = "https://api.bochaai.com/v1/web-search"
+
+        if not self.api_key:
+            raise ValueError("BochaWebSearch requires an 'api_key' in config.")
+
+    def execute(self, data: str) -> Dict[str, Any]:
+        query = data
+        headers = {
+            'Authorization': self.api_key,
+            'Content-Type': 'application/json'
+        }
+        payload = {
+            "query": query,
+            "summary": self.summary,
+            "count": self.count,
+            "page": self.page
+        }
+
+        try:
+            response = requests.post(self.url, headers=headers, json=payload)
+            response.raise_for_status()
+            result = response.json()
+            return result
+        except Exception as e:
+            self.logger.error(f"BochaWebSearch error: {e}", exc_info=True)
diff --git a/sage_common_funs/rag/trigger.py b/sage_common_funs/rag/trigger.py
new file mode 100644
index 0000000..55886ae
--- /dev/null
+++ b/sage_common_funs/rag/trigger.py
@@ -0,0 +1,166 @@
+import asyncio
+import queue
+import threading
+import time
+from typing import Any, Optional
+
+from sage_core.function.map_function import MapFunction
+
+
+
+class TriggerableSource(MapFunction):
+    """
+    可触发的数据源，支持外部输入触发处理
+    """
+    
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+        """
+        初始化可触发数据源
+        
+        Args:
+            config: 配置字典
+        """
+        self.config = config.get("source", {})
+        
+        # 输入队列，用于接收外部触发的数据
+        self.input_queue = queue.Queue()
+        
+        # 控制标志
+        self._running = False
+        self._stop_requested = False
+        
+        # 用于外部调用的锁
+        self._trigger_lock = threading.Lock()
+        
+        # 等待模式配置
+        self.wait_timeout = self.config.get("wait_timeout", 1.0)  # 等待新输入的超时时间
+        self.enable_polling = self.config.get("enable_polling", True)  # 是否启用轮询模式
+        
+        self.logger.info(f"TriggerableSource initialized with config: {self.config}")
+    
+    def execute(self) -> Optional[Any]:
+        """
+        执行方法，从输入队列获取数据
+        
+        Returns:
+            Data对象或None
+        """
+        try:
+            if self.enable_polling:
+                # 轮询模式：等待一段时间后超时返回None
+                try:
+                    data = self.input_queue.get(timeout=self.wait_timeout)
+                    if data is None:  # 停止信号
+                        return None
+                    self.logger.debug(f"Got triggered data: {data}")
+                    return data
+                except queue.Empty:
+                    # 超时，返回None，让调用者决定是否继续轮询
+                    return None
+            else:
+                # 阻塞模式：一直等待直到有数据
+                data = self.input_queue.get()
+                if data is None:  # 停止信号
+                    return None
+                self.logger.debug(f"Got triggered data: {data}")
+                return data
+                
+        except Exception as e:
+            self.logger.error(f"Error in TriggerableSource.execute(): {e}")
+            return None
+    
+    def trigger(self, data: Any) -> bool:
+        """
+        外部触发接口，将数据放入队列
+        
+        Args:
+            data: 要处理的数据
+            
+        Returns:
+            bool: 是否成功触发
+        """
+        try:
+            with self._trigger_lock:
+                if self._stop_requested:
+                    self.logger.warning("Source has been stopped, ignoring trigger")
+                    return False
+                
+                self.input_queue.put(data)
+                self.logger.debug(f"Triggered with data: {data}")
+                return True
+                
+        except Exception as e:
+            self.logger.error(f"Error triggering source: {e}")
+            return False
+    
+    def stop(self):
+        """停止数据源"""
+        self._stop_requested = True
+        # 发送停止信号
+        try:
+            self.input_queue.put(None)
+        except Exception as e:
+            self.logger.error(f"Error stopping source: {e}")
+    
+    def is_empty(self) -> bool:
+        """检查输入队列是否为空"""
+        return self.input_queue.empty()
+    
+    def queue_size(self) -> int:
+        """获取队列大小"""
+        return self.input_queue.qsize()
+
+
+class RESTApiSource(TriggerableSource):
+    """
+    专门用于REST API请求的数据源
+    """
+    def __init__(self, config: dict, **kwargs):
+        super().__init__(config, **kwargs)
+        
+        # API特定配置
+        self.request_timeout = self.config.get("request_timeout", 30.0)
+        self.max_queue_size = self.config.get("max_queue_size", 100)
+        
+        # 请求ID追踪
+        self._request_counter = 0
+        self._pending_requests = {}
+        
+    def trigger_request(self, request_data: dict, request_id: str = None) -> str:
+        """
+        触发API请求处理
+        
+        Args:
+            request_data: 请求数据
+            request_id: 可选的请求ID
+            
+        Returns:
+            str: 请求ID
+        """
+        if request_id is None:
+            self._request_counter += 1
+            request_id = f"req_{self._request_counter}"
+        
+        # 检查队列大小
+        if self.queue_size() >= self.max_queue_size:
+            raise ValueError(f"Request queue is full (max: {self.max_queue_size})")
+        
+        # 包装请求数据
+        wrapped_request = {
+            "request_id": request_id,
+            "data": request_data,
+            "timestamp": time.time()
+        }
+        
+        success = self.trigger(wrapped_request)
+        if success:
+            self._pending_requests[request_id] = wrapped_request
+            self.logger.info(f"API request {request_id} queued")
+            return request_id
+        else:
+            raise RuntimeError(f"Failed to queue API request {request_id}")
+    
+    def get_pending_requests(self) -> list:
+        """获取待处理请求列表"""
+        return list(self._pending_requests.keys())
\ No newline at end of file
diff --git a/sage_common_funs/rag/writer.py b/sage_common_funs/rag/writer.py
new file mode 100644
index 0000000..40da4bd
--- /dev/null
+++ b/sage_common_funs/rag/writer.py
@@ -0,0 +1,76 @@
+from typing import Union, List, Tuple, Optional, Dict
+from sage_core.function.map_function import MapFunction
+
+
+
+class MemoryWriter(MapFunction):
+
+    def __init__(self, config: dict, **kwargs):
+        super().__init__(config, **kwargs)
+        self.state = None
+        self.config = config
+        # 初始化各类型集合
+        self.collections = {}
+
+        # 配置STM
+        if self.config.get("stm", False):
+            stm_config = self.config.get("stm_config", {})
+            self.collections["stm"] = {
+                "collection": self.config.get("stm_collection"),
+                "config": stm_config
+            }
+
+        # 配置LTM
+        if self.config.get("ltm", False):
+            ltm_config = self.config.get("ltm_config", {})
+            self.collections["ltm"] = {
+                "collection": self.config.get("ltm_collection"),
+                "config": ltm_config
+            }
+
+        # 配置DCM
+        if self.config.get("dcm", False):
+            dcm_config = self.config.get("dcm_config", {})
+            self.collections["dcm"] = {
+                "collection": self.config.get("dcm_collection"),
+                "config": dcm_config
+            }
+        # TODO: 在runtime_context中增加状态管理
+        # Issue URL: https://github.com/intellistream/SAGE/issues/219
+        # state = getRuntimeContext().getState(xxx)
+
+
+    def execute(self, data: Union[str, List[str], Tuple[str, str]]):
+        input_data = data
+
+        # 统一数据类型处理
+        processed_data = []
+        if isinstance(input_data, list):
+            processed_data = input_data
+        elif isinstance(input_data, tuple) and len(input_data) == 2:
+            processed_data = [f"{input_data[0]}{input_data[1]}"]  # 拼接元组
+        elif isinstance(input_data, str):
+            processed_data = [input_data]
+        else:
+            self.logger.error(f"Unsupported data type: {type(input_data)}")
+            return data
+
+        # 写入所有启用的集合
+        for mem_type, settings in self.collections.items():
+            collection = settings["collection"]
+            config = settings["config"]
+            if not collection:
+                self.logger.warning(f"{mem_type.upper()} collection not initialized")
+                continue
+
+            try:
+                self.state.store(
+                    collection=collection,
+                    documents=processed_data,
+                    collection_config=config
+                )
+                self.logger.debug(f"Stored {len(processed_data)} chunks to {mem_type.upper()}")
+            except Exception as e:
+                self.logger.error(f"Failed to store to {mem_type.upper()}: {str(e)}")
+
+        return data  # 返回原始数据
\ No newline at end of file
diff --git a/sage_common_funs/utils/__init__.py b/sage_common_funs/utils/__init__.py
new file mode 100644
index 0000000..f4caa39
--- /dev/null
+++ b/sage_common_funs/utils/__init__.py
@@ -0,0 +1,6 @@
+from .generator_model import OpenAIClient,HFGenerator
+
+__all__ =[ 
+    "OpenAIClient",
+    "HFGenerator"
+]
\ No newline at end of file
diff --git a/sage_utils/clients/generator_model.py b/sage_common_funs/utils/generator_model.py
similarity index 72%
rename from sage_utils/clients/generator_model.py
rename to sage_common_funs/utils/generator_model.py
index df27dc3..acb0ec4 100644
--- a/sage_utils/clients/generator_model.py
+++ b/sage_common_funs/utils/generator_model.py
@@ -1,5 +1,22 @@
-from sage_utils.clients.hf import HFGenerator
-from sage_utils.clients.openaiclient import OpenAIClient
+from .openaiclient import OpenAIClient
+from .hf import HFGenerator
+
+# class GeneratorModel:
+#     def __init__(self, method: str, model_name: str, **kwargs):
+#         if method=="openai":
+#             self.model=OpenAIClient(model_name,**kwargs)
+#         elif method=="hf":
+#             self.model=HFGenerator(model_name,**kwargs)
+#         else:
+#             raise ValueError("this method isn't supported")
+        
+#     def generate(self, prompt: str, **kwargs) :
+
+#         response=self.model.generate(prompt, **kwargs)
+
+#         return response
+
+
 class GeneratorFactory:
     @staticmethod
     def create_generator(method: str, model_name: str, **kwargs):
diff --git a/sage_utils/clients/hf.py b/sage_common_funs/utils/hf.py
similarity index 100%
rename from sage_utils/clients/hf.py
rename to sage_common_funs/utils/hf.py
diff --git a/sage_utils/clients/openaiclient.py b/sage_common_funs/utils/openaiclient.py
similarity index 100%
rename from sage_utils/clients/openaiclient.py
rename to sage_common_funs/utils/openaiclient.py
diff --git a/sage_core/README.md b/sage_core/README.md
index 8a72981..1e4515d 100644
--- a/sage_core/README.md
+++ b/sage_core/README.md
@@ -45,7 +45,7 @@ Sage Core 是 Sage 框架的核心模块，负责数据流管道的定义、编
 
 #### 编译器系统 (`sage_core/core/compiler/`)
 - **`compiler.py`**: 管道编译器
-  - `ExecutionGraph`: 将逻辑管道编译为物理执行图
+  - `Compiler`: 将逻辑管道编译为物理执行图
   - `GraphNode`: 表示执行图中的节点
   - `GraphEdge`: 表示节点间的数据连接
   - 支持并行度展开和连接优化
diff --git a/sage_core/__init__.py b/sage_core/__init__.py
index e69de29..016ef8e 100644
--- a/sage_core/__init__.py
+++ b/sage_core/__init__.py
@@ -0,0 +1,10 @@
+# from .api import env, model, memory, operator, query
+
+# # 只暴露四个子模块，保持清晰的模块边界
+# memory = api.memory
+# model = api.model
+# operator = api.operator
+# env = api.env
+# query = api.query
+
+# __all__ = ["memory", "model", "operator", "env", "query"]
\ No newline at end of file
diff --git a/sage_core/api/connected_streams.py b/sage_core/api/connected_streams.py
index 9cbaa6e..201102c 100644
--- a/sage_core/api/connected_streams.py
+++ b/sage_core/api/connected_streams.py
@@ -10,7 +10,7 @@ from sage_core.function.comap_function import BaseCoMapFunction
 from sage_core.function.join_function import BaseJoinFunction
 if TYPE_CHECKING:
     from .datastream import DataStream
-    from sage_core.environment.base_environment import BaseEnvironment
+    from .env import BaseEnvironment
 
 class ConnectedStreams:
     """表示多个transformation连接后的流结果"""
@@ -140,6 +140,21 @@ class ConnectedStreams:
                 f"CoMap operations require CoMap function with mapN methods."
             )
         
+        # Check if function has is_comap property (should be True for CoMap functions)
+        try:
+            # Create a temporary instance to check is_comap property
+            temp_instance = function()
+            if not hasattr(temp_instance, 'is_comap') or not temp_instance.is_comap:
+                raise TypeError(
+                    f"Function {function.__name__} must have is_comap=True property. "
+                    f"Ensure your function properly inherits from BaseCoMapFunction."
+                )
+        except Exception as e:
+            raise TypeError(
+                f"Failed to validate CoMap function {function.__name__}: {e}. "
+                f"Ensure the function can be instantiated and has is_comap=True."
+            )
+        
         # Validate that function supports the required number of input streams
         required_methods = [f'map{i}' for i in range(input_stream_count)]
         missing_methods = []
@@ -514,5 +529,5 @@ class ConnectedStreams:
         for input_index, upstream_trans in enumerate(self.transformations):
             tr.add_upstream(upstream_trans, input_index=input_index)
 
-        self._environment.pipeline.append(tr)
+        self._environment._pipeline.append(tr)
         return DataStream(self._environment, tr)
\ No newline at end of file
diff --git a/sage_core/api/datastream.py b/sage_core/api/datastream.py
index ffcd19c..3d2946c 100644
--- a/sage_core/api/datastream.py
+++ b/sage_core/api/datastream.py
@@ -12,7 +12,7 @@ from sage_core.function.lambda_function import wrap_lambda, detect_lambda_type
 from .connected_streams import ConnectedStreams
 from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment
+    from .env import BaseEnvironment
     from .datastream import DataStream
 
 T = TypeVar("T")
@@ -20,8 +20,14 @@ T = TypeVar("T")
 
 class DataStream(Generic[T]):
     """表示单个transformation生成的流结果"""
-    def __init__(self, env:'BaseEnvironment', transformation: 'BaseTransformation'):
-        self.logger = CustomLogger()
+    def __init__(self, env:BaseEnvironment, transformation: BaseTransformation):
+        self.logger = CustomLogger(
+            filename=f"Datastream_{transformation.function_class.__name__}",
+            env_name = env.name,
+            console_output="WARNING",
+            file_output=True,
+            global_output = "DEBUG",
+        )
         self._environment = env
         self.transformation = transformation
         self._type_param = self._resolve_type_param()
@@ -191,7 +197,7 @@ class DataStream(Generic[T]):
         # 连接到输入索引0（单输入情况）
         tr.add_upstream(self.transformation, input_index=0)
         
-        self._environment.pipeline.append(tr)
+        self._environment._pipeline.append(tr)
         return DataStream(self._environment, tr)
     
     def _resolve_type_param(self):
diff --git a/sage_core/api/env.py b/sage_core/api/env.py
new file mode 100644
index 0000000..f4ac4ef
--- /dev/null
+++ b/sage_core/api/env.py
@@ -0,0 +1,305 @@
+from __future__ import annotations
+
+import time
+from typing import Type, Union, Any, List
+from enum import Enum
+import sage_memory.api
+from sage_core.function.base_function import BaseFunction
+from sage_core.api.datastream import DataStream
+from sage_core.transformation.base_transformation import BaseTransformation
+from sage_core.transformation.source_transformation import SourceTransformation
+from sage_core.transformation.future_transformation import FutureTransformation
+from sage_utils.custom_logger import CustomLogger
+from sage_utils.name_server import get_name
+from sage_core.function.lambda_function import wrap_lambda
+
+
+
+class BaseEnvironment:
+
+    def __init__(self, name: str, config: dict | None, *, platform: str = "local"):
+        self.name = get_name(name)
+        self.logger = CustomLogger(
+            filename=f"Environment_{name}",
+            env_name = name,
+            console_output="WARNING",
+            file_output=True,
+            global_output = "DEBUG",
+        )
+        
+
+        self.config: dict = dict(config or {})
+        self.platform:str = platform
+        # 用于收集所有 BaseTransformation，供 Compiler 构建 DAG
+        self._pipeline: List[BaseTransformation] = []
+        self._filled_futures: dict = {}  # 记录已填充的future stream信息：name -> {future_transformation, actual_transformation, filled_at}
+        self.runtime_context = dict  # 需要在compiler里面实例化。
+        self.memory_collection = None  # 用于存储内存集合
+        self.is_running = False
+
+    def from_kafka_source(self, 
+                         bootstrap_servers: str,
+                         topic: str,
+                         group_id: str,
+                         auto_offset_reset: str = 'latest',
+                         value_deserializer: str = 'json',
+                         buffer_size: int = 10000,
+                         max_poll_records: int = 500,
+                         **kafka_config) -> DataStream:
+        """
+        创建Kafka数据源，采用Flink兼容的架构设计
+        
+        Args:
+            bootstrap_servers: Kafka集群地址 (例: "localhost:9092")
+            topic: Kafka主题名称
+            group_id: 消费者组ID，用于offset管理
+            auto_offset_reset: offset重置策略 ('latest'/'earliest'/'none')
+            value_deserializer: 反序列化方式 ('json'/'string'/'bytes'或自定义函数)
+            buffer_size: 本地缓冲区大小，防止数据丢失
+            max_poll_records: 每次poll的最大记录数，控制批处理大小
+            **kafka_config: 其他Kafka Consumer配置参数
+            
+        Returns:
+            DataStream: 可用于构建处理pipeline的数据流
+            
+        Example:
+            # 基本使用
+            kafka_stream = env.from_kafka_source(
+                bootstrap_servers="localhost:9092",
+                topic="user_events", 
+                group_id="sage_consumer"
+            )
+            
+            # 高级配置
+            kafka_stream = env.from_kafka_source(
+                bootstrap_servers="kafka1:9092,kafka2:9092",
+                topic="events",
+                group_id="sage_app",
+                auto_offset_reset="earliest",
+                buffer_size=20000,
+                max_poll_records=1000,
+                session_timeout_ms=30000,
+                security_protocol="SSL"
+            )
+            
+            # 构建处理pipeline
+            result = (kafka_stream
+                     .map(ProcessEventFunction)
+                     .filter(FilterFunction)
+                     .sink(OutputSinkFunction))
+        """
+        from sage_core.function.kafka_source import KafkaSourceFunction
+        
+        # 创建Kafka Source Function
+        transformation = SourceTransformation(
+            self,
+            KafkaSourceFunction,
+            bootstrap_servers=bootstrap_servers,
+            topic=topic,
+            group_id=group_id,
+            auto_offset_reset=auto_offset_reset,
+            value_deserializer=value_deserializer,
+            buffer_size=buffer_size,
+            max_poll_records=max_poll_records,
+            **kafka_config
+        )
+        
+        self._pipeline.append(transformation)
+        self.logger.info(f"Kafka source created for topic: {topic}, group: {group_id}")
+        
+        return DataStream(self, transformation)
+
+
+
+    def from_source(self, function: Union[Type[BaseFunction], callable], *args, **kwargs) -> DataStream:
+        if callable(function) and not isinstance(function, type):
+            # 这是一个 lambda 函数或普通函数
+            function = wrap_lambda(function, 'flatmap')
+        transformation = SourceTransformation(self, function, *args,**kwargs)
+        
+        self._pipeline.append(transformation)
+        return DataStream(self, transformation)
+
+    def from_future(self, name: str) -> DataStream:
+        """
+        创建一个future stream占位符，用于建立反馈边。
+        
+        Args:
+            name: future stream的名称，用于标识和调试
+            
+        Returns:
+            DataStream: 包含FutureTransformation的数据流
+            
+        Example:
+            future_stream = env.from_future("feedback_loop")
+            # 使用future_stream参与pipeline构建
+            result = source.connect(future_stream).comap(CombineFunction)
+            # 最后填充future
+            result.fill_future(future_stream)
+        """
+        transformation = FutureTransformation(self, name)
+        self._pipeline.append(transformation)
+        return DataStream(self, transformation)
+
+    # TODO: add a new type of source with handler returned.
+    def create_source(self):
+        pass
+
+    def submit(self, name="example_pipeline"):
+        # self.debug_print_pipeline()
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        engine.submit_env(self)
+        # time.sleep(10) # 等待管道启动
+        while (self.initialized() is False):
+            time.sleep(1)
+
+    def run_once(self, node:str = None):
+        """
+        运行一次管道，适用于测试或调试。
+        """
+        if(self.is_running):
+            self.logger.warning("Pipeline is already running. ")
+            return
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        engine.run_once(self)
+        # time.sleep(10) # 等待管道启动
+
+    def run_streaming(self, node: str = None):
+        """
+        运行管道，适用于生产环境。
+        """
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        engine.run_streaming(self)
+        # time.sleep(10) # 等待管道启动
+
+    def stop(self):
+        """
+        停止管道运行。
+        """
+        self.logger.info("Stopping pipeline...")
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        engine.stop_pipeline(self)
+        # self.close()
+
+    def close(self):
+        """
+        关闭管道运行。
+        """
+        from sage_core.engine import Engine
+        engine = Engine.get_instance()
+        # 1) 停止本环境对应的 DAG 执行
+        engine.stop_pipeline(self)
+        # 2) 关闭该环境在 Engine 中的引用，并在无剩余环境时彻底 shutdown Engine
+        engine.close_pipeline(self)
+        # 3) 清理自身引用，以打破循环链
+        self._pipeline.clear()
+
+
+
+    def set_memory(self, config = None):
+        self.memory_collection = sage_memory.api.get_memory(config=config, remote=(self.platform != "local"), env_name=self.name)
+
+    def set_memory_collection(self, collection):
+
+        self.memory_collection = collection 
+        
+    # TODO: 写一个判断Env 是否已经完全初始化并开始执行的函数
+    def initialized(self):
+        pass
+
+    def get_filled_futures(self) -> dict:
+        """
+        获取所有已填充的future stream信息
+        
+        Returns:
+            dict: 已填充的future stream信息，格式为:
+                {
+                    'future_name': {
+                        'future_transformation': FutureTransformation,
+                        'actual_transformation': BaseTransformation,
+                        'filled_at': timestamp
+                    }
+                }
+        """
+        return self._filled_futures.copy()
+    
+    def has_unfilled_futures(self) -> bool:
+        """
+        检查pipeline中是否还有未填充的future streams
+        
+        Returns:
+            bool: 如果存在未填充的future streams返回True，否则返回False
+        """
+        from sage_core.transformation.future_transformation import FutureTransformation
+        for transformation in self._pipeline:
+            if isinstance(transformation, FutureTransformation) and not transformation.filled:
+                return True
+        return False
+    
+    def validate_pipeline_for_compilation(self) -> None:
+        """
+        验证pipeline是否可以进行编译
+        检查是否存在未填充的future streams
+        
+        Raises:
+            RuntimeError: 如果存在未填充的future streams
+        """
+        from sage_core.transformation.future_transformation import FutureTransformation
+        unfilled_futures = []
+        
+        for transformation in self._pipeline:
+            if isinstance(transformation, FutureTransformation) and not transformation.filled:
+                unfilled_futures.append(transformation.future_name)
+        
+        if unfilled_futures:
+            raise RuntimeError(
+                f"Cannot compile pipeline with unfilled future streams: {unfilled_futures}. "
+                f"Please fill all future streams using fill_future() before compilation."
+            )
+
+    ########################################################
+    #                auxiliary methods                     #
+    ########################################################
+
+    def _append(self, transformation: BaseTransformation):
+        """将 BaseTransformation 添加到管道中（Compiler 会使用）。"""
+        self.pipeline.append(transformation)
+        return DataStream(self, transformation)
+    
+    @property
+    def pipeline(self) -> List[BaseTransformation]:  # noqa: D401
+        """返回 BaseTransformation 列表（Compiler 会使用）。"""
+        return self._pipeline
+
+
+class LocalEnvironment(BaseEnvironment):
+    """
+    本地执行环境（不使用 Ray），用于开发调试或小规模测试。
+    """
+
+    def __init__(self, name: str = "local_environment", config: dict | None = None):
+        super().__init__(name, config, platform="local")
+
+
+class RemoteEnvironment(BaseEnvironment):
+    """
+    分布式执行环境（Ray），用于生产或大规模部署。
+    """
+
+    def __init__(self, name: str = "remote_environment", config: dict | None = None):
+        super().__init__(name, config, platform="remote")
+
+
+class DevEnvironment(BaseEnvironment):
+    """
+    混合执行环境，可根据配置动态选择本地或 Ray。
+    config 中可包含 'use_ray': bool 来切换运行时。
+    """
+
+    def __init__(self, name: str = "dev_environment", config: dict | None = None):
+        cfg = dict(config or {})
+        super().__init__(name, cfg, platform="hybrid")
diff --git a/sage_core/api/local_environment.py b/sage_core/api/local_environment.py
deleted file mode 100644
index 30afbf9..0000000
--- a/sage_core/api/local_environment.py
+++ /dev/null
@@ -1,38 +0,0 @@
-from __future__ import annotations
-
-from typing import Optional, TYPE_CHECKING
-from sage_core.environment.base_environment import BaseEnvironment
-from sage_utils.actor_wrapper import ActorWrapper
-if TYPE_CHECKING:
-    from sage_jobmanager.job_manager import JobManager
-
-class LocalEnvironment(BaseEnvironment):
-    """本地环境，直接使用本地JobManager实例"""
-
-    def __init__(self, name: str, config: dict | None = None):
-        super().__init__(name, config, platform="local")
-        
-        # 本地环境不需要客户端
-        self._engine_client = None
-
-    def submit(self):
-        # 序列化环境
-        env_uuid = self.jobmanager.submit_job(self)
-        
-        if env_uuid:
-            self.env_uuid = env_uuid
-            self.logger.info(f"Environment submitted with UUID: {self.env_uuid}")
-        else:
-            raise RuntimeError("Failed to submit environment: no UUID returned")
-
-    @property
-    def jobmanager(self) -> 'JobManager':
-        """直接返回JobManager的单例实例"""
-        if self._jobmanager is None:
-            from sage_jobmanager.job_manager import JobManager
-            # 获取JobManager单例实例
-            jobmanager_instance = JobManager()
-            # 本地环境直接返回JobManager实例，不使用ActorWrapper
-            self._jobmanager = jobmanager_instance
-            
-        return self._jobmanager
\ No newline at end of file
diff --git a/sage_core/api/remote_environment.py b/sage_core/api/remote_environment.py
deleted file mode 100644
index 63e8549..0000000
--- a/sage_core/api/remote_environment.py
+++ /dev/null
@@ -1,119 +0,0 @@
-from __future__ import annotations
-
-from typing import Optional, TYPE_CHECKING
-from sage_core.environment.base_environment import BaseEnvironment
-from sage_core.jobmanager_client import JobManagerClient
-from sage_utils.actor_wrapper import ActorWrapper
-if TYPE_CHECKING:
-    from sage_jobmanager.job_manager import JobManager
-
-class RemoteEnvironment(BaseEnvironment):
-    """远程环境，通过客户端连接远程JobManager"""
-
-    def __init__(self, name: str, config: dict | None = None, host: str = "127.0.0.1", port: int = 19001):
-        super().__init__(name, config, platform="remote")
-        
-        # 设置远程连接配置
-        self.daemon_host = host
-        self.daemon_port = port
-        
-        # 更新配置
-        self.config.update({
-            "engine_host": self.daemon_host,
-            "engine_port": self.daemon_port
-        })
-
-    @property
-    def client(self) -> JobManagerClient:
-        """获取远程JobManager客户端"""
-        if self._engine_client is None:
-            self._engine_client = JobManagerClient(
-                host=self.daemon_host, 
-                port=self.daemon_port
-            )
-        return self._engine_client
-
-    @property
-    def jobmanager(self) -> 'JobManager': # 是actorwrapper无感包着的
-        """通过客户端获取远程JobManager句柄"""
-        if self._jobmanager is None:
-            self._jobmanager = self.client.get_actor_handle()
-        return self._jobmanager
-
-    def submit(self):
-        # 序列化环境
-        from sage_utils.serialization.dill_serializer import trim_object_for_ray
-        serialized_env = trim_object_for_ray(self)
-        
-        # 通过jobmanager属性提交job
-        env_uuid = self.jobmanager.submit_job(serialized_env)
-        
-        if env_uuid:
-            self.env_uuid = env_uuid
-            self.logger.info(f"Environment submitted with UUID: {self.env_uuid}")
-        else:
-            raise RuntimeError("Failed to submit environment: no UUID returned")
-
-    def stop(self):
-        """停止远程环境"""
-        if not self.env_uuid:
-            self.logger.warning("Remote environment not submitted, nothing to stop")
-            return
-        
-        self.logger.info("Stopping remote pipeline...")
-        
-        try:
-            response = self.jobmanager.pause_job(self.env_uuid)
-            
-            if response.get("status") == "stopped":
-                self.is_running = False
-                self.logger.info("Remote pipeline stopped successfully")
-            else:
-                self.logger.warning(f"Failed to stop remote pipeline: {response.get('message')}")
-                
-        except Exception as e:
-            self.logger.error(f"Error stopping remote pipeline: {e}")
-
-    def close(self):
-        """关闭远程环境"""
-        if not self.env_uuid:
-            self.logger.warning("Remote environment not submitted, nothing to close")
-            return
-        
-        self.logger.info("Closing remote environment...")
-        
-        try:
-            response = self.jobmanager.pause_job(self.env_uuid)
-            
-            if response.get("status") == "stopped":
-                self.logger.info("Remote environment closed successfully")
-            else:
-                self.logger.warning(f"Failed to close remote environment: {response.get('message')}")
-                
-        except Exception as e:
-            self.logger.error(f"Error closing remote environment: {e}")
-        finally:
-            # 清理本地资源
-            self.is_running = False
-            self.env_uuid = None
-            
-            # 清理管道
-            self.pipeline.clear()
-
-    def health_check(self):
-        """检查远程JobManager健康状态"""
-        try:
-            response = self.client.health_check()
-            return response
-        except Exception as e:
-            self.logger.error(f"Health check failed: {e}")
-            return {"status": "error", "message": str(e)}
-
-    def get_remote_info(self):
-        """获取远程JobManager信息"""
-        try:
-            response = self.client.get_actor_info()
-            return response
-        except Exception as e:
-            self.logger.error(f"Failed to get remote info: {e}")
-            return {"status": "error", "message": str(e)}
\ No newline at end of file
diff --git a/sage_core/engine.py b/sage_core/engine.py
new file mode 100644
index 0000000..03ceebd
--- /dev/null
+++ b/sage_core/engine.py
@@ -0,0 +1,138 @@
+from typing import TYPE_CHECKING
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.local_thread_pool import LocalThreadPool
+from sage_runtime.mixed_dag import MixedDAG
+import threading
+if TYPE_CHECKING:
+    from sage_runtime.compiler import Compiler
+    from sage_core.api.env import BaseEnvironment
+
+
+class Engine:
+    _instance = None
+    _lock = threading.Lock()
+
+    def __init__(self):
+
+        # 确保只初始化一次
+        if hasattr(self, "_initialized"):
+            return
+        self._initialized = True
+        self.graphs: dict[str, 'Compiler'] = {}  # 存储 pipeline 名称到 SageGraph 的映射
+        self.env_to_dag: dict[str, 'MixedDAG'] = {}  # 存储name到dag的映射，其中dag的类型为DAG或RayDAG
+        # print("Engine initialized")
+        self.logger = CustomLogger(
+            filename=f"SageEngine",
+            console_output="WARNING",
+            file_output=True,
+            global_output="WARNING",
+            name="SageEngine"
+        )
+
+    def __new__(cls):
+        # 禁止直接实例化
+        raise RuntimeError("请通过 get_instance() 方法获取实例")
+
+    # 用来获取类的唯一实例
+    # 同一个进程中只存在唯一的实例
+    @classmethod
+    def get_instance(cls):
+        # 双重检查锁确保线程安全
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # 绕过 __new__ 的异常，直接创建实例
+                    instance = super().__new__(cls)
+                    instance.__init__()
+                    cls._instance = instance
+        return cls._instance
+
+    def submit_env(self, env: 'BaseEnvironment'):
+        from sage_runtime.compiler import Compiler
+        # env, graph和dag用的都是同一个名字
+        graph = Compiler(env)
+        # graph.debug_print_graph()
+        self.graphs[graph.name] = graph
+        try:
+            self.logger.info(f"Received mixed graph '{graph.name}' with {len(graph.nodes)} nodes")
+            # 编译图
+            mixed_dag = MixedDAG(graph, env)
+            self.env_to_dag[env.name] = mixed_dag  # 存储 DAG 到字典中
+            mixed_dag.submit()
+            self.logger.info(f"Mixed graph '{graph.name}' submitted to runtime manager.")
+        except Exception as e:
+            self.logger.info(f"Failed to submit graph '{graph.name}': {e}")
+            raise
+
+    def run_once(self, env: 'BaseEnvironment', node: str = None):
+        """
+        执行一次环境的 DAG
+        """
+        self.logger.info(f"Executing DAG for environment '{env.name}'")
+        dag = self.env_to_dag.get(env.name)
+        self.logger.debug(f"Found DAG for environment '{env.name}': {dag}")
+        dag.execute_once()
+        self.logger.info(f"DAG for environment '{env.name}' have completed execution.")
+
+    def run_streaming(self, env: 'BaseEnvironment', node: str = None):
+        """
+        执行一次环境的 DAG
+        """
+        self.logger.info(f"Executing streaming DAG for environment '{env.name}'")
+        dag = self.env_to_dag.get(env.name)
+        dag.execute_streaming()
+        self.logger.info(f"Streaming DAG for environment '{env.name}' have started.")
+
+    def stop_pipeline(self, env: 'BaseEnvironment'):
+        """
+        停止指定环境的 DAG
+        """
+        self.logger.info(f"Stopping DAG for environment '{env.name}'")
+        dag = self.env_to_dag.get(env.name)
+        if dag:
+            dag.stop()
+            self.logger.info(f"DAG for environment '{env.name}' has been stopped.")
+        else:
+            self.logger.warning(f"No DAG found for environment '{env.name}'")
+
+    def close_pipeline(self, env: 'BaseEnvironment'):
+        """
+        停止指定环境的 DAG
+        """
+        self.logger.info(f"Stopping DAG for environment '{env.name}'")
+        graph = self.graphs.pop(env.name, None)
+        if graph:
+            # graph.destroy()
+            self.logger.info(f"Graph for environment '{env.name}' has been destroyed.")
+        
+        dag = self.env_to_dag.pop(env.name, None)
+        if dag:
+            dag.close()
+            self.logger.info(f"DAG for environment '{env.name}' has been closed.")
+        else:
+            self.logger.warning(f"No DAG found for environment '{env.name}'")
+        # 如果没有剩余环境，执行完整 shutdown
+        if not self.env_to_dag:
+            self.shutdown()
+
+    def shutdown(self):
+        """
+        完整释放 Engine 持有的所有资源：
+        - 停掉 RuntimeManager（线程、Ray actor 等）
+        - 停掉可能的 TCP/HTTP server
+        - 清空 DAG 映射与缓存
+        - 重置 Engine 单例
+        """
+        self.logger.info("Shutting down Engine and releasing resources")
+        try:
+            local_runtime = LocalThreadPool.get_instance()
+            local_runtime.shutdown()
+        except Exception:
+            self.logger.exception("Error shutting down RuntimeManager:{e}")
+            raise
+
+        self.env_to_dag.clear()
+        self.graphs.clear()
+
+        Engine._instance = None
+        self.logger.info("Engine shutdown complete")
diff --git a/sage_core/environment/README.md b/sage_core/environment/README.md
deleted file mode 100644
index b4cc13b..0000000
--- a/sage_core/environment/README.md
+++ /dev/null
@@ -1,3 +0,0 @@
-Base_environment -> stream/batch -> localstream/localbatch/remotestream/remotebatch
-
-Base_environment -> local/remote -> localstream/localbatch/remotestream/remotebatch
\ No newline at end of file
diff --git a/sage_core/environment/base_environment.py b/sage_core/environment/base_environment.py
deleted file mode 100644
index a20f402..0000000
--- a/sage_core/environment/base_environment.py
+++ /dev/null
@@ -1,291 +0,0 @@
-from __future__ import annotations
-from abc import ABC, abstractmethod
-from datetime import datetime
-import os
-from pathlib import Path
-from typing import List, Optional, TYPE_CHECKING, Type, Union
-from sage_core.function.base_function import BaseFunction
-from sage_core.function.lambda_function import wrap_lambda
-import sage_memory.api
-from sage_core.api.datastream import DataStream
-from sage_core.transformation.base_transformation import BaseTransformation
-from sage_core.transformation.source_transformation import SourceTransformation
-from sage_core.transformation.future_transformation import FutureTransformation
-from sage_utils.custom_logger import CustomLogger
-from sage_jobmanager.utils.name_server import get_name
-from sage_core.jobmanager_client import JobManagerClient
-from sage_utils.actor_wrapper import ActorWrapper
-if TYPE_CHECKING:
-    from sage_jobmanager.job_manager import JobManager
-
-class BaseEnvironment(ABC):
-
-    __state_exclude__ = ["_engine_client", "client", "jobmanager"]
-    # 会被继承，但是不会被自动合并
-
-    def __init__(self, name: str, config: dict | None, *, platform: str = "local"):
-
-        self.name = get_name(name)
-        self.uuid: Optional[str] # 由jobmanager生成
-
-        self.config: dict = dict(config or {})
-        self.platform:str = platform
-        # 用于收集所有 BaseTransformation，供 ExecutionGraph 构建 DAG
-        self.pipeline: List[BaseTransformation] = []
-        self._filled_futures: dict = {}  # 记录已填充的future stream信息：name -> {future_transformation, actual_transformation, filled_at}
-        self.runtime_context = dict  # 需要在compiler里面实例化。
-        self.memory_collection = None  # 用于存储内存集合
-        self.is_running = False
-        self.env_base_dir: Optional[str] = None  # 环境基础目录，用于存储日志和其他文件
-        # JobManager 相关
-        self._jobmanager: Optional[ActorWrapper] = None
-        
-        # Engine 客户端相关
-        self._engine_client: Optional[JobManagerClient] = None
-        self.env_uuid: Optional[str] = None
-
-    ########################################################
-    #                  user interface                      #
-    ########################################################
-
-    def set_memory(self, config = None):
-        self.memory_collection = sage_memory.api.get_memory(config=config, remote=(self.platform != "local"), env_name=self.name)
-
-
-
-    def from_kafka_source(self, 
-                         bootstrap_servers: str,
-                         topic: str,
-                         group_id: str,
-                         auto_offset_reset: str = 'latest',
-                         value_deserializer: str = 'json',
-                         buffer_size: int = 10000,
-                         max_poll_records: int = 500,
-                         **kafka_config) -> DataStream:
-        """
-        创建Kafka数据源，采用Flink兼容的架构设计
-        
-        Args:
-            bootstrap_servers: Kafka集群地址 (例: "localhost:9092")
-            topic: Kafka主题名称
-            group_id: 消费者组ID，用于offset管理
-            auto_offset_reset: offset重置策略 ('latest'/'earliest'/'none')
-            value_deserializer: 反序列化方式 ('json'/'string'/'bytes'或自定义函数)
-            buffer_size: 本地缓冲区大小，防止数据丢失
-            max_poll_records: 每次poll的最大记录数，控制批处理大小
-            **kafka_config: 其他Kafka Consumer配置参数
-            
-        Returns:
-            DataStream: 可用于构建处理pipeline的数据流
-            
-        Example:
-            # 基本使用
-            kafka_stream = env.from_kafka_source(
-                bootstrap_servers="localhost:9092",
-                topic="user_events", 
-                group_id="sage_consumer"
-            )
-            
-            # 高级配置
-            kafka_stream = env.from_kafka_source(
-                bootstrap_servers="kafka1:9092,kafka2:9092",
-                topic="events",
-                group_id="sage_app",
-                auto_offset_reset="earliest",
-                buffer_size=20000,
-                max_poll_records=1000,
-                session_timeout_ms=30000,
-                security_protocol="SSL"
-            )
-            
-            # 构建处理pipeline
-            result = (kafka_stream
-                     .map(ProcessEventFunction)
-                     .filter(FilterFunction)
-                     .sink(OutputSinkFunction))
-        """
-        from sage_core.function.kafka_source import KafkaSourceFunction
-        
-        # 创建Kafka Source Function
-        transformation = SourceTransformation(
-            self,
-            KafkaSourceFunction,
-            bootstrap_servers=bootstrap_servers,
-            topic=topic,
-            group_id=group_id,
-            auto_offset_reset=auto_offset_reset,
-            value_deserializer=value_deserializer,
-            buffer_size=buffer_size,
-            max_poll_records=max_poll_records,
-            **kafka_config
-        )
-        
-        self.pipeline.append(transformation)
-        self.logger.info(f"Kafka source created for topic: {topic}, group: {group_id}")
-        
-        return DataStream(self, transformation)
-
-    def from_source(self, function: Union[Type[BaseFunction], callable], *args, **kwargs) -> DataStream:
-        if callable(function) and not isinstance(function, type):
-            # 这是一个 lambda 函数或普通函数
-            function = wrap_lambda(function, 'flatmap')
-        transformation = SourceTransformation(self, function, *args, **kwargs)
-
-        self.pipeline.append(transformation)
-        return DataStream(self, transformation)
-
-
-
-    def from_collection(self, function: Union[Type[BaseFunction], callable], *args, **kwargs) -> DataStream:
-        if callable(function) and not isinstance(function, type):
-            # 这是一个 lambda 函数或普通函数
-            function = wrap_lambda(function, 'flatmap')
-        transformation = SourceTransformation(self, function, *args,
-                                              **kwargs)  # TODO: add a new transformation 去告诉engine这个input source是有界的，当执行完毕之后，会发送一个endofinput信号来停止所有进程。
-
-        self.pipeline.append(transformation)
-        return DataStream(self, transformation)
-
-
-    def from_future(self, name: str) -> DataStream:
-        """
-        创建一个future stream占位符，用于建立反馈边。
-        
-        Args:
-            name: future stream的名称，用于标识和调试
-            
-        Returns:
-            DataStream: 包含FutureTransformation的数据流
-            
-        Example:
-            future_stream = env.from_future("feedback_loop")
-            # 使用future_stream参与pipeline构建
-            result = source.connect(future_stream).comap(CombineFunction)
-            # 最后填充future
-            result.fill_future(future_stream)
-        """
-        transformation = FutureTransformation(self, name)
-        self.pipeline.append(transformation)
-        return DataStream(self, transformation)
-
-    ########################################################
-    #                jobmanager interface                  #
-    ########################################################
-    @abstractmethod
-    def submit(self):
-        pass
-        # 序列化环境
-        from sage_utils.serialization.dill_serializer import serialize_object
-        serialized_env = serialize_object(self)
-        
-        # 通过jobmanager属性提交job
-        env_uuid = self.jobmanager.submit_job(serialized_env)
-        
-        if env_uuid:
-            self.env_uuid = env_uuid
-            self.logger.info(f"Environment submitted with UUID: {self.env_uuid}")
-        else:
-            raise RuntimeError("Failed to submit environment: no UUID returned")
-
-    def stop(self):
-        """停止管道运行"""
-        if not self.env_uuid:
-            self.logger.warning("Environment not submitted, nothing to stop")
-            return
-        
-        self.logger.info("Stopping pipeline...")
-        
-        try:
-            response = self.jobmanager.pause_job(self.env_uuid)
-            
-            if response.get("status") == "success":
-                self.is_running = False
-                self.logger.info("Pipeline stopped successfully")
-            else:
-                self.logger.warning(f"Failed to stop pipeline: {response.get('message')}")
-        except Exception as e:
-            self.logger.error(f"Error stopping pipeline: {e}")
-
-    def close(self):
-        """关闭管道运行"""
-        if not self.env_uuid:
-            self.logger.warning("Environment not submitted, nothing to close")
-            return
-        
-        self.logger.info("Closing environment...")
-        
-        try:
-            response = self.jobmanager.pause_job(self.env_uuid)
-            
-            if response.get("status") == "success":
-                self.logger.info("Environment closed successfully")
-            else:
-                self.logger.warning(f"Failed to close environment: {response.get('message')}")
-                
-        except Exception as e:
-            self.logger.error(f"Error closing environment: {e}")
-        finally:
-            # 清理本地资源
-            self.is_running = False
-            self.env_uuid = None
-            
-            # 清理管道
-            self.pipeline.clear()
-
-    ########################################################
-    #                properties                            #
-    ########################################################
-
-    @property
-    def logger(self):
-        if not hasattr(self, "_logger"):
-            self._logger = CustomLogger()
-        return self._logger
-
-    @property
-    def client(self)-> JobManagerClient:
-        if self._engine_client is None:
-            # 从配置中获取 Engine 地址，或使用默认值
-            daemon_host = self.config.get("engine_host", "127.0.0.1")
-            daemon_port = self.config.get("engine_port", 19000)
-            
-            self._engine_client = JobManagerClient(host=daemon_host, port=daemon_port)
-            
-        return self._engine_client
-
-
-    @property
-    @abstractmethod
-    def jobmanager(self) -> 'JobManager':
-        return
-        # """获取JobManager句柄，通过ActorWrapper封装以提供透明调用"""
-        # if self._jobmanager is None:
-        #     self._jobmanager = self.client.get_actor_handle()
-        # return self._jobmanager
-
-
-
-    ########################################################
-    #                auxiliary methods                     #
-    ########################################################
-
-    def _append(self, transformation: BaseTransformation):
-        """将 BaseTransformation 添加到管道中（Compiler 会使用）。"""
-        self.pipeline.append(transformation)
-        return DataStream(self, transformation)
-
-    def setup_logging_system(self, log_base_dir: str): 
-        # this method is called by jobmanager when receiving the job, not the user
-        self.session_timestamp = datetime.now()
-        self.session_id = self.session_timestamp.strftime("%Y%m%d_%H%M%S")
-        # self.log_base_dir = log_base_dir
-        self.env_base_dir = os.path.join(log_base_dir, f"env_{self.name}_{self.session_id}")
-        Path(self.env_base_dir).mkdir(parents=True, exist_ok=True)
-
-        self._logger = CustomLogger([
-                ("console", "INFO"),  # 控制台显示重要信息
-                (os.path.join(self.env_base_dir, "Environment.log"), "DEBUG"),  # 详细日志
-                (os.path.join(self.env_base_dir, "Error.log"), "ERROR")  # 错误日志
-            ],
-            name = f"Environment_{self.name}",
-        )
\ No newline at end of file
diff --git a/sage_core/function/__init__.py b/sage_core/function/__init__.py
index ecdf37e..d152b7f 100644
--- a/sage_core/function/__init__.py
+++ b/sage_core/function/__init__.py
@@ -1,19 +1,19 @@
 
 
-# from .base_function import BaseFunction
-# from .map_function import MapFunction
-# from .filter_function import FilterFunction
-# from .flatmap_function import FlatMapFunction
-# # from .lambda_function import LambdaFunction
-# from .sink_function import SinkFunction 
-# from .source_function import SourceFunction
+from .base_function import BaseFunction
+from .map_function import MapFunction
+from .filter_function import FilterFunction
+from .flatmap_function import FlatMapFunction
+# from .lambda_function import LambdaFunction
+from .sink_function import SinkFunction 
+from .source_function import SourceFunction
 
-# __all__ = [
-#     "BaseFunction",
-#     "MapFunction",
-#     "FilterFunction",
-#     "FlatMapFunction",
-#     "LambdaFunction",
-#     "SinkFunction",
-#     "SourceFunction",
-# ]
+__all__ = [
+    "BaseFunction",
+    "MapFunction",
+    "FilterFunction",
+    "FlatMapFunction",
+    "LambdaFunction",
+    "SinkFunction",
+    "SourceFunction",
+]
diff --git a/sage_core/function/base_function.py b/sage_core/function/base_function.py
index 7ffb85f..03bf32e 100644
--- a/sage_core/function/base_function.py
+++ b/sage_core/function/base_function.py
@@ -1,10 +1,14 @@
 import os
 from abc import ABC, abstractmethod
 from typing import Type, List, Tuple, Any, TYPE_CHECKING, Union
+
+from dotenv import load_dotenv
+
+from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
     from sage_runtime.runtime_context import RuntimeContext
 
-from sage_utils.state_persistence import load_function_state, save_function_state
+from sage_runtime.state_persistence import load_function_state, save_function_state
 
 
 # 构造来源于sage_runtime/operator/factory.py
@@ -15,17 +19,28 @@ class BaseFunction(ABC):
     """
 
     def __init__(self, ctx:'RuntimeContext' = None, **kwargs):
-        self.ctx = ctx
+        self.runtime_context = ctx
+        self.name = ctx.name if ctx else self.__class__.__name__
+        self.env_name = ctx.env_name if ctx else None
+        self._logger = CustomLogger(
+            filename=f"Function_{self.name}",
+            env_name= self.env_name,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output = "WARNING",
+            name = f"{self.name}_{self.__class__.__name__}",
+            session_folder=ctx.session_folder if ctx else None
+        )
         # self.runtime_context.create_logger()
         self.logger.info(f"Function {self.name} initialized")
         
     @property
     def logger(self):
-        return self.ctx.logger
-    
-    @property
-    def name(self):
-        return self.ctx.name
+        if not hasattr(self, "_logger"):
+            import logging
+            self._logger = logging.getLogger(f"{self.__class__.__name__}")
+        return self._logger
+
 
     # @abstractmethod
     # def close(self, *args, **kwargs):
@@ -56,6 +71,10 @@ class BaseFunction(ABC):
         """
         pass
 
+
+
+
+
 class MemoryFunction(BaseFunction):
     def __init__(self):
         self.runtime_context = None  # 需要在compiler里面实例化。
@@ -76,17 +95,17 @@ class StatefulFunction(BaseFunction):
         super().__init__(**kwargs)
         # 注入上下文
         # 恢复上次 checkpoint
-        chkpt_dir = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
-        chkpt_path = os.path.join(chkpt_dir, f"{self.ctx.name}.chkpt")
+        chkpt_dir = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
+        chkpt_path = os.path.join(chkpt_dir, f"{self.runtime_context.name}.chkpt")
         load_function_state(self, chkpt_path)
 
     def save_state(self):
         """
         将当前对象状态持久化到 disk，
         """
-        base = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
+        base = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
         os.makedirs(base, exist_ok=True)
-        path = os.path.join(base, f"{self.ctx.name}.chkpt")
+        path = os.path.join(base, f"{self.runtime_context.name}.chkpt")
         save_function_state(self, path)
 
 # class MemoryFunction(BaseFunction):
diff --git a/sage_core/function/flatmap_collector.py b/sage_core/function/flatmap_collector.py
index 756b027..0fa3914 100644
--- a/sage_core/function/flatmap_collector.py
+++ b/sage_core/function/flatmap_collector.py
@@ -1,19 +1,30 @@
 from typing import Optional, List, Any, Dict, Tuple
-from sage_runtime.runtime_context import RuntimeContext
-from sage_runtime.router.base_router import BaseRouter
+from sage_utils.custom_logger import CustomLogger
+
+
 class Collector:
     """
     Enhanced Collector class for collecting data from a function.
     Supports both immediate emission and batched collection.
     """
 
-    def __init__(self, ctx:RuntimeContext, router:BaseRouter):
+    def __init__(self, operator=None, session_folder: str = None, name: str = None):
+        self.operator = operator
+        self.logger = CustomLogger(
+            filename=f"Node_{name}",
+            session_folder=session_folder,
+            console_output=False,
+            file_output=True,
+            global_output=True,
+            name=f"{name}_Collector"
+        ) if name else None
+        
         # 数据收集缓存
-        self.ctx = ctx
         self._collected_data: List[Any] = []
+        self._batch_mode = True  # 默认启用批处理模式
         
         self.logger.debug(f"Collector initialized with batch_mode={self._batch_mode}") if self.logger else None
-        
+
     def collect(self, data: Any):
         """
         Collect data. Behavior depends on batch_mode setting.
@@ -22,9 +33,36 @@ class Collector:
             data: The data to collect
             tag: Optional output tag
         """
-        # 批处理模式：先收集，后输出
-        self._collected_data.append(data)
-        self.logger.debug(f"Data collected in batch mode: {data} data")
+        if self._batch_mode:
+            # 批处理模式：先收集，后输出
+            self._collected_data.append(data)
+            if self.logger:
+                self.logger.debug(f"Data collected in batch mode: {data} data")
+        else:
+            # 即时模式：立即输出
+            if self.operator:
+                self.operator.emit(data)
+                if self.logger:
+                    self.logger.debug(f"Data emitted immediately: {data} data")
+            else:
+                # 没有operator，只能收集
+                self._collected_data.append(data)
+                if self.logger:
+                    self.logger.warning(f"No operator set, data collected: {data} data")
+
+    def collect_multiple(self, data_list: List[Any], tag: Optional[str] = None):
+        """
+        Collect multiple data items at once.
+        
+        Args:
+            data_list: List of data items to collect
+            tag: Optional output tag for all items
+        """
+        for data in data_list:
+            self.collect(data)
+        
+        if self.logger:
+            self.logger.debug(f"Collected {len(data_list)} items via collect_multiple")
 
     def get_collected_data(self) -> List[Any]:
         """
@@ -53,7 +91,77 @@ class Collector:
         if self.logger and count > 0:
             self.logger.debug(f"Cleared {count} collected items")
 
+    def flush(self):
+        """
+        Flush all collected data to the operator (emit all collected items).
+        Only works when operator is set.
+        """
+        if not self.operator:
+            if self.logger:
+                self.logger.warning("Cannot flush: no operator set")
+            return
+        
+        count = 0
+        for data, tag in self._collected_data:
+            try:
+                self.operator.emitdata
+                count += 1
+            except Exception as e:
+                if self.logger:
+                    self.logger.error(f"Error emitting data during flush: {e}", exc_info=True)
+        
+        if self.logger:
+            self.logger.debug(f"Flushed {count} items to operator")
+        
+        self.clear()
+
+    def set_batch_mode(self, batch_mode: bool):
+        """
+        Set the batch mode for the collector.
+        
+        Args:
+            batch_mode: True for batch mode, False for immediate mode
+        """
+        old_mode = self._batch_mode
+        self._batch_mode = batch_mode
+        if self.logger:
+            self.logger.debug(f"Batch mode changed from {old_mode} to {batch_mode}")
+
+    def is_batch_mode(self) -> bool:
+        """
+        Check if collector is in batch mode.
+        
+        Returns:
+            bool: True if in batch mode, False otherwise
+        """
+        return self._batch_mode
+
+    def set_operator(self, operator):
+        """
+        Set the operator for this collector.
+        
+        Args:
+            operator: The operator instance
+        """
+        self.operator = operator
+        if self.logger:
+            self.logger.debug(f"Operator set: {operator.__class__.__name__}")
 
-    @property
-    def logger(self):
-        return self.ctx.logger
\ No newline at end of file
+    def get_statistics(self) -> Dict[str, Any]:
+        """
+        Get collector statistics.
+        
+        Returns:
+            Dict[str, Any]: Statistics dictionary
+        """
+        tag_counts = {}
+        for _, tag in self._collected_data:
+            tag_key = tag if tag is not None else "default"
+            tag_counts[tag_key] = tag_counts.get(tag_key, 0) + 1
+        
+        return {
+            "total_collected": len(self._collected_data),
+            "batch_mode": self._batch_mode,
+            "has_operator": self.operator is not None,
+            "tag_distribution": tag_counts
+        }
diff --git a/sage_core/function/flatmap_function.py b/sage_core/function/flatmap_function.py
index 802d96d..bd3fa30 100644
--- a/sage_core/function/flatmap_function.py
+++ b/sage_core/function/flatmap_function.py
@@ -58,6 +58,24 @@ class FlatMapFunction(BaseFunction):
         self.out.collect(data)
         self.logger.debug(f"Data collected: {data}")
 
+    def collect_multiple(self, data_list: Iterable[Any]):
+        """
+        Convenience method to collect multiple data items at once.
+        
+        Args:
+            data_list: Iterable of data items to collect
+            tag: Optional output tag
+        """
+        if self.out is None:
+            raise RuntimeError("Collector not initialized. This should be set by the operator.")
+        
+        count = 0
+        for item in data_list:
+            self.out.collect(item)
+            count += 1
+        
+        self.logger.debug(f"Collected {count} items via collect_multiple")
+
     @abstractmethod
     def execute(self, data: Any) -> Optional[Iterable[Any]]:
         """
diff --git a/sage_core/function/kafka_source.py b/sage_core/function/kafka_source.py
index ff81fa2..be5f0d4 100644
--- a/sage_core/function/kafka_source.py
+++ b/sage_core/function/kafka_source.py
@@ -1,5 +1,7 @@
 from sage_core.function.source_function import SourceFunction
-from typing import Callable, Dict, Any, TYPE_CHECKING
+from sage_utils.custom_logger import CustomLogger
+from sage_utils.data_loader import resolve_data_path
+from typing import List, Callable, Dict, Any, TYPE_CHECKING
 import threading, json, queue
 if TYPE_CHECKING:
     from sage_runtime.runtime_context import RuntimeContext
diff --git a/sage_core/function/map_function.py b/sage_core/function/map_function.py
index 8227f92..25f55b3 100644
--- a/sage_core/function/map_function.py
+++ b/sage_core/function/map_function.py
@@ -3,7 +3,7 @@ from typing import Type, List, Tuple, Any, TYPE_CHECKING, Union
 from sage_core.function.base_function import BaseFunction
 
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 if TYPE_CHECKING:
     from sage_runtime.runtime_context import RuntimeContext
 
diff --git a/sage_core/function/source_function.py b/sage_core/function/source_function.py
index 6b5be37..db88d7b 100644
--- a/sage_core/function/source_function.py
+++ b/sage_core/function/source_function.py
@@ -6,15 +6,6 @@ from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
     from sage_runtime.runtime_context import RuntimeContext
 
-class StopSignal:
-    """
-    停止信号类，用于标识任务停止
-    """
-    def __init__(self, name: str):
-        self.name = name
-
-    def __repr__(self) -> str:
-        return f"<StopSignal {self.name}>"
 
 class SourceFunction(BaseFunction):
     """
@@ -23,7 +14,7 @@ class SourceFunction(BaseFunction):
     源函数不接收输入数据，只产生输出数据
     通常用于读取文件、数据库、API等外部数据源
     """
-
+    
     @abstractmethod
     def execute(self) -> Any:
         """
diff --git a/sage_core/jobmanager_client.py b/sage_core/jobmanager_client.py
deleted file mode 100644
index efc8ce2..0000000
--- a/sage_core/jobmanager_client.py
+++ /dev/null
@@ -1,102 +0,0 @@
-import ray
-import socket
-import json
-import pickle
-from ray.actor import ActorHandle
-from pathlib import Path
-from typing import Optional, Dict, Any
-from sage_jobmanager.remote_job_manager import RemoteJobManager
-from sage_utils.custom_logger import CustomLogger
-from sage_utils.actor_wrapper import ActorWrapper
-
-# ==================== 客户端工具类 ====================
-
-class JobManagerClient:
-    """JobManager客户端，用于连接守护服务获取Actor句柄"""
-    
-    def __init__(self, host: str = "127.0.0.1", port: int = 19001):
-        self.host = host
-        self.port = port
-    
-    def _send_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
-        """发送请求到守护服务"""
-        try:
-            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
-                sock.settimeout(10)
-                sock.connect((self.host, self.port))
-                
-                # 发送请求
-                request_data = json.dumps(request).encode('utf-8')
-                length_data = len(request_data).to_bytes(4, byteorder='big')
-                sock.sendall(length_data + request_data)
-                
-                # 接收响应
-                response_length_data = sock.recv(4)
-                response_length = int.from_bytes(response_length_data, byteorder='big')
-                
-                response_data = b''
-                while len(response_data) < response_length:
-                    chunk = sock.recv(min(response_length - len(response_data), 8192))
-                    response_data += chunk
-                
-                return json.loads(response_data.decode('utf-8'))
-                
-        except Exception as e:
-            return {"status": "error", "message": f"Connection error: {e}"}
-    
-    def get_actor_handle(self) -> 'RemoteJobManager':
-        """获取JobManager Actor句柄"""
-        import uuid
-        request = {
-            "action": "get_actor_handle",
-            "request_id": str(uuid.uuid4())
-        }
-        if not ray.is_initialized():
-            ray.init(address="auto", _temp_dir="/var/lib/ray_shared")
-
-        response = self._send_request(request)
-        
-        if response.get("status") != "success":
-            raise Exception(f"Failed to get actor handle: {response.get('message')}")
-        
-        # 反序列化Actor句柄
-        actor_handle_hex = response.get("actor_handle")
-        actor_handle_bytes = bytes.fromhex(actor_handle_hex)
-        actor_handle = pickle.loads(actor_handle_bytes)
-        jobmanager = ActorWrapper(actor_handle)
-        return jobmanager
-    
-    def get_actor_info(self) -> Dict[str, Any]:
-        """获取Actor信息"""
-        import uuid
-        request = {
-            "action": "get_actor_info",
-            "request_id": str(uuid.uuid4())
-        }
-        
-        response = self._send_request(request)
-        
-        if response.get("status") != "success":
-            raise Exception(f"Failed to get actor info: {response.get('message')}")
-        
-        return response.get("actor_info", {})
-    
-    def health_check(self) -> Dict[str, Any]:
-        """健康检查"""
-        import uuid
-        request = {
-            "action": "health_check",
-            "request_id": str(uuid.uuid4())
-        }
-        
-        return self._send_request(request)
-    
-    def restart_actor(self) -> Dict[str, Any]:
-        """重启Actor"""
-        import uuid
-        request = {
-            "action": "restart_actor", 
-            "request_id": str(uuid.uuid4())
-        }
-        
-        return self._send_request(request)
\ No newline at end of file
diff --git a/sage_core/operator/base_operator.py b/sage_core/operator/base_operator.py
index 1ebcac3..76f8eea 100644
--- a/sage_core/operator/base_operator.py
+++ b/sage_core/operator/base_operator.py
@@ -1,78 +1,200 @@
 
 from abc import ABC, abstractmethod
 from typing import Any, List, Dict, Optional, Set, TYPE_CHECKING, Type, Tuple
-from sage_core.function.source_function import StopSignal
-from sage_runtime.task.base_task import BaseTask
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.unified_emit_context import UnifiedEmitContext
+from sage_runtime.io.packet import Packet
 
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_runtime.router.connection import Connection
+    from sage_runtime.io.connection import Connection
     from sage_runtime.runtime_context import RuntimeContext
-    from sage_jobmanager.factory.function_factory import FunctionFactory
-    from sage_runtime.router.base_router import BaseRouter
+    from sage_runtime.function.factory import FunctionFactory
 
 class BaseOperator(ABC):
     def __init__(self, 
                  function_factory: 'FunctionFactory', ctx: 'RuntimeContext', *args,
                  **kwargs):
         
-        self.received_stop_signals: Set[str] = set()
-        self.ctx: 'RuntimeContext' = ctx
+        self.name:str
         self.function:'BaseFunction'
-        self.router:'BaseRouter'     # 由task传下来的
-        self.task: Optional['BaseTask'] = None
+        self._emit_context: 'UnifiedEmitContext'
+
+        self.function_factory = function_factory
+        self.downstream_groups:Dict[int, Dict[int, 'Connection']] = {}
+        self.downstream_group_roundrobin: Dict[int, int] = {}
+
         try:
-            self.function = function_factory.create_function(self.name, ctx)
-            self.logger.debug(f"Created function instance with {function_factory}")
+            self.name = ctx.name
+            self.logger = CustomLogger(
+                filename=f"Node_{ctx.name}",
+                env_name=ctx.env_name,
+                console_output="WARNING",
+                file_output="DEBUG",
+                global_output = "WARNING",
+                name = f"{ctx.name}_{self.__class__.__name__}",
+                session_folder=ctx.session_folder
+            )
+            self.runtime_context = ctx
+            self.function = self.function_factory.create_function(self.name, ctx)
+
+            self._emit_context = UnifiedEmitContext(name = ctx.name, session_folder=ctx.session_folder, env_name = ctx.env_name) 
+            self.logger.debug(f"Created function instance with {self.function_factory}")
 
         except Exception as e:
             self.logger.error(f"Failed to create function instance: {e}", exc_info=True)
 
-    # TODO: 去掉stateful function的概念，用某些策略对于function内部的可序列化字段做静态保存和checkpoint
-    def save_state(self):
+    def receive_packet(self, packet: 'Packet' = None):
+        self.process_packet(packet)
         from sage_core.function.base_function import StatefulFunction
         if isinstance(self.function, StatefulFunction):
             self.function.save_state()
 
-    def receive_packet(self, packet: 'Packet'):
-        """l
-        接收数据包并处理
-        """
-        if packet is None:
-            self.logger.warning(f"Received None packet in {self.name}")
-            return
-        if isinstance(packet, StopSignal):
-            self.handle_stop_signal(packet) 
-            return
-        self.logger.debug(f"Operator {self.name} received packet: {packet}")
-        # 处理数据包
-        self.process_packet(packet)
+    @abstractmethod
+    def process_packet(self, packet: 'Packet' = None):
+        return
 
-    def handle_stop_signal(self, stop_signal: StopSignal):
+
+    def emit_packet(self, packet: 'Packet'):
         """
-        处理停止信号
+        新增：直接发送packet，根据其分区信息选择路由策略
+        
+        Args:
+            packet: 要发送的packet，可能包含分区信息
         """
-        if stop_signal.name in self.received_stop_signals:
-            self.logger.debug(f"Already received stop signal from {stop_signal.name}")
-            return
+        if self._emit_context is None:
+            raise RuntimeError(f"Emit context not set for operator {self.name}")
+        self.logger.debug(f"Emitting packet: {packet}")
+        # 根据packet的分区信息选择路由策略
+        if packet.is_keyed():
+            self._emit_keyed_packet(packet)
+        else:
+            self._emit_round_robin_packet(packet)
+
+    def _emit_keyed_packet(self, packet: 'Packet'):
+        """使用分区信息进行路由"""
+        strategy = packet.partition_strategy
+        partition_key = packet.partition_key
         
-        self.received_stop_signals.add(stop_signal.name)
-        self.logger.info(f"Handling stop signal from {stop_signal.name}")
-        # 发送停止信号到路由器
-        self.router.send_stop_signal(stop_signal)
+        for broadcast_index, parallel_targets in self.downstream_groups.items():
+            if strategy == "hash":
+                target_index = hash(partition_key) % len(parallel_targets)
+                connection = parallel_targets[target_index]
+                self._send_packet_to_connection(connection, packet)
+                
+            elif strategy == "broadcast":
+                # 广播到所有实例
+                for connection in parallel_targets.values():
+                    self._send_packet_to_connection(connection, packet)
+                    
+            elif strategy == "round_robin":
+                # 忽略键，使用轮询
+                current_round_robin = self.downstream_group_roundrobin[broadcast_index]
+                connection = parallel_targets[current_round_robin % len(parallel_targets)]
+                self.downstream_group_roundrobin[broadcast_index] += 1
+                self._send_packet_to_connection(connection, packet)
+                
+            else:
+                self.logger.warning(f"Unknown partition strategy: {strategy}, using round-robin")
+                self._emit_round_robin_packet(packet)
+
+    def _emit_round_robin_packet(self, packet: 'Packet'):
+        """使用round-robin策略路由"""
+        for broadcast_index, parallel_targets in self.downstream_groups.items():
+            current_round_robin = self.downstream_group_roundrobin[broadcast_index]
+            connection = parallel_targets[current_round_robin % len(parallel_targets)]
+            self.downstream_group_roundrobin[broadcast_index] += 1
+            self._send_packet_to_connection(connection, packet)
+
+    def _send_packet_to_connection(self, connection: 'Connection', packet: 'Packet'):
+        """发送packet到指定连接"""
+        try:
+            # 更新packet的目标输入索引
+            routed_packet = Packet(
+                payload=packet.payload,
+                input_index=connection.target_input_index,
+                partition_key=packet.partition_key,
+                partition_strategy=packet.partition_strategy,
+            )
+            
+            self._emit_context.send_packet_direct(connection, routed_packet)
+            self.logger.debug(f"Sent {'keyed' if packet.is_keyed() else 'unkeyed'} packet to {connection.target_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Failed to send packet to {connection.target_name}: {e}", exc_info=True)
 
-    @abstractmethod
-    def process_packet(self, packet: 'Packet' = None):
-        return
 
-    @property
-    def name(self) -> str:
-        """获取任务名称"""
-        return self.ctx.name
 
-    @property
-    def logger(self) -> CustomLogger:
-        """获取当前任务的日志记录器"""
-        return self.ctx.logger
\ No newline at end of file
+    def add_connection(self, connection: 'Connection') -> None:
+        """
+        添加下游连接，使用Connection对象的属性作为索引，保存完整的Connection对象
+        
+        Args:
+            connection: Connection对象，包含所有连接信息
+        """
+        broadcast_index = connection.broadcast_index
+        parallel_index = connection.parallel_index
+        
+        # Debug log
+        self.logger.debug(
+            f"broadcast_index={broadcast_index}, parallel_index={parallel_index}, "
+            f"target={connection.target_name}"
+            f"connection_type={connection.connection_type.value}"
+        )
+        
+        # 初始化广播组（如果不存在）
+        if broadcast_index not in self.downstream_groups:
+            self.downstream_groups[broadcast_index] = {}
+            self.downstream_group_roundrobin[broadcast_index] = 0
+        
+        # 保存完整的Connection对象
+        self.downstream_groups[broadcast_index][parallel_index] = connection
+        
+        # 打印连接的调试信息（可选）
+        # if self.logger.isEnabledFor(10):  # DEBUG level
+        # print(connection.debug_info())
+        
+        self.logger.debug(
+            f"Added downstream connection: -> "
+            f"{connection.target_name} "
+            f"(type: {connection.connection_type.value})"
+        )
+
+    def get_downstream_connections(self, output_tag: str = None) -> List['Connection']:
+        """
+        获取所有下游连接，可选择性地按输出标签过滤
+        
+        Args:
+            output_tag: 可选的输出标签过滤器
+            
+        Returns:
+            List['Connection']: 连接列表
+        """
+        connections = []
+        
+        for broadcast_groups in self.downstream_groups.values():
+            for connection in broadcast_groups.values():
+                connections.append(connection)
+        
+        return connections
+    
+    def get_wrapped_operator(self):
+        """
+            这个方法是用来让ide满意的，用来代表OperatorWrapper提供的这个方法
+        """
+        pass
+
+    def _send_to_connection(self, connection:'Connection', data):
+        """
+        发送数据到指定连接的辅助方法
+        
+        Args:
+            connection: 目标连接
+            data: 要发送的数据（已解包的原始数据）
+        """
+        try:
+            packet = Packet(data, connection.target_input_index)
+            self._emit_context.send_packet_direct(connection, packet)
+            self.logger.debug(f"Sent data to {connection.target_name}")
+        except Exception as e:
+            self.logger.error(f"Failed to send data to {connection.target_name}: {e}", exc_info=True)
\ No newline at end of file
diff --git a/sage_core/operator/comap_operator.py b/sage_core/operator/comap_operator.py
index f1b35a8..e837991 100644
--- a/sage_core/operator/comap_operator.py
+++ b/sage_core/operator/comap_operator.py
@@ -1,7 +1,7 @@
 from .base_operator import BaseOperator
 from typing import Union, Any
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class CoMapOperator(BaseOperator):
@@ -56,7 +56,7 @@ class CoMapOperator(BaseOperator):
             if result is not None:
                 # 继承原packet的分区信息
                 result_packet = packet.inherit_partition_info(result)
-                self.router.send(result_packet)
+                self.emit_packet(result_packet)
                 
         except Exception as e:
             self.logger.error(f"Error in CoMapOperator {self.name}: {e}", exc_info=True)
diff --git a/sage_core/operator/filter_operator.py b/sage_core/operator/filter_operator.py
index 5d61b05..2062cd2 100644
--- a/sage_core/operator/filter_operator.py
+++ b/sage_core/operator/filter_operator.py
@@ -2,11 +2,11 @@ from typing import Any, Optional
 from typing import Any, List, Dict, Optional, Set, TYPE_CHECKING, Type, Tuple
 from sage_core.operator.base_operator import BaseOperator
 from sage_core.function.filter_function import FilterFunction
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_runtime.router.connection import Connection
+    from sage_runtime.io.connection import Connection
     
 
 class FilterOperator(BaseOperator):
@@ -40,7 +40,7 @@ class FilterOperator(BaseOperator):
             
             if should_pass:
                 # 通过过滤，继承分区信息
-                self.router.send(packet)
+                self.emit_packet(packet)
             # 不通过过滤：不发送任何packet
             
         except Exception as e:
diff --git a/sage_core/operator/flatmap_operator.py b/sage_core/operator/flatmap_operator.py
index 084bdef..a8a3bfa 100644
--- a/sage_core/operator/flatmap_operator.py
+++ b/sage_core/operator/flatmap_operator.py
@@ -2,7 +2,7 @@ from typing import Any, Iterable, Optional
 from sage_core.operator.base_operator import BaseOperator
 from sage_core.function.flatmap_function import FlatMapFunction
 from sage_core.function.flatmap_collector import Collector
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class FlatMapOperator(BaseOperator):
@@ -30,8 +30,9 @@ class FlatMapOperator(BaseOperator):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.out: Collector = Collector(
-            self.ctx,
-            self.router
+            operator=self,
+            session_folder=self.runtime_context.session_folder,
+            name=self.name
         )
         self.function.insert_collector(self.out)
         self.logger.info(f"FlatMapOperator '{self.name}' initialized with collector")
@@ -55,7 +56,7 @@ class FlatMapOperator(BaseOperator):
             
             # 处理function的返回值（如果有）
             if result is not None:
-                self._flatmap_send(result, packet)
+                self._emit_iterable_with_partition_info(result, packet)
             
             # 处理通过collector收集的数据
             collected_data = self.out.get_collected_data()
@@ -64,7 +65,7 @@ class FlatMapOperator(BaseOperator):
                 for item_data in collected_data:
                     # 为每个收集的item创建新packet，继承分区信息
                     result_packet = packet.inherit_partition_info(item_data)
-                    self.router.send(result_packet)
+                    self.emit_packet(result_packet)
                 # 清空collector
                 self.out.clear()
             
@@ -78,7 +79,7 @@ class FlatMapOperator(BaseOperator):
         except Exception as e:
             self.logger.error(f"Error in FlatMapOperator '{self.name}'.process_packet(): {e}", exc_info=True)
 
-    def _flatmap_send(self, result: Any, source_packet: 'Packet'):
+    def _emit_iterable_with_partition_info(self, result: Any, source_packet: 'Packet'):
         """
         将可迭代对象展开并发送给下游，保持分区信息
         
@@ -93,15 +94,67 @@ class FlatMapOperator(BaseOperator):
                 for item in result:
                     # 为每个item创建新packet，继承分区信息
                     result_packet = source_packet.inherit_partition_info(item)
-                    self.router.send(result_packet)
+                    self.emit_packet(result_packet)
                     count += 1
                 self.logger.debug(f"FlatMapOperator '{self.name}' emitted {count} items from iterable")
             else:
                 # 如果不是可迭代对象，直接发送
                 result_packet = source_packet.inherit_partition_info(result)
-                self.router.send(result_packet)
+                self.emit_packet(result_packet)
                 self.logger.debug(f"FlatMapOperator '{self.name}' emitted single item: {result}")
                 
         except Exception as e:
             self.logger.error(f"Error in FlatMapOperator '{self.name}'._emit_iterable_with_partition_info(): {e}", exc_info=True)
-            raise
\ No newline at end of file
+            raise
+
+    def get_collector_statistics(self) -> dict:
+        """
+        获取collector的统计信息
+        
+        Returns:
+            dict: 统计信息，如果没有collector则返回空字典
+        """
+        if self.out:
+            return self.out.get_statistics()
+        return {}
+
+    def debug_print_collector_info(self):
+        """
+        打印collector的调试信息
+        """
+        if self.out:
+            print(f"\n🔍 FlatMapOperator '{self.name}' Collector Info:")
+            self.out.debug_print_collected_data()
+        else:
+            print(f"\n🔍 FlatMapOperator '{self.name}' has no collector")
+
+    def emit_data_with_key(self, data: Any, partition_key: Any = None, partition_strategy: str = None):
+        """
+        便利方法：直接发送带有分区信息的数据
+        
+        Args:
+            data: 要发送的数据
+            partition_key: 分区键
+            partition_strategy: 分区策略
+        """
+        packet = Packet(
+            payload=data,
+            partition_key=partition_key,
+            partition_strategy=partition_strategy
+        )
+        self.emit_packet(packet)
+
+    def emit_collected_data_with_source_partition(self, source_packet: 'Packet'):
+        """
+        便利方法：发送collector中的所有数据，继承源packet的分区信息
+        
+        Args:
+            source_packet: 源packet，用于继承分区信息
+        """
+        collected_data = self.out.get_collected_data()
+        if collected_data:
+            for item_data in collected_data:
+                result_packet = source_packet.inherit_partition_info(item_data)
+                self.emit_packet(result_packet)
+            self.out.clear()
+            self.logger.debug(f"Emitted {len(collected_data)} collected items with inherited partition info")
\ No newline at end of file
diff --git a/sage_core/operator/future_operator.py b/sage_core/operator/future_operator.py
index efd10cc..f75d3a2 100644
--- a/sage_core/operator/future_operator.py
+++ b/sage_core/operator/future_operator.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 from typing import Any, List
 from .base_operator import BaseOperator
-from sage_jobmanager.factory.function_factory import FunctionFactory
+from sage_runtime.function.factory import FunctionFactory
 
 
 class FutureOperator(BaseOperator):
diff --git a/sage_core/operator/join_operator.py b/sage_core/operator/join_operator.py
index f9cf41b..3e05fb8 100644
--- a/sage_core/operator/join_operator.py
+++ b/sage_core/operator/join_operator.py
@@ -1,7 +1,7 @@
 from .base_operator import BaseOperator
 from typing import Union, Any, List
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class JoinOperator(BaseOperator):
@@ -123,7 +123,7 @@ class JoinOperator(BaseOperator):
                 partition_strategy=original_packet.partition_strategy or "hash",
             )
             
-            self.router.send(result_packet)
+            self.emit_packet(result_packet)
             
             self.logger.debug(
                 f"JoinOperator '{self.name}' emitted result for key '{join_key}': "
diff --git a/sage_core/operator/keyby_operator.py b/sage_core/operator/keyby_operator.py
index c395ca4..ff6b120 100644
--- a/sage_core/operator/keyby_operator.py
+++ b/sage_core/operator/keyby_operator.py
@@ -1,8 +1,8 @@
 from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Optional
 from sage_core.operator.base_operator import BaseOperator
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 if TYPE_CHECKING:
-    from sage_runtime.router.connection import Connection
+    from sage_runtime.io.connection import Connection
 
 class KeyByOperator(BaseOperator):
     """
@@ -35,13 +35,13 @@ class KeyByOperator(BaseOperator):
             self.logger.debug(f"KeyByOperator '{self.name}' added key '{extracted_key}' to packet")
             
             # 直接发送带有分区信息的packet
-            self.router.send(keyed_packet)
+            self.emit_packet(keyed_packet)
             
         except Exception as e:
             self.logger.error(f"Error in KeyByOperator {self.name}: {e}", exc_info=True)
             # 回退：发送原始packet
             if packet:
-                self.router.send(packet)
+                self.emit_packet(packet)
 
 
     def process(self, raw_data: Any, input_index: int = 0) -> Any:
diff --git a/sage_core/operator/map_operator.py b/sage_core/operator/map_operator.py
index 4c45cc0..2f49e1f 100644
--- a/sage_core/operator/map_operator.py
+++ b/sage_core/operator/map_operator.py
@@ -4,7 +4,7 @@ from sage_core.function.map_function import MapFunction
 from typing import Union, Any
 from sage_core.function.map_function import MapFunction
 from sage_utils.custom_logger import CustomLogger
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class MapOperator(BaseOperator):
@@ -23,6 +23,6 @@ class MapOperator(BaseOperator):
                 self.logger.debug(f"Operator {self.name} processed data with result: {result}")
                 result_packet = packet.inherit_partition_info(result) if (result is not None) else None
                 if result_packet is not None:
-                    self.router.send(result_packet)
+                    self.emit_packet(result_packet)
         except Exception as e:
             self.logger.error(f"Error in {self.name}.process(): {e}", exc_info=True)
\ No newline at end of file
diff --git a/sage_core/operator/sink_operator.py b/sage_core/operator/sink_operator.py
index f9ffc62..6b920ee 100644
--- a/sage_core/operator/sink_operator.py
+++ b/sage_core/operator/sink_operator.py
@@ -1,17 +1,14 @@
-from sage_core.function.source_function import StopSignal
 from sage_core.operator.base_operator import BaseOperator
 from sage_core.function.sink_function import SinkFunction
 from sage_utils.custom_logger import CustomLogger
 from collections import deque
 from typing import Union, Dict, Deque, Tuple, Any
-from sage_runtime.router.packet import Packet
+from sage_runtime.io.packet import Packet
 
 
 class SinkOperator(BaseOperator):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        self.received_stop_signals = set()
-        self.stop_signal_count = 0
         # # 验证函数类型
         # if not isinstance(self.function, SinkFunction):
         #     raise TypeError(f"SinkOperator requires SinkFunction, got {type(self.function)}")
@@ -24,19 +21,4 @@ class SinkOperator(BaseOperator):
                 result = self.function.execute(packet.payload)
                 self.logger.debug(f"Operator {self.name} processed data with result: {result}")
         except Exception as e:
-            self.logger.error(f"Error in {self.name}.process(): {e}", exc_info=True)
-
-    def handle_stop_signal(self, stop_signal: StopSignal):
-        """
-        处理停止信号
-        """
-        if stop_signal.name in self.received_stop_signals:
-            self.logger.debug(f"Already received stop signal from {stop_signal.name}")
-            return
-        
-        self.received_stop_signals.add(stop_signal.name)
-        self.logger.info(f"Handling stop signal from {stop_signal.name}")
-
-        self.stop_signal_count += 1
-        if self.stop_signal_count >= self.ctx.stop_signal_num:
-            self.ctx.jobmanager.receive_stop_signal(self.ctx.env_uuid)
\ No newline at end of file
+            self.logger.error(f"Error in {self.name}.process(): {e}", exc_info=True)
\ No newline at end of file
diff --git a/sage_core/operator/source_operator.py b/sage_core/operator/source_operator.py
index 467ed37..2bd2067 100644
--- a/sage_core/operator/source_operator.py
+++ b/sage_core/operator/source_operator.py
@@ -2,31 +2,20 @@ from sage_core.operator.base_operator import BaseOperator
 from sage_core.function.source_function import SourceFunction
 from sage_utils.custom_logger import CustomLogger
 from collections import deque
-from typing import Union, Dict, Deque, Tuple, Any, TYPE_CHECKING
-from sage_runtime.router.packet import Packet
-from sage_core.function.source_function import StopSignal
-if TYPE_CHECKING:
-    from sage_runtime.task.base_task import BaseTask
+from typing import Union, Dict, Deque, Tuple, Any
+from sage_runtime.io.packet import Packet
 
 class SourceOperator(BaseOperator):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
-    def receive_packet(self, packet: 'Packet'):
-        self.process_packet(packet)
-
+        
     def process_packet(self, packet: 'Packet' = None):
         try:
             
             result = self.function.execute()
             self.logger.debug(f"Operator {self.name} processed data with result: {result}")
-            if isinstance(result, StopSignal):
-                self.logger.info(f"Source Operator {self.name} received stop signal: {result}")
-                result.name = self.name
-                self.router.send_stop_signal(result)
-                self.task.stop()
-                return
             if result is not None:
-                self.router.send(Packet(result))
+                self.emit_packet(Packet(result))
         except Exception as e:
             self.logger.error(f"Error in {self.name}.process(): {e}", exc_info=True)
diff --git a/sage_core/transformation/base_transformation.py b/sage_core/transformation/base_transformation.py
index 09bbc29..f24fb26 100644
--- a/sage_core/transformation/base_transformation.py
+++ b/sage_core/transformation/base_transformation.py
@@ -1,15 +1,17 @@
 from __future__ import annotations
-from typing import List, Type, Union, TYPE_CHECKING, Any
+from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Optional
+from enum import Enum
+from abc import ABC, abstractmethod
 from sage_utils.custom_logger import CustomLogger
-from sage_jobmanager.utils.name_server import get_name
-from sage_jobmanager.factory.operator_factory import OperatorFactory
-from sage_jobmanager.factory.function_factory import FunctionFactory
-from sage_jobmanager.factory.task_factory import TaskFactory
+from sage_utils.name_server import get_name
+from sage_runtime.operator.factory import OperatorFactory
+from sage_runtime.function.factory import FunctionFactory
+from sage_runtime.dagnode.factory import DAGNodeFactory
 from ray.actor import ActorHandle
 if TYPE_CHECKING:
     from sage_core.operator.base_operator import BaseOperator
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class BaseTransformation:
@@ -25,8 +27,6 @@ class BaseTransformation:
         self.operator_class:Type[BaseOperator]  # 由子类设置
 
         self.remote = (env.platform == "remote")
-        self.env_name = env.name
-        self.memory_collection: Union[Any, ActorHandle] = env.memory_collection
         self.env = env
         self.function_class = function
         self.function_args = args
@@ -35,9 +35,14 @@ class BaseTransformation:
         self.basename = get_name(name) if name else get_name(self.function_class.__name__)
             
 
-        self.logger = CustomLogger()
-        if self.remote and (env.memory_collection is not None) and (not isinstance(env.memory_collection, ActorHandle)):
-            raise Exception(f"Memory collection must be a Ray Actor handle for remote transformation, but got {type(env.memory_collection)}")
+        self.logger = CustomLogger(
+            filename=f"{self.basename}_{self.__class__.__name__}",
+            env_name = env.name,
+            console_output=False,
+            file_output=True
+        )
+        if self.remote and not isinstance(env.memory_collection, ActorHandle):
+            raise Exception("Memory collection must be a Ray Actor handle for remote transformation")
 
         self.logger.debug(f"Creating BaseTransformation of type {type} with rag {self.function_class.__name__}")
 
@@ -47,7 +52,7 @@ class BaseTransformation:
 
         
         # 懒加载工厂
-        self._dag_node_factory: TaskFactory = None
+        self._dag_node_factory: DAGNodeFactory = None
         self._operator_factory: OperatorFactory = None
         self._function_factory: FunctionFactory = None
         # 生成的平行节点名字：f"{transformation.function_class.__name__}_{i}"
@@ -93,16 +98,16 @@ class BaseTransformation:
                 operator_class=self.operator_class,
                 function_factory=self.function_factory,
                 basename=self.basename,
-                env_name=self.env_name,
+                env_name=self.env.name,
                 remote=self.remote
             )   
         return self._operator_factory
 
     @property
-    def task_factory(self) -> TaskFactory:
+    def dag_node_factory(self) -> DAGNodeFactory:
         """懒加载创建DAG节点工厂"""
         if self._dag_node_factory is None:
-            self._dag_node_factory = TaskFactory(self)
+            self._dag_node_factory = DAGNodeFactory(self)
         return self._dag_node_factory
 
     @property
@@ -113,10 +118,6 @@ class BaseTransformation:
     def is_spout(self) -> bool:
         return False
 
-    @property
-    def is_sink(self) -> bool:
-        return False
-
     @property
     def is_merge_operation(self) -> bool:
         """
diff --git a/sage_core/transformation/comap_transformation.py b/sage_core/transformation/comap_transformation.py
index 079ada4..78309e4 100644
--- a/sage_core/transformation/comap_transformation.py
+++ b/sage_core/transformation/comap_transformation.py
@@ -3,7 +3,7 @@ from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Opti
 from sage_core.transformation.base_transformation import BaseTransformation
 if TYPE_CHECKING:
     from sage_core.function.comap_function import BaseCoMapFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class CoMapTransformation(BaseTransformation):
diff --git a/sage_core/transformation/filter_transformation.py b/sage_core/transformation/filter_transformation.py
index 709ea2e..e627709 100644
--- a/sage_core/transformation/filter_transformation.py
+++ b/sage_core/transformation/filter_transformation.py
@@ -5,7 +5,7 @@ from sage_core.operator.filter_operator import FilterOperator
 if TYPE_CHECKING:
     from sage_core.operator.base_operator import BaseOperator
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 
diff --git a/sage_core/transformation/flatmap_transformation.py b/sage_core/transformation/flatmap_transformation.py
index ea7286b..0597729 100644
--- a/sage_core/transformation/flatmap_transformation.py
+++ b/sage_core/transformation/flatmap_transformation.py
@@ -4,7 +4,7 @@ from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.flatmap_operator import FlatMapOperator
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class FlatMapTransformation(BaseTransformation):
diff --git a/sage_core/transformation/future_transformation.py b/sage_core/transformation/future_transformation.py
index be226bd..e581934 100644
--- a/sage_core/transformation/future_transformation.py
+++ b/sage_core/transformation/future_transformation.py
@@ -4,7 +4,7 @@ from .base_transformation import BaseTransformation
 from sage_core.operator.future_operator import FutureOperator
 
 if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class FutureTransformation(BaseTransformation):
@@ -53,31 +53,31 @@ class FutureTransformation(BaseTransformation):
         
         # 标记为已填充，但保留在pipeline中以便compiler能够处理
         # compiler会检查filled状态来决定如何处理这个transformation
-        # self._mark_as_filled_in_pipeline()
+        self._mark_as_filled_in_pipeline()
         
         self.logger.debug(f"Filled FutureTransformation '{self.future_name}' with {actual_transformation.basename}")
     
-    # def _mark_as_filled_in_pipeline(self) -> None:
-    #     """
-    #     将已填充的future transformation从pipeline中移除，并保存到filled_futures中
-    #     这样compiler就看不到future transformations，只看到实际的反馈边连接
-    #     """
-    #     # 将该future transformation从pipeline中移除
-    #     if self in self.env._pipeline:
-    #         self.env._pipeline.remove(self)
-    #         self.logger.debug(f"Removed FutureTransformation '{self.future_name}' from pipeline")
+    def _mark_as_filled_in_pipeline(self) -> None:
+        """
+        将已填充的future transformation从pipeline中移除，并保存到filled_futures中
+        这样compiler就看不到future transformations，只看到实际的反馈边连接
+        """
+        # 将该future transformation从pipeline中移除
+        if self in self.env._pipeline:
+            self.env._pipeline.remove(self)
+            self.logger.debug(f"Removed FutureTransformation '{self.future_name}' from pipeline")
         
-    #     # 保存填充信息到环境中，供调试和管理使用
-    #     if not hasattr(self.env, '_filled_futures'):
-    #         self.env._filled_futures = {}
+        # 保存填充信息到环境中，供调试和管理使用
+        if not hasattr(self.env, '_filled_futures'):
+            self.env._filled_futures = {}
         
-    #     self.env._filled_futures[self.future_name] = {
-    #         'future_transformation': self,
-    #         'actual_transformation': self.actual_transformation,
-    #         'filled_at': self._get_current_timestamp()
-    #     }
+        self.env._filled_futures[self.future_name] = {
+            'future_transformation': self,
+            'actual_transformation': self.actual_transformation,
+            'filled_at': self._get_current_timestamp()
+        }
         
-    #     self.logger.info(f"Future transformation '{self.future_name}' filled and removed from pipeline")
+        self.logger.info(f"Future transformation '{self.future_name}' filled and removed from pipeline")
     
     def _get_current_timestamp(self) -> str:
         """获取当前时间戳"""
@@ -113,7 +113,7 @@ class FutureTransformation(BaseTransformation):
         """
         在pipeline中查找指定名称的transformation
         """
-        for trans in self.env.pipeline:
+        for trans in self.env._pipeline:
             if trans.basename == name:
                 return trans
         return None
diff --git a/sage_core/transformation/join_transformation.py b/sage_core/transformation/join_transformation.py
index cf62d67..3d1b348 100644
--- a/sage_core/transformation/join_transformation.py
+++ b/sage_core/transformation/join_transformation.py
@@ -3,7 +3,7 @@ from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Opti
 from sage_core.transformation.base_transformation import BaseTransformation
 if TYPE_CHECKING:
     from sage_core.function.join_function import BaseJoinFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class JoinTransformation(BaseTransformation):
diff --git a/sage_core/transformation/keyby_transformation.py b/sage_core/transformation/keyby_transformation.py
index 72fe096..d7cba9a 100644
--- a/sage_core/transformation/keyby_transformation.py
+++ b/sage_core/transformation/keyby_transformation.py
@@ -1,6 +1,6 @@
 from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.keyby_operator import KeyByOperator
-from sage_jobmanager.factory.operator_factory import OperatorFactory
+from sage_runtime.operator.factory import OperatorFactory
 
 class KeyByTransformation(BaseTransformation):
     """
@@ -30,7 +30,7 @@ class KeyByTransformation(BaseTransformation):
             operator_class=self.operator_class,
             function_factory=self.function_factory,
             basename=self.basename,
-            env_name=self.env_name,
+            env_name=self.env.name,
             remote=self.remote,
             partition_strategy=self.partition_strategy  # KeyBy特有参数
         )
diff --git a/sage_core/transformation/map_transformation.py b/sage_core/transformation/map_transformation.py
index cc34f5c..9d979d2 100644
--- a/sage_core/transformation/map_transformation.py
+++ b/sage_core/transformation/map_transformation.py
@@ -4,7 +4,7 @@ from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.map_operator import MapOperator
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 class MapTransformation(BaseTransformation):
     """映射变换 - 一对一数据变换"""
diff --git a/sage_core/transformation/sink_transformation.py b/sage_core/transformation/sink_transformation.py
index 765687e..a40f02c 100644
--- a/sage_core/transformation/sink_transformation.py
+++ b/sage_core/transformation/sink_transformation.py
@@ -4,7 +4,7 @@ from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.sink_operator import SinkOperator
 if TYPE_CHECKING:
     from sage_core.function.sink_function import SinkFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class SinkTransformation(BaseTransformation):
@@ -23,7 +23,5 @@ class SinkTransformation(BaseTransformation):
         super().__init__(env, function, *args, **kwargs)
 
 
-    @property
-    def is_sink(self) -> bool:
-        return True
+
 
diff --git a/sage_core/transformation/source_transformation.py b/sage_core/transformation/source_transformation.py
index 8fccc84..1405e4f 100644
--- a/sage_core/transformation/source_transformation.py
+++ b/sage_core/transformation/source_transformation.py
@@ -4,7 +4,7 @@ from sage_core.transformation.base_transformation import BaseTransformation
 from sage_core.operator.source_operator import SourceOperator
 if TYPE_CHECKING:
     from sage_core.function.base_function import BaseFunction
-    from sage_core.environment.base_environment import BaseEnvironment
+    from sage_core.api.env import BaseEnvironment
 
 
 class SourceTransformation(BaseTransformation):
diff --git a/sage_examples/batch_example.py b/sage_examples/batch_example.py
deleted file mode 100644
index 712a2d2..0000000
--- a/sage_examples/batch_example.py
+++ /dev/null
@@ -1,242 +0,0 @@
-from sage_core.api.local_environment import LocalEnvironment
-from sage_core.api.remote_environment import RemoteEnvironment
-from sage_core.function.sink_function import SinkFunction
-from sage_core.function.source_function import SourceFunction, StopSignal
-import time
-import random
-
-class NumberSequenceSource(SourceFunction):
-    """
-    数字序列源 - 生成有限数量的数字，然后发送停止信号
-    """
-    def __init__(self, max_count=10, **kwargs):
-        super().__init__(**kwargs)
-        self.counter = 0
-        self.max_count = max_count
-        
-    def execute(self):
-        if self.counter >= self.max_count:
-            # 数据耗尽，发送停止信号
-            return StopSignal(f"NumberSequence_{self.counter}")
-        
-        self.counter += 1
-        number = self.counter * 10 + random.randint(1, 9)
-        print(f"[Source] Generating number {self.counter}/{self.max_count}: {number}")
-        return number
-
-class FileLineSource(SourceFunction):
-    """
-    文件行源 - 逐行读取文件，读完后发送停止信号
-    """
-    def __init__(self, lines_data=None, **kwargs):
-        super().__init__(**kwargs)
-        # 模拟文件内容
-        self.lines = lines_data or [
-            "Hello, SAGE batch processing!",
-            "Processing line by line...",
-            "Each line is processed independently.",
-            "This is a test of batch termination.",
-            "End of file reached."
-        ]
-        self.current_index = 0
-        
-    def execute(self):
-        if self.current_index >= len(self.lines):
-            # 文件读完，发送停止信号
-            return StopSignal(f"FileReader_EOF")
-        
-        line = self.lines[self.current_index]
-        self.current_index += 1
-        print(f"[FileSource] Reading line {self.current_index}/{len(self.lines)}: {line}")
-        return line
-
-class CountdownSource(SourceFunction):
-    """
-    倒计时源 - 从指定数字倒数到0，然后发送停止信号
-    """
-    def __init__(self, start_from=5, **kwargs):
-        super().__init__(**kwargs)
-        self.current_number = start_from
-        
-    def execute(self):
-        if self.current_number < 0:
-            # 倒计时结束，发送停止信号
-            return StopSignal(f"Countdown_Finished")
-        
-        result = self.current_number
-        print(f"[Countdown] T-minus {self.current_number}")
-        self.current_number -= 1
-        return result
-
-class BatchProcessor(SinkFunction):
-    """
-    批处理数据接收器
-    """
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-        self.processed_count = 0
-        
-    def execute(self, data):
-        self.processed_count += 1
-        print(f"[Processor-{self.name}] Processed item #{self.processed_count}: {data}")
-        return data
-
-def run_simple_batch_test():
-    """测试1: 简单的数字序列批处理"""
-    print("🔢 Test 1: Simple Number Sequence Batch Processing")
-    print("=" * 50)
-    
-    env = LocalEnvironment("simple_batch_test")
-    
-    # 创建有限数据源
-    source_stream = env.from_source(NumberSequenceSource, max_count=5, delay=0.5)
-    
-    # 处理管道
-    result = (source_stream
-        .map(lambda x: x * 2)  # 数字翻倍
-        .filter(lambda x: x > 50)  # 过滤大于50的数字
-        .sink(BatchProcessor, name="NumberProcessor")
-    )
-    
-    print("🚀 Starting simple batch processing...")
-    print("📊 Processing sequence: generate → double → filter → sink")
-    print("⏹️  Source will automatically stop after 5 numbers\n")
-    
-    # 提交并运行
-    env.submit()
-    
-    print("\n✅ Simple batch test completed!\n")
-
-def run_file_processing_test():
-    """测试2: 文件行批处理"""
-    print("📄 Test 2: File Line Batch Processing") 
-    print("=" * 50)
-    
-    env = LocalEnvironment("file_batch_test")
-    
-    # 模拟文件数据
-    file_data = [
-        "SAGE Framework",
-        "Distributed Stream Processing", 
-        "Batch Processing Support",
-        "Ray-based Architecture",
-        "Python Implementation"
-    ]
-    
-    source_stream = env.from_source(FileLineSource, lines_data=file_data, delay=0.8)
-    
-    # 文本处理管道
-    result = (source_stream
-        .map(lambda line: line.upper())  # 转大写
-        .map(lambda line: f"📝 {line}")   # 添加前缀
-        .sink(BatchProcessor, name="TextProcessor")
-    )
-    
-    print("🚀 Starting file batch processing...")
-    print("📊 Processing pipeline: read → uppercase → prefix → sink")  
-    print("⏹️  Source will automatically stop after reading all lines\n")
-    
-    # 提交并运行
-    env.submit()
-    
-    print("\n✅ File batch test completed!\n")
-
-def run_multi_source_batch_test():
-    """测试3: 多源批处理（展示不同源的终止时机）"""
-    print("🔀 Test 3: Multi-Source Batch Processing")
-    print("=" * 50)
-    
-    env = LocalEnvironment("multi_source_batch_test")
-    
-    # 创建多个不同速度的数据源
-    numbers_stream = env.from_source(NumberSequenceSource, max_count=3, delay=0.5)
-    countdown_stream = env.from_source(CountdownSource, start_from=2, delay=0.7)
-    
-    # 合并流处理
-    combined_result = (numbers_stream
-        .connect(countdown_stream)  # 合并两个流
-        .map(lambda x: f"Combined: {x}")
-        .sink(BatchProcessor, name="MultiSourceProcessor")
-    )
-    
-    print("🚀 Starting multi-source batch processing...")
-    print("📊 Two independent sources will terminate at different times")
-    print("⏹️  Job will complete when ALL sources send stop signals\n")
-    
-    # 提交并运行
-    env.submit()
-    
-    print("\n✅ Multi-source batch test completed!\n")
-
-def run_processing_chain_test():
-    """测试4: 复杂处理链批处理"""
-    print("⛓️  Test 4: Complex Processing Chain Batch")
-    print("=" * 50)
-    
-    env = RemoteEnvironment("complex_batch_test")  # 使用远程环境测试分布式批处理
-    
-    source_stream = env.from_source(NumberSequenceSource, max_count=8, delay=0.3)
-    
-    # 复杂的处理链
-    result = (source_stream
-        .map(lambda x: x + 100)           # +100
-        .filter(lambda x: x % 2 == 0)     # 只保留偶数
-        .map(lambda x: x / 2)             # 除以2
-        .map(lambda x: f"Result: {int(x)}")  # 格式化
-        .sink(BatchProcessor, name="ChainProcessor")
-    )
-    
-    print("🚀 Starting complex processing chain...")
-    print("📊 Chain: source → +100 → filter_even → /2 → format → sink")
-    print("🌐 Running on distributed Ray cluster")
-    print("⏹️  Automatic termination with batch lifecycle management\n")
-    
-    # 提交并运行
-    env.submit()
-    
-    print("\n✅ Complex batch test completed!\n")
-
-def main():
-    """主测试函数"""
-    print("🎯 SAGE Batch Processing Tests with StopSignal")
-    print("=" * 60)
-    print("🧪 Testing automatic batch termination using StopSignal interface")
-    print("📈 Each test demonstrates different batch processing scenarios\n")
-    
-    try:
-        # 运行所有测试
-        run_simple_batch_test()
-        time.sleep(2)
-        
-        run_file_processing_test() 
-        time.sleep(2)
-        
-        run_multi_source_batch_test()
-        time.sleep(2)
-        
-        run_processing_chain_test()
-        
-    except KeyboardInterrupt:
-        print("\n\n🛑 Tests interrupted by user")
-        
-    finally:
-        print("\n📋 Batch Processing Tests Summary:")
-        print("✅ Test 1: Simple sequence - PASSED")
-        print("✅ Test 2: File processing - PASSED") 
-        print("✅ Test 3: Multi-source - PASSED")
-        print("✅ Test 4: Complex chain - PASSED")
-        print("\n💡 Key Features Demonstrated:")
-        print("   - StopSignal automatic termination")
-        print("   - Source-driven batch lifecycle")
-        print("   - Multi-source coordination")
-        print("   - Distributed batch processing")
-        print("   - Graceful job completion")
-        print("\n🔄 StopSignal Workflow:")
-        print("   1. Source detects data exhaustion")
-        print("   2. Source returns StopSignal")
-        print("   3. SourceOperator propagates signal")
-        print("   4. Downstream nodes receive termination")
-        print("   5. Job gracefully completes")
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/sage_examples/comap_function_example.py b/sage_examples/comap_function_example.py
index 8b30897..618554d 100644
--- a/sage_examples/comap_function_example.py
+++ b/sage_examples/comap_function_example.py
@@ -1,5 +1,4 @@
-from sage_core.api.local_environment import LocalEnvironment
-from sage_core.api.remote_environment import RemoteEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.sink_function import SinkFunction
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.comap_function import BaseCoMapFunction
@@ -102,6 +101,8 @@ class TypeSpecificProcessor(BaseCoMapFunction):
 
 # 汇总输出函数
 class SensorSink(SinkFunction):
+    def __init__(self, **kwargs):
+        self.name = kwargs.get('name', 'SensorSink')
         
     def execute(self, data):
         if isinstance(data, dict) and 'alert' in data:
@@ -113,7 +114,7 @@ class SensorSink(SinkFunction):
 
 def main():
     # 创建环境
-    env = RemoteEnvironment("comap_function_example")
+    env = LocalEnvironment("comap_function_example")
     
     print("🚀 Starting CoMap Function Example")
     print("🌡️  Demonstrating multi-sensor data processing with CoMap")
@@ -145,7 +146,6 @@ def main():
     simple_result = (connected_sensors
         .comap(TypeSpecificProcessor)
         .print("🎯 Formatted Output")
-        
     )
     
     print("\n📈 All sensors connected and processing with CoMap...\n")
@@ -159,8 +159,8 @@ def main():
     try:
         # 运行流处理
         env.submit()
-        
-        # time.sleep(40)  # 运行15秒以观察不同频率的数据
+        env.run_streaming()
+        time.sleep(15)  # 运行15秒以观察不同频率的数据
         
     except KeyboardInterrupt:
         print("\n\n🛑 Stopping CoMap Function Example...")
@@ -176,6 +176,7 @@ def main():
         print("\n🔄 Comparison with regular map():")
         print("   - Regular map(): All inputs merged → single execute() method")
         print("   - CoMap: Each input stream → dedicated mapN() method")
+        env.close()
 
 if __name__ == "__main__":
     main()
diff --git a/sage_examples/comap_lambda_example.py b/sage_examples/comap_lambda_example.py
index b9a45ba..2893f8e 100644
--- a/sage_examples/comap_lambda_example.py
+++ b/sage_examples/comap_lambda_example.py
@@ -14,7 +14,7 @@ from typing import List, Any
 # Add the project root to Python path for imports
 sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 
 
@@ -41,7 +41,7 @@ def main():
     print("=" * 60)
     
     # Create environment
-    env1 = LocalStreamEnvironment()
+    env1 = LocalEnvironment()
     
     # Example 1: Lambda List Approach
     print("\n📋 Example 1: Lambda List Approach")
@@ -75,7 +75,7 @@ def main():
     print("-" * 40)
     
     # Reset environment for new example
-    env2 = LocalStreamEnvironment()
+    env2 = LocalEnvironment()
     
     # Define named functions
     def format_temperature(data: float) -> str:
@@ -116,7 +116,7 @@ def main():
     print("-" * 40)
     
     # Reset environment for new example
-    env3 = LocalStreamEnvironment()
+    env3 = LocalEnvironment()
     
     # Define a complex processing function
     def process_numeric_data(data: float) -> str:
@@ -161,7 +161,7 @@ def main():
     print("-" * 40)
     
     # Reset environment for new example
-    env4 = LocalStreamEnvironment()
+    env4 = LocalEnvironment()
     
     # Create numeric data sources
     input1_source = env4.from_source(ListSource, [1, 2, 3, 4, 5])
@@ -196,7 +196,7 @@ def main():
     print("-" * 40)
     
     # Reset environment for new example
-    env5 = LocalStreamEnvironment()
+    env5 = LocalEnvironment()
     
     # Create data with potential issues
     mixed_data1 = env5.from_source(ListSource, [5, -3, 0, 12, -1])
diff --git a/sage_examples/connected_stream_example.py b/sage_examples/connected_stream_example.py
index 2d29b1a..f62dec8 100644
--- a/sage_examples/connected_stream_example.py
+++ b/sage_examples/connected_stream_example.py
@@ -1,8 +1,8 @@
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.sink_function import SinkFunction
 from sage_core.function.source_function import SourceFunction
 import time
-
+import random
 
 # 简单的数字源
 class NumberSource(SourceFunction):
@@ -28,7 +28,7 @@ class StatsSink(SinkFunction):
 
 def main():
     # 创建环境
-    env = LocalStreamEnvironment("simple_connected_example")
+    env = LocalEnvironment("simple_connected_example")
     
     print("🚀 Starting Simple Connected Streams Example")
     print("📊 Demonstrating multiple stream processing and connection")
@@ -85,7 +85,7 @@ def main():
     try:
         # 运行流处理
         env.submit()
-        
+        env.run_streaming()
         time.sleep(5)  # 运行5秒
         
     except KeyboardInterrupt:
diff --git a/sage_examples/external_memory_ingestion_pipeline.py b/sage_examples/external_memory_ingestion_pipeline.py
index bfc37f9..11157aa 100644
--- a/sage_examples/external_memory_ingestion_pipeline.py
+++ b/sage_examples/external_memory_ingestion_pipeline.py
@@ -1,16 +1,15 @@
 import logging
 import time
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.sink import MemWriteSink
 from sage_common_funs.io.source import FileSource
-
-from sage_libs.rag.chunk import CharacterSplitter
-from sage_libs.rag.writer import MemoryWriter
+from sage_libs.rag import CharacterSplitter
+from sage_libs.rag import MemoryWriter
 from sage_utils.config_loader import load_config
 
 
 def pipeline_run():
-    env = LocalStreamEnvironment(name="example_pipeline")
+    env = LocalEnvironment(name="example_pipeline")
     env.set_memory(config=None)  # 初始化内存配置
 
     # 构建数据处理流程
@@ -19,7 +18,7 @@ def pipeline_run():
     memwrite_stream= chunk_stream.map(MemoryWriter,config["writer"])
     sink_stream= memwrite_stream.sink(MemWriteSink,config["sink"])
     env.submit()
-      # 启动管道
+    env.run_streaming()  # 启动管道
     time.sleep(100)  # 等待管道运行
 
 if __name__ == '__main__':
diff --git a/sage_examples/future_stream_example.py b/sage_examples/future_stream_example.py
index 4c2069c..407808a 100644
--- a/sage_examples/future_stream_example.py
+++ b/sage_examples/future_stream_example.py
@@ -1,4 +1,4 @@
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.sink_function import SinkFunction
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.comap_function import BaseCoMapFunction
@@ -93,7 +93,7 @@ class CounterSink(SinkFunction):
 
 def main():
     # 创建环境
-    env = LocalStreamEnvironment("future_stream_example")
+    env = LocalEnvironment("future_stream_example")
     
     print("🚀 Starting Future Stream Example")
     print("🔄 Demonstrating feedback edges with a counting loop")
@@ -145,16 +145,21 @@ def main():
     print()
     
     print("✅ Pipeline validation:")
-    print(f"   - Pipeline transformations: {len(env.pipeline)}")
+    print(f"   - Pipeline transformations: {len(env._pipeline)}")
+    print(f"   - Filled future streams: {list(env.get_filled_futures().keys())}")
+    print(f"   - Has unfilled futures: {env.has_unfilled_futures()}")
     
     try:
+        # 验证pipeline可编译性
+        env.validate_pipeline_for_compilation()
+        print("✅ Pipeline validation passed - ready to run!\n")
         
         print("🎬 Starting feedback loop execution...")
         print("📈 Watch the counter increment in a feedback loop:\n")
         
         # 运行流处理
         env.submit()
-        
+        env.run_streaming()
         time.sleep(15)  # 运行15秒，足够计数到10
         
     except KeyboardInterrupt:
diff --git a/sage_examples/kafka_query.py b/sage_examples/kafka_query.py
index ce0a343..4915348 100644
--- a/sage_examples/kafka_query.py
+++ b/sage_examples/kafka_query.py
@@ -2,13 +2,15 @@ import logging
 import json
 import threading
 import time
+from kafka import KafkaProducer
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
 from sage_utils.config_loader import load_config
+from sage_utils.custom_logger import CustomLogger
 from sage_utils.logging_utils import configure_logging
 
 
@@ -111,7 +113,7 @@ def extract_query_from_kafka(kafka_data):
 
 def pipeline_run():
     """创建并运行基于Kafka的数据处理管道"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)  # 初始化内存配置
     
     # 创建Kafka数据源
@@ -134,12 +136,23 @@ def pipeline_run():
     response_stream.sink(TerminalSink, config["sink"])
 
     # 提交管道并运行
-    env.submit()
-
+    env.submit(name="kafka_rag_pipeline")
+    
+    # 在后台线程启动流处理
+    def run_pipeline():
+        try:
+            env.run_streaming()
+        except Exception as e:
+            logging.error(f"Pipeline error: {e}")
+    
+    pipeline_thread = threading.Thread(target=run_pipeline, daemon=True)
+    pipeline_thread.start()
     
     # 等待pipeline启动
     time.sleep(2)
     logging.info("Kafka RAG pipeline started successfully")
+    
+    return pipeline_thread
 
 
 def interactive_mode():
@@ -211,6 +224,7 @@ def main():
 
 if __name__ == '__main__':
     # 配置日志
+    CustomLogger.disable_global_console_debug()
     configure_logging(level=logging.INFO)
     
     # 加载配置
diff --git a/sage_examples/multiagent_app.py b/sage_examples/multiagent_app.py
index 83bc86d..6291986 100644
--- a/sage_examples/multiagent_app.py
+++ b/sage_examples/multiagent_app.py
@@ -1,5 +1,6 @@
-import time
-from sage_core.api.local_environment import LocalStreamEnvironment
+from dotenv import load_dotenv
+import os, time
+from sage_core.api.env import LocalEnvironment, RemoteEnvironment
 from sage_utils.config_loader import load_config
 from sage_libs.agents.question_bot import QuestionBot
 from sage_libs.agents.chief_bot import ChiefBot
@@ -10,9 +11,11 @@ from sage_libs.agents.answer_bot import AnswerBot
 from sage_libs.agents.critic_bot import CriticBot
 from sage_libs.utils.tool_filter import ToolFilter
 
+
+
 def pipeline_run():
     """创建并运行数据处理管道"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     chief_stream = (
@@ -36,12 +39,25 @@ def pipeline_run():
 
 
     env.submit()
+    env.run_once()
     try:
         time.sleep(60)
     except KeyboardInterrupt:
         print("\n🛑 收到中断信号，正在关闭...")
     env.stop()
     env.close()
+    # try:
+    #     env.submit()
+    #     env.run_streaming() # 开销有点大，最好只润一次做测试
+    #     print("🌱 管道已启动，按 Ctrl+C 中断")
+    #     while True:
+    #         time.sleep(1)  # 持续运行直到被打断
+    # except KeyboardInterrupt:
+    #     print("\n🛑 收到中断信号，正在关闭...")
+    #     env.stop()
+    # finally:
+    #     env.close()
+    #     print("✅ 管道已安全关闭")
 
 if __name__ == '__main__':
     config = load_config("multiagent_config.yaml")
diff --git a/sage_examples/multiple_pipeline.py b/sage_examples/multiple_pipeline.py
index e032771..5f0330a 100644
--- a/sage_examples/multiple_pipeline.py
+++ b/sage_examples/multiple_pipeline.py
@@ -1,30 +1,29 @@
 import logging
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.sink import MemWriteSink, FileSink
 from sage_common_funs.io.source import FileSource
-
-from sage_libs.rag.chunk import CharacterSplitter
+from sage_libs.rag import CharacterSplitter
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
-from sage_libs.rag.writer import MemoryWriter
+from sage_libs.rag import MemoryWriter
 
 from sage_utils.config_loader import load_config
 
 def ingest_pipeline_run():
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     # 构建数据处理流程
     source_stream = env.from_source(FileSource, config_for_ingest["source"])
     chunk_stream = source_stream.map(CharacterSplitter,config_for_ingest["chunk"])
     memwrite_stream= chunk_stream.map(MemoryWriter,config_for_ingest["writer"])
     sink_stream= memwrite_stream.sink(MemWriteSink,config_for_ingest["sink"])
     env.submit()
-      # 启动管道
+    env.run_streaming()  # 启动管道
 
 def qa_pipeline_run():
     """创建并运行数据处理管道"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory()
     # 构建数据处理流程
     query_stream = env.from_source(FileSource, config_for_qa["source"])
@@ -34,7 +33,7 @@ def qa_pipeline_run():
     response_stream.sink(FileSink, config_for_qa["sink"])
     # 提交管道并运行
     env.submit()
-      # 启动管道
+    env.run_streaming()  # 启动管道
 
 if __name__ == '__main__':
     # 加载配置并初始化日志
diff --git a/sage_examples/neuromem_examples/experiment/memrun/locomo_memprompt_run.py b/sage_examples/neuromem_examples/experiment/memrun/locomo_memprompt_run.py
index 2344aa9..570ba1b 100644
--- a/sage_examples/neuromem_examples/experiment/memrun/locomo_memprompt_run.py
+++ b/sage_examples/neuromem_examples/experiment/memrun/locomo_memprompt_run.py
@@ -6,6 +6,9 @@ import os
 os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
 
 from tqdm import tqdm
+from sage_memory.api import get_memory, get_manager
+from sage_utils.config_loader import load_config
+from sage_utils.embedding_model import apply_embedding_model
 from data.neuromem_datasets.locomo_dataloader import LocomoDataLoader
 
 # manager = get_manager()
diff --git a/sage_examples/neuromem_examples/experiment/prefill/locomo_memprompt_prefill.py b/sage_examples/neuromem_examples/experiment/prefill/locomo_memprompt_prefill.py
index 9bbb9d3..5af70ab 100644
--- a/sage_examples/neuromem_examples/experiment/prefill/locomo_memprompt_prefill.py
+++ b/sage_examples/neuromem_examples/experiment/prefill/locomo_memprompt_prefill.py
@@ -83,7 +83,7 @@ manager.store_collection()
 #     response_stream.sink(TerminalSink, config["sink"])
 #     # 提交管道并运行
 #     env.submit()
-#       # 启动管道
+#     env.run_streaming()  # 启动管道
 
 #     # time.sleep(100)  # 等待管道运行
 
diff --git a/sage_examples/persistent_jobmanager_example.py b/sage_examples/persistent_jobmanager_example.py
deleted file mode 100644
index 085f7da..0000000
--- a/sage_examples/persistent_jobmanager_example.py
+++ /dev/null
@@ -1,297 +0,0 @@
-"""
-JobManager持久化Actor实现示例
-展示如何在SAGE系统中创建持久化的JobManager Actor
-"""
-import ray
-import time
-import threading
-from typing import Dict, Any, Optional
-from sage_jobmanager.job_manager import JobManager
-from draft.persistent_actor_manager import create_persistent_actor
-from sage_utils.custom_logger import CustomLogger
-
-
-@ray.remote
-class PersistentJobManagerActor(JobManager):
-    """持久化JobManager Actor - 绕过_ray_weak_ref限制"""
-    
-    def __init__(self, *args, **kwargs):
-        # 禁用TCP服务器，使用Ray通信
-        kwargs['enable_tcp_server'] = False
-        super().__init__(*args, **kwargs)
-        
-        # Ray Actor特有的设置
-        self.actor_id = ray.get_runtime_context().get_actor_id()
-        self.start_time = time.time()
-        self.heartbeat_count = 0
-        
-        self.logger.info(f"PersistentJobManagerActor initialized: {self.actor_id}")
-
-    def keep_alive(self) -> Dict[str, Any]:
-        """心跳保活方法 - 防止Actor被回收"""
-        self.heartbeat_count += 1
-        current_time = time.time()
-        uptime = current_time - self.start_time
-        
-        return {
-            "status": "alive",
-            "actor_id": self.actor_id,
-            "timestamp": current_time,
-            "uptime_seconds": uptime,
-            "heartbeat_count": self.heartbeat_count
-        }
-
-    def get_actor_info(self) -> Dict[str, Any]:
-        """获取Actor信息"""
-        return {
-            "actor_id": self.actor_id,
-            "start_time": self.start_time,
-            "uptime": time.time() - self.start_time,
-            "heartbeat_count": self.heartbeat_count,
-            "weak_ref_info": "Using persistent actor strategies to bypass _ray_weak_ref"
-        }
-
-    def health_check(self) -> Dict[str, Any]:
-        """健康检查"""
-        try:
-            # 执行一些基本的健康检查
-            status = {
-                "status": "healthy",
-                "actor_id": self.actor_id,
-                "uptime": time.time() - self.start_time,
-                "active_environments": len(getattr(self, 'environments', {})),
-                "memory_usage": "N/A"  # 可以添加内存使用情况
-            }
-            return status
-        except Exception as e:
-            return {
-                "status": "unhealthy",
-                "error": str(e),
-                "actor_id": self.actor_id
-            }
-
-    def graceful_shutdown(self) -> Dict[str, Any]:
-        """优雅关闭Actor"""
-        self.logger.info(f"PersistentJobManagerActor {self.actor_id} shutting down...")
-        try:
-            # 清理资源
-            if hasattr(self, 'environments'):
-                for env_uuid in list(self.jobs.keys()):
-                    try:
-                        self.pause_job(env_uuid)
-                    except Exception as e:
-                        self.logger.error(f"Error stopping environment {env_uuid}: {e}")
-            
-            return {
-                "status": "shutdown_complete",
-                "actor_id": self.actor_id,
-                "final_heartbeat_count": self.heartbeat_count
-            }
-        except Exception as e:
-            return {
-                "status": "shutdown_error",
-                "error": str(e),
-                "actor_id": self.actor_id
-            }
-
-
-class JobManagerActorService:
-    """JobManager Actor服务管理器"""
-    
-    def __init__(self, actor_name: str = "sage_jobmanager", namespace: str = "sage"):
-        self.actor_name = actor_name
-        self.namespace = namespace
-        self.logger = CustomLogger()
-        self._actor_handle: Optional[ray.actor.ActorHandle] = None
-        self._persistence_method = "detached"  # 默认使用detached方法
-
-    def start_persistent_jobmanager(
-        self,
-        method: str = "detached",
-        **actor_options
-    ) -> ray.actor.ActorHandle:
-        """
-        启动持久化JobManager Actor
-        
-        Args:
-            method: 持久化方法 ("detached", "named", "resource_locked")
-            **actor_options: 其他Actor选项
-        
-        Returns:
-            Ray ActorHandle
-        """
-        try:
-            self.logger.info(f"Starting persistent JobManager using method: {method}")
-            
-            # 根据不同方法设置不同的选项
-            if method == "detached":
-                # 方案1：使用detached生命周期（最推荐）
-                actor_handle = create_persistent_actor(
-                    PersistentJobManagerActor,
-                    name=self.actor_name,
-                    method="detached",
-                    namespace=self.namespace,
-                    max_restarts=3,
-                    resources={"jobmanager": 1.0},  # 资源占用防止回收
-                    **actor_options
-                )
-                
-            elif method == "named":
-                # 方案2：命名Actor + 心跳保活
-                actor_handle = create_persistent_actor(
-                    PersistentJobManagerActor,
-                    name=self.actor_name,
-                    method="named",
-                    namespace=self.namespace,
-                    keepalive_interval=30,  # 30秒心跳
-                    **actor_options
-                )
-                
-            elif method == "resource_locked":
-                # 方案3：资源锁定
-                actor_handle = create_persistent_actor(
-                    PersistentJobManagerActor,
-                    name=self.actor_name,
-                    method="resource_locked",
-                    namespace=self.namespace,
-                    resource_name="jobmanager_lock",
-                    resource_amount=1.0,
-                    **actor_options
-                )
-            else:
-                raise ValueError(f"Unknown persistence method: {method}")
-
-            self._actor_handle = actor_handle
-            self._persistence_method = method
-            
-            # 验证Actor是否正常启动
-            actor_info = ray.get(actor_handle.get_actor_info.remote(), timeout=10)
-            self.logger.info(f"JobManager Actor started successfully: {actor_info}")
-            
-            return actor_handle
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start persistent JobManager: {e}")
-            raise
-
-    def get_or_create_jobmanager(self, **options) -> ray.actor.ActorHandle:
-        """获取现有JobManager或创建新的"""
-        try:
-            # 尝试获取现有Actor
-            existing_actor = ray.get_actor(self.actor_name, namespace=self.namespace)
-            
-            # 检查Actor是否健康
-            health_status = ray.get(existing_actor.health_check.remote(), timeout=5)
-            if health_status.get("status") == "healthy":
-                self.logger.info("Found existing healthy JobManager Actor")
-                self._actor_handle = existing_actor
-                return existing_actor
-            else:
-                self.logger.warning(f"Existing Actor unhealthy: {health_status}")
-                
-        except (ValueError, Exception) as e:
-            self.logger.info(f"No existing JobManager Actor found: {e}")
-        
-        # 创建新的持久化Actor
-        return self.start_persistent_jobmanager(**options)
-
-    def stop_jobmanager(self, graceful: bool = True) -> bool:
-        """停止JobManager Actor"""
-        if not self._actor_handle:
-            self.logger.warning("No JobManager Actor to stop")
-            return True
-            
-        try:
-            if graceful:
-                # 优雅关闭
-                shutdown_info = ray.get(
-                    self._actor_handle.graceful_shutdown.remote(), 
-                    timeout=30
-                )
-                self.logger.info(f"JobManager graceful shutdown: {shutdown_info}")
-            
-            # 杀死Actor
-            ray.kill(self._actor_handle)
-            self._actor_handle = None
-            
-            self.logger.info("JobManager Actor stopped successfully")
-            return True
-            
-        except Exception as e:
-            self.logger.error(f"Error stopping JobManager Actor: {e}")
-            return False
-
-    def get_actor_handle(self) -> Optional[ray.actor.ActorHandle]:
-        """获取当前Actor句柄"""
-        return self._actor_handle
-
-    def monitor_actor_health(self, interval: int = 60) -> threading.Thread:
-        """启动Actor健康监控线程"""
-        def monitor_worker():
-            while self._actor_handle:
-                try:
-                    health_status = ray.get(
-                        self._actor_handle.health_check.remote(), 
-                        timeout=10
-                    )
-                    
-                    if health_status.get("status") != "healthy":
-                        self.logger.warning(f"JobManager Actor unhealthy: {health_status}")
-                        # 可以在这里实现重启逻辑
-                    else:
-                        self.logger.debug(f"JobManager Actor healthy: {health_status}")
-                    
-                    time.sleep(interval)
-                    
-                except Exception as e:
-                    self.logger.error(f"Health check failed: {e}")
-                    time.sleep(interval)
-        
-        thread = threading.Thread(
-            target=monitor_worker,
-            name="JobManagerHealthMonitor",
-            daemon=True
-        )
-        thread.start()
-        self.logger.info("Started JobManager health monitoring")
-        return thread
-
-
-# 使用示例
-def example_usage():
-    """展示如何使用持久化JobManager Actor"""
-    
-    # 确保Ray已初始化
-    if not ray.is_initialized():
-        ray.init(address="auto", ignore_reinit_error=True)
-    
-    # 创建JobManager服务
-    jm_service = JobManagerActorService()
-    
-    # 方法1：使用detached生命周期（推荐）
-    print("=== 方法1：Detached生命周期 ===")
-    jobmanager_actor = jm_service.start_persistent_jobmanager(method="detached")
-    
-    # 测试Actor功能
-    actor_info = ray.get(jobmanager_actor.get_actor_info.remote())
-    print(f"Actor Info: {actor_info}")
-    
-    # 心跳测试
-    heartbeat = ray.get(jobmanager_actor.keep_alive.remote())
-    print(f"Heartbeat: {heartbeat}")
-    
-    # 健康检查
-    health = ray.get(jobmanager_actor.health_check.remote())
-    print(f"Health: {health}")
-    
-    # 启动健康监控
-    monitor_thread = jm_service.monitor_actor_health(interval=30)
-    
-    print("JobManager Actor is now persistent and resistant to _ray_weak_ref issues")
-    print("The actor will survive even if the original handle goes out of scope")
-    
-    return jobmanager_actor
-
-
-if __name__ == "__main__":
-    example_usage()
diff --git a/sage_examples/qa_bm25_retrieval.py b/sage_examples/qa_bm25_retrieval.py
index 7f80180..abff4bb 100644
--- a/sage_examples/qa_bm25_retrieval.py
+++ b/sage_examples/qa_bm25_retrieval.py
@@ -1,7 +1,7 @@
 
 import logging
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.sink import TerminalSink
 from sage_common_funs.io.source import FileSource
 from sage_libs.rag.generator import OpenAIGenerator
@@ -13,7 +13,7 @@ from sage_utils.logging_utils import configure_logging
 
 def pipeline_run():
     """创建并运行数据处理管道"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
     # 构建数据处理流程
     query_stream = env.from_source(FileSource, config["source"])
@@ -23,7 +23,7 @@ def pipeline_run():
     response_stream.sink(TerminalSink, config["sink"])
     # 提交管道并运行
     env.submit()
-      # 启动管道
+    env.run_streaming()  # 启动管道
 
     # time.sleep(100)  # 等待管道运行
 
diff --git a/sage_examples/qa_bm25_retrieval_edit.py b/sage_examples/qa_bm25_retrieval_edit.py
deleted file mode 100644
index 7f80180..0000000
--- a/sage_examples/qa_bm25_retrieval_edit.py
+++ /dev/null
@@ -1,35 +0,0 @@
-
-import logging
-
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_common_funs.io.sink import TerminalSink
-from sage_common_funs.io.source import FileSource
-from sage_libs.rag.generator import OpenAIGenerator
-from sage_libs.rag.promptor import QAPromptor
-from sage_libs.rag.retriever import BM25sRetriever
-from sage_utils.config_loader import load_config
-from sage_utils.logging_utils import configure_logging
-
-
-def pipeline_run():
-    """创建并运行数据处理管道"""
-    env = LocalStreamEnvironment()
-    env.set_memory(config=None)
-    # 构建数据处理流程
-    query_stream = env.from_source(FileSource, config["source"])
-    query_and_chunks_stream = query_stream.map(BM25sRetriever, config["retriever"])
-    prompt_stream = query_and_chunks_stream.map(QAPromptor, config["promptor"])
-    response_stream = prompt_stream.map(OpenAIGenerator, config["generator"])
-    response_stream.sink(TerminalSink, config["sink"])
-    # 提交管道并运行
-    env.submit()
-      # 启动管道
-
-    # time.sleep(100)  # 等待管道运行
-
-if __name__ == '__main__':
-    # 加载配置并初始化日志
-    config = load_config('config_bm25s.yaml')
-    configure_logging(level=logging.INFO)
-    # 初始化内存并运行管道
-    pipeline_run()
\ No newline at end of file
diff --git a/sage_examples/qa_dense_retrieval.py b/sage_examples/qa_dense_retrieval.py
index 25a0d37..316d73e 100644
--- a/sage_examples/qa_dense_retrieval.py
+++ b/sage_examples/qa_dense_retrieval.py
@@ -1,6 +1,7 @@
+import time
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
-from sage_core.api.remote_environment import RemoteBatchEnvironment
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -9,34 +10,21 @@ from sage_utils.config_loader import load_config
 
 def pipeline_run():
     """创建并运行数据处理管道"""
-    # env = LocalBatchEnvironment() #DEBUG and Batch -- Client 拥有后续程序的全部handler（包括JM）
-    env = RemoteBatchEnvironment("JM-IP")  # Deployment to JM. -- Client 不拥有后续程序的全部handler（包括JM）
-
-    # Batch Environment.
-
-    query_stream = (env
-                    .process(FileSource, config["source"]) # 处理且处理一整个file 一次。
-                    .map(DenseRetriever, config["retriever"])
-                    .map(QAPromptor, config["promptor"])
-                    .map(OpenAIGenerator, config["generator"])
-                    .sink(TerminalSink, config["sink"]) # TM (JVM) --> 会打印在某一台机器的console里
-                    )
-
-    # Streaming Environment.
+    env = LocalEnvironment()
     env.set_memory(config=None)
-    # env = LocalStreamEnvironment() #DEBUG and Streaming
-    # env = RemoteStreamEnvironment("JM-IP")  # Deployment to JM.
-
     # 构建数据处理流程
     query_stream = (env
                     .from_source(FileSource, config["source"])
                     .map(DenseRetriever, config["retriever"])
                     .map(QAPromptor, config["promptor"])
                     .map(OpenAIGenerator, config["generator"])
-                    .sink(TerminalSink, config["sink"]) # TM (JVM) --> 会打印在某一台机器的console里
+                    .sink(TerminalSink, config["sink"])
                     )
     try:
         env.submit()
+        env.run_once()  # 启动管道
+        time.sleep(15)  # 等待管道运行
+        env.stop()
     finally:
         env.close()
 
diff --git a/sage_examples/qa_dense_retrieval_mixed.py b/sage_examples/qa_dense_retrieval_mixed.py
index f91d5c3..756c5e6 100644
--- a/sage_examples/qa_dense_retrieval_mixed.py
+++ b/sage_examples/qa_dense_retrieval_mixed.py
@@ -2,7 +2,7 @@ import logging
 import time
 from dotenv import load_dotenv
 import os
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
@@ -13,7 +13,7 @@ from sage_utils.logging_utils import configure_logging
 
 def pipeline_run():
     """创建并运行数据处理管道"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
     # 构建数据处理流程
     query_stream = env.from_source(FileSource, config["source"])
@@ -23,6 +23,8 @@ def pipeline_run():
     response_stream.sink(TerminalSink, config["sink"])
     # 提交管道并运行
     env.submit()
+    env.run_streaming()  # 启动管道
+
     time.sleep(100)  # 等待管道运行
 
 
diff --git a/sage_examples/qa_dense_retrieval_ray.py b/sage_examples/qa_dense_retrieval_ray.py
index 391a4a9..97c3e0f 100644
--- a/sage_examples/qa_dense_retrieval_ray.py
+++ b/sage_examples/qa_dense_retrieval_ray.py
@@ -2,9 +2,9 @@ import logging
 import time
 from dotenv import load_dotenv
 import os
-from sage_core.api.remote_environment import RemoteStreamEnvironment
+from sage_core.api.env import RemoteEnvironment
 from sage_common_funs.io.source import FileSource
-from sage_common_funs.io.sink import FileSink
+from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -13,32 +13,33 @@ from sage_utils.logging_utils import configure_logging
 
 def pipeline_run():
     """创建并运行数据处理管道"""
-    env = RemoteStreamEnvironment(name="example_pipeline")
+    env = RemoteEnvironment(name="example_pipeline")
     env.set_memory(config = {"collection_name": "example_collection"})
     # 构建数据处理流程
     query_stream = env.from_source(FileSource, config["source"])
     query_and_chunks_stream = query_stream.map(DenseRetriever, config["retriever"])
     prompt_stream = query_and_chunks_stream.map(QAPromptor, config["promptor"])
     response_stream = prompt_stream.map(OpenAIGenerator, config["generator"])
-    response_stream.sink(FileSink, config["sink"])
+    response_stream.sink(TerminalSink, config["sink"])
     # 提交管道并运行
     env.submit()
-      # 启动管道
+    env.run_streaming()  # 启动管道
     time.sleep(5)
-
-
-
-
-    env2 = RemoteStreamEnvironment(name="example_pipeline2")
+    env.stop()  # 停止管道
+    time.sleep(2)
+    env2 = RemoteEnvironment(name="example_pipeline2")
     env2.set_memory(config={"collection_name": "example_collection2"})
     # 构建数据处理流程
     query_stream2 = env2.from_source(FileSource, config["source"])
     query_and_chunks_stream2 = query_stream2.map(DenseRetriever, config["retriever"])
     prompt_stream2 = query_and_chunks_stream2.map(QAPromptor, config["promptor"])
     response_stream2 = prompt_stream2.map(OpenAIGenerator, config["generator"])
-    response_stream2.sink(FileSink, config["sink"])
+    response_stream2.sink(TerminalSink, config["sink"])
     # 提交管道并运行
     env2.submit()
+    env2.run_streaming()  # 启动管道
+    time.sleep(50)
+    env.stop()  # 停止管道
     time.sleep(1000)
 
 if __name__ == '__main__':
diff --git a/sage_examples/qa_evaluate.py b/sage_examples/qa_evaluate.py
index 075bcdf..c146bd1 100644
--- a/sage_examples/qa_evaluate.py
+++ b/sage_examples/qa_evaluate.py
@@ -1,5 +1,5 @@
 import time
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.map_function import MapFunction
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
@@ -36,7 +36,7 @@ class ResultFormatter(MapFunction):
         return (reference, generated)
 
 def pipeline_run(config):
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     (env
@@ -47,6 +47,7 @@ def pipeline_run(config):
      )
     try:
         env.submit()
+        env.run_streaming()
         time.sleep(5)
         env.stop()
     finally:
diff --git a/sage_examples/qa_hf.py b/sage_examples/qa_hf.py
index 83f2cf6..baab21c 100644
--- a/sage_examples/qa_hf.py
+++ b/sage_examples/qa_hf.py
@@ -1,11 +1,10 @@
 import logging
 import time
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
-
-from sage_libs.rag.generator import HFGenerator
+from sage_libs.rag import HFGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
 from sage_utils.config_loader import load_config
@@ -19,7 +18,7 @@ def pipeline_run(config: dict) -> None:
     Args:
         config (dict): 包含各个组件配置的字典。
     """
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     # 构建数据处理流程
@@ -33,7 +32,7 @@ def pipeline_run(config: dict) -> None:
 
     # 提交管道并运行一次
     env.submit()
-    
+    env.run_streaming()
     time.sleep(5)  # 等待管道运行
     env.close()
 
diff --git a/sage_examples/qa_multiplex.py b/sage_examples/qa_multiplex.py
index 88443e1..324e810 100644
--- a/sage_examples/qa_multiplex.py
+++ b/sage_examples/qa_multiplex.py
@@ -1,7 +1,7 @@
 import time
 from dotenv import load_dotenv
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink, FileSink
 from sage_libs.rag.generator import OpenAIGenerator
@@ -19,7 +19,7 @@ def pipeline_run(config):
         config (dict): The configuration parameters loaded from the config file.
     """
     try:
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         env.set_memory(config=None)  # Set environment memory if required.
 
         # Constructing the data processing pipeline
@@ -47,7 +47,7 @@ def pipeline_run(config):
 
         # Submit and run the pipeline
         env.submit()
-        
+        env.run_streaming()
 
         # Optional: Wait for 10 seconds before ending the pipeline (if necessary)
         time.sleep(10)
diff --git a/sage_examples/qa_openai.py b/sage_examples/qa_openai.py
index 1ce7de7..c47db74 100644
--- a/sage_examples/qa_openai.py
+++ b/sage_examples/qa_openai.py
@@ -1,9 +1,9 @@
 import time
 from dotenv import load_dotenv
 
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_libs.io.sink import TerminalSink
-from sage_libs.io.source import FileSource
+from sage_core.api.env import LocalEnvironment
+from sage_common_funs.io.source import FileSource
+from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -17,7 +17,7 @@ def pipeline_run(config: dict) -> None:
     Args:
         config (dict): 包含各模块配置的配置字典。
     """
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     # 构建数据处理流程
@@ -30,9 +30,8 @@ def pipeline_run(config: dict) -> None:
     )
 
     env.submit()
-
-    time.sleep(5)  # 等待管道运行5秒
-
+    env.run_once()
+    time.sleep(5)  # 等待管道运行
     env.close()
 
 
diff --git a/sage_examples/qa_openai_chat_history.py b/sage_examples/qa_openai_chat_history.py
index 3addcde..56697c7 100644
--- a/sage_examples/qa_openai_chat_history.py
+++ b/sage_examples/qa_openai_chat_history.py
@@ -1,11 +1,10 @@
 import time
 from dotenv import load_dotenv
 
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
-
-from sage_libs.rag.generator import OpenAIGeneratorWithHistory
+from sage_libs.rag import OpenAIGeneratorWithHistory
 from sage_libs.rag.promptor import QAPromptor
 from sage_utils.config_loader import load_config
 
@@ -17,7 +16,7 @@ def pipeline_run(config: dict) -> None:
     Args:
         config (dict): 包含各模块配置的配置字典。
     """
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
 
     # 构建数据处理流程
@@ -29,7 +28,7 @@ def pipeline_run(config: dict) -> None:
     )
 
     env.submit()
-    
+    env.run_streaming()
     time.sleep(5)  # 等待管道运行
     env.close()
 
diff --git a/sage_tests/example_tests/qa_recycle_test.py b/sage_examples/qa_recycle_test.py
similarity index 95%
rename from sage_tests/example_tests/qa_recycle_test.py
rename to sage_examples/qa_recycle_test.py
index ea01a6a..21f16db 100644
--- a/sage_tests/example_tests/qa_recycle_test.py
+++ b/sage_examples/qa_recycle_test.py
@@ -1,6 +1,6 @@
 from dotenv import load_dotenv
 import os, time
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_common_funs.io.source import FileSource
 from sage_common_funs.io.sink import TerminalSink
 from sage_libs.rag.generator import OpenAIGenerator
@@ -44,7 +44,7 @@ def run_gc_report(verbose: bool = True):
 
 def pipeline_run():
     """创建并运行数据处理管道"""
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
     # 构建数据处理流程
     query_stream = (env
@@ -56,7 +56,7 @@ def pipeline_run():
                     )
     try:
         env.submit()
-          # 启动管道
+        env.run_streaming()  # 启动管道
         time.sleep(30)  # 等待管道运行
         env.stop()
         time.sleep(60)
diff --git a/sage_examples/qa_refiner.py b/sage_examples/qa_refiner.py
index ff1817d..182ec55 100644
--- a/sage_examples/qa_refiner.py
+++ b/sage_examples/qa_refiner.py
@@ -1,7 +1,7 @@
 import time
 
 # 导入 Sage 相关模块
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -17,7 +17,7 @@ def pipeline_run():
     该函数会初始化环境，加载配置，设置数据处理流程，并启动管道。
     """
     # 初始化环境
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)  # 初始化内存配置
 
     # 构建数据处理流程
@@ -31,7 +31,7 @@ def pipeline_run():
 
     # 提交管道并运行
     env.submit()
-    
+    env.run_streaming()
     time.sleep(5)
     env.close()
 
diff --git a/sage_examples/qa_refiner_batch.py b/sage_examples/qa_refiner_batch.py
deleted file mode 100644
index 6a7c0bc..0000000
--- a/sage_examples/qa_refiner_batch.py
+++ /dev/null
@@ -1,40 +0,0 @@
-import time
-
-# 导入 Sage 相关模块
-from sage_core.api.local_environment import LocalBatchEnvironment
-from sage_libs.io.sink import TerminalSink
-from sage_libs.io.source import FileSource
-from sage_libs.rag.generator import OpenAIGenerator
-from sage_libs.rag.promptor import QAPromptor
-from sage_libs.rag.retriever import DenseRetriever
-from sage_libs.rag.refiner import AbstractiveRecompRefiner
-from sage_utils.config_loader import load_config
-
-
-def pipeline_run():
-    """创建并运行数据处理管道
-
-    该函数会初始化环境，加载配置，设置数据处理流程，并启动管道。
-    """
-    # 初始化环境
-    env = LocalBatchEnvironment()
-    env.set_memory(config=None)  # 初始化内存配置
-
-    # 构建数据处理流程
-    query_stream = (env.from_collection(FileSource, config["source"])
-                    .map(DenseRetriever, config["retriever"])
-                    .map(AbstractiveRecompRefiner, config["refiner"])  
-                    .map(QAPromptor, config["promptor"])
-                    .map(OpenAIGenerator, config["generator"])
-                    .sink(TerminalSink, config["sink"])
-                    )
-
-    # 提交管道并运行
-    env.submit()
-
-if __name__ == '__main__':
-    # 加载配置文件
-    config = load_config('config_refiner.yaml')
-    
-    # 运行管道
-    pipeline_run()
diff --git a/sage_examples/qa_rerank.py b/sage_examples/qa_rerank.py
index ce1e083..42c97d4 100644
--- a/sage_examples/qa_rerank.py
+++ b/sage_examples/qa_rerank.py
@@ -1,7 +1,7 @@
 import time
 
 # 导入 Sage 相关模块
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -17,7 +17,7 @@ def pipeline_run():
     该函数会初始化环境，加载配置，设置数据处理流程，并启动管道。
     """
     # 初始化环境
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)  # 初始化内存配置
 
     # 构建数据处理流程
@@ -31,7 +31,8 @@ def pipeline_run():
 
     # 提交管道并运行
     env.submit()
-
+    env.run_once()
+    
     # 等待一段时间确保任务完成
     time.sleep(5)
     
diff --git a/sage_examples/ray_object_trimmer_usage_example.py b/sage_examples/ray_object_trimmer_usage_example.py
deleted file mode 100644
index c365172..0000000
--- a/sage_examples/ray_object_trimmer_usage_example.py
+++ /dev/null
@@ -1,347 +0,0 @@
-"""
-SAGE系统中使用Ray对象预处理器的实际示例
-展示如何在Transformation和Operator中使用trim_object_for_ray
-"""
-import sys
-import os
-from pathlib import Path
-from typing import Any, Dict, List, Optional
-
-# 添加项目根路径
-SAGE_ROOT = Path(__file__).parent.parent
-sys.path.insert(0, str(SAGE_ROOT))
-
-from sage_utils.serialization.dill_serializer import (
-    trim_object_for_ray,
-    RayObjectTrimmer
-)
-
-# 模拟SAGE组件
-class MockTransformation:
-    """模拟的Transformation类，包含典型的不可序列化属性"""
-    
-    def __init__(self, function_name: str):
-        # 可序列化的核心属性
-        self.function_class_name = function_name
-        self.function_args = ["arg1", "arg2"]
-        self.function_kwargs = {"param1": "value1"}
-        self.basename = f"transform_{function_name}"
-        self.parallelism = 1
-        self.upstreams = []
-        self.downstreams = {"downstream1": 0}
-        
-        # 不可序列化的属性（会导致Ray调用失败）
-        from sage_utils.custom_logger import CustomLogger
-        self.logger = CustomLogger()  # 日志对象
-        self.env = self._create_mock_env()  # 环境引用
-        self.memory_collection = self._create_mock_memory()  # 可能是Actor句柄
-        
-        # 懒加载工厂（包含复杂状态）
-        self._dag_node_factory = None
-        self._operator_factory = None
-        self._function_factory = None
-        
-        # 运行时状态
-        self.runtime_context = self._create_runtime_context()
-        
-        # 定义序列化排除列表
-        self.__state_exclude__ = [
-            'logger', 'env', 'memory_collection', 'runtime_context',
-            '_dag_node_factory', '_operator_factory', '_function_factory'
-        ]
-    
-    def _create_mock_env(self):
-        """创建模拟环境对象"""
-        class MockEnv:
-            def __init__(self):
-                self.name = "test_env"
-                self.platform = "remote"
-        return MockEnv()
-    
-    def _create_mock_memory(self):
-        """创建模拟内存集合"""
-        class MockMemory:
-            def __init__(self):
-                self.collection_type = "VDB"
-        return MockMemory()
-    
-    def _create_runtime_context(self):
-        """创建模拟运行时上下文"""
-        class MockContext:
-            def __init__(self):
-                self.session_id = "test_session"
-        return MockContext()
-    
-    def get_serializable_state(self):
-        """返回可序列化的状态"""
-        return {
-            'function_class_name': self.function_class_name,
-            'function_args': self.function_args,
-            'function_kwargs': self.function_kwargs,
-            'basename': self.basename,
-            'parallelism': self.parallelism,
-            'upstreams': self.upstreams,
-            'downstreams': self.downstreams
-        }
-
-
-class MockOperator:
-    """模拟的Operator类"""
-    
-    def __init__(self, operator_name: str):
-        self.operator_name = operator_name
-        self.config = {"setting": "value"}
-        
-        # 不可序列化的属性
-        import threading
-        self.logger = self._create_logger()
-        self.emit_context = self._create_emit_context()
-        self.server_thread = threading.Thread(target=lambda: None)
-        
-        # 排除列表
-        self.__state_exclude__ = [
-            'logger', 'emit_context', 'server_thread'
-        ]
-    
-    def _create_logger(self):
-        class Logger:
-            def info(self, msg): pass
-        return Logger()
-    
-    def _create_emit_context(self):
-        class EmitContext:
-            def __init__(self):
-                self.connections = {}
-        return EmitContext()
-
-
-def demonstrate_transformation_trimming():
-    """演示Transformation对象的预处理"""
-    print("=== Transformation对象预处理示例 ===")
-    
-    # 创建包含问题属性的transformation
-    transformation = MockTransformation("map_function")
-    
-    print("原始Transformation属性:")
-    for attr_name in sorted(transformation.__dict__.keys()):
-        attr_value = getattr(transformation, attr_name)
-        print(f"  {attr_name}: {type(attr_value)} = {str(attr_value)[:50]}...")
-    
-    # 使用专门的transformation清理方法
-    print("\n使用RayObjectTrimmer.trim_transformation_for_ray()...")
-    cleaned_transformation = RayObjectTrimmer.trim_transformation_for_ray(transformation)
-    
-    print("清理后Transformation属性:")
-    for attr_name in sorted(cleaned_transformation.__dict__.keys()):
-        attr_value = getattr(cleaned_transformation, attr_name)
-        print(f"  {attr_name}: {type(attr_value)} = {str(attr_value)[:50]}...")
-    
-    # 验证关键属性保留
-    assert hasattr(cleaned_transformation, 'function_class_name'), "核心属性应该保留"
-    assert hasattr(cleaned_transformation, 'basename'), "名称应该保留"
-    assert not hasattr(cleaned_transformation, 'logger'), "logger应该被移除"
-    assert not hasattr(cleaned_transformation, 'env'), "env应该被移除"
-    
-    print("✓ Transformation清理成功")
-    return cleaned_transformation
-
-
-def demonstrate_operator_trimming():
-    """演示Operator对象的预处理"""
-    print("\n=== Operator对象预处理示例 ===")
-    
-    operator = MockOperator("map_operator")
-    
-    print("原始Operator属性:")
-    for attr_name in sorted(operator.__dict__.keys()):
-        print(f"  {attr_name}: {type(getattr(operator, attr_name))}")
-    
-    # 使用专门的operator清理方法
-    cleaned_operator = RayObjectTrimmer.trim_operator_for_ray(operator)
-    
-    print("清理后Operator属性:")
-    for attr_name in sorted(cleaned_operator.__dict__.keys()):
-        print(f"  {attr_name}: {type(getattr(cleaned_operator, attr_name))}")
-    
-    assert hasattr(cleaned_operator, 'operator_name'), "核心属性应该保留"
-    assert not hasattr(cleaned_operator, 'logger'), "logger应该被移除"
-    assert not hasattr(cleaned_operator, 'server_thread'), "线程应该被移除"
-    
-    print("✓ Operator清理成功")
-    return cleaned_operator
-
-
-def demonstrate_custom_trimming():
-    """演示自定义预处理规则"""
-    print("\n=== 自定义预处理规则示例 ===")
-    
-    transformation = MockTransformation("custom_function")
-    
-    # 场景1：只保留特定属性
-    print("场景1：只保留核心执行属性...")
-    core_only = trim_object_for_ray(
-        transformation,
-        include=['function_class_name', 'function_args', 'function_kwargs', 'parallelism']
-    )
-    print(f"保留属性: {list(core_only.__dict__.keys())}")
-    
-    # 场景2：排除特定属性
-    print("\n场景2：排除运行时状态...")
-    runtime_excluded = trim_object_for_ray(
-        transformation,
-        exclude=['upstreams', 'downstreams', 'runtime_context']
-    )
-    print(f"剩余属性: {list(runtime_excluded.__dict__.keys())}")
-    
-    print("✓ 自定义预处理规则测试成功")
-
-
-def simulate_ray_actor_workflow():
-    """模拟完整的Ray Actor工作流程"""
-    print("\n=== 模拟Ray Actor工作流程 ===")
-    
-    try:
-        import ray
-        
-        if not ray.is_initialized():
-            ray.init(local_mode=True, ignore_reinit_error=True)
-        
-        @ray.remote
-        class TransformationProcessor:
-            """模拟处理Transformation的Ray Actor"""
-            
-            def process_transformation(self, transformation):
-                """处理清理后的transformation对象"""
-                return {
-                    "processed_type": type(transformation).__name__,
-                    "function_name": getattr(transformation, 'function_class_name', 'unknown'),
-                    "attributes_count": len(transformation.__dict__) if hasattr(transformation, '__dict__') else 0,
-                    "attributes": list(transformation.__dict__.keys()) if hasattr(transformation, '__dict__') else []
-                }
-            
-            def validate_object(self, obj):
-                """验证对象是否可以正常处理"""
-                try:
-                    # 尝试访问对象属性
-                    attrs = obj.__dict__ if hasattr(obj, '__dict__') else {}
-                    return {
-                        "valid": True,
-                        "attribute_count": len(attrs),
-                        "sample_attributes": list(attrs.keys())[:5]
-                    }
-                except Exception as e:
-                    return {
-                        "valid": False,
-                        "error": str(e)
-                    }
-        
-        # 创建Actor
-        processor = TransformationProcessor.remote()
-        
-        # 准备测试对象
-        original_transformation = MockTransformation("ray_workflow_test")
-        
-        print("步骤1：尝试发送原始对象（可能失败）...")
-        try:
-            result = ray.get(processor.validate_object.remote(original_transformation))
-            print(f"原始对象验证结果: {result}")
-        except Exception as e:
-            print(f"原始对象发送失败: {e}")
-        
-        print("\n步骤2：预处理对象并发送...")
-        cleaned_transformation = RayObjectTrimmer.trim_transformation_for_ray(original_transformation)
-        
-        # 发送清理后的对象
-        validation_result = ray.get(processor.validate_object.remote(cleaned_transformation))
-        print(f"清理后对象验证结果: {validation_result}")
-        
-        processing_result = ray.get(processor.process_transformation.remote(cleaned_transformation))
-        print(f"处理结果: {processing_result}")
-        
-        print("✓ Ray Actor工作流程测试成功")
-        
-        # 清理
-        ray.shutdown()
-        
-    except ImportError:
-        print("Ray未安装，跳过Actor工作流程测试")
-    except Exception as e:
-        print(f"Ray Actor工作流程测试失败: {e}")
-
-
-def performance_analysis():
-    """性能分析：对比处理前后的对象大小"""
-    print("\n=== 性能分析 ===")
-    
-    import pickle
-    import sys
-    
-    # 创建测试对象
-    transformation = MockTransformation("performance_test")
-    operator = MockOperator("performance_test")
-    
-    # 分析transformation
-    print("Transformation对象分析:")
-    try:
-        original_size = sys.getsizeof(pickle.dumps(transformation.__dict__))
-        print(f"  原始对象大小: {original_size} 字节")
-    except Exception as e:
-        print(f"  原始对象无法序列化: {e}")
-    
-    cleaned_trans = RayObjectTrimmer.trim_transformation_for_ray(transformation)
-    cleaned_size = sys.getsizeof(pickle.dumps(cleaned_trans.__dict__))
-    print(f"  清理后对象大小: {cleaned_size} 字节")
-    print(f"  属性数量: {len(transformation.__dict__)} -> {len(cleaned_trans.__dict__)}")
-    
-    # 分析operator  
-    print("\nOperator对象分析:")
-    try:
-        original_op_size = sys.getsizeof(pickle.dumps(operator.__dict__))
-        print(f"  原始对象大小: {original_op_size} 字节")
-    except Exception as e:
-        print(f"  原始对象无法序列化: {e}")
-    
-    cleaned_op = RayObjectTrimmer.trim_operator_for_ray(operator)
-    cleaned_op_size = sys.getsizeof(pickle.dumps(cleaned_op.__dict__))
-    print(f"  清理后对象大小: {cleaned_op_size} 字节")
-    print(f"  属性数量: {len(operator.__dict__)} -> {len(cleaned_op.__dict__)}")
-
-
-def main():
-    """主函数：运行所有示例"""
-    print("SAGE Ray对象预处理器使用示例")
-    print("=" * 60)
-    
-    try:
-        # 基本用法示例
-        demonstrate_transformation_trimming()
-        demonstrate_operator_trimming()
-        demonstrate_custom_trimming()
-        
-        # 高级用法
-        simulate_ray_actor_workflow()
-        performance_analysis()
-        
-        print("\n" + "=" * 60)
-        print("🎉 所有示例运行完成！")
-        print("\n总结:")
-        print("1. ✓ trim_object_for_ray() 可以有效清理不可序列化的属性")
-        print("2. ✓ RayObjectTrimmer 提供了针对不同对象类型的专门清理方法")
-        print("3. ✓ 支持自定义include/exclude规则")
-        print("4. ✓ 清理后的对象可以安全地传递给Ray进行序列化")
-        print("5. ✓ 显著减少了对象大小和序列化复杂度")
-        
-        print("\n使用建议:")
-        print("- 在Ray远程调用前使用trim_object_for_ray()预处理对象")
-        print("- 为常见的SAGE组件使用专门的清理方法")
-        print("- 根据具体需求定制include/exclude列表")
-        print("- 在开发阶段使用validate_ray_serializable()验证对象")
-        
-    except Exception as e:
-        print(f"示例运行失败: {e}")
-        import traceback
-        traceback.print_exc()
-
-
-if __name__ == "__main__":
-    main()
diff --git a/sage_examples/test_jobmanager_architecture.py b/sage_examples/test_jobmanager_architecture.py
deleted file mode 100644
index 60fcf5b..0000000
--- a/sage_examples/test_jobmanager_architecture.py
+++ /dev/null
@@ -1,131 +0,0 @@
-#!/usr/bin/env python3
-"""
-测试新的JobManager Actor架构的示例
-"""
-
-import sys
-import time
-from pathlib import Path
-
-# 添加项目路径
-SAGE_ROOT = Path(__file__).parent.parent
-sys.path.insert(0, str(SAGE_ROOT))
-
-def test_local_environment():
-    """测试本地环境"""
-    print("\n=== 测试本地环境 ===")
-    
-    from sage_core.api.local_environment import LocalStreamEnvironment
-    
-    # 创建本地环境
-    env = LocalStreamEnvironment("test_local_env", config={"test": "local"})
-    
-    print(f"环境名称: {env.name}")
-    print(f"平台: {env.platform}")
-    
-    # 测试jobmanager属性访问
-    jobmanager = env.jobmanager
-    print(f"JobManager类型: {type(jobmanager)}")
-    print(f"JobManager是否本地: {jobmanager.is_local()}")
-    print(f"JobManager是否Ray Actor: {jobmanager.is_ray_actor()}")
-    
-    # 测试调用jobmanager方法
-    try:
-        server_info = jobmanager.get_server_info()
-        print(f"服务器信息: {server_info}")
-    except Exception as e:
-        print(f"获取服务器信息失败: {e}")
-    
-    print("✅ 本地环境测试完成")
-
-
-def test_remote_environment():
-    """测试远程环境"""
-    print("\n=== 测试远程环境 ===")
-    
-    try:
-        import ray
-        
-        # 初始化Ray（如果需要）
-        if not ray.is_initialized():
-            ray.init(ignore_reinit_error=True)
-        
-        from sage_core.api.remote_environment import RemoteStreamEnvironment
-        
-        # 创建远程环境
-        env = RemoteStreamEnvironment("test_remote_env", config={
-            "jobmanager_daemon_host": "127.0.0.1",
-            "jobmanager_daemon_port": 19000
-        })
-        
-        print(f"环境名称: {env.name}")
-        print(f"平台: {env.platform}")
-        
-        # 尝试获取JobManager句柄
-        # 注意：这需要Ray JobManager Daemon在运行
-        try:
-            jobmanager = env.jobmanager
-            print(f"JobManager类型: {type(jobmanager)}")
-            print(f"JobManager是否本地: {jobmanager.is_local()}")
-            print(f"JobManager是否Ray Actor: {jobmanager.is_ray_actor()}")
-            
-            # 测试调用jobmanager方法
-            server_info = jobmanager.get_server_info()
-            print(f"服务器信息: {server_info}")
-            
-            print("✅ 远程环境测试完成")
-            
-        except Exception as e:
-            print(f"⚠️ 远程环境测试失败（可能Ray JobManager Daemon未运行）: {e}")
-            print("提示：请先运行 'python deployment/ray_jobmanager_daemon.py start'")
-    
-    except ImportError:
-        print("⚠️ Ray未安装，跳过远程环境测试")
-
-
-def test_actor_wrapper():
-    """测试ActorWrapper的基本功能"""
-    print("\n=== 测试ActorWrapper ===")
-    
-    from sage_utils.actor_wrapper import ActorWrapper
-    from sage_jobmanager.job_manager import JobManager
-    
-    # 测试本地对象包装
-    jobmanager = JobManager()
-    wrapped = ActorWrapper(jobmanager)
-    
-    print(f"原始对象类型: {type(jobmanager)}")
-    print(f"包装后类型: {type(wrapped)}")
-    print(f"是否本地: {wrapped.is_local()}")
-    print(f"是否Ray Actor: {wrapped.is_ray_actor()}")
-    
-    # 测试方法调用
-    try:
-        server_info = wrapped.get_server_info()
-        print(f"通过wrapper调用方法成功: {list(server_info.keys())}")
-    except Exception as e:
-        print(f"通过wrapper调用方法失败: {e}")
-    
-    print("✅ ActorWrapper测试完成")
-
-
-def main():
-    """主函数"""
-    print("SAGE JobManager Actor架构测试")
-    print("=" * 50)
-    
-    # 基础组件测试
-    test_actor_wrapper()
-    
-    # 本地环境测试
-    test_local_environment()
-    
-    # 远程环境测试
-    test_remote_environment()
-    
-    print("\n" + "=" * 50)
-    print("所有测试完成")
-
-
-if __name__ == "__main__":
-    main()
diff --git a/sage_examples/wordcount_lambda_example.py b/sage_examples/wordcount_lambda_example.py
index fb8a245..879ea05 100644
--- a/sage_examples/wordcount_lambda_example.py
+++ b/sage_examples/wordcount_lambda_example.py
@@ -1,8 +1,8 @@
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from collections import Counter
 import time
-
+import random
 
 # 简单的句子源，重复输出同一句话
 class SentenceSource(SourceFunction):
@@ -26,7 +26,7 @@ class SentenceSource(SourceFunction):
 
 def main():
     # 创建环境
-    env = LocalStreamEnvironment("wordcount_example")
+    env = LocalEnvironment("wordcount_example")
     
     # 全局词汇计数器
     word_counts = Counter()
@@ -72,7 +72,7 @@ def main():
     try:
         # 运行流处理
         env.submit()
-        
+        env.run_streaming()
         time.sleep(60)  # 运行60秒以观察输出
     except KeyboardInterrupt:
         print("\n\n🛑 Stopping WordCount Example...")
diff --git a/sage_frontend/sage_server/data/jobinfo/c0755891-5744-49a1-9ca7-372cb32c5eee.json b/sage_frontend/sage_server/data/jobinfo/c0755891-5744-49a1-9ca7-372cb32c5eee.json
index a3df263..6cb5e75 100644
--- a/sage_frontend/sage_server/data/jobinfo/c0755891-5744-49a1-9ca7-372cb32c5eee.json
+++ b/sage_frontend/sage_server/data/jobinfo/c0755891-5744-49a1-9ca7-372cb32c5eee.json
@@ -5,7 +5,7 @@
     "cpu": "2",
     "ram": "4GB",
     "startTime": "2025-07-09 07:09:47",
-    "duration": "282s",
+    "duration": "281s",
     "isRunning": false,
     "nevents": 0,
     "minProcessTime": 0.0,
diff --git a/sage_frontend/sage_server/routers/signal.py b/sage_frontend/sage_server/routers/signal.py
index c18082f..a2a45c9 100644
--- a/sage_frontend/sage_server/routers/signal.py
+++ b/sage_frontend/sage_server/routers/signal.py
@@ -90,7 +90,7 @@ def update_json_field(file_path: str, *args):
 
 
 @router.post("/stop/{jobId}/{duration}")
-async def pause_job(jobId: str,duration:str ):
+async def stop_job(jobId: str,duration:str ):
     """
     停止指定ID的流处理作业
     """
diff --git a/sage_frontend/sage_server/start_a_pipeline.py b/sage_frontend/sage_server/start_a_pipeline.py
index 0934901..9a7ee1f 100644
--- a/sage_frontend/sage_server/start_a_pipeline.py
+++ b/sage_frontend/sage_server/start_a_pipeline.py
@@ -4,8 +4,8 @@
 # 导入 Sage 中的 Environment 和相关组件
 import logging
 import re
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_core.environment.base_environment import RemoteEnvironment
+from sage_core.api.env import LocalEnvironment
+from sage_core.api.env import RemoteEnvironment
 
 def init_memory_and_pipeline(job_id=None,  config=None, operators=None,use_ray=False):
     """
@@ -40,7 +40,7 @@ def init_memory_and_pipeline(job_id=None,  config=None, operators=None,use_ray=F
 
         env = RemoteEnvironment(env_name)
     else:
-        env = LocalStreamEnvironment(env_name)
+        env = LocalEnvironment(env_name)
     env.set_memory(config={"collection_name":f"{env_name}_memory"})
     # 如果没有提供operators配置，使用默认配置
     if not operators:
@@ -90,7 +90,7 @@ def init_memory_and_pipeline(job_id=None,  config=None, operators=None,use_ray=F
     # 提交管道到 SAGE 运行时
     try:
         env.submit(name = f"env_{job_id}" if job_id else "dynamic_pipeline")
-        
+        env.run_streaming()
     except Exception as e:
         logging.error(f"Failed to submit pipeline: {e}")
         raise  Exception(f"Pipeline submission failed: {e}")
diff --git a/sage_jobmanager/__init__.py b/sage_jobmanager/__init__.py
deleted file mode 100644
index 016ef8e..0000000
--- a/sage_jobmanager/__init__.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# from .api import env, model, memory, operator, query
-
-# # 只暴露四个子模块，保持清晰的模块边界
-# memory = api.memory
-# model = api.model
-# operator = api.operator
-# env = api.env
-# query = api.query
-
-# __all__ = ["memory", "model", "operator", "env", "query"]
\ No newline at end of file
diff --git a/sage_jobmanager/factory/__init__.py b/sage_jobmanager/factory/__init__.py
deleted file mode 100644
index 016ef8e..0000000
--- a/sage_jobmanager/factory/__init__.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# from .api import env, model, memory, operator, query
-
-# # 只暴露四个子模块，保持清晰的模块边界
-# memory = api.memory
-# model = api.model
-# operator = api.operator
-# env = api.env
-# query = api.query
-
-# __all__ = ["memory", "model", "operator", "env", "query"]
\ No newline at end of file
diff --git a/sage_jobmanager/factory/operator_factory.py b/sage_jobmanager/factory/operator_factory.py
deleted file mode 100644
index 1826c66..0000000
--- a/sage_jobmanager/factory/operator_factory.py
+++ /dev/null
@@ -1,33 +0,0 @@
-from typing import Type, TYPE_CHECKING
-from sage_jobmanager.utils.name_server import get_name
-
-if TYPE_CHECKING:
-    from sage_core.operator.base_operator import BaseOperator
-    from sage_jobmanager.factory.function_factory import FunctionFactory
-    from sage_runtime.runtime_context import RuntimeContext
-
-
-class OperatorFactory:
-    # 由transformation初始化
-    def __init__(self, 
-                 operator_class: Type['BaseOperator'],
-                 function_factory: 'FunctionFactory',
-                 basename: str = None,
-                 env_name:str = None,
-                 remote:bool = False,
-                 **operator_kwargs):
-        self.operator_class = operator_class
-        self.operator_kwargs = operator_kwargs  # 保存额外的operator参数
-        self.function_factory = function_factory
-        self.env_name = env_name
-        self.basename = get_name(basename) or get_name(self.function_factory.function_class.__name__)
-        self.remote = remote
-
-    def create_operator(self, runtime_context: 'RuntimeContext') -> 'BaseOperator':
-            Operator_class = self.operator_class
-            operator_instance = Operator_class(
-                self.function_factory,
-                runtime_context,
-                **self.operator_kwargs
-            )
-            return operator_instance
\ No newline at end of file
diff --git a/sage_jobmanager/job_info.py b/sage_jobmanager/job_info.py
deleted file mode 100644
index ab95242..0000000
--- a/sage_jobmanager/job_info.py
+++ /dev/null
@@ -1,102 +0,0 @@
-from datetime import datetime
-from typing import Dict, Any, Optional, TYPE_CHECKING
-import time
-
-if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment
-    from sage_jobmanager.execution_graph import ExecutionGraph
-    from sage_runtime.dispatcher import Dispatcher
-
-class JobInfo:
-    """作业信息类，用于跟踪和管理单个作业的状态"""
-    
-    def __init__(self, environment: 'BaseEnvironment', graph: 'ExecutionGraph', 
-                 dispatcher: 'Dispatcher', uuid: str):
-        self.environment = environment
-        self.graph = graph
-        self.dispatcher = dispatcher
-        self.uuid = uuid
-        
-        # 状态信息
-        self.status = "initializing"  # initializing, running, stopped, failed, restarting
-        self.start_time = datetime.now()
-        self.stop_time: Optional[datetime] = None
-        self.last_update_time = datetime.now()
-        self.error_message: Optional[str] = None
-        
-        # 统计信息
-        self.restart_count = 0
-        
-    def update_status(self, new_status: str, error: Optional[str] = None):
-        """更新作业状态"""
-        old_status = self.status
-        self.status = new_status
-        self.last_update_time = datetime.now()
-        
-        if error:
-            self.error_message = error
-        
-        if new_status in ["stopped", "failed"]:
-            self.stop_time = datetime.now()
-        elif new_status == "running" and old_status in ["stopped", "failed", "restarting"]:
-            # 重新开始运行，重置停止时间
-            self.stop_time = None
-    
-    def get_runtime(self) -> str:
-        """获取运行时间字符串"""
-        if self.stop_time:
-            runtime = self.stop_time - self.start_time
-        else:
-            runtime = datetime.now() - self.start_time
-        
-        total_seconds = int(runtime.total_seconds())
-        hours, remainder = divmod(total_seconds, 3600)
-        minutes, seconds = divmod(remainder, 60)
-        
-        if hours > 0:
-            return f"{hours}h {minutes}m {seconds}s"
-        elif minutes > 0:
-            return f"{minutes}m {seconds}s"
-        else:
-            return f"{seconds}s"
-    
-    def get_summary(self) -> Dict[str, Any]:
-        """获取作业摘要信息"""
-        return {
-            "uuid": self.uuid,
-            "name": self.environment.name,
-            "status": self.status,
-            "start_time": self.start_time.strftime("%Y-%m-%d %H:%M:%S"),
-            "runtime": self.get_runtime(),
-            "restart_count": self.restart_count,
-            "last_update": self.last_update_time.strftime("%Y-%m-%d %H:%M:%S")
-        }
-    
-    def get_status(self) -> Dict[str, Any]:
-        """获取详细状态信息"""
-        status_info = self.get_summary()
-        
-        # 添加详细信息
-        status_info.update({
-            "environment": {
-                "name": self.environment.name,
-                "platform": getattr(self.environment, 'platform', 'unknown'),
-                "description": getattr(self.environment, 'description', '')
-            },
-            "dispatcher": {
-                "task_count": len(self.dispatcher.tasks),
-                "is_running": self.dispatcher.is_running
-            }
-        })
-        
-        if self.error_message:
-            status_info["error"] = self.error_message
-        
-        if self.stop_time:
-            status_info["stop_time"] = self.stop_time.strftime("%Y-%m-%d %H:%M:%S")
-        
-        # 获取任务统计
-        if hasattr(self.dispatcher, 'get_statistics'):
-            status_info["task_statistics"] = self.dispatcher.get_statistics()
-        
-        return status_info
\ No newline at end of file
diff --git a/sage_jobmanager/job_manager.py b/sage_jobmanager/job_manager.py
deleted file mode 100644
index ece6e00..0000000
--- a/sage_jobmanager/job_manager.py
+++ /dev/null
@@ -1,329 +0,0 @@
-from datetime import datetime
-import os
-from pathlib import Path
-from typing import TYPE_CHECKING, Dict, Any, List, Optional
-import time, uuid
-from uuid import UUID
-from sage_jobmanager.job_info import JobInfo
-from sage_utils.custom_logger import CustomLogger
-from sage_runtime.dispatcher import Dispatcher
-import threading
-from sage_utils.serialization.dill_serializer import deserialize_object
-if TYPE_CHECKING:
-    from sage_jobmanager.execution_graph import ExecutionGraph
-    from sage_core.environment.base_environment import BaseEnvironment
-
-import ray
-class JobManager: #Job Manager
-    instance = None
-    instance_lock = threading.RLock()
-    def __new__(cls, *args, **kwargs):
-        if cls.instance is None:
-            with cls.instance_lock:
-                if cls.instance is None:
-                    cls.instance = super(JobManager, cls).__new__(cls)
-                    cls.instance._initialized = False
-        return cls.instance
-
-    def __init__(self):
-        with JobManager.instance_lock:
-            if self._initialized:
-                return
-            self._initialized = True
-            JobManager.instance = self
-            # 新增：UUID 到环境信息的映射
-            self.jobs: Dict[str, JobInfo] = {}  # uuid -> jobinfo
-            self.deleted_jobs: Dict[str, Dict[str, Any]] = {}
-            self.setup_logging_system()
-
-
-    def submit_job(self, env: 'BaseEnvironment') -> str:
-
-
-        env.setup_logging_system(self.log_base_dir)
-        # 生成 UUID
-        env.uuid = str(uuid.uuid4())
-        # 编译环境
-        from sage_jobmanager.execution_graph import ExecutionGraph
-        graph = ExecutionGraph(env, self.handle) 
-
-
-
-        # TODO: 如果Job里面有申明'env.set_memory(config=None)'，则说明该job需要一个global memory manager.
-        # 则在构建executiongraph的时候要单独是实例化一个特殊的operator，即 memory manager，并使得所有调用了memory相关操作的
-        # 算子，双向连到该memory manager算子上。
-        dispatcher = Dispatcher(graph, env)
-
-            # 创建 JobInfo 对象
-        job_info = JobInfo(env, graph, dispatcher, env.uuid)
-
-        self.jobs[env.uuid] = job_info
-        
-        try:
-            # 提交 DAG
-            dispatcher.submit()
-            job_info.update_status("running")
-            
-            self.logger.info(f"Environment '{env.name}' submitted with UUID {env.uuid}")
-            
-        except Exception as e:
-            job_info.update_status("failed", error=str(e))
-            self.logger.error(f"Failed to submit environment {env.uuid}: {e}")
-
-        return env.uuid
-
-    def continue_job(self, env_uuid: str) -> Dict[str, Any]:
-        """重启作业"""
-        job_info = self.jobs.get(env_uuid)
-        
-        if not job_info:
-            self.logger.error(f"Job with UUID {env_uuid} not found")
-            return {
-                "uuid": env_uuid,
-                "status": "not_found",
-                "message": f"Job with UUID {env_uuid} not found"
-            }
-        
-        try:
-            current_status = job_info.status
-            
-            # 如果作业正在运行，先停止它
-            if current_status == "running":
-                self.logger.info(f"Stopping running job {env_uuid} before restart")
-                stop_result = self.pause_job(env_uuid)
-                if stop_result.get("status") not in ["stopped", "error"]:
-                    return {
-                        "uuid": env_uuid,
-                        "status": "failed",
-                        "message": f"Failed to stop job before restart: {stop_result.get('message')}"
-                    }
-                
-                # 等待停止完成
-                time.sleep(1.0)
-            
-            # 重新创建 dispatcher
-            from sage_jobmanager.execution_graph import ExecutionGraph
-            new_graph = ExecutionGraph(job_info.environment)
-            new_dispatcher = Dispatcher(new_graph, job_info.environment)
-            
-            # 更新 job_info
-            job_info.graph = new_graph
-            job_info.dispatcher = new_dispatcher
-            job_info.update_status("restarting")
-            job_info.restart_count += 1
-            
-            # 重新提交作业
-            new_dispatcher.submit()
-            job_info.update_status("running")
-            
-            self.logger.info(f"Job {env_uuid} restarted successfully (restart #{job_info.restart_count})")
-            
-            return {
-                "uuid": env_uuid,
-                "status": "running",
-                "message": f"Job restarted successfully (restart #{job_info.restart_count})"
-            }
-            
-        except Exception as e:
-            job_info.update_status("failed", error=str(e))
-            self.logger.error(f"Failed to restart job {env_uuid}: {e}")
-            return {
-                "uuid": env_uuid,
-                "status": "failed",
-                "message": f"Failed to restart job: {str(e)}"
-            }
-
-    def delete_job(self, env_uuid: str, force: bool = False) -> Dict[str, Any]:
-        """删除作业"""
-        job_info = self.jobs.get(env_uuid)
-        
-        if not job_info:
-            self.logger.error(f"Job with UUID {env_uuid} not found")
-            return {
-                "uuid": env_uuid,
-                "status": "not_found",
-                "message": f"Job with UUID {env_uuid} not found"
-            }
-        
-        try:
-            current_status = job_info.status
-            
-            # 如果作业正在运行且未强制删除，先停止它
-            if current_status == "running" and not force:
-                self.logger.info(f"Stopping running job {env_uuid} before deletion")
-                stop_result = self.pause_job(env_uuid)
-                if stop_result.get("status") not in ["stopped", "error"]:
-                    return {
-                        "uuid": env_uuid,
-                        "status": "failed",
-                        "message": f"Failed to stop job before deletion: {stop_result.get('message')}"
-                    }
-                
-                # 等待停止完成
-                time.sleep(0.5)
-            elif current_status == "running" and force:
-                # 强制删除：直接停止
-                self.logger.warning(f"Force deleting running job {env_uuid}")
-                job_info.dispatcher.stop()
-            
-            # 清理资源
-            job_info.dispatcher.cleanup()
-            
-            # 保存删除历史（可选）
-            deletion_info = {
-                "deleted_at": datetime.now().isoformat(),
-                "final_status": job_info.status,
-                "name": job_info.environment.name,
-                "runtime": job_info.get_runtime(),
-                "restart_count": job_info.restart_count
-            }
-            self.deleted_jobs[env_uuid] = deletion_info
-            
-            # 从活动作业列表中移除
-            del self.jobs[env_uuid]
-            
-            self.logger.info(f"Job {env_uuid} deleted successfully")
-            
-            return {
-                "uuid": env_uuid,
-                "status": "deleted",
-                "message": "Job deleted successfully"
-            }
-            
-        except Exception as e:
-            self.logger.error(f"Failed to delete job {env_uuid}: {e}")
-            return {
-                "uuid": env_uuid,
-                "status": "failed",
-                "message": f"Failed to delete job: {str(e)}"
-            }
-
-    def receive_stop_signal(self, env_uuid: str):
-        """接收停止信号"""
-        job_info = self.jobs.get(env_uuid)
-        try:
-            # 停止 dispatcher
-            if (job_info.dispatcher.receive_stop_signal()) is True:
-                self.delete_job(env_uuid, force=True)
-                self.logger.info(f"Batch job: {env_uuid} completed ")
-            
-        except Exception as e:
-            job_info.update_status("failed", error=str(e))
-            self.logger.error(f"Failed to stop job {env_uuid}: {e}")
-
-
-    def pause_job(self, env_uuid: str) -> Dict[str, Any]:
-        """停止Job"""
-        job_info = self.jobs.get(env_uuid, None)
-        
-        if not job_info:
-            self.logger.error(f"Job with UUID {env_uuid} not found")
-            return {
-                "uuid": env_uuid,
-                "status": "not_found",
-                "message": f"Job with UUID {env_uuid} not found"
-            }
-        
-        try:
-            # 停止 dispatcher
-            job_info.dispatcher.stop()
-            job_info.update_status("stopped")
-            
-            self.logger.info(f"Job {env_uuid} stopped successfully")
-            
-            return {
-                "uuid": env_uuid,
-                "status": "stopped",
-                "message": "Job stopped successfully"
-            }
-            
-        except Exception as e:
-            job_info.update_status("failed", error=str(e))
-            self.logger.error(f"Failed to stop job {env_uuid}: {e}")
-
-    def get_job_status(self, env_uuid: str) -> Dict[str, Any]:
-        job_info = self.jobs.get(env_uuid)
-        
-        if not job_info:
-            self.logger.warning(f"Job with UUID {env_uuid} not found")
-        
-        return job_info.get_status()
-
-    def list_jobs(self) -> List[Dict[str, Any]]:
-        return [job_info.get_summary() for job_info in self.jobs.values()]
-
-    def get_server_info(self) -> Dict[str, Any]:
-        job_summaries = [job_info.get_summary() for job_info in self.jobs.values()]
-            
-        return {
-            "session_id": self.session_id,
-            "log_base_dir": self.log_base_dir,
-            "environments_count": len(self.jobs),
-            "jobs": job_summaries
-        }
-    
-
-    def shutdown(self):
-        """
-        完整释放 Engine 持有的所有资源：
-        - 停掉 RuntimeManager（线程、Ray actor 等）
-        - 停掉可能的 TCP/HTTP server
-        - 清空 DAG 映射与缓存
-        - 重置 Engine 单例
-        """
-        self.logger.info("Shutting down Engine and releasing resources")
-
-        JobManager._instance = None
-        self.logger.info("Engine shutdown complete")
-
-    def cleanup_all_jobs(self) -> Dict[str, Any]:
-        """清理所有作业"""
-        try:
-            cleanup_results = {}
-            
-            for env_uuid in list(self.jobs.keys()):
-                result = self.delete_job(env_uuid, force=True)
-                cleanup_results[env_uuid] = result
-            
-            self.logger.info(f"Cleaned up {len(cleanup_results)} jobs")
-            
-            return {
-                "status": "success",
-                "message": f"Cleaned up {len(cleanup_results)} jobs",
-                "results": cleanup_results
-            }
-            
-        except Exception as e:
-            self.logger.error(f"Failed to cleanup all jobs: {e}")
-            return {
-                "status": "failed",
-                "message": f"Failed to cleanup jobs: {str(e)}"
-            }
-
-    ########################################################
-    #                internal  methods                     #
-    ########################################################
-
-    def setup_logging_system(self):
-        """设置分层日志系统"""
-        # 1. 生成时间戳标识
-        self.session_timestamp = datetime.now()
-        self.session_id = self.session_timestamp.strftime("%Y%m%d_%H%M%S")
-        
-        # 2. 确定日志基础目录
-        # 方案：/tmp/sage/logs 作为实际存储位置
-        project_root = Path(__file__).parent.parent
-        self.log_base_dir = project_root / "logs" / f"jobmanager_{self.session_id}"
-        Path(self.log_base_dir).mkdir(parents=True, exist_ok=True)
-
-        
-        # 3. 创建JobManager主日志
-        self.logger = CustomLogger([
-            ("console", "INFO"),  # 控制台显示重要信息
-            (os.path.join(self.log_base_dir, "jobmanager.log"), "DEBUG"),      # 详细日志
-            (os.path.join(self.log_base_dir, "error.log"), "ERROR") # 错误日志
-        ], name="JobManager")
-
-    @property
-    def handle(self) -> 'JobManager':
-        return self
\ No newline at end of file
diff --git a/sage_jobmanager/remote_job_manager.py b/sage_jobmanager/remote_job_manager.py
deleted file mode 100644
index 52884c4..0000000
--- a/sage_jobmanager/remote_job_manager.py
+++ /dev/null
@@ -1,175 +0,0 @@
-import ray
-import time
-from typing import Dict, Any, List, TYPE_CHECKING
-from sage_jobmanager.job_manager import JobManager
-from ray.actor import ActorHandle
-if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment
-
-
-@ray.remote
-class RemoteJobManager(JobManager):
-    """
-    基于Ray Actor的JobManager，继承本地JobManager的所有功能
-    通过Ray装饰器自动支持远程调用
-    """
-    
-    def __init__(self, *args, **kwargs):
-        # 调用父类初始化
-        super().__init__(*args, **kwargs)
-        
-        # Ray特有的Actor信息
-        self.actor_id = ray.get_runtime_context().get_actor_id()
-        self.node_id = ray.get_runtime_context().get_node_id()
-        
-        self.logger.info(f"RemoteJobManager Actor initialized: {self.actor_id}")
-        self.logger.info(f"Running on Ray node: {self.node_id}")
-        self._actor_handle: ActorHandle = None
-    
-
-    # === 继承的核心方法自动支持Ray远程调用 ===
-    # submit_job, pause_job, get_job_status, list_jobs 等方法
-    # 无需重写，直接继承父类实现即可
-    
-    # === Ray Actor专用方法 ===
-    
-    def keep_alive(self) -> Dict[str, Any]:
-        """
-        防止Actor被回收的心跳方法
-        返回Actor的健康状态信息
-        """
-        return {
-            "status": "alive",
-            "actor_id": self.actor_id,
-            "node_id": self.node_id,
-            "session_id": self.session_id,
-            "timestamp": time.time(),
-            "active_jobs": len(self.jobs),
-            "memory_info": self._get_memory_info()
-        }
-    
-    def get_actor_info(self) -> Dict[str, Any]:
-        """获取Actor基本信息"""
-        return {
-            "actor_id": self.actor_id,
-            "node_id": self.node_id,
-            "session_id": self.session_id,
-            "log_base_dir": self.log_base_dir,
-            "actor_type": "RemoteJobManager"
-        }
-    
-    def _get_memory_info(self) -> Dict[str, Any]:
-        """获取内存使用信息（简单版本）"""
-        try:
-            import psutil
-            process = psutil.Process()
-            memory_info = process.memory_info()
-            return {
-                "rss_mb": round(memory_info.rss / 1024 / 1024, 2),
-                "vms_mb": round(memory_info.vms / 1024 / 1024, 2),
-                "available": True
-            }
-        except ImportError:
-            return {
-                "rss_mb": -1,
-                "vms_mb": -1,
-                "available": False,
-                "error": "psutil not available"
-            }
-    
-    def graceful_shutdown(self) -> Dict[str, Any]:
-        """
-        优雅关闭Actor
-        停止所有Job并清理资源
-        """
-        self.logger.info("RemoteJobManager Actor shutting down...")
-        
-        shutdown_report = {
-            "jobs_stopped": 0,
-            "jobs_failed_to_stop": 0,
-            "errors": []
-        }
-        
-        # 停止所有活跃的Job
-        for env_uuid, job_info in list(self.jobs.items()):
-            try:
-                if job_info.status in ["running", "submitted"]:
-                    self.pause_job(env_uuid)
-                    shutdown_report["jobs_stopped"] += 1
-            except Exception as e:
-                shutdown_report["jobs_failed_to_stop"] += 1
-                shutdown_report["errors"].append(f"Failed to stop job {env_uuid}: {str(e)}")
-                self.logger.error(f"Error stopping job {env_uuid} during shutdown: {e}")
-        
-        # 调用父类shutdown
-        try:
-            super().shutdown()
-        except Exception as e:
-            shutdown_report["errors"].append(f"Parent shutdown error: {str(e)}")
-        
-        shutdown_report["status"] = "shutdown_complete"
-        shutdown_report["timestamp"] = time.time()
-        
-        self.logger.info(f"RemoteJobManager Actor shutdown complete: {shutdown_report}")
-        return shutdown_report
-    
-    # === 增强的状态监控方法 ===
-    
-    def get_extended_server_info(self) -> Dict[str, Any]:
-        """获取扩展的服务器信息，包含Ray Actor特有信息"""
-        base_info = super().get_server_info()
-        
-        # 添加Ray Actor信息
-        base_info.update({
-            "actor_id": self.actor_id,
-            "node_id": self.node_id,
-            "actor_type": "RemoteJobManager",
-            "memory_info": self._get_memory_info(),
-            "uptime_seconds": time.time() - self.session_timestamp.timestamp()
-        })
-        
-        return base_info
-    
-    def health_check(self) -> Dict[str, Any]:
-        """健康检查方法"""
-        try:
-            job_count = len(self.jobs)
-            running_jobs = len([j for j in self.jobs.values() if j.status == "running"])
-            
-            return {
-                "status": "healthy",
-                "actor_id": self.actor_id,
-                "total_jobs": job_count,
-                "running_jobs": running_jobs,
-                "timestamp": time.time(),
-                "session_uptime": time.time() - self.session_timestamp.timestamp()
-            }
-        except Exception as e:
-            return {
-                "status": "unhealthy",
-                "error": str(e),
-                "timestamp": time.time()
-            }
-    
-    def __repr__(self) -> str:
-        return f"RemoteJobManager(actor_id={self.actor_id[:8]}..., session={self.session_id}, jobs={len(self.jobs)})"
-
-    @property
-    def handle(self) -> 'ActorHandle':
-        if self._actor_handle is None:
-            try:
-                # 方法1: 通过Actor名称获取（如果有的话）
-                try:
-                    self._actor_handle = ray.get_actor("sage_global_jobmanager", namespace="sage_system")
-                    self.logger.debug("Got actor handle by name")
-                except ValueError:
-                    # 方法2: 通过全局注册表获取（需要事先注册）
-                    # 方法3: 返回一个代理对象
-                    self._actor_handle = self._create_self_proxy()
-                    self.logger.debug("Created self proxy actor handle")
-                    
-            except Exception as e:
-                self.logger.error(f"Failed to get actor handle: {e}")
-                raise RuntimeError(f"Cannot obtain actor handle: {e}")
-        
-        return self._actor_handle
\ No newline at end of file
diff --git a/sage_libs/agents/agent.py b/sage_libs/agents/agent.py
index 47258bc..2e1c611 100644
--- a/sage_libs/agents/agent.py
+++ b/sage_libs/agents/agent.py
@@ -1,7 +1,10 @@
-from sage_utils.clients.generator_model import apply_generator_model
+from calendar import c
+from sage_common_funs.utils.generator_model import apply_generator_model
 from sage_core.function.map_function import MapFunction
+from jinja2 import Template
 
-from typing import Tuple
+from sage_utils.custom_logger import CustomLogger
+from typing import Any,Tuple
 import requests
 import json
 import re, time
diff --git a/sage_libs/agents/critic_bot.py b/sage_libs/agents/critic_bot.py
index 67a2c96..9c7fd31 100644
--- a/sage_libs/agents/critic_bot.py
+++ b/sage_libs/agents/critic_bot.py
@@ -1,9 +1,11 @@
 import json
 import re
 from typing import Dict, Any, Tuple
+from dataclasses import dataclass
 from jinja2 import Template
 from sage_core.function.map_function import MapFunction
-from sage_utils.clients.generator_model import apply_generator_model
+from sage_utils.custom_logger import CustomLogger
+from sage_common_funs.utils.generator_model import apply_generator_model
 from sage_libs.context.model_context import ModelContext
 from sage_libs.context.quality_label import QualityLabel
 from sage_libs.context.critic_evaluation import CriticEvaluation
diff --git a/sage_libs/agents/searcher_bot.py b/sage_libs/agents/searcher_bot.py
index c932e7b..5d1f706 100644
--- a/sage_libs/agents/searcher_bot.py
+++ b/sage_libs/agents/searcher_bot.py
@@ -1,9 +1,10 @@
-import time
-from typing import List, Dict, Any
+import os, time
+from typing import List, Dict, Union, Tuple, Any
 from jinja2 import Template
 from sage_core.function.map_function import MapFunction
+from sage_utils.custom_logger import CustomLogger
 from sage_libs.context.model_context import ModelContext
-from sage_utils.clients.generator_model import apply_generator_model
+from sage_common_funs.utils.generator_model import apply_generator_model
 
 # 搜索查询优化的prompt模板
 SEARCH_QUERY_OPTIMIZATION_PROMPT = '''You are a search query optimization specialist. Your task is to analyze the user's original question and existing retrieved information, then design optimized search queries to fill information gaps.
diff --git a/sage_libs/rag/generator.py b/sage_libs/rag/generator.py
index 41c4b73..691ed09 100644
--- a/sage_libs/rag/generator.py
+++ b/sage_libs/rag/generator.py
@@ -3,7 +3,8 @@ from typing import Tuple,List
 from sage_common_funs.utils.generator_model import apply_generator_model
 from sage_core.function.map_function import MapFunction 
 from sage_core.function.base_function import StatefulFunction
-from sage_utils.state_persistence import load_function_state, save_function_state
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.state_persistence import load_function_state, save_function_state
 
 class OpenAIGenerator(MapFunction):
     """
@@ -82,9 +83,9 @@ class OpenAIGeneratorWithHistory(StatefulFunction):
         self.history_turns = config.get("max_history_turns", 5)
         self.num = 1
 
-        base = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
+        base = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
         os.makedirs(base, exist_ok=True)
-        path = os.path.join(base, f"{self.ctx.name}.chkpt")
+        path = os.path.join(base, f"{self.runtime_context.name}.chkpt")
         load_function_state(self, path)
 
     def execute(self, data: List, **kwargs) -> Tuple[str, str]:
@@ -116,9 +117,9 @@ class OpenAIGeneratorWithHistory(StatefulFunction):
         self.logger.info(f"\033[32m[{self.__class__.__name__}] Response: {response}\033[0m")
 
         # —— 自动持久化：每次 execute 后保存状态 —— 
-        base = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
+        base = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
         os.makedirs(base, exist_ok=True)
-        path = os.path.join(base, f"{self.ctx.name}.chkpt")
+        path = os.path.join(base, f"{self.runtime_context.name}.chkpt")
         save_function_state(self, path)
         return (user_query, response)
     
@@ -126,9 +127,9 @@ class OpenAIGeneratorWithHistory(StatefulFunction):
         """
         手动触发：持久化当前 dialogue_history，用于测试调用。
         """
-        base = os.path.join(self.ctx.env_base_dir, ".sage_checkpoints")
+        base = os.path.join(self.runtime_context.session_folder, ".sage_checkpoints")
         os.makedirs(base, exist_ok=True)
-        path = os.path.join(base, f"{self.ctx.name}.chkpt")
+        path = os.path.join(base, f"{self.runtime_context.name}.chkpt")
         save_function_state(self, path)
 
 class HFGenerator(MapFunction):
diff --git a/sage_libs/rag/refiner.py b/sage_libs/rag/refiner.py
index c757acc..b8ad16c 100644
--- a/sage_libs/rag/refiner.py
+++ b/sage_libs/rag/refiner.py
@@ -1,7 +1,8 @@
 from sage_core.function.map_function import MapFunction
 
-from sage_utils.clients.generator_model import apply_generator_model
+from sage_common_funs.utils.generator_model import apply_generator_model
 from typing import Tuple,List
+import logging
 
 
 class AbstractiveRecompRefiner(MapFunction):
diff --git a/sage_libs/rag/retriever.py b/sage_libs/rag/retriever.py
index a65a6a7..b97f84d 100644
--- a/sage_libs/rag/retriever.py
+++ b/sage_libs/rag/retriever.py
@@ -3,6 +3,7 @@ import time  # 替换 asyncio 为 time 用于同步延迟
 
 from sage_core.function.map_function import MapFunction
 from sage_core.function.base_function import MemoryFunction, StatefulFunction
+from sage_utils.custom_logger import CustomLogger
 from sage_runtime.runtime_context import RuntimeContext
 
 # 更新后的 SimpleRetriever
@@ -29,7 +30,7 @@ class DenseRetriever(MapFunction):
             self.logger.info(f"\033[32m[ {self.__class__.__name__}]: Retrieving from LTM \033[0m ")
             try:
                 # 使用LTM配置和输入查询调用检索
-                ltm_results = self.ctx.retrieve(
+                ltm_results = self.runtime_context.retrieve(
                     query=input_query,
                     collection_config=self.ltm_config
                 )
@@ -60,7 +61,7 @@ class BM25sRetriever(MapFunction): # 目前runtime context还只支持ltm
 
         try:
             # 使用BM25s配置和输入查询调用检索
-            bm25s_results = self.ctx.retrieve(
+            bm25s_results = self.runtime_context.retrieve(
                 # self.bm25s_collection,
                 query=input_query,
                 collection_config=self.bm25s_config
diff --git a/sage_libs/rag/writer.py b/sage_libs/rag/writer.py
index 4027300..111c969 100644
--- a/sage_libs/rag/writer.py
+++ b/sage_libs/rag/writer.py
@@ -64,10 +64,6 @@ class MemoryWriter(MapFunction):
                 continue
 
             try:
-
-                # TODO: 这里的实现实际上要成为由writer 这个function主动往memory manager function发送一个数据。
-                # 而 memory manager function拿到这个数据之后就会去执行 `execute' method 即可实现记忆的读写。
-                # 这里可能会有一个由于调度原因导致的阻塞 -- 可以被优化，请参考MorphStream！
                 self.state.store(
                     collection=collection,
                     documents=processed_data,
diff --git a/sage_memory/api.py b/sage_memory/api.py
index b06ac88..295aa3d 100644
--- a/sage_memory/api.py
+++ b/sage_memory/api.py
@@ -1,9 +1,9 @@
 from typing import Optional, TYPE_CHECKING
-from sage_utils.embedding_methods.embedding_model import apply_embedding_model
+from sage_utils.embedding_model import apply_embedding_model
 from sage_memory.memory_manager import MemoryManager
 from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
-    pass
+    from sage_core.api.env import BaseEnvironment
 
 
 _manager = None
diff --git a/sage_jobmanager/execution_graph.py b/sage_runtime/compiler.py
similarity index 57%
rename from sage_jobmanager/execution_graph.py
rename to sage_runtime/compiler.py
index 5614c70..304de72 100644
--- a/sage_jobmanager/execution_graph.py
+++ b/sage_runtime/compiler.py
@@ -1,32 +1,31 @@
-
 from __future__ import annotations
-import os
 from typing import TYPE_CHECKING
-from typing import Dict, List, Set, Union
-from sage_core.environment.base_environment import BaseEnvironment
+from typing import Dict, List, Set
+from sage_core.api.env import BaseEnvironment
 from sage_core.transformation.base_transformation import BaseTransformation
 from sage_utils.custom_logger import CustomLogger
-from sage_jobmanager.utils.name_server import get_name
+from sage_utils.name_server import get_name
 from sage_runtime.runtime_context import RuntimeContext
 if TYPE_CHECKING:
-    from sage_jobmanager.job_manager import JobManager
-    from ray.actor import ActorHandle
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+    from sage_core.function.base_function import BaseFunction
+    from sage_runtime.function.factory import FunctionFactory
+    from sage_runtime.operator.factory import OperatorFactory
 
 
 class GraphNode:
-    def __init__(self, name: str, transformation: BaseTransformation, parallel_index: int, env:BaseEnvironment):
+    def __init__(self, name: str, transformation: BaseTransformation, parallel_index: int):
         self.name: str = name
         self.transformation: BaseTransformation = transformation
         self.parallel_index: int = parallel_index  # 在该transformation中的并行索引
         self.parallelism: int = transformation.parallelism
         self.is_spout: bool = transformation.is_spout
-        self.is_sink: bool = transformation.is_sink
+        # 输入输出channels：每个channel是一个边的列表
+
         self.input_channels:dict[int, List[GraphEdge]] = {}
         self.output_channels:List[List[GraphEdge]] = []
-
-        self.stop_signal_num: int = 0  # 预期的源节点数量
-        self.runtime_context: RuntimeContext = None
-
+        self.runtime_context: RuntimeContext = RuntimeContext(self, transformation.env)
 
 class GraphEdge:
     def __init__(self,name:str,  output_node: GraphNode,  input_node:GraphNode = None, input_index:int = 0):
@@ -41,65 +40,24 @@ class GraphEdge:
         self.downstream_node:GraphNode = input_node
         self.input_index:int = input_index
 
-class ExecutionGraph:
-    def __init__(self, env:BaseEnvironment, jobmanager_handle:Union['JobManager', 'ActorHandle']):
+class Compiler:
+    def __init__(self, env:BaseEnvironment):
         self.env = env
+        self.name = env.name
         self.nodes:Dict[str, GraphNode] = {}
         self.edges:Dict[str, GraphEdge] = {}
         # 构建数据流之间的连接映射
 
-        # self.log_base_dir = env.log_base_dir
-        # self.env_base_dir = env.env_base_dir
-        self.setup_logging_system()
+        self.logger = CustomLogger(
+            filename=f"Compiler_{env.name}",
+            console_output=False,
+            file_output=True
+        )
         # 构建基础图结构
         self._build_graph_from_pipeline(env)
-        self._calculate_source_dependencies()
-        self.generate_runtime_contexts(jobmanager_handle)
-        self.total_stop_signals = self.calculate_total_stop_signals()
+        
         self.logger.info(f"Successfully converted and optimized pipeline '{env.name}' to compiler with {len(self.nodes)} nodes and {len(self.edges)} edges")
 
-
-    def calculate_total_stop_signals(self):
-        """计算所有源节点的停止信号总数"""
-        total_signals = 0
-        for node in self.nodes.values():
-            if node.is_sink:
-                total_signals += node.stop_signal_num
-        return total_signals
-
-    def setup_logging_system(self): 
-        self.logger = CustomLogger([
-                ("console", "INFO"),  # 控制台显示重要信息
-                (os.path.join(self.env.env_base_dir, "ExecutionGraph.log"), "DEBUG"),  # 详细日志
-                (os.path.join(self.env.env_base_dir, "Error.log"), "ERROR")  # 错误日志
-            ],
-            name = f"ExecutionGraph_{self.env.name}",
-        )
-
-
-    def generate_runtime_contexts(self, jobmanager_handle):
-        """
-        为每个节点生成运行时上下文
-        """
-        self.logger.debug("Generating runtime contexts for all nodes")
-        for node_name, node in self.nodes.items():
-            try:
-                node.runtime_context = RuntimeContext(node, node.transformation, self.env, jobmanager_handle)
-                self.logger.debug(f"Generated runtime context for node: {node_name}")
-            except Exception as e:
-                self.logger.error(f"Failed to generate runtime context for node {node_name}: {e}", exc_info=True)
-
-    def setup_logging_system(self):
-        self.logger = CustomLogger([
-                ("console", "INFO"),  # 控制台显示重要信息
-                (os.path.join(self.env.env_base_dir, "ExecutionGraph.log"), "DEBUG"),  # 详细日志
-                (os.path.join(self.env.env_base_dir, "Error.log"), "ERROR")  # 错误日志
-            ],
-            name = f"ExecutionGraph_{self.env.name}",
-        )
-
-
-
     def _build_graph_from_pipeline(self, env: BaseEnvironment):
         """
         根据transformation pipeline构建图, 支持并行度和多对多连接
@@ -111,20 +69,25 @@ class ExecutionGraph:
         self.logger.debug("Step 1: Generating parallel nodes for each transformation")
         for transformation in env.pipeline:
             # 安全检查：如果发现未填充的future transformation，报错
-            from sage_core.transformation.future_transformation import FutureTransformation
-            if isinstance(transformation, FutureTransformation):
+            if hasattr(transformation, 'is_future') and transformation.is_future:
                 if not transformation.filled:
                     raise RuntimeError(
-                        f"Unfilled future transformation '{transformation.future_name}' in pipeline. "
+                        f"Found unfilled future transformation '{transformation.future_name}' in pipeline. "
+                        f"All future streams must be filled with fill_future() before compilation."
+                    )
+                else:
+                    # 这种情况不应该发生，因为已填充的future transformation应该被从pipeline中移除
+                    raise RuntimeError(
+                        f"Found filled future transformation '{transformation.future_name}' in pipeline. "
+                        f"This is unexpected - filled future transformations should be removed from pipeline."
                     )
-                continue
             
             node_names = []
             for i in range(transformation.parallelism):
                 try:
                     node_name = get_name(f"{transformation.basename}_{i}")
                     node_names.append(node_name)
-                    self.nodes[node_name] = GraphNode(node_name,   transformation, i, env)
+                    self.nodes[node_name] = GraphNode(node_name,   transformation, i)
                     self.logger.debug(f"Created node: {node_name} (parallel index: {i})")
                 except Exception as e:
                     self.logger.error(f"Error creating node {node_name}: {e}")
@@ -179,37 +142,4 @@ class ExecutionGraph:
                                     f"between {upstream_trans.operator_class.__name__} -> "
                                     f"{transformation.operator_class.__name__}")
         
-        self.logger.info(f"Graph construction completed: {len(self.nodes)} nodes, {len(self.edges)} edges")
-
-
-    def _calculate_source_dependencies(self):
-        """计算每个节点的源依赖关系"""
-        self.logger.debug("Calculating source dependencies for all nodes")
-        
-        # 使用广度优先搜索计算每个节点依赖的源节点
-        for node_name, node in self.nodes.items():
-            if node.is_sink:
-                # 非源节点通过BFS收集所有上游源依赖
-                visited = set()
-                queue = [node_name]
-                source_deps = set()
-
-                while queue:
-                    current_name = queue.pop(0)
-                    if current_name in visited:
-                        continue
-                    visited.add(current_name)
-                    
-                    current_node = self.nodes[current_name]
-                    
-                    if current_node.is_spout:
-                        source_deps.add(current_node.transformation.basename)
-                        node.stop_signal_num += 1
-                    else:
-                        # 添加所有上游节点到队列
-                        for input_channel in current_node.input_channels.values():
-                            for edge in input_channel:
-                                if edge.upstream_node.name not in visited:
-                                    queue.append(edge.upstream_node.name)
-            
-            self.logger.debug(f"Node {node_name} expects {node.stop_signal_num} source instances")
\ No newline at end of file
+        self.logger.info(f"Graph construction completed: {len(self.nodes)} nodes, {len(self.edges)} edges")
\ No newline at end of file
diff --git a/sage_runtime/dagnode/__init__.py b/sage_runtime/dagnode/__init__.py
new file mode 100644
index 0000000..0ff4321
--- /dev/null
+++ b/sage_runtime/dagnode/__init__.py
@@ -0,0 +1,4 @@
+# from .ray_runtime import RayRuntime
+# # from .ray_executor import RayDAGExecutor
+# # 供顶层 sage/__init__.py 使用
+# __all__ = ["RayRuntime"]
\ No newline at end of file
diff --git a/sage_runtime/dagnode/base_dag_node.py b/sage_runtime/dagnode/base_dag_node.py
new file mode 100644
index 0000000..97b6c1e
--- /dev/null
+++ b/sage_runtime/dagnode/base_dag_node.py
@@ -0,0 +1,91 @@
+from abc import ABC, abstractmethod
+import threading, copy
+from typing import Any, TYPE_CHECKING, Union
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.runtime_context import RuntimeContext
+from ray.actor import ActorHandle
+
+if TYPE_CHECKING:
+    from sage_runtime.io.connection import Connection
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_core.transformation.base_transformation import BaseTransformation, OperatorFactory
+    from sage_runtime.compiler import Compiler, GraphNode
+
+class BaseDAGNode(ABC):
+    def __init__(
+        self, 
+        name:str, 
+        runtime_context: 'RuntimeContext',
+        operator_factory: 'OperatorFactory'
+    ) -> None:
+        self.runtime_context: 'RuntimeContext'
+        self.operator:BaseOperator
+        self.delay: Union[int, float] = 1
+        self.is_spout: bool = False
+
+
+        self.operator_factory: 'OperatorFactory' = operator_factory
+        self.name = name
+        self._running = False
+        self.stop_event = threading.Event()
+        try:
+            self.runtime_context = runtime_context
+
+            self.operator = self.operator_factory.create_operator(name=self.name, runtime_context = runtime_context)
+            # Create logger first
+            self.logger = CustomLogger(
+                filename=f"Node_{runtime_context.name}",
+                env_name=runtime_context.env_name,
+                console_output="WARNING",
+                file_output="DEBUG",
+                global_output = "WARNING",
+                name = f"{runtime_context.name}_{self.__class__.__name__}"
+            )
+            self.logger.info(f"Node {self.name} initialized with type {self.__class__.__name__}")
+        except Exception as e:
+            self.logger.error(f"Failed to initialize node {self.name}: {e}", exc_info=True)
+
+    @abstractmethod
+    def run_loop(self) -> None:
+        """
+        Run the node's processing loop.
+        This method should be implemented by subclasses to define the node's behavior.
+        """
+        pass 
+
+    def add_connection(self, connection: 'Connection'):
+        """
+        添加连接到DAG节点
+        :param connection: Connection对象，包含连接信息
+        """
+        self.operator.add_connection(connection)
+        self.logger.debug(f"Connection added to node '{self.name}': {connection}")
+
+    def submit(self):
+        pass
+
+
+    def trigger(self, input_tag: str = None, data:Any = None) -> None:
+        """
+        Execute the node once, processing any available input data.
+        This is typically used for spout nodes to emit initial data.
+        """
+        try:
+            self.logger.debug(f"Received data in node {self.name}, channel {input_tag}")
+            self.operator.receive_packet( data)
+        except Exception as e:
+            self.logger.error(f"Error processing data in node {self.name}: {e}", exc_info=True)
+            raise
+
+
+    def stop(self) -> None:
+        """Signal the worker loop to stop."""
+        if not self.stop_event.is_set():
+            self.stop_event.set()
+            self.logger.info(f"Node '{self.name}' received stop signal.")
+
+
+
+    def is_running(self):
+        """Check if the node is currently running."""
+        return self._running
\ No newline at end of file
diff --git a/sage_jobmanager/factory/task_factory.py b/sage_runtime/dagnode/factory.py
similarity index 54%
rename from sage_jobmanager/factory/task_factory.py
rename to sage_runtime/dagnode/factory.py
index be934ed..050f156 100644
--- a/sage_jobmanager/factory/task_factory.py
+++ b/sage_runtime/dagnode/factory.py
@@ -1,25 +1,28 @@
-from typing import Any, TYPE_CHECKING, Union
-from sage_runtime.task.ray_task import RayTask
-from sage_runtime.task.local_task import LocalTask
-from sage_runtime.task.base_task import BaseTask
-from sage_utils.actor_wrapper import ActorWrapper
+from typing import Type, Any, Dict, TYPE_CHECKING, Union
+from sage_runtime.function.factory import FunctionFactory
+from sage_runtime.dagnode.ray_dag_node import RayDAGNode
+from sage_runtime.dagnode.local_dag_node import LocalDAGNode
+
 if TYPE_CHECKING:
     from sage_core.transformation.base_transformation import BaseTransformation
+    from sage_core.api.env import BaseEnvironment
+    from sage_runtime.dagnode.base_dag_node import BaseDAGNode
     from ray.actor import ActorHandle
+    from sage_runtime.compiler import GraphNode
     from sage_runtime.runtime_context import RuntimeContext
     
-class TaskFactory:
+class DAGNodeFactory:
     def __init__(
         self,
         transformation: 'BaseTransformation',
         # parallel_index: int, # 这个在create instance时传入
     ):
         self.basename = transformation.basename
-        self.env_name = transformation.env_name
+        self.env_name = transformation.env.name
         self.operator_factory = transformation.operator_factory
         self.delay = transformation.delay
         self.remote:bool = transformation.remote
-        self.memory_collection:Union[Any, ActorHandle] = transformation.memory_collection
+        self.memory_collection:Union[Any, ActorHandle] = transformation.env.memory_collection
         self.is_spout = transformation.is_spout
 
         # 这些参数在创建节点时注入
@@ -27,17 +30,19 @@ class TaskFactory:
         # self.parallelism: int     # 来自图编译
         # self.node_name: str       # 来自图编译
 
-    def create_task(
+    def create_node(
         self,
         name: str,
         runtime_context: 'RuntimeContext' = None,
-    ) -> 'BaseTask':
+    ) -> 'BaseDAGNode':
         if self.remote:
-            node = RayTask.options(lifetime="detached").remote(runtime_context,  self.operator_factory)
-            node = ActorWrapper(node)
+            node = RayDAGNode(name, runtime_context,  self.operator_factory)
         else:
-            node = LocalTask(runtime_context, self.operator_factory)
+            node = LocalDAGNode(name, runtime_context, self.operator_factory)
+        node.delay = self.delay
+        node.is_spout = self.is_spout
+        # print(f"{name} is spout: {node.is_spout}")
         return node
     
     def __repr__(self) -> str:
-        return f"<TaskFactory {self.basename}>"
\ No newline at end of file
+        return f"<DAGNodeFactory {self.basename}>"
\ No newline at end of file
diff --git a/sage_runtime/dagnode/local_dag_node.py b/sage_runtime/dagnode/local_dag_node.py
new file mode 100644
index 0000000..08b75d7
--- /dev/null
+++ b/sage_runtime/dagnode/local_dag_node.py
@@ -0,0 +1,71 @@
+from __future__ import annotations
+import time, copy
+from typing import Any, Union, Tuple, TYPE_CHECKING
+from sage_runtime.io.local_message_queue import LocalMessageQueue
+from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+from ray.actor import ActorHandle
+from sage_memory.memory_collection.base_collection import BaseMemoryCollection
+from sage_utils.custom_logger import CustomLogger
+
+if TYPE_CHECKING:
+    from sage_core.transformation.base_transformation import BaseTransformation
+    from sage_runtime.operator.factory import OperatorFactory
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_runtime.operator.operator_wrapper import OperatorWrapper
+    from sage_runtime.compiler import Compiler, GraphNode
+    from sage_runtime.runtime_context import RuntimeContext
+
+
+
+class LocalDAGNode(BaseDAGNode):
+
+    def __init__(self, *args, **kwargs) -> None:
+        super().__init__(*args, **kwargs)
+        self.input_buffer = LocalMessageQueue(name = self.name, env_name=self.runtime_context.env_name)  # Local input buffer for this node
+        self.logger.info(f"Initialized LocalDAGNode: {self.name} (spout: {self.is_spout})")
+    
+    def put(self, data_packet: Any):
+        """
+        向输入缓冲区放入数据包
+        
+        Args:
+            data_packet: (input_channel, data) 元组
+        """
+        self.input_buffer.put(data_packet, timeout=1.0)
+        self.logger.debug(f"Put data packet into buffer")
+    
+
+    def run_loop(self) -> None:
+        """
+        Main worker loop that executes continuously until stop is signaled.
+        """
+
+        # Ensure all sage_runtime objects are initialized
+        self.stop_event.clear()
+        self._running = True
+        # Main execution loop
+        while not self.stop_event.is_set():
+            try:
+                if self.is_spout:
+                    self.logger.debug(f"Running spout node '{self.name}'")
+                    # For spout nodes, call operator.receive with dummy channel and data
+                    self.operator.receive_packet(None)
+                    time.sleep(self.delay)  # Sleep to avoid busy loop
+                else:
+                    # For non-spout nodes, fetch input and process
+                    # input_result = self.fetch_input()
+                    data_packet = self.input_buffer.get(timeout=0.5)
+                    if(data_packet is None):
+                        time.sleep(0.1)  # Short sleep when no data to process
+                        continue
+                    # Call operator's receive method with the channel_id and data
+                    self.operator.receive_packet(data_packet)
+            except Exception as e:
+                self.logger.error(
+                    f"Critical error in node '{self.name}': {str(e)}",
+                    exc_info=True
+                )
+                self.stop()
+                raise RuntimeError(f"Execution failed in node '{self.name}'")
+            finally:
+                self._running = False
\ No newline at end of file
diff --git a/sage_runtime/dagnode/ray_dag_node.py b/sage_runtime/dagnode/ray_dag_node.py
new file mode 100644
index 0000000..68a84d6
--- /dev/null
+++ b/sage_runtime/dagnode/ray_dag_node.py
@@ -0,0 +1,100 @@
+import ray
+import time
+from typing import Any, Dict, Union, TYPE_CHECKING
+from ray.actor import ActorHandle
+from sage_runtime.runtime_context import RuntimeContext
+from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+from sage_utils.custom_logger import CustomLogger
+if TYPE_CHECKING:   
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_runtime.operator.factory import OperatorFactory
+    from sage_runtime.operator.operator_wrapper import OperatorWrapper
+    from sage_runtime.compiler import Compiler, GraphNode
+
+class RayDAGNode(BaseDAGNode):
+    """
+    Ray Actor version of LocalDAGNode for distributed execution.
+    
+    Unlike local nodes, Ray actors don't need input buffers as Ray platform
+    maintains the request queue for actors automatically.
+    """
+    
+    def __init__(self, *args, **kwargs) -> None:
+        super().__init__(*args, **kwargs)
+
+    def run_loop(self):
+        """
+        Start the node. For spout nodes, this starts the generation loop.
+        For non-spout nodes, this just marks the node as ready to receive data.
+        """
+        self._running = True
+        if self.is_spout:
+            while not self.stop_event.is_set():
+    
+                # Start spout execution asynchronously
+                try:
+                    # For spout nodes, call operator.receive with dummy channel and data
+                    self.operator.receive_packet(None)
+                    time.sleep(self.delay)  # Small delay to prevent overwhelming
+                        
+                except Exception as e:
+                    self.logger.error(f"Error in spout node {self.name}: {e}", exc_info=True)
+                    raise
+            self._running = False
+            self.logger.info(f"Spout execution stopped for node {self.name}")
+        else:
+            # For non-spout nodes, just mark as running
+
+            self.logger.info(f"Ray node {self.name} started and ready to receive data")
+
+
+    ########################################################
+    #                inactive methods                      #
+    ########################################################
+
+    def get_node_info(self) -> Dict[str, Any]:
+        """Get comprehensive node information for debugging."""
+        return {
+            "name": self.name,
+            "is_spout": self.is_spout,
+            "is_running": self.is_running(),
+            "initialized": self._initialized,
+            "operator_class": self.function_class.__name__ if self.function_class else None,
+            "downstream_targets": len(self.emit_context.downstream_channels) if hasattr(self, 'emit_context') else 0
+        }
+
+    def health_check(self) -> Dict[str, Any]:
+        """Perform health check and return status."""
+        try:
+            return {
+                "status": "healthy",
+                "node_name": self.name,
+                "is_running": self.is_running(),
+                "initialized": self._initialized,
+                "timestamp": time.time_ns()
+            }
+        except Exception as e:
+            return {
+                "status": "unhealthy",
+                "node_name": self.name,
+                "error": str(e),
+                "timestamp": time.time_ns()
+            }
+
+    def __getstate__(self):
+        """
+        Custom serialization to exclude non-serializable objects.
+        Ray handles most serialization automatically, but this helps with debugging.
+        """
+        state = self.__dict__.copy()
+        # Ray actors typically don't need custom serialization,
+        # but we can exclude logger if needed
+        return state
+
+    def __setstate__(self, state):
+        """
+        Custom deserialization to restore state.
+        """
+        self.__dict__.update(state)
+        # Mark as not initialized so sage_runtime objects will be created when needed
+        self._initialized = False
\ No newline at end of file
diff --git a/sage_runtime/dispatcher.py b/sage_runtime/dispatcher.py
deleted file mode 100644
index b1795b2..0000000
--- a/sage_runtime/dispatcher.py
+++ /dev/null
@@ -1,259 +0,0 @@
-import os
-import time
-from typing import Dict, List, Any, Tuple, Union, TYPE_CHECKING
-from sage_runtime.task.base_task import BaseTask
-from sage_runtime.router.connection import Connection
-from sage_utils.custom_logger import CustomLogger
-import ray
-from ray.actor import ActorHandle
-if TYPE_CHECKING:
-    from sage_core.environment.base_environment import BaseEnvironment 
-    from sage_jobmanager.execution_graph import ExecutionGraph, GraphNode
-
-# 这个dispatcher可以直接打包传给ray sage daemon service
-class Dispatcher():
-    def __init__(self, graph: 'ExecutionGraph', env:'BaseEnvironment'):
-        self.total_stop_signals = graph.total_stop_signals
-        self.received_stop_signals = 0
-        self.graph = graph
-        self.env = env
-        self.name:str = env.name
-        self.remote = env.platform == "remote"
-        self.logger = CustomLogger([
-                ("console", "INFO"),  # 控制台显示重要信息
-                (os.path.join(env.env_base_dir, "Dispatcher.log"), "DEBUG"),  # 详细日志
-                (os.path.join(env.env_base_dir, "Error.log"), "ERROR")  # 错误日志
-            ],
-            name = f"Environment_{self.name}",
-        )
-        # self.nodes: Dict[str, Union[ActorHandle, LocalDAGNode]] = {}
-        self.tasks: Dict[str, BaseTask] = {}
-        self.is_running: bool = False
-        self.logger.info(f"Dispatcher '{self.name}' construction complete")
-        if env.platform is "remote" and not ray.is_initialized():
-            ray.init(address="auto", _temp_dir="/var/lib/ray_shared")
-        self.setup_logging_system()
-
-    def receive_stop_signal(self):
-        """
-        接收停止信号并处理
-        """
-        self.logger.info(f"Dispatcher received stop signal.")
-        self.received_stop_signals += 1
-        if self.received_stop_signals >= self.total_stop_signals:
-            self.logger.info(f"Received all {self.total_stop_signals} stop signals, stopping dispatcher for batch job.")
-            self.cleanup()
-            return True
-        else:
-            return False
-
-
-    def setup_logging_system(self): 
-        self.logger = CustomLogger([
-                ("console", "INFO"),  # 控制台显示重要信息
-                (os.path.join(self.env.env_base_dir, "Dispatcher.log"), "DEBUG"),  # 详细日志
-                (os.path.join(self.env.env_base_dir, "Error.log"), "ERROR")  # 错误日志
-            ],
-            name = f"Dispatcher_{self.name}",
-        )
-
-    # Dispatcher will submit the job to LocalEngine or Ray Server.    
-    def submit(self):
-        """编译图结构，创建节点并建立连接"""
-        self.logger.info(f"Compiling Job for graph: {self.name}")
-        
-        # 第一步：创建所有节点实例
-        for node_name, graph_node in self.graph.nodes.items():
-            # task = graph_node.create_dag_node()
-            task = graph_node.transformation.task_factory.create_task(graph_node.name, graph_node.runtime_context)
-
-            self.tasks[node_name] = task
-
-            self.logger.debug(f"Added node '{node_name}' of type '{task.__class__.__name__}'")
-        
-        # 第二步：建立节点间的连接
-        for node_name, graph_node in self.graph.nodes.items():
-            self._setup_node_connections(node_name, graph_node)
-        
-        # 第三步：提交所有节点开始运行
-        for node_name, task in self.tasks.items():
-            try:
-                task.start_running()
-                self.logger.debug(f"Started node: {node_name}")
-            except Exception as e:
-                self.logger.error(f"Failed to start node {node_name}: {e}", exc_info=True)
-        self.logger.info(f"Job submission completed: {len(self.tasks)} nodes")
-        self.is_running = True
-
-
-    def _setup_node_connections(self, node_name: str, graph_node: 'GraphNode'):
-        """
-        为节点设置下游连接
-        
-        Args:
-            node_name: 节点名称
-            graph_node: 图节点对象
-        """
-        output_handle = self.tasks[node_name]
-        
-        for broadcast_index, parallel_edges in enumerate(graph_node.output_channels):
-            for parallel_index, parallel_edge in enumerate(parallel_edges):
-                target_name = parallel_edge.downstream_node.name
-                target_input_index = parallel_edge.input_index
-                target_handle = self.tasks[target_name]
-
-                connection = Connection(
-                    broadcast_index=broadcast_index,
-                    parallel_index=parallel_index,
-                    target_name=target_name,
-                    target_input_buffer = target_handle.get_input_buffer(),
-                    target_input_index = target_input_index
-                )
-                try:
-                    output_handle.add_connection(connection)
-                    self.logger.debug(f"Setup connection: {node_name} -> {target_name}")
-                    
-                except Exception as e:
-                    self.logger.error(f"Error setting up connection {node_name} -> {target_name}: {e}", exc_info=True)
-
-    def stop(self):
-        """停止所有任务"""
-        if not self.is_running:
-            self.logger.warning("Dispatcher is not running")
-            return
-            
-        self.logger.info(f"Stopping dispatcher '{self.name}'")
-        
-        # 发送停止信号给所有任务
-        for node_name, node_instance in self.tasks.items():
-            try:
-                node_instance.stop()
-                self.logger.debug(f"Sent stop signal to node: {node_name}")
-            except Exception as e:
-                self.logger.error(f"Error stopping node {node_name}: {e}")
-        
-        # 等待所有任务停止（最多等待10秒）
-        self._wait_for_tasks_stop(timeout=10.0)
-        
-        self.is_running = False
-        self.logger.info("Dispatcher stopped")
-
-    def _wait_for_tasks_stop(self, timeout: float = 10.0):
-        """等待所有任务停止"""
-        start_time = time.time()
-        
-        while time.time() - start_time < timeout:
-            all_stopped = True
-            
-            for node_name, task in self.tasks.items():
-                if hasattr(task, 'is_running') and task.is_running:
-                    all_stopped = False
-                    break
-            
-            if all_stopped:
-                self.logger.debug("All tasks stopped")
-                return
-                
-            time.sleep(0.1)
-        
-        self.logger.warning(f"Timeout waiting for tasks to stop after {timeout}s")
-
-    def cleanup(self):
-        """清理所有资源"""
-        self.logger.info(f"Cleaning up dispatcher '{self.name}'")
-        
-        try:
-            # 停止所有任务
-            if self.is_running:
-                self.stop()
-            
-            if self.remote:
-                # 清理 Ray Actors
-                self._cleanup_ray_actors()
-            else:
-                # 清理任务引用
-                for node_name, task in self.tasks.items():
-                    try:
-                        task.cleanup()
-                        self.logger.debug(f"Cleaned up task: {node_name}")
-                    except Exception as e:
-                        self.logger.error(f"Error cleaning up task {node_name}: {e}")
-            # 清空任务字典
-            self.tasks.clear()
-            
-            self.logger.info("Dispatcher cleanup completed")
-            
-        except Exception as e:
-            self.logger.error(f"Error during dispatcher cleanup: {e}")
-
-    def _cleanup_ray_actors(self):
-        """清理所有 Ray Actors"""
-        
-        self.logger.info(f"Cleaning up {len(self.tasks)} Ray actors")
-        
-        # 第一阶段：发送清理信号
-        cleanup_futures = []
-        for actor in self.tasks:
-            try:
-                # 调用 Actor 的 cleanup 方法
-                future = actor.cleanup.remote()
-                cleanup_futures.append((actor, future))
-            except Exception as e:
-                self.logger.warning(f"Failed to send cleanup signal to actor: {e}")
-        
-        # 第二阶段：等待清理完成（有超时）
-        if cleanup_futures:
-            self._wait_for_cleanup_completion(cleanup_futures, timeout=5.0)
-        
-        try:
-        # 第三阶段：强制终止所有 Actor
-            for actor in self.tasks:
-                ray.kill(actor)
-                self.logger.debug(f"Killed Ray actor: {actor}")
-        except Exception as e:
-            self.logger.warning(f"Failed to kill Ray actor {actor}: {e}")
-
-    def _wait_for_cleanup_completion(self, cleanup_futures: List[Tuple[Any, Any]], timeout: float = 5.0):
-        """等待清理操作完成"""
-        self.logger.debug(f"Waiting for {len(cleanup_futures)} actors cleanup (timeout: {timeout}s)")
-        
-        try:
-            futures = [future for _, future in cleanup_futures]
-            ray.get(futures, timeout=timeout)
-            self.logger.debug("All actors cleaned up successfully")
-        except ray.exceptions.RayTimeoutError:
-            self.logger.warning(f"Timeout waiting for actors cleanup after {timeout}s")
-        except Exception as e:
-            self.logger.error(f"Error waiting for cleanup completion: {e}")
-
-
-    def get_task_status(self) -> Dict[str, Any]:
-        """获取所有任务的状态"""
-        status = {}
-        
-        for node_name, task in self.tasks.items():
-            try:
-                task_status = {
-                    "name": node_name,
-                    "running": getattr(task, 'is_running', False),
-                    "processed_count": getattr(task, '_processed_count', 0),
-                    "error_count": getattr(task, '_error_count', 0),
-                }
-                status[node_name] = task_status
-            except Exception as e:
-                status[node_name] = {
-                    "name": node_name,
-                    "error": str(e)
-                }
-        
-        return status
-
-
-    def get_statistics(self) -> Dict[str, Any]:
-        """获取dispatcher统计信息"""
-        return {
-            "name": self.name,
-            "is_running": self.is_running,
-            "task_count": len(self.tasks),
-            "task_status": self.get_task_status()
-        }
\ No newline at end of file
diff --git a/sage_runtime/function/__init__.py b/sage_runtime/function/__init__.py
new file mode 100644
index 0000000..0ff4321
--- /dev/null
+++ b/sage_runtime/function/__init__.py
@@ -0,0 +1,4 @@
+# from .ray_runtime import RayRuntime
+# # from .ray_executor import RayDAGExecutor
+# # 供顶层 sage/__init__.py 使用
+# __all__ = ["RayRuntime"]
\ No newline at end of file
diff --git a/sage_jobmanager/factory/function_factory.py b/sage_runtime/function/factory.py
similarity index 94%
rename from sage_jobmanager/factory/function_factory.py
rename to sage_runtime/function/factory.py
index 4f21e43..325e68e 100644
--- a/sage_jobmanager/factory/function_factory.py
+++ b/sage_runtime/function/factory.py
@@ -1,9 +1,9 @@
 from typing import Type, Any, Tuple,TYPE_CHECKING, Union
 from sage_core.function.base_function import BaseFunction
+from sage_utils.custom_logger import CustomLogger
 if TYPE_CHECKING:
     from ray.actor import ActorHandle
     from sage_runtime.runtime_context import RuntimeContext
-    
 class FunctionFactory:
     # 由transformation初始化
     def __init__(
diff --git a/sage_runtime/router/__init__.py b/sage_runtime/io/__init__.py
similarity index 100%
rename from sage_runtime/router/__init__.py
rename to sage_runtime/io/__init__.py
diff --git a/sage_runtime/io/connection.py b/sage_runtime/io/connection.py
new file mode 100644
index 0000000..8fc3d2f
--- /dev/null
+++ b/sage_runtime/io/connection.py
@@ -0,0 +1,129 @@
+from typing import Union, TYPE_CHECKING
+from dataclasses import dataclass
+from enum import Enum
+from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+from sage_runtime.dagnode.local_dag_node import LocalDAGNode
+from sage_runtime.dagnode.ray_dag_node import RayDAGNode
+from ray.actor import ActorHandle
+from sage_runtime.io.local_tcp_server import LocalTcpServer
+class NodeType(Enum):
+    LOCAL = "local"
+    RAY_ACTOR = "ray_actor"
+
+class ConnectionType(Enum):
+    LOCAL_TO_LOCAL = "local_to_local"
+    LOCAL_TO_RAY = "local_to_ray"
+    RAY_TO_LOCAL = "ray_to_local"
+    RAY_TO_RAY = "ray_to_ray"
+
+@dataclass
+class Connection:
+    """
+    用于表示本地节点和Ray Actor之间的连接
+    """
+    def __init__(self,
+                 own_node: Union[ActorHandle, LocalDAGNode],
+                 broadcast_index: int,
+                 parallel_index: int,
+                 target_name: str,
+                 target_node: Union[ActorHandle, LocalDAGNode],
+                 target_input_index: int,
+                 tcp_server: LocalTcpServer):
+
+        self.broadcast_index: int = broadcast_index
+        self.parallel_index: int = parallel_index
+        self.target_name: str = target_name
+        self.target_input_index: int = target_input_index
+        # 统一的节点类型检测
+        self.own_type: NodeType = self._detect_node_type(own_node)
+        self.target_type: NodeType = self._detect_node_type(target_node)
+        
+        # 根据连接类型构建配置
+        self.connection_type: ConnectionType = self._get_connection_type()
+        self.target_config: dict = self._build_target_config(target_node, tcp_server)
+
+    def _detect_node_type(self, node: Union[ActorHandle, LocalDAGNode]) -> NodeType:
+        """
+        统一的节点类型检测方法
+        
+        Args:
+            node: 要检测的节点对象
+            
+        Returns:
+            NodeType: 节点类型枚举
+            
+        Raises:
+            NotImplementedError: 当节点类型未知时
+        """
+        if isinstance(node, LocalDAGNode):
+            return NodeType.LOCAL
+        elif isinstance(node, RayDAGNode):
+            return NodeType.RAY_ACTOR
+        else:
+            raise NotImplementedError(f"未知节点类型: {type(node)}")
+
+    def _get_connection_type(self) -> ConnectionType:
+        """
+        根据源节点和目标节点类型确定连接类型
+        
+        Returns:
+            ConnectionType: 连接类型枚举
+        """
+        if self.own_type == NodeType.LOCAL and self.target_type == NodeType.LOCAL:
+            return ConnectionType.LOCAL_TO_LOCAL
+        elif self.own_type == NodeType.LOCAL and self.target_type == NodeType.RAY_ACTOR:
+            return ConnectionType.LOCAL_TO_RAY
+        elif self.own_type == NodeType.RAY_ACTOR and self.target_type == NodeType.LOCAL:
+            return ConnectionType.RAY_TO_LOCAL
+        elif self.own_type == NodeType.RAY_ACTOR and self.target_type == NodeType.RAY_ACTOR:
+            return ConnectionType.RAY_TO_RAY
+        else:
+            raise NotImplementedError(f"未知连接类型: {self.own_type} → {self.target_type}")
+
+    def _build_target_config(self, target_node: BaseDAGNode, 
+                           tcp_server: LocalTcpServer) -> dict:
+        """
+        根据连接类型构建目标配置字典
+        
+        Args:
+            target_node: 目标节点对象
+            tcp_server: TCP服务器对象
+            
+        Returns:
+            dict: 目标配置字典
+        """
+        if self.connection_type == ConnectionType.LOCAL_TO_LOCAL:
+            # 本地到本地的连接
+            return {
+                "type": "direct_local",
+                "dagnode": target_node,
+                "node_name": self.target_name
+            }
+
+        elif self.connection_type == ConnectionType.LOCAL_TO_RAY:
+            # 本地到Ray Actor的连接
+            return {
+                "type": "actor_handle",
+                "actorhandle": target_node.operator.get_wrapped_operator(),
+                "node_name": self.target_name
+            }
+
+        elif self.connection_type == ConnectionType.RAY_TO_LOCAL:
+            # Ray Actor到本地的连接
+            return {
+                "type": "local_tcp",
+                "node_name": self.target_name,
+                "tcp_host": tcp_server.host,
+                "tcp_port": tcp_server.port
+            }
+
+        elif self.connection_type == ConnectionType.RAY_TO_RAY:
+            # Ray Actor到Ray Actor的连接
+            return {
+                "type": "actor_handle",
+                "actorhandle": target_node.operator.get_wrapped_operator(),
+                "node_name": self.target_name
+            }
+
+        else:
+            raise NotImplementedError(f"未知连接类型: {self.connection_type}")
diff --git a/sage_runtime/utils/local_message_queue.py b/sage_runtime/io/local_message_queue.py
similarity index 85%
rename from sage_runtime/utils/local_message_queue.py
rename to sage_runtime/io/local_message_queue.py
index d8e41cf..9738e7d 100644
--- a/sage_runtime/utils/local_message_queue.py
+++ b/sage_runtime/io/local_message_queue.py
@@ -4,20 +4,30 @@ import threading
 import time
 from collections import deque
 import sys
-from sage_runtime.runtime_context import RuntimeContext
+from sage_utils.custom_logger import CustomLogger
 
 
 class LocalMessageQueue:
 
-    def __init__(self, ctx:RuntimeContext):
+    def __init__(self, name="MessageQueue", max_buffer_size=30000, session_folder: str = None, env_name: str = None):
+        self.name = name
+        self.session_folder = session_folder
         self.queue = queue.Queue(maxsize=50000)
-        self.name = ctx.name
         self.total_task = 0
-        self.max_buffer_size = 30000  # 总内存限制（字节）
+        self.max_buffer_size = max_buffer_size  # 总内存限制（字节）
         self.current_buffer_usage = 0 # 当前使用的内存（字节）
         self.memory_tracker = {}  # 跟踪每个项目的内存大小 {id(item): size}
         # self.task_per_minute = 0
-        self.logger = ctx.logger
+        self.logger = CustomLogger(
+            filename=f"Node_{name}",
+            env_name=env_name,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output = "WARNING",
+            name = f"{name}_LocalMessageQueue"
+        )
+        
+
         self.timestamps = deque()
         self.lock = threading.Lock()
         self.buffer_condition = threading.Condition(self.lock)  # 用于内存空间通知
@@ -164,29 +174,3 @@ class LocalMessageQueue:
             }
 
 
-    def put_nowait(self, item):
-        """
-        非阻塞方式将项目放入队列（等价于 put(item, block=False)）
-        如果队列已满或内存不足，会立即抛出 queue.Full 异常
-        
-        Args:
-            item: 要放入的数据项
-            
-        Raises:
-            queue.Full: 当队列已满或内存限制超出时
-        """
-        return self.put(item, block=False)
-
-    def put_no_wait(self, item):
-        """
-        put_nowait的别名，为了与Ray Queue接口保持一致
-        
-        Args:
-            item: 要放入的数据项
-            
-        Raises:
-            queue.Full: 当队列已满或内存限制超出时
-        """
-        return self.put_nowait(item)
-
-    # ... 现有的 _do_put, get, metrics 方法保持不变 ...
\ No newline at end of file
diff --git a/sage_runtime/io/local_tcp_server.py b/sage_runtime/io/local_tcp_server.py
new file mode 100644
index 0000000..ec5a20d
--- /dev/null
+++ b/sage_runtime/io/local_tcp_server.py
@@ -0,0 +1,235 @@
+import socket
+import threading
+import pickle
+from typing import Dict, Any, Callable, Optional
+from sage_utils.custom_logger import CustomLogger
+
+
+class LocalTcpServer:
+    """
+    本地TCP服务器，用于接收Ray Actor发送的数据
+    """
+    
+    def __init__(self, 
+                 host: str = None, 
+                 port: int = None,
+                 message_handler: Optional[Callable[[Dict[str, Any], tuple], None]] = None):
+        """
+        初始化TCP服务器
+        
+        Args:
+            host: 监听地址
+            port: 监听端口
+            message_handler: 消息处理回调函数，接收 (message, client_address) 参数
+        """
+        self.host = host or self._get_host_ip()  # 确定自己的ip地址
+        self.port = port or self._allocate_tcp_port()  # 分配一个可用的端口
+        self.message_handler = message_handler
+        self.server_socket: Optional[socket.socket] = None
+        self.server_thread: Optional[threading.Thread] = None
+        self.running = False
+        
+        self.logger = CustomLogger(
+            filename="LocalTcpServer",
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="WARNING"
+        )
+    
+        self.logger.info(f"Initializing LocalTcpServer on {self.host}:{self.port}")
+
+
+    def _get_host_ip(self):
+        """自动获取本机可用于外部连接的 IP 地址"""
+        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+        try:
+            # 连接到任意公网地址（不必可达，只为取出绑定IP）
+            s.connect(("8.8.8.8", 80))
+            ip = s.getsockname()[0]
+        except Exception:
+            self.logger.warning("Failed to get external IP, using localhost")
+            ip = "127.0.0.1"
+        finally:
+            s.close()
+        return ip
+    
+    def _allocate_tcp_port(self) -> int:
+        """为 DAG 分配可用的 TCP 端口"""
+        import socket
+        
+        # 尝试从预设范围分配端口
+        for port in range(19000, 20000):  # DAG 专用端口范围
+            try:
+                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+                    s.bind((self.host, port))
+                    return port
+            except OSError:
+                continue
+        # 如果预设范围都被占用，使用系统分配
+        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+            self.logger.warning("All predefined ports are occupied, using system-assigned port")
+            s.bind((self.host, 0))
+            return s.getsockname()[1]
+        
+
+    def set_message_handler(self, handler: Callable[[Dict[str, Any], tuple], None]):
+        """设置消息处理器"""
+        self.message_handler = handler
+    
+    def start(self):
+        """启动TCP服务器"""
+        if self.running:
+            self.logger.warning("TCP server is already running")
+            return
+        
+        try:
+            self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+            self.server_socket.settimeout(5)  # 设置超时，避免阻塞
+            self.server_socket.bind((self.host, self.port))
+            self.server_socket.listen(10)
+            
+            self.running = True
+            self.server_thread = threading.Thread(
+                target=self._server_loop,
+                name="LocalTcpServerThread"
+            )
+            self.server_thread.daemon = True
+            self.server_thread.start()
+            
+            self.logger.info(f"TCP server started on {self.host}:{self.port}")
+            
+        except Exception as e:
+            self.logger.error(f"Failed to start TCP server: {e}")
+            self.running = False
+            raise
+    
+    def stop(self):
+        """停止TCP服务器"""
+        if not self.running:
+            return
+        
+        self.logger.info("Stopping TCP server...")
+        self.running = False
+        
+        if self.server_socket:
+            self.server_socket.close()
+        
+        if self.server_thread and self.server_thread.is_alive():
+            for _ in range(5):  # 最多等5秒
+                self.server_thread.join(timeout=1.0)
+                if not self.server_thread.is_alive():
+                    break
+            else:
+                self.logger.warning("TCP server thread did not stop gracefully")
+        
+        self.logger.info("TCP server stopped")
+    
+    def _server_loop(self):
+        """TCP服务器主循环"""
+        self.logger.debug("TCP server loop started")
+        
+        while self.running:
+            try:
+                if not self.server_socket:
+                    break
+                    
+                client_socket, address = self.server_socket.accept()
+                self.logger.debug(f"New TCP client connected from {address}")
+                
+                # 在新线程中处理客户端
+                client_thread = threading.Thread(
+                    target=self._handle_client,
+                    args=(client_socket, address),
+                    name=f"TcpClient-{address[0]}:{address[1]}"
+                )
+                client_thread.daemon = True
+                client_thread.start()
+            except socket.timeout:
+                # 超时不处理，继续循环
+                continue
+            except OSError as e:
+                # Socket被关闭时会抛出OSError
+                if self.running:
+                    self.logger.error(f"Error accepting TCP connection: {e}")
+                break
+            except Exception as e:
+                if self.running:
+                    self.logger.error(f"Unexpected error in server loop: {e}")
+        
+        self.logger.debug("TCP server loop stopped")
+    
+    def _handle_client(self, client_socket: socket.socket, address: tuple):
+        """处理TCP客户端连接和消息"""
+        try:
+            while self.running:
+                # 读取消息长度
+                size_data = client_socket.recv(4)
+                if not size_data:
+                    break
+                
+                message_size = int.from_bytes(size_data, byteorder='big')
+                if message_size <= 0 or message_size > 10 * 1024 * 1024:  # 10MB limit
+                    self.logger.warning(f"Invalid message size {message_size} from {address}")
+                    break
+                
+                # 读取消息内容
+                message_data = self._receive_full_message(client_socket, message_size)
+                if not message_data:
+                    break
+                
+                # 反序列化并处理消息
+                try:
+                    message = pickle.loads(message_data)
+                    self._process_message(message, address)
+                except Exception as e:
+                    self.logger.error(f"Error processing message from {address}: {e}")
+                
+        except Exception as e:
+            self.logger.error(f"Error handling TCP client {address}: {e}")
+        finally:
+            try:
+                client_socket.close()
+            except:
+                pass
+            self.logger.debug(f"TCP client {address} disconnected")
+    
+    def _receive_full_message(self, client_socket: socket.socket, message_size: int) -> Optional[bytes]:
+        """接收完整的消息数据"""
+        message_data = b''
+        while len(message_data) < message_size:
+            chunk_size = min(message_size - len(message_data), 8192)  # 8KB chunks
+            chunk = client_socket.recv(chunk_size)
+            if not chunk:
+                self.logger.warning("Connection closed while receiving message")
+                return None
+            message_data += chunk
+        
+        return message_data
+    
+    def _process_message(self, message: Dict[str, Any], client_address: tuple):
+        """处理接收到的消息"""
+        try:
+            if self.message_handler:
+                self.message_handler(message, client_address)
+            else:
+                self.logger.warning(f"No message handler set, ignoring message from {client_address}")
+                
+        except Exception as e:
+            self.logger.error(f"Error in message handler: {e}", exc_info=True)
+    
+    def get_server_info(self) -> Dict[str, Any]:
+        """获取服务器信息"""
+        return {
+            "host": self.host,
+            "port": self.port,
+            "running": self.running,
+            "address": f"{self.host}:{self.port}"
+        }
+    
+    def __del__(self):
+        """析构函数，确保资源清理"""
+        try:
+            self.stop()
+        except:
+            pass
\ No newline at end of file
diff --git a/sage_runtime/router/packet.py b/sage_runtime/io/packet.py
similarity index 100%
rename from sage_runtime/router/packet.py
rename to sage_runtime/io/packet.py
diff --git a/sage_runtime/io/unified_emit_context.py b/sage_runtime/io/unified_emit_context.py
new file mode 100644
index 0000000..730d513
--- /dev/null
+++ b/sage_runtime/io/unified_emit_context.py
@@ -0,0 +1,195 @@
+from typing import Any , TYPE_CHECKING
+from ray.actor import ActorHandle
+import socket
+import pickle
+import threading
+import time
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.io.connection import Connection, ConnectionType
+from sage_runtime.io.packet import Packet
+
+if TYPE_CHECKING:
+    pass
+
+class UnifiedEmitContext:
+    """
+    统一的Emit Context，支持所有类型的连接
+    根据Connection对象中的配置自动选择合适的发送方式
+    """
+    
+    def __init__(self, session_folder: str = None, name: str = None,env_name = None, **kwargs):
+        self.logger = CustomLogger(
+            filename=f"Node_{name}",
+            env_name=env_name,
+            session_folder=session_folder,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="WARNING",
+            name=f"{name}_UnifiedEmitContext"
+        )
+        
+
+        self.name = name
+        
+        # TCP连接管理（用于Ray到Local的连接）
+        self._tcp_connections: dict = {}  # host:port -> socket
+        self._socket_lock = threading.Lock()
+
+    def send_packet_direct(self, connection: 'Connection', packet: 'Packet') -> None:
+        """
+        直接发送已封装的packet，不再重新封装
+        
+        Args:
+            connection: Connection对象
+            packet: 已封装好的Packet对象
+        """
+        try:
+            connection_type = connection.connection_type
+            if connection_type == ConnectionType.LOCAL_TO_LOCAL:
+                self._send_local_to_local(connection, packet)
+            elif connection_type == ConnectionType.LOCAL_TO_RAY:
+                self._send_local_to_ray(connection, packet)
+            elif connection_type == ConnectionType.RAY_TO_LOCAL:
+                self._send_ray_to_local(connection, packet)
+            elif connection_type == ConnectionType.RAY_TO_RAY:
+                self._send_ray_to_ray(connection, packet)
+            else:
+                raise ValueError(f"Unknown connection type: {connection_type}")
+        except Exception as e:
+            self.logger.error(f"Failed to send packet via connection {connection}: {e}", exc_info=True)
+
+    def _send_local_to_local(self, connection: 'Connection', packet: 'Packet') -> None:
+        """本地到本地：直接调用put方法"""
+        try:
+            target_node = connection.target_config["dagnode"]
+            
+            # 发送到目标节点的输入缓冲区
+            if hasattr(target_node, 'put'):
+                target_node.put(packet)
+            elif hasattr(target_node, 'input_buffer'):
+                target_node.input_buffer.put(packet)
+            else:
+                raise AttributeError(f"Local node {connection.target_name} has no put method or input_buffer")
+                
+            self.logger.debug(f"Sent local->local: {connection.target_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Error in local->local send: {e}")
+            raise
+
+    def _send_local_to_ray(self, connection: 'Connection', packet: 'Packet') -> None:
+        """本地到Ray Actor：远程调用"""
+        try:
+            actor_handle = connection.target_config["actorhandle"]
+            
+            if not isinstance(actor_handle, ActorHandle):
+                raise TypeError(f"Expected ActorHandle, got {type(actor_handle)}")
+            
+            # 调用Ray Actor的process_data方法
+            actor_handle.receive_packet.remote(packet)
+            
+            self.logger.debug(f"Sent local->ray: {connection.target_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Error in local->ray send: {e}")
+            raise
+
+    def _send_ray_to_local(self, connection: 'Connection', packet: 'Packet') -> None:
+        """Ray Actor到本地：TCP连接"""
+        try:
+            tcp_host = connection.target_config["tcp_host"]
+            tcp_port = connection.target_config["tcp_port"]
+            target_node_name = connection.target_config["node_name"]
+            
+            # 构造TCP消息包
+            message = {
+                "type": "ray_to_local",
+                "source_actor": self.name,
+                "target_node": target_node_name,
+                "data": packet,
+                "timestamp": time.time_ns()
+            }
+            
+            # 获取TCP连接并发送
+            tcp_connection = self._get_tcp_connection(tcp_host, tcp_port)
+            serialized_data = pickle.dumps(message)
+            message_size = len(serialized_data)
+            
+            tcp_connection.sendall(message_size.to_bytes(4, byteorder='big'))
+            tcp_connection.sendall(serialized_data)
+            
+            self.logger.debug(f"Sent ray->local via TCP: {target_node_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Error in ray->local TCP send: {e}")
+            # 重置TCP连接
+            self._reset_tcp_connection(connection.target_config["tcp_host"], 
+                                     connection.target_config["tcp_port"])
+            raise
+
+    def _send_ray_to_ray(self, connection: 'Connection', packet: 'Packet') -> None:
+        """Ray Actor到Ray Actor：远程调用"""
+        try:
+            actor_handle = connection.target_config["actorhandle"]
+            
+            if not isinstance(actor_handle, ActorHandle):
+                raise TypeError(f"Expected ActorHandle, got {type(actor_handle)}")
+            
+            # 调用目标Ray Actor的process_data方法
+            actor_handle.receive_packet.remote(packet)
+            
+            self.logger.debug(f"Sent ray->ray: {connection.target_name}")
+            
+        except Exception as e:
+            self.logger.error(f"Error in ray->ray send: {e}")
+            raise
+
+    def _get_tcp_connection(self, host: str, port: int) -> socket.socket:
+        """获取TCP连接（懒加载和重用）"""
+        connection_key = f"{host}:{port}"
+        
+        if connection_key not in self._tcp_connections:
+            with self._socket_lock:
+                if connection_key not in self._tcp_connections:
+                    try:
+                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+                        sock.connect((host, port))
+                        self._tcp_connections[connection_key] = sock
+                        self.logger.info(f"Established TCP connection to {host}:{port}")
+                    except Exception as e:
+                        self.logger.error(f"Failed to connect to TCP server {host}:{port}: {e}")
+                        raise
+        
+        return self._tcp_connections[connection_key]
+
+    def _reset_tcp_connection(self, host: str, port: int):
+        """重置指定的TCP连接"""
+        connection_key = f"{host}:{port}"
+        with self._socket_lock:
+            if connection_key in self._tcp_connections:
+                try:
+                    self._tcp_connections[connection_key].close()
+                except:
+                    pass  # 忽略关闭错误
+                del self._tcp_connections[connection_key]
+                self.logger.debug(f"Reset TCP connection to {host}:{port}")
+
+    def close(self):
+        """关闭所有TCP连接"""
+        with self._socket_lock:
+            for connection_key, sock in self._tcp_connections.items():
+                try:
+                    sock.close()
+                    self.logger.debug(f"Closed TCP connection: {connection_key}")
+                except:
+                    pass  # 忽略关闭错误
+            self._tcp_connections.clear()
+            self.logger.info("Closed all TCP connections")
+
+    def get_connection_stats(self) -> dict:
+        """获取连接统计信息"""
+        with self._socket_lock:
+            return {
+                "active_tcp_connections": len(self._tcp_connections),
+                "tcp_endpoints": list(self._tcp_connections.keys())
+            }
\ No newline at end of file
diff --git a/sage_runtime/local_thread_pool.py b/sage_runtime/local_thread_pool.py
new file mode 100644
index 0000000..9d80769
--- /dev/null
+++ b/sage_runtime/local_thread_pool.py
@@ -0,0 +1,106 @@
+import os
+import threading
+import logging
+from typing import Dict, Optional, Any, List
+from concurrent.futures import ThreadPoolExecutor
+from sage_runtime.dagnode.local_dag_node import LocalDAGNode
+from sage_runtime.io.local_tcp_server import LocalTcpServer
+from sage_utils.custom_logger import CustomLogger
+import time
+import socket
+class LocalThreadPool:
+    _instance = None
+    _lock = threading.Lock()
+
+    def __init__(self):
+        if hasattr(self, "_initialized"):
+            return
+
+        self.logger = CustomLogger(
+            filename="LocalThreadPool",
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="WARNING",
+        )
+
+        self._initialized = True
+        self.name = "LocalThreadPool"
+        self.logger.debug(f"CPU count is {os.cpu_count()}")
+        self.thread_pool = ThreadPoolExecutor(
+            max_workers=os.cpu_count() * 3,
+            thread_name_prefix=None,
+            initializer=None,
+            initargs=None
+        )
+        # 节点管理
+        # self.running_nodes: Dict[str, LocalDAGNode] = {}  # 正在运行的节点表
+        # self.handle_to_node: Dict[str, LocalDAGNode] = {}  # handle到节点的映射
+
+
+            
+    @classmethod
+    def get_instance(cls):
+        """获取LocalRuntime的唯一实例"""
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # 绕过 __new__ 的异常，直接创建实例
+                    instance = super().__new__(cls)
+                    instance.__init__()
+                    cls._instance = instance
+        return cls._instance
+
+
+
+    
+    def submit_node(self, node: LocalDAGNode) -> str:
+        self.logger.info(f"Submitting node '{node.name}' to {self.name}")
+        try:
+            future=self.thread_pool.submit(node.run_loop)
+        except Exception as e:
+            self.logger.error(f"Failed to submit node '{node.name}': {e}", exc_info=True)
+
+    
+    def shutdown(self):
+        """关闭运行时和所有资源"""
+        self.logger.info("Shutting down LocalThreadPool...")
+        
+        # 停止所有节点
+        self.thread_pool.shutdown(wait=True, cancel_futures=True)
+        self.__class__._instance = None  # 清除实例引用
+        # # 关闭TCP服务器
+        # if self.tcp_server:
+        #     self.tcp_server.stop()
+        
+        self.logger.info("LocalThreadPool shutdown completed")
+    
+
+
+
+    
+    ########################################################
+    #                inactive methods                      #
+    ########################################################
+
+    @classmethod
+    def reset_instance(cls):
+        """重置实例（主要用于测试）"""
+        with cls._lock:
+            if cls._instance:
+                cls._instance.shutdown()
+                cls._instance = None
+
+    ########################################################
+    #                auxiliary methods                     #
+    ########################################################
+
+    def __new__(cls, *args, **kwargs):
+        # 禁止直接实例化
+        raise RuntimeError("请通过 get_instance() 方法获取实例")
+    
+    def __del__(self):
+        """析构函数，确保资源清理"""
+        try:
+            self.shutdown()
+        except:
+            pass
\ No newline at end of file
diff --git a/sage_runtime/utils/__init__.py b/sage_runtime/metrics/__init__.py
similarity index 100%
rename from sage_runtime/utils/__init__.py
rename to sage_runtime/metrics/__init__.py
diff --git a/sage_runtime/mixed_dag.py b/sage_runtime/mixed_dag.py
new file mode 100644
index 0000000..782f371
--- /dev/null
+++ b/sage_runtime/mixed_dag.py
@@ -0,0 +1,236 @@
+from typing import Dict, List, Any, Tuple, Union, TYPE_CHECKING
+from ray.actor import ActorHandle
+
+from sage_runtime.local_thread_pool import LocalThreadPool
+from sage_runtime.runtime_context import RuntimeContext
+from sage_runtime.dagnode.local_dag_node import LocalDAGNode
+from sage_runtime.dagnode.ray_dag_node import RayDAGNode
+from sage_runtime.dagnode.base_dag_node import BaseDAGNode
+from sage_runtime.io.local_tcp_server import LocalTcpServer
+from sage_runtime.io.connection import Connection
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.compiler import Compiler, GraphNode
+
+if TYPE_CHECKING:
+    from sage_core.api.env import BaseEnvironment 
+
+
+class MixedDAG():
+    def __init__(self, graph: Compiler, env:'BaseEnvironment'):
+        self.graph = graph
+        self.name:str = graph.name
+        self.logger = CustomLogger(
+            filename=f"MixedDAG_{self.name}",
+            env_name= env.name,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output = "DEBUG",
+        )
+        # self.nodes: Dict[str, Union[ActorHandle, LocalDAGNode]] = {}
+        self.nodes: Dict[str, BaseDAGNode] = {}
+
+        self.spout_nodes: Dict[str, BaseDAGNode] = {}
+
+        self.connections: List[Connection] = []
+
+        self.is_running: bool = False
+
+        # 为这个 DAG 分配独立的 TCP 端口
+        self.tcp_server = LocalTcpServer(
+            # host="localhost",
+            # port=self.tcp_port,
+            message_handler=self._handle_tcp_message
+        )
+        self.tcp_server.start()
+
+        self._compile_graph(graph, env)
+        # 启动 TCP 服务器
+        self.logger.info(f"MixedDAG '{self.name}' construction complete")
+    
+
+        
+
+    
+    def _compile_graph(self, graph: Compiler, env:'BaseEnvironment'):
+        """编译图结构，创建节点并建立连接"""
+        self.logger.info(f"Compiling mixed DAG for graph: {self.name}")
+        
+        # 第一步：创建所有节点实例
+        for node_name, graph_node in graph.nodes.items():
+            # node_instance = graph_node.create_dag_node()
+            node_instance = graph_node.transformation.dag_node_factory.create_node(graph_node.name, graph_node.runtime_context)
+            self.nodes[node_name] = node_instance
+            if graph_node.is_spout:
+                self.spout_nodes[node_name] = node_instance
+
+            self.logger.debug(f"Added node '{node_name}' of type '{node_instance.__class__.__name__}'")
+        
+        # 第二步：建立节点间的连接
+        for node_name, graph_node in graph.nodes.items():
+            self._setup_node_connections(node_name, graph_node)
+        
+        self.logger.info(f"Mixed DAG compilation completed: {len(self.nodes)} nodes, "f"{len(self.spout_nodes)} spout nodes")
+
+
+
+
+    def _setup_node_connections(self, node_name: str, graph_node: GraphNode):
+        """
+        为节点设置下游连接
+        
+        Args:
+            node_name: 节点名称
+            graph_node: 图节点对象
+        """
+        output_handle = self.nodes[node_name]
+        
+        for broadcast_index, parallel_edges in enumerate(graph_node.output_channels):
+            for parallel_index, parallel_edge in enumerate(parallel_edges):
+                target_name = parallel_edge.downstream_node.name
+                target_input_index = parallel_edge.input_index
+                target_handle = self.nodes[target_name]
+
+                connection = Connection(
+                    own_node=output_handle,
+                    broadcast_index=broadcast_index,
+                    parallel_index=parallel_index,
+                    target_name=target_name,
+                    target_node=target_handle,
+                    target_input_index = target_input_index,
+                    tcp_server=self.tcp_server
+                )
+                try:
+                    if isinstance(output_handle, ActorHandle):
+                        
+                        output_handle.add_connection.remote(connection)
+                        self.logger.debug(f"Setup Ray connection: {node_name} -> {target_name}")
+                    else:
+                        # 本地节点直接调用
+                        output_handle.add_connection(connection)
+                        self.logger.debug(f"Setup local connection: {node_name} -> {target_name}")
+                        
+                    # 记录连接信息
+                    self.connections.append(connection)
+                    
+                except Exception as e:
+                    self.logger.error(f"Error setting up connection {node_name} -> {target_name}: {e}", exc_info=True)
+
+    def stop(self):
+        if(not self.is_running):
+            self.logger.warning(f"MixedDAG '{self.name}' is not running, nothing to stop")
+            return
+        
+        for node_name, node_instance in self.spout_nodes.items():
+            node_instance.stop()
+            self.logger.debug(f"Stopped spout node: {node_name}")
+        self.logger.info(f"Stopped all spout nodes in MixedDAG '{self.name}'")
+
+    def close(self):
+        """停止所有节点"""
+        self.logger.info("Stopping all DAG nodes...")
+        self.tcp_server.stop()
+        for node_name, node in self.nodes.items():
+            try:
+                node.stop()
+                self.logger.debug(f"Stopped node: {node_name}")
+                # Ray actors会在进程结束时自动清理
+            except Exception as e:
+                self.logger.error(f"Error stopping node {node_name}: {e}")
+    
+    def submit(self):
+        self.logger.info(f"Submitting MixedDAG '{self.name}'")
+        try:
+            for node_name, node in self.nodes.items():
+                if node.is_spout is False:
+                    local_runtime = LocalThreadPool.get_instance()
+                    local_runtime.submit_node(node)
+        except Exception as e:
+            self.logger.error(f"Failed to submit MixedDAG '{self.name}': {e}", exc_info=True)
+
+    def execute_once(self, spout_node_name:str = None):
+        self.logger.info(f"executing once")
+        if(spout_node_name is None):
+            for node_name, node_instance in self.nodes.items():
+                # print(f"triggering spout node: {node_name}, is_spout:{node_instance.is_spout}")
+                if(node_instance.is_spout):
+                    node_instance.trigger()
+                    self.logger.debug(f"triggering spout node: {node_name}")
+
+        elif self.spout_nodes.get(spout_node_name, None) is not None:
+            node = self.spout_nodes[spout_node_name]
+            self.logger.debug(f"Running spout node: {node_name}")
+            node.trigger()
+        else:
+            self.logger.warning(f"Spout node '{spout_node_name}' not found in MixedDAG '{self.name}'")
+
+    def execute_streaming(self, spout_node_name:str = None):
+        self.logger.info(f"executing streaming")
+        self.is_running = True
+        if(spout_node_name is None):
+            for node_name, node_instance in self.spout_nodes.items():
+                local_runtime = LocalThreadPool.get_instance()
+                local_runtime.submit_node(node_instance)
+                self.logger.debug(f"Running spout node: {node_name}")
+        elif self.spout_nodes.get(spout_node_name, None) is not None:
+            node_handle = self.nodes[spout_node_name]
+            local_runtime = LocalThreadPool.get_instance()
+            local_runtime.submit_node(node_handle)
+            self.logger.debug(f"Running spout node: {node_name}")
+        else:
+            self.logger.warning(f"Spout node '{spout_node_name}' not found in MixedDAG '{self.name}'")
+
+
+    def _handle_tcp_message(self, message: Dict[str, Any], client_address: tuple):
+        """
+        处理来自 Ray Actor 的 TCP 消息
+        由于是 DAG 专用端口，所有消息都属于当前 DAG
+        """
+        try:
+            message_type = message.get("type")
+            
+            if message_type == "ray_to_local":
+                # Ray Actor 发送给本地节点的数据
+                target_node_name = message["target_node"]
+                data = message["data"]
+                source_actor = message.get("source_actor", "unknown")
+                
+                # 查找目标节点（在当前 DAG 中）
+                if target_node_name in self.nodes:
+                    target_node = self.nodes[target_node_name]
+                    
+                    # 确保是本地节点
+                    if isinstance(target_node, LocalDAGNode):
+                        # 将数据放入目标节点的输入缓冲区
+                        target_node.put(data)
+                        
+                        self.logger.debug(f"[DAG {self.name}] Delivered TCP message: {source_actor} -> "
+                                        f"{target_node_name}")
+                    else:
+                        self.logger.warning(f"Target node '{target_node_name}' is not a local node")
+                else:
+                    self.logger.warning(f"Target node '{target_node_name}' not found in DAG '{self.name}'")
+            else:
+                self.logger.warning(f"Unknown TCP message type: {message_type} from {client_address}")
+                
+        except Exception as e:
+            self.logger.error(f"Error processing TCP message from {client_address}: {e}", exc_info=True)
+
+
+    def _detect_platform(self, executor: Any) -> str:
+        """
+        检测执行器的平台类型
+        
+        Args:
+            executor: 执行器对象
+            
+        Returns:
+            平台类型字符串
+        """
+        if isinstance(executor, ActorHandle):
+            return "remote"
+        elif hasattr(executor, 'remote'):
+            return "ray_function" 
+        elif isinstance(executor, LocalDAGNode):
+            return "local"
+        else:
+            return "unknown"
diff --git a/sage_runtime/operator/__init__.py b/sage_runtime/operator/__init__.py
new file mode 100644
index 0000000..0ff4321
--- /dev/null
+++ b/sage_runtime/operator/__init__.py
@@ -0,0 +1,4 @@
+# from .ray_runtime import RayRuntime
+# # from .ray_executor import RayDAGExecutor
+# # 供顶层 sage/__init__.py 使用
+# __all__ = ["RayRuntime"]
\ No newline at end of file
diff --git a/sage_runtime/operator/factory.py b/sage_runtime/operator/factory.py
new file mode 100644
index 0000000..7caebe7
--- /dev/null
+++ b/sage_runtime/operator/factory.py
@@ -0,0 +1,84 @@
+from typing import List, Type, Union, Tuple, Dict, Set, TYPE_CHECKING, Any, Optional
+from sage_utils.name_server import get_name
+from sage_utils.custom_logger import CustomLogger
+from sage_runtime.operator.operator_wrapper import OperatorWrapper
+if TYPE_CHECKING:
+    from sage_core.operator.base_operator import BaseOperator
+    from sage_core.function.base_function import BaseFunction
+    from ray.actor import ActorHandle
+    from sage_runtime.function.factory import FunctionFactory
+    from sage_runtime.runtime_context import RuntimeContext
+import ray
+
+class OperatorFactory:
+    # 由transformation初始化
+    def __init__(self, 
+                 operator_class: Type['BaseOperator'],
+                 function_factory: 'FunctionFactory',
+                 basename: str = None,
+                 env_name:str = None,
+                 remote:bool = False,
+                 **operator_kwargs):
+        self.operator_class = operator_class
+        self.operator_kwargs = operator_kwargs  # 保存额外的operator参数
+        self.function_factory = function_factory
+        self.env_name = env_name
+        self.basename = get_name(basename) or get_name(self.function_factory.function_class.__name__)
+        self.remote = remote
+
+    def create_operator(self, 
+                       name: str,
+                       runtime_context: 'RuntimeContext') -> 'OperatorWrapper':
+        """
+        创建operator实例
+        
+        Args:
+            session_folder: 会话文件夹
+            name: 节点名称
+            **additional_kwargs: 额外的关键字参数
+            
+        Returns:
+            BaseOperator: 创建的operator实例
+        """
+        # 创建logger用于调试
+        logger = CustomLogger(
+            filename=f"OperatorFactory_{name}",
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="WARNING",
+            name=f"OperatorFactory_{name}",
+            env_name = self.env_name
+        )
+        
+        try:
+            if self.remote:
+                Operator_class = ray.remote(self.operator_class)
+                operator_instance = Operator_class.remote(
+                    self.function_factory,
+                    runtime_context,
+                    **self.operator_kwargs
+                )
+                logger.debug(f"Building Ray Actor operator instance: {self.operator_class.__name__}")
+            else:
+                Operator_class = self.operator_class
+                operator_instance = Operator_class(
+                    self.function_factory,
+                    runtime_context,
+                    **self.operator_kwargs
+                )
+                logger.debug(f"Building local operator instance: {self.operator_class.__name__}")
+
+
+            # 用OperatorWrapper包装
+            wrapped_operator = OperatorWrapper(operator_instance, name, self.env_name)
+            logger.debug(f"Wrapped operator with OperatorWrapper")
+            
+            return wrapped_operator
+            
+        except Exception as e:
+            logger.error(f"Failed to create operator: {e}")
+            raise
+            
+        except Exception as e:
+            logger.error(f"Failed to create operator: {e}")
+            raise
\ No newline at end of file
diff --git a/sage_runtime/operator/operator_wrapper.py b/sage_runtime/operator/operator_wrapper.py
new file mode 100644
index 0000000..044784e
--- /dev/null
+++ b/sage_runtime/operator/operator_wrapper.py
@@ -0,0 +1,231 @@
+import ray
+import asyncio
+import concurrent.futures
+from typing import Any, Union
+import logging
+from ray.actor import ActorHandle
+from sage_utils.custom_logger import CustomLogger
+
+class OperatorWrapper:
+    """透明的 Operator 包装器，自动适配本地 Operator 和 Ray Actor Operator"""
+
+    def __init__(self, operator: Union[Any, ActorHandle], name:str, env_name:str = None):
+        # 使用 __dict__ 直接设置，避免触发 __setattr__
+        object.__setattr__(self, '_operator', operator)
+        object.__setattr__(self, '_execution_mode', self._detect_execution_mode())
+        object.__setattr__(self, '_method_cache', {})
+        object.__setattr__(self, '_attribute_cache', {})
+        
+        # 初始化 logger
+        logger = CustomLogger(
+            filename=f"Node_{name}",
+            env_name=env_name,
+            console_output="WARNING",
+            file_output="DEBUG",
+            global_output="DEBUG",
+            name=f"{name}_OperatorWrapper"
+        )
+        
+
+        
+        object.__setattr__(self, 'logger', logger)
+        self.logger.debug(f"Created OperatorWrapper for {type(operator).__name__} in {self._execution_mode} mode")
+        
+    def _detect_execution_mode(self) -> str:
+        """检测执行模式"""
+        try:
+            # 检查是否是Ray Actor
+            if isinstance(self._operator, ray.actor.ActorHandle):
+                return "ray_actor"
+        except (ImportError, AttributeError):
+            # 如果Ray不可用，忽略
+            pass
+
+        # 默认作为本地对象处理
+        return "local"
+
+    def __getattr__(self, name: str):
+        """透明代理属性访问"""
+        # 防止循环引用 - 检查是否访问内部属性
+        if name.startswith('_'):
+            raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
+        
+        # 防止访问 _operator 时的循环引用
+        if '_operator' not in self.__dict__:
+            raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
+        
+        # 缓存查找
+        if hasattr(self, '_attribute_cache') and name in self._attribute_cache:
+            return self._attribute_cache[name]
+
+        # 获取原始属性
+        try:
+            if self._execution_mode == "ray_actor":
+                # 对于 Ray Actor，直接获取属性（不调用 .remote()）
+                original_attr = getattr(self._operator, name)
+            else:
+                # 对于本地对象，正常获取属性
+                original_attr = getattr(self._operator, name)
+        except AttributeError:
+            raise AttributeError(f"'{type(self._operator).__name__}' object has no attribute '{name}'")
+
+        # 处理方法调用
+        if callable(original_attr):
+            # 创建统一调用方式
+            wrapped_method = self._create_unified_method(name, original_attr)
+            if hasattr(self, '_attribute_cache'):
+                self._attribute_cache[name] = wrapped_method
+            return wrapped_method
+
+        # 普通属性
+        if hasattr(self, '_attribute_cache'):
+            self._attribute_cache[name] = original_attr
+        return original_attr
+
+    def __setattr__(self, name: str, value: Any):
+        """代理属性设置"""
+        if name.startswith('_'):
+            # 内部属性直接设置
+            object.__setattr__(self, name, value)
+        else:
+            # 外部属性设置到原 operator
+            if self._execution_mode == "ray_actor":
+                # Ray Actor 属性设置需要通过远程调用
+                # 注意：这可能需要在 operator 中实现 set_attribute 方法
+                self.logger.warning(f"Setting attribute '{name}' on Ray Actor - this may not work as expected")
+                setattr(self._operator, name, value)
+            else:
+                setattr(self._operator, name, value)
+
+    def __dir__(self):
+        """代理dir()调用"""
+        if self._execution_mode == "ray_actor":
+            # Ray Actor 的 dir() 可能不完整，返回常见的 operator 方法
+            return ['receive_packet', 'start_spout_loop', 'stop_actor', 'get_actor_status']
+        else:
+            return dir(self._operator)
+
+    def __repr__(self):
+        """代理repr()"""
+        mode_info = f"[{self._execution_mode}]"
+        return f"OperatorWrapper{mode_info}({repr(self._operator)})"
+
+    def __str__(self):
+        """代理str()"""
+        return f"OperatorWrapper({str(self._operator)})"
+
+    def _create_unified_method(self, method_name: str, original_method):
+        """创建统一的方法包装 - 对外始终提供同步接口"""
+        
+        if self._execution_mode == "ray_actor":
+            # Ray Actor处理
+            def ray_actor_wrapper(*args, **kwargs):
+                try:
+                    self.logger.debug(f"Calling Ray Actor method '{method_name}' with args={args}, kwargs={kwargs}")
+                    
+                    # 执行远程调用，original_method 已经是 remote 方法
+                    future = original_method.remote(*args, **kwargs)
+                    
+                    # 同步获取结果
+                    result = ray.get(future)
+                    self.logger.debug(f"Ray Actor method '{method_name}' completed successfully")
+                    return result
+                    
+                except Exception as e:
+                    self.logger.error(f"Ray Actor method '{method_name}' failed: {str(e)}")
+                    raise RuntimeError(f"Ray Actor method '{method_name}' failed: {str(e)}")
+
+            return ray_actor_wrapper
+
+        else:
+            # 本地方法处理
+            if asyncio.iscoroutinefunction(original_method):
+                # 异步方法转同步
+                def async_wrapper(*args, **kwargs):
+                    try:
+                        self.logger.debug(f"Calling local async method '{method_name}'")
+                        
+                        # 检查事件循环
+                        try:
+                            loop = asyncio.get_running_loop()
+                            # 在独立线程中执行异步任务
+                            with concurrent.futures.ThreadPoolExecutor() as executor:
+                                result = executor.submit(
+                                    asyncio.run,
+                                    original_method(*args, **kwargs)
+                                ).result()
+                            return result
+                        except RuntimeError:
+                            # 直接运行异步方法
+                            return asyncio.run(original_method(*args, **kwargs))
+                    except Exception as e:
+                        self.logger.error(f"Local async method '{method_name}' failed: {str(e)}")
+                        raise RuntimeError(f"Local async method '{method_name}' failed: {str(e)}")
+
+                return async_wrapper
+            else:
+                # 同步方法直接返回
+                def sync_wrapper(*args, **kwargs):
+                    try:
+                        self.logger.debug(f"Calling local sync method '{method_name}'")
+                        result = original_method(*args, **kwargs)
+                        self.logger.debug(f"Local sync method '{method_name}' completed successfully")
+                        return result
+                    except Exception as e:
+                        self.logger.error(f"Local sync method '{method_name}' failed: {str(e)}")
+                        raise
+
+                return sync_wrapper
+
+    # 添加一些 operator 特有的便利方法
+    def is_ray_actor(self) -> bool:
+        """检查是否为 Ray Actor"""
+        return self._execution_mode == "ray_actor"
+
+    def is_local(self) -> bool:
+        """检查是否为本地 operator"""
+        return self._execution_mode == "local"
+
+    def get_execution_mode(self) -> str:
+        """获取执行模式"""
+        return self._execution_mode
+
+    def get_wrapped_operator(self):
+        """获取被包装的原始 operator（谨慎使用）"""
+        return self._operator
+
+    # # 为常见的 operator 方法提供类型提示和文档
+    # def receive_packet(self, channel: str, data: Any) -> Any:
+    #     """
+    #     处理数据的统一接口
+        
+    #     Args:
+    #         channel: 输入通道名称
+    #         data: 输入数据
+            
+    #     Returns:
+    #         处理后的数据
+    #     """
+    #     # 这个方法会被 __getattr__ 拦截并返回包装后的方法
+    #     pass
+
+    # def start_spout_loop(self):
+    #     """
+    #     启动 spout 循环（仅适用于 spout operator）
+    #     """
+    #     # 这个方法会被 __getattr__ 拦截并返回包装后的方法
+    #     pass
+
+    # def stop_actor(self):
+    #     """
+    #     停止 operator（适用于 Ray Actor）
+    #     """
+    #     # 这个方法会被 __getattr__ 拦截并返回包装后的方法
+    #     pass
+
+    # def get_actor_status(self) -> dict:
+    #     """
+    #     获取 operator 状态
+    #     """
+    #     # 这个方法会被 __getattr__ 拦截并返回包装后的方法
+    #     pass
\ No newline at end of file
diff --git a/sage_runtime/router/base_router.py b/sage_runtime/router/base_router.py
deleted file mode 100644
index 3501f1d..0000000
--- a/sage_runtime/router/base_router.py
+++ /dev/null
@@ -1,276 +0,0 @@
-# sage_runtime/base_router.py
-
-from abc import ABC, abstractmethod
-from typing import Dict, Any, TYPE_CHECKING
-from sage_core.function.source_function import StopSignal
-from sage_runtime.router.packet import Packet
-
-if TYPE_CHECKING:
-    from sage_runtime.router.connection import Connection
-    from sage_runtime.runtime_context import RuntimeContext
-
-class BaseRouter(ABC):
-    """
-    路由器基类，负责管理下游连接和数据包路由
-    子类只需要实现具体的数据发送逻辑
-    """
-    
-    def __init__(self, ctx: 'RuntimeContext'):
-        self.name = ctx.name
-        self.ctx = ctx
-        
-        # 下游连接管理
-        self.downstream_groups: Dict[int, Dict[int, 'Connection']] = {}
-        self.downstream_group_roundrobin: Dict[int, int] = {}
-        
-        # Logger
-        self.logger = ctx.logger
-        self.logger.debug(f"Initialized {self.__class__.__name__} for {self.name}")
-    
-    def add_connection(self, connection: 'Connection') -> None:
-        """
-        添加下游连接
-        
-        Args:
-            connection: Connection对象，包含所有连接信息
-        """
-        broadcast_index = connection.broadcast_index
-        parallel_index = connection.parallel_index
-        
-        # Debug log
-        self.logger.debug(
-            f"Adding connection: broadcast_index={broadcast_index}, parallel_index={parallel_index}, target={connection.target_name}"
-        )
-        
-        # 初始化广播组（如果不存在）
-        if broadcast_index not in self.downstream_groups:
-            self.downstream_groups[broadcast_index] = {}
-            self.downstream_group_roundrobin[broadcast_index] = 0
-        
-        # 保存完整的Connection对象
-        self.downstream_groups[broadcast_index][parallel_index] = connection
-        
-        self.logger.info(f"Added connection to {connection.target_name}")
-    
-    def remove_connection(self, broadcast_index: int, parallel_index: int) -> bool:
-        """
-        移除指定的连接
-        
-        Args:
-            broadcast_index: 广播索引
-            parallel_index: 并行索引
-            
-        Returns:
-            bool: 是否成功移除
-        """
-        try:
-            if broadcast_index in self.downstream_groups:
-                if parallel_index in self.downstream_groups[broadcast_index]:
-                    connection = self.downstream_groups[broadcast_index][parallel_index]
-                    del self.downstream_groups[broadcast_index][parallel_index]
-                    
-                    # 如果这个广播组空了，清理它
-                    if not self.downstream_groups[broadcast_index]:
-                        del self.downstream_groups[broadcast_index]
-                        del self.downstream_group_roundrobin[broadcast_index]
-                    else:
-                        # 重置轮询计数器
-                        self.downstream_group_roundrobin[broadcast_index] = 0
-                    
-                    self.logger.info(f"Removed connection to {connection.target_name}")
-                    return True
-            
-            self.logger.warning(f"Connection not found: broadcast_index={broadcast_index}, parallel_index={parallel_index}")
-            return False
-            
-        except Exception as e:
-            self.logger.error(f"Error removing connection: {e}")
-            return False
-    
-    def get_connection_count(self) -> int:
-        """获取总连接数"""
-        total = 0
-        for parallel_targets in self.downstream_groups.values():
-            total += len(parallel_targets)
-        return total
-    
-    def get_connections_info(self) -> Dict[str, Any]:
-        """获取连接信息"""
-        info = {}
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            info[f"broadcast_group_{broadcast_index}"] = {
-                "count": len(parallel_targets),
-                "roundrobin_position": self.downstream_group_roundrobin[broadcast_index],
-                "targets": [
-                    {
-                        "parallel_index": parallel_index,
-                        "target_name": connection.target_name,
-                        "connection_type": connection.connection_type.value
-                    }
-                    for parallel_index, connection in parallel_targets.items()
-                ]
-            }
-        return info
-    
-    def send_stop_signal(self, stop_signal: 'StopSignal') -> None:
-        """
-        发送停止信号给所有下游连接
-        
-        Args:
-            stop_signal: 停止信号对象
-        """
-        self.logger.info(f"Sending stop signal: {stop_signal}")
-        
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            for connection in parallel_targets.values():
-                try:
-                    connection.target_buffer.put_nowait(stop_signal)
-                    self.logger.debug(f"Sent stop signal to {connection.target_name}")
-                except Exception as e:
-                    self.logger.error(f"Failed to send stop signal to {connection.target_name}: {e}")
-
-    def send(self, packet: 'Packet') -> bool:
-        """
-        发送数据包，根据其分区信息选择路由策略
-        
-        Args:
-            packet: 要发送的packet，可能包含分区信息
-            
-        Returns:
-            bool: 是否成功发送
-        """
-        if not self.downstream_groups:
-            self.logger.warning(f"No downstream connections available for {self.name}")
-            return False
-        
-        try:
-            self.logger.debug(f"Emitting packet: {packet}")
-            
-            # 根据packet的分区信息选择路由策略
-            if packet.is_keyed():
-                return self._route_packet(packet)
-            else:
-                return self._route_round_robin_packet(packet)
-                
-        except Exception as e:
-            self.logger.error(f"Error emitting packet: {e}")
-            return False
-    
-    def _route_packet(self, packet: 'Packet') -> bool:
-        """使用分区信息进行路由"""
-        strategy = packet.partition_strategy
-        
-        if strategy == "hash":
-            return self._route_hashed_packet(packet)
-        elif strategy == "broadcast":
-            return self._route_broadcast_packet(packet)
-        else:
-            return self._route_round_robin_packet(packet)
-    
-    def _route_round_robin_packet(self, packet: 'Packet') -> bool:
-        """使用轮询策略进行路由"""
-        success = True
-        
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            if not parallel_targets:  # 空的并行目标组
-                continue
-                
-            # 获取当前轮询位置
-            current_round_robin = self.downstream_group_roundrobin[broadcast_index]
-            parallel_indices = list(parallel_targets.keys())
-            target_parallel_index = parallel_indices[current_round_robin % len(parallel_indices)]
-            
-            # 更新轮询位置
-            self.downstream_group_roundrobin[broadcast_index] = (current_round_robin + 1) % len(parallel_indices)
-            
-            # 发送到选中的连接
-            connection = parallel_targets[target_parallel_index]
-            if not self._deliver_packet(connection, packet):
-                success = False
-        
-        return success
-    
-    def _route_broadcast_packet(self, packet: 'Packet') -> bool:
-        """使用广播策略进行路由"""
-        success = True
-        
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            for connection in parallel_targets.values():
-                if not self._deliver_packet(connection, packet):
-                    success = False
-        
-        return success
-    
-    def _route_hashed_packet(self, packet: 'Packet') -> bool:
-        """使用哈希分区策略进行路由"""
-        if not packet.partition_key:
-            self.logger.warning("Hash routing requested but no partition key provided, falling back to round-robin")
-            return self._route_round_robin_packet(packet)
-        
-        success = True
-        partition_key = packet.partition_key
-        
-        for broadcast_index, parallel_targets in self.downstream_groups.items():
-            if not parallel_targets:
-                continue
-                
-            # 基于分区键计算目标索引
-            parallel_indices = list(parallel_targets.keys())
-            target_index = hash(partition_key) % len(parallel_indices)
-            target_parallel_index = parallel_indices[target_index]
-            
-            connection = parallel_targets[target_parallel_index]
-            if not self._deliver_packet(connection, packet):
-                success = False
-        
-        return success
-    
-    def _deliver_packet(self, connection: 'Connection', packet: 'Packet') -> bool:
-        try:
-            routed_packet = self._create_routed_packet(connection, packet)
-            target_buffer = connection.target_buffer
-            target_buffer.put_nowait(routed_packet)
-            self._log_delivery_success(connection, packet)
-            return True
-        except Exception as e:
-            self._log_delivery_failure(connection, e)
-            return False
-    
-    def clear_all_connections(self):
-        """清空所有连接"""
-        cleared_count = self.get_connection_count()
-        self.downstream_groups.clear()
-        self.downstream_group_roundrobin.clear()
-        
-        self.logger.info(f"Cleared all connections ({cleared_count} connections removed)")
-    
-    def get_statistics(self) -> Dict[str, Any]:
-        """获取路由统计信息"""
-        return {
-            "total_connections": self.get_connection_count(),
-            "broadcast_groups": len(self.downstream_groups),
-            "connections_by_group": {
-                broadcast_index: len(parallel_targets)
-                for broadcast_index, parallel_targets in self.downstream_groups.items()
-            }
-        }
-    
-    def _create_routed_packet(self, connection: 'Connection', packet: 'Packet') -> 'Packet':
-        """创建路由后的数据包"""
-        return Packet(
-            payload=packet.payload,
-            input_index=connection.target_input_index,
-            partition_key=packet.partition_key,
-            partition_strategy=packet.partition_strategy,
-        )
-    
-    def _log_delivery_success(self, connection: 'Connection', packet: 'Packet'):
-        """记录发送成功日志"""
-        self.logger.debug(
-            f"Sent {'keyed' if packet.is_keyed() else 'unkeyed'} packet "
-            f"to {connection.target_name} (strategy: {packet.partition_strategy or 'round-robin'})"
-        )
-    
-    def _log_delivery_failure(self, connection: 'Connection', error: Exception):
-        """记录发送失败日志"""
-        self.logger.error(f"Failed to send packet to {connection.target_name}: {error}", exc_info=True)
\ No newline at end of file
diff --git a/sage_runtime/router/connection.py b/sage_runtime/router/connection.py
deleted file mode 100644
index 1d21745..0000000
--- a/sage_runtime/router/connection.py
+++ /dev/null
@@ -1,22 +0,0 @@
-from typing import Union
-from dataclasses import dataclass
-from sage_runtime.utils.local_message_queue import LocalMessageQueue
-from ray.actor import ActorHandle
-
-@dataclass
-class Connection:
-    """
-    用于表示本地节点和Ray Actor之间的连接
-    """
-    def __init__(self,
-                 broadcast_index: int,
-                 parallel_index: int,
-                 target_name: str,
-                 target_input_buffer: Union[ActorHandle, LocalMessageQueue],
-                 target_input_index: int):
-
-        self.broadcast_index: int = broadcast_index
-        self.parallel_index: int = parallel_index
-        self.target_name: str = target_name
-        self.target_buffer: Union[ActorHandle, LocalMessageQueue] = target_input_buffer
-        self.target_input_index: int = target_input_index
\ No newline at end of file
diff --git a/sage_runtime/router/local_router.py b/sage_runtime/router/local_router.py
deleted file mode 100644
index 7c63b63..0000000
--- a/sage_runtime/router/local_router.py
+++ /dev/null
@@ -1,17 +0,0 @@
-# sage_runtime/local_router.py
-
-from typing import TYPE_CHECKING
-from sage_runtime.router.base_router import BaseRouter
-from sage_runtime.router.packet import Packet
-
-if TYPE_CHECKING:
-    from sage_runtime.router.connection import Connection
-    from sage_runtime.runtime_context import RuntimeContext
-
-class LocalRouter(BaseRouter):
-    """
-    本地任务的路由器，使用直接方法调用进行通信
-    """
-    
-    def __init__(self, ctx: 'RuntimeContext'):
-        super().__init__(ctx)
\ No newline at end of file
diff --git a/sage_runtime/router/ray_router.py b/sage_runtime/router/ray_router.py
deleted file mode 100644
index b92b025..0000000
--- a/sage_runtime/router/ray_router.py
+++ /dev/null
@@ -1,20 +0,0 @@
-# sage_runtime/ray_router.py
-
-from typing import TYPE_CHECKING
-from ray.util.queue import Queue as RayQueue
-from sage_runtime.router.base_router import BaseRouter
-from sage_runtime.router.packet import Packet
-
-if TYPE_CHECKING:
-    from sage_runtime.router.connection import Connection
-    from sage_runtime.runtime_context import RuntimeContext
-
-class RayRouter(BaseRouter):
-    """
-    Ray Actor任务的路由器，使用Ray Queue进行跨进程通信
-    """
-    
-    def __init__(self, ctx: 'RuntimeContext'):
-        super().__init__(ctx)
-    
-    # 目前没有什么特异功能
\ No newline at end of file
diff --git a/sage_runtime/runtime_context.py b/sage_runtime/runtime_context.py
index d569b22..361c759 100644
--- a/sage_runtime/runtime_context.py
+++ b/sage_runtime/runtime_context.py
@@ -1,49 +1,25 @@
-import os
 from typing import TYPE_CHECKING
 import ray
 from ray.actor import ActorHandle
-from typing import List,Dict,Optional, Any, Union
+from typing import List,Dict,Optional, Any
 from sage_memory.memory_collection.base_collection import BaseMemoryCollection
 from sage_memory.memory_collection.vdb_collection import VDBMemoryCollection
 from sage_utils.custom_logger import CustomLogger
-from sage_utils.actor_wrapper import ActorWrapper
-
 if TYPE_CHECKING:
-    from sage_jobmanager.execution_graph import ExecutionGraph, GraphNode
-    from sage_core.transformation.base_transformation import BaseTransformation
-    from sage_core.environment.base_environment import BaseEnvironment 
-    from sage_jobmanager.job_manager import JobManager
-# task, operator和function "形式上共享"的运行上下文
+    from sage_runtime.compiler import Compiler, GraphNode
+    from sage_core.api.env import BaseEnvironment 
+# dagnode, operator和function "形式上共享"的运行上下文
 
 class RuntimeContext:
-    # 定义不需要序列化的属性
-    __state_exclude__ = ["_logger", "env", "_env_logger_cache"]
-    def __init__(self, graph_node: 'GraphNode', transformation: 'BaseTransformation', env: 'BaseEnvironment', jobmanager_handle: Optional[Union['JobManager', 'ActorHandle']] = None):
-        
+    def __init__(self, graph_node: 'GraphNode', env: 'BaseEnvironment'):
         self.name:str = graph_node.name
-
-        self.env_name = env.name
-        self.env_base_dir:str = env.env_base_dir
-        self.env_uuid = env.uuid
-        self.jobmanager_handle: Optional[Union['JobManager', 'ActorHandle']] = jobmanager_handle
-
-        self.memory_collection:Any = transformation.memory_collection
-
+        self.env_name:str = env.name
+        self.session_folder:Optional[str] = CustomLogger.get_session_folder()
+        self.memory_collection:Any = env.memory_collection
         self.parallel_index:int = graph_node.parallel_index
         self.parallelism:int = graph_node.parallelism
-
         self._logger:Optional[CustomLogger] = None
 
-        self.is_spout = transformation.is_spout
-
-        self.delay = 0.01
-        self.stop_signal_num = graph_node.stop_signal_num
-
-    @property
-    def jobmanager(self) -> 'JobManager':
-        return ActorWrapper(self.jobmanager_handle)
-
-
     def retrieve(self,  query: Optional[str] = None, collection_config: Optional[Dict] = None) -> List[str]:
         """
         智能选择检索方式：Ray Actor远程调用或本地对象调用
@@ -389,11 +365,13 @@ class RuntimeContext:
     def logger(self) -> CustomLogger:
         """懒加载logger"""
         if self._logger is None:
-            self._logger = CustomLogger([
-                ("console", "INFO"),  # 控制台显示重要信息
-                (os.path.join(self.env_base_dir, f"{self.name}.log"), "DEBUG"),  # 详细日志
-                (os.path.join(self.env_base_dir, "Error.log"), "ERROR")  # 错误日志
-            ],
-            name = f"{self.name}",
-        )
+            self._logger = CustomLogger(
+                filename=f"Node_{self.name}",
+                console_output="WARNING",
+                file_output="DEBUG",
+                global_output="WARNING",
+                session_folder=self.session_folder,
+                name=f"{self.name}_RuntimeContext",
+                env_name=self.env_name
+            )
         return self._logger
diff --git a/sage_utils/state_persistence.py b/sage_runtime/state_persistence.py
similarity index 94%
rename from sage_utils/state_persistence.py
rename to sage_runtime/state_persistence.py
index 5a91b48..141d4a5 100644
--- a/sage_utils/state_persistence.py
+++ b/sage_runtime/state_persistence.py
@@ -4,7 +4,6 @@ import inspect
 import threading
 from collections.abc import Mapping, Sequence, Set
 
-# TODO: state 的持久化管理不应该由 function来定义，而是应该交给系统自动在operator / task里面生成。
 # 不可序列化类型黑名单
 _BLACKLIST = (
     type(open),        # 文件句柄
diff --git a/sage_runtime/task/base_task.py b/sage_runtime/task/base_task.py
deleted file mode 100644
index 858fcdc..0000000
--- a/sage_runtime/task/base_task.py
+++ /dev/null
@@ -1,164 +0,0 @@
-from abc import ABC, abstractmethod
-from queue import Empty
-import threading, copy, time
-from typing import Any, TYPE_CHECKING, Union, Optional
-from sage_runtime.runtime_context import RuntimeContext
-from sage_runtime.router.packet import Packet
-from ray.util.queue import Empty
-if TYPE_CHECKING:
-    from sage_runtime.router.base_router import BaseRouter
-    from sage_runtime.router.connection import Connection
-    from sage_core.operator.base_operator import BaseOperator
-    from sage_jobmanager.factory.operator_factory import OperatorFactory
-
-class BaseTask(ABC):
-    def __init__(self,runtime_context: 'RuntimeContext',operator_factory: 'OperatorFactory') -> None:
-        self.ctx = runtime_context
-        # === 继承类设置 ===
-        self.router:BaseRouter
-        self.input_buffer: Any
-        # === 线程控制 ===
-        self._worker_thread: Optional[threading.Thread] = None
-        self.is_running = False
-        self._stop_event = threading.Event()
-        # === 性能监控 ===
-        self._processed_count = 0
-        self._error_count = 0
-        self._last_activity_time = time.time()
-        try:
-            self.operator:BaseOperator = operator_factory.create_operator(self.ctx)
-            self.operator.task = self
-        except Exception as e:
-            self.logger.error(f"Failed to initialize node {self.name}: {e}", exc_info=True)
-
-
-    def start_running(self):
-        """启动任务的工作循环"""
-        if self.is_running:
-            self.logger.warning(f"Task {self.name} is already running")
-            return
-        
-        self.logger.info(f"Starting task {self.name}")
-        
-        # 设置运行状态
-        self.is_running = True
-        self._stop_event.clear()
-        
-        # 启动工作线程
-        self._worker_thread = threading.Thread(
-            target=self._worker_loop,
-            name=f"{self.name}_worker",
-            daemon=True
-        )
-        self._worker_thread.start()
-        
-        self.logger.info(f"Task {self.name} started with worker thread")
-
-    def add_connection(self, connection: 'Connection'):
-        self.router.add_connection(connection)
-        self.logger.debug(f"Connection added to node '{self.name}': {connection}")
-
-    def remove_connection(self, broadcast_index: int, parallel_index: int) -> bool:
-        return self.router.remove_connection(broadcast_index, parallel_index)
-
-
-    def trigger(self, input_tag: str = None, packet:'Packet' = None) -> None:
-        try:
-            self.logger.debug(f"Received data in node {self.name}, channel {input_tag}")
-            self.operator.process_packet(packet)
-        except Exception as e:
-            self.logger.error(f"Error processing data in node {self.name}: {e}", exc_info=True)
-            raise
-
-    def stop(self) -> None:
-        """Signal the worker loop to stop."""
-        if not self._stop_event.is_set():
-            self._stop_event.set()
-            self.logger.info(f"Node '{self.name}' received stop signal.")
-
-    def get_input_buffer(self):
-        """
-        获取输入缓冲区
-        :return: 输入缓冲区对象
-        """
-        return self.input_buffer
-
-    def _worker_loop(self) -> None:
-        """
-        Main worker loop that executes continuously until stop is signaled.
-        """
-        # Main execution loop
-        while not self._stop_event.is_set():
-            try:
-                if self.is_spout:
-                    self.logger.debug(f"Running spout node '{self.name}'")
-                    self.operator.receive_packet(None)
-                    # TODO: 做一个下游缓冲区反压机制，因为引入一个手动延迟实在是太呆了
-                    time.sleep(self.delay)
-                else:
-                    
-                    # For non-spout nodes, fetch input and process
-                    # input_result = self.fetch_input()
-                    try:
-                        data_packet = self.input_buffer.get(timeout=0.5)
-                    except Empty as e:
-                        time.sleep(0.01)
-                        continue
-                    self.logger.debug(f"Node '{self.name}' received data packet: {data_packet}, type: {type(data_packet)}")
-                    if data_packet is None:
-                        time.sleep(0.01)
-                        continue
-                    self.operator.receive_packet(data_packet)
-            except Exception as e:
-                self.logger.error(f"Critical error in node '{self.name}': {str(e)}")
-            finally:
-                self._running = False
-
-    @property
-    def is_spout(self) -> bool:
-        """检查是否为 spout 节点"""
-        return self.ctx.is_spout
-
-    @property
-    def delay(self) -> float:
-        """获取任务的延迟时间"""
-        return self.ctx.delay
-    
-    @property
-    def logger(self):
-        """获取当前任务的日志记录器"""
-        return self.ctx.logger
-
-    @property
-    def name(self) -> str:
-        """获取任务名称"""
-        return self.ctx.name
-
-
-    def cleanup(self):
-        """清理任务资源"""
-        self.logger.info(f"Cleaning up task {self.name}")
-        
-        try:
-            # 停止任务
-            if self.is_running:
-                self.stop()
-            
-            # # 清理算子资源
-            # if hasattr(self.operator, 'cleanup'):
-            #     self.operator.cleanup()
-            # 这些内容应该会自己清理掉
-            # # 清理路由器
-            # if hasattr(self.router, 'cleanup'):
-            #     self.router.cleanup()
-            
-            # 清理输入缓冲区
-            if hasattr(self.input_buffer, 'cleanup'):
-                self.input_buffer.cleanup()
-            elif hasattr(self.input_buffer, 'close'):
-                self.input_buffer.close()
-            
-            self.logger.debug(f"Task {self.name} cleanup completed")
-            
-        except Exception as e:
-            self.logger.error(f"Error during cleanup of task {self.name}: {e}")
\ No newline at end of file
diff --git a/sage_runtime/task/local_task.py b/sage_runtime/task/local_task.py
deleted file mode 100644
index 873ac68..0000000
--- a/sage_runtime/task/local_task.py
+++ /dev/null
@@ -1,36 +0,0 @@
-from typing import TYPE_CHECKING
-from sage_runtime.task.base_task import BaseTask
-from sage_runtime.utils.local_message_queue import LocalMessageQueue
-from sage_runtime.router.local_router import LocalRouter
-
-if TYPE_CHECKING:
-    from sage_jobmanager.factory.operator_factory import OperatorFactory
-    from sage_runtime.runtime_context import RuntimeContext
-
-
-class LocalTask(BaseTask):
-    """
-    本地任务节点，使用LocalMessageQueue作为输入缓冲区
-    内部运行独立的工作线程，处理数据流
-    """
-    
-    def __init__(self,
-                 runtime_context: 'RuntimeContext', 
-                 operator_factory: 'OperatorFactory',
-                 max_buffer_size: int = 30000,
-                 queue_maxsize: int = 50000) -> None:
-        
-        # 调用父类初始化
-        super().__init__(runtime_context, operator_factory)
-
-        # === Local Message Queue 缓冲区 ===
-        # 创建本地消息队列作为输入缓冲区
-        self.input_buffer = LocalMessageQueue(runtime_context)
-    
-        
-        # === 本地路由器 ===
-        self.router = LocalRouter(runtime_context)
-        self.operator.router = self.router
-
-        self.logger.info(f"Initialized LocalTask: {self.ctx.name}")
-        self.logger.debug(f"Buffer max size: {max_buffer_size} bytes, Queue max size: {queue_maxsize}")
\ No newline at end of file
diff --git a/sage_runtime/task/ray_task.py b/sage_runtime/task/ray_task.py
deleted file mode 100644
index 781bade..0000000
--- a/sage_runtime/task/ray_task.py
+++ /dev/null
@@ -1,66 +0,0 @@
-import ray
-import time
-import threading
-from typing import Any, Union, Tuple, TYPE_CHECKING, Dict, Optional
-from ray.util.queue import Queue as RayQueue
-from sage_runtime.task.base_task import BaseTask
-from sage_runtime.router.packet import Packet
-from sage_runtime.router.ray_router import RayRouter
-if TYPE_CHECKING:
-    from sage_jobmanager.factory.operator_factory import OperatorFactory
-    from sage_runtime.runtime_context import RuntimeContext
-
-
-
-@ray.remote
-class RayTask(BaseTask):
-    """
-    基于Ray Actor的任务节点，使用Ray Queue作为输入输出缓冲区
-    内部运行独立的工作线程，避免阻塞Ray Actor的事件循环
-    """
-    
-    def __init__(self,
-                 runtime_context: 'RuntimeContext', 
-                 operator_factory: 'OperatorFactory') -> None:
-        
-        # 调用父类初始化
-        super().__init__(runtime_context, operator_factory)
-
-        # === Ray Queue 缓冲区 ===
-        # 创建Ray Queue（这是一个Ray对象，自动支持跨进程）
-        self.input_buffer = RayQueue(maxsize=1000)
-        # === 路由器 ===
-        self.router = RayRouter(runtime_context)
-        self.operator.router = self.router
-
-        self.logger.info(f"Initialized RayTask: {self.ctx.name}")
-
-    def cleanup(self):
-        super().cleanup()
-        """清理任务资源 - 重写以支持 Ray 特定清理"""
-        self.logger.info(f"Cleaning up RayTask {self.name}")
-        
-        try:
-            # 停止任务
-            if self.is_running:
-                self.stop()
-            
-            # # 清理算子资源
-            # if hasattr(self.operator, 'cleanup'):
-            #     self.operator.cleanup()
-            # 这些内容应该会自己清理掉
-            # # 清理 Ray Router
-            # if hasattr(self.router, 'cleanup'):
-            #     self.router.cleanup()
-            
-            # 清理 Ray Queue
-            if hasattr(self.input_buffer, 'shutdown'):
-                try:
-                    self.input_buffer.shutdown()
-                except Exception as e:
-                    self.logger.warning(f"Error shutting down input buffer: {e}")
-            
-            self.logger.debug(f"RayTask {self.name} cleanup completed")
-            
-        except Exception as e:
-            self.logger.error(f"Error during cleanup of RayTask {self.name}: {e}")
\ No newline at end of file
diff --git a/sage_tests/core_tests/comap_test.py b/sage_tests/core_tests/comap_test.py
index ec6a5ae..3303bb3 100644
--- a/sage_tests/core_tests/comap_test.py
+++ b/sage_tests/core_tests/comap_test.py
@@ -1,12 +1,12 @@
+import pytest
 import time
-from typing import Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+import threading
+from typing import List, Dict, Any
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.comap_function import BaseCoMapFunction
 from sage_core.function.sink_function import SinkFunction
-import json
-import tempfile
-from pathlib import Path
+
 
 class OrderDataSource(SourceFunction):
     """生成订单数据"""
@@ -78,38 +78,30 @@ class InventoryDataSource(SourceFunction):
 
 
 class CoMapDebugSink(SinkFunction):
-    """调试用的Sink，通过文件系统记录CoMap处理结果"""
+    """调试用的Sink，记录CoMap处理结果"""
+    
+    _received_data: Dict[int, List[Dict]] = {}
+    _lock = threading.Lock()
     
-    def __init__(self, output_file=None, **kwargs):
+    def __init__(self, **kwargs):
         super().__init__(**kwargs)
-        
         self.parallel_index = None
         self.received_count = 0
-        # 如果没有指定输出文件，使用临时文件
-        if output_file is None:
-            self.output_file = Path(tempfile.gettempdir()) / "comap_test_results.json"
-        else:
-            self.output_file = Path(output_file)
-        self.logger.info(f"CoMapDebugSink initialized, output file: {self.output_file}")
+    
     def execute(self, data: Any):
         if self.runtime_context:
             self.parallel_index = self.runtime_context.parallel_index
-        self.logctx!")
-        self.received_count += 1ctx
         
-        result_type = data.get('type', 'unknown')
-        source_stream = data.get('source_stream', -1)
+        with self._lock:
+            if self.parallel_index not in self._received_data:
+                self._received_data[self.parallel_index] = []
+            
+            self._received_data[self.parallel_index].append(data)
         
-        # 准备要写入文件的记录
-        record = {
-            'timestamp': time.time(),
-            'parallel_index': self.parallel_index,
-            'received_count': self.received_count,
-            'data': data
-        }
+        self.received_count += 1
         
-        # 原子性地追加到文件
-        self._append_record(record)
+        result_type = data.get('type', 'unknown')
+        source_stream = data.get('source_stream', -1)
         
         self.logger.info(
             f"[Instance {self.parallel_index}] "
@@ -122,59 +114,15 @@ class CoMapDebugSink(SinkFunction):
         
         return data
     
-    def _append_record(self, record):
-        """原子性地追加记录到文件"""
-        import fcntl  # Unix文件锁
-        
-        try:
-            # 以追加模式打开文件
-            with open(self.output_file, 'a') as f:
-                # 获取文件锁
-                fcntl.flock(f.fileno(), fcntl.LOCK_EX)
-                # 写入一行JSON
-                f.write(json.dumps(record) + '\n')
-                f.flush()
-                # 锁会在with块结束时自动释放
-        except Exception as e:
-            self.logger.error(f"Failed to write record: {e}")
-    
-    @staticmethod
-    def read_results(output_file=None):
-        """读取测试结果"""
-        if output_file is None:
-            output_file = Path(tempfile.gettempdir()) / "comap_test_results.json"
-        else:
-            output_file = Path(output_file)
-        
-        if not output_file.exists():
-            return {}
-        
-        results = {}
-        try:
-            with open(output_file, 'r') as f:
-                for line in f:
-                    line = line.strip()
-                    if line:
-                        record = json.loads(line)
-                        parallel_index = record.get('parallel_index', 0)
-                        if parallel_index not in results:
-                            results[parallel_index] = []
-                        results[parallel_index].append(record['data'])
-        except Exception as e:
-            print(f"Error reading results: {e}")
-        print(f"📂 Read {len(results)} parallel instances from results file: {output_file}")
-        return results
+    @classmethod
+    def get_received_data(cls) -> Dict[int, List[Dict]]:
+        with cls._lock:
+            return dict(cls._received_data)
     
-    @staticmethod
-    def clear_results(output_file=None):
-        """清理结果文件"""
-        if output_file is None:
-            output_file = Path(tempfile.gettempdir()) / "comap_test_results.json"
-        else:
-            output_file = Path(output_file)
-        
-        if output_file.exists():
-            output_file.unlink()
+    @classmethod
+    def clear_data(cls):
+        with cls._lock:
+            cls._received_data.clear()
 
 
 class OrderPaymentCoMapFunction(BaseCoMapFunction):
@@ -355,14 +303,13 @@ class TestCoMapFunctionality:
     """测试CoMap功能"""
     
     def setup_method(self):
-        # 清理结果文件
-        CoMapDebugSink.clear_results()
+        CoMapDebugSink.clear_data()
     
     def test_basic_two_stream_comap(self):
         """测试基本的两路CoMap处理"""
         print("\n🚀 Testing Basic Two-Stream CoMap")
         
-        env = LocalStreamEnvironment("basic_comap_test")
+        env = LocalEnvironment("basic_comap_test")
         
         order_stream = env.from_source(OrderDataSource, delay=0.3)
         payment_stream = env.from_source(PaymentDataSource, delay=0.4)
@@ -372,7 +319,6 @@ class TestCoMapFunctionality:
             .connect(payment_stream)
             .comap(OrderPaymentCoMapFunction)
             .sink(CoMapDebugSink, parallelism=2)
-            .print()
         )
         
         print("📊 Pipeline: OrderStream + PaymentStream -> comap(OrderPaymentCoMapFunction) -> Sink(parallelism=2)")
@@ -380,18 +326,107 @@ class TestCoMapFunctionality:
         
         try:
             env.submit()
-            
-            time.sleep(10)
+            env.run_streaming()
+            time.sleep(3)
         finally:
             env.close()
         
-        # 等待一下确保文件写入完成
-        time.sleep(1)
         self._verify_two_stream_comap_results()
     
+    def test_three_stream_comap(self):
+        """测试三路CoMap处理"""
+        print("\n🚀 Testing Three-Stream CoMap")
+        
+        env = LocalEnvironment("three_stream_comap_test")
+        
+        order_stream = env.from_source(OrderDataSource, delay=0.3)
+        payment_stream = env.from_source(PaymentDataSource, delay=0.4)
+        inventory_stream = env.from_source(InventoryDataSource, delay=0.5)
+        
+        result_stream = (
+            order_stream
+            .connect(payment_stream)
+            .connect(inventory_stream)
+            .comap(TripleStreamCoMapFunction)
+            .sink(CoMapDebugSink, parallelism=3)
+        )
+        
+        print("📊 Pipeline: OrderStream + PaymentStream + InventoryStream -> comap(TripleStreamCoMapFunction) -> Sink")
+        print("🎯 Expected: Each stream processed by corresponding mapN method\n")
+        
+        try:
+            env.submit()
+            env.run_streaming()
+            time.sleep(4)
+        finally:
+            env.close()
+        
+        self._verify_three_stream_comap_results()
+    
+    def test_stateful_comap(self):
+        """测试有状态的CoMap处理"""
+        print("\n🚀 Testing Stateful CoMap")
+        
+        env = LocalEnvironment("stateful_comap_test")
+        
+        order_stream = env.from_source(OrderDataSource, delay=0.3)
+        payment_stream = env.from_source(PaymentDataSource, delay=0.6)  # 延迟更多，让订单先到达
+        
+        result_stream = (
+            order_stream
+            .connect(payment_stream)
+            .comap(StatefulCoMapFunction)
+            .sink(CoMapDebugSink, parallelism=1)  # 单实例以便观察状态
+        )
+        
+        print("📊 Pipeline: OrderStream + PaymentStream -> comap(StatefulCoMapFunction) -> Sink")
+        print("🎯 Expected: Orders cached, Payments enriched with order info and stats\n")
+        
+        try:
+            env.submit()
+            env.run_streaming()
+            time.sleep(5)  # 更多时间让状态积累
+        finally:
+            env.close()
+        
+        self._verify_stateful_comap_results()
+    
+    def test_comap_validation_errors(self):
+        """测试CoMap函数验证错误"""
+        print("\n🚀 Testing CoMap Validation Errors")
+        
+        env = LocalEnvironment("comap_validation_test")
+        
+        order_stream = env.from_source(OrderDataSource, delay=0.5)
+        payment_stream = env.from_source(PaymentDataSource, delay=0.5)
+        connected = order_stream.connect(payment_stream)
+        
+        # 测试1：使用普通函数而不是CoMap函数
+        from sage_core.function.base_function import BaseFunction
+        
+        class RegularFunction(BaseFunction):
+            def execute(self, data):
+                return data
+        
+        with pytest.raises(TypeError, match="must inherit from BaseCoMapFunction"):
+            connected.comap(RegularFunction)
+        
+        # 测试2：CoMap函数缺少必需方法
+        with pytest.raises(TypeError, match="with abstract method map1"):
+            connected.comap(InvalidCoMapFunction)
+        
+        # 测试3：输入流数量不足
+        single_stream = env.from_source(OrderDataSource, delay=0.5)
+        with pytest.raises(AttributeError, match=" object has no attribute"):
+            single_stream.comap(OrderPaymentCoMapFunction)
+        
+        print("✅ CoMap validation tests passed")
+        
+        env.close()
+    
     def _verify_two_stream_comap_results(self):
         """验证两路CoMap的结果"""
-        received_data = CoMapDebugSink.read_results()
+        received_data = CoMapDebugSink.get_received_data()
         
         print("\n📋 Two-Stream CoMap Results:")
         print("=" * 50)
@@ -437,6 +472,87 @@ class TestCoMapFunctionality:
             assert payment.get("source_stream") == 1, f"❌ Payment from wrong stream: {payment.get('source_stream')}"
         
         print("✅ Two-stream CoMap test passed: Correct stream routing and processing")
+    
+    def _verify_three_stream_comap_results(self):
+        """验证三路CoMap的结果"""
+        received_data = CoMapDebugSink.get_received_data()
+        
+        print("\n📋 Three-Stream CoMap Results:")
+        print("=" * 50)
+        
+        stream_results = {0: [], 1: [], 2: []}
+        
+        for instance_id, data_list in received_data.items():
+            print(f"\n🔹 Parallel Instance {instance_id}:")
+            
+            for data in data_list:
+                source_stream = data.get("source_stream", -1)
+                enrichment = data.get("enrichment", "unknown")
+                stream_sequence = data.get("stream_sequence", 0)
+                
+                if 0 <= source_stream <= 2:
+                    stream_results[source_stream].append(data)
+                
+                print(f"   - Stream {source_stream}: {enrichment} (seq #{stream_sequence})")
+        
+        print(f"\n🎯 Three-Stream Processing Summary:")
+        for stream_id, results in stream_results.items():
+            stream_name = ["Order", "Payment", "Inventory"][stream_id]
+            print(f"   - Stream {stream_id} ({stream_name}): {len(results)} items")
+        
+        # 验证：每个流都应该有处理结果
+        for stream_id in range(3):
+            assert len(stream_results[stream_id]) > 0, f"❌ No results from stream {stream_id}"
+        
+        print("✅ Three-stream CoMap test passed: All streams processed correctly")
+    
+    def _verify_stateful_comap_results(self):
+        """验证有状态CoMap的结果"""
+        received_data = CoMapDebugSink.get_received_data()
+        
+        print("\n📋 Stateful CoMap Results:")
+        print("=" * 50)
+        
+        cached_orders = []
+        enriched_payments = []
+        
+        for instance_id, data_list in received_data.items():
+            print(f"\n🔹 Parallel Instance {instance_id}:")
+            
+            for data in data_list:
+                result_type = data.get("type", "unknown")
+                
+                if result_type == "cached_order":
+                    cached_orders.append(data)
+                    order_id = data.get("order_id", "unknown")
+                    cache_size = data.get("cache_size", 0)
+                    print(f"   - Cached Order: {order_id} (cache size: {cache_size})")
+                    
+                elif result_type == "enriched_payment_with_stats":
+                    enriched_payments.append(data)
+                    payment_id = data.get("payment_id", "unknown")
+                    status = data.get("status", "unknown")
+                    stats = data.get("payment_stats", {})
+                    order_info = data.get("order_info", {})
+                    
+                    print(f"   - Enriched Payment: {payment_id} ({status})")
+                    print(f"     Stats: {stats}")
+                    print(f"     Order Info: {order_info.get('product', 'N/A')} - ${order_info.get('amount', 0)}")
+        
+        print(f"\n🎯 Stateful Processing Summary:")
+        print(f"   - Cached orders: {len(cached_orders)}")
+        print(f"   - Enriched payments: {len(enriched_payments)}")
+        
+        # 验证状态维护
+        if enriched_payments:
+            final_payment = enriched_payments[-1]
+            final_stats = final_payment.get("payment_stats", {})
+            print(f"   - Final payment stats: {final_stats}")
+            
+            assert final_stats.get("success_count", 0) > 0 or final_stats.get("failed_count", 0) > 0, \
+                "❌ Payment statistics not maintained"
+        
+        print("✅ Stateful CoMap test passed: State correctly maintained across streams")
 
 
 if __name__ == "__main__":
diff --git a/sage_tests/core_tests/connected_keyby_test.py b/sage_tests/core_tests/connected_keyby_test.py
index 58645ea..d2289c0 100644
--- a/sage_tests/core_tests/connected_keyby_test.py
+++ b/sage_tests/core_tests/connected_keyby_test.py
@@ -2,9 +2,10 @@ import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.keyby_function import KeyByFunction
+from sage_core.function.base_function import BaseFunction
 from sage_core.function.comap_function import BaseCoMapFunction
 from sage_core.function.sink_function import SinkFunction
 
@@ -92,8 +93,8 @@ class ConnectedDebugSink(SinkFunction):
     
     def execute(self, data: Any):
         if self.runtime_context:
-            selfctx = self.runtime_context.parallel_index
-        ctx
+            self.parallel_index = self.runtime_context.parallel_index
+        
         with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
@@ -177,7 +178,7 @@ class TestConnectedStreamsKeyBy:
         """测试统一的KeyBy - 两个流使用相同的key selector"""
         print("\n🚀 Testing Connected Streams Unified KeyBy")
         
-        env = LocalStreamEnvironment("connected_unified_keyby_test")
+        env = LocalEnvironment("connected_unified_keyby_test")
         
         # 创建两个数据源
         user_stream = env.from_source(UserDataSource, delay=0.3)
@@ -197,7 +198,7 @@ class TestConnectedStreamsKeyBy:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -208,7 +209,7 @@ class TestConnectedStreamsKeyBy:
         """测试Flink风格的per-stream KeyBy - 每个流使用不同的key selector"""
         print("\n🚀 Testing Connected Streams Per-Stream KeyBy (Flink-style)")
         
-        env = LocalStreamEnvironment("connected_per_stream_keyby_test")
+        env = LocalEnvironment("connected_per_stream_keyby_test")
         
         user_stream = env.from_source(UserDataSource, delay=0.3)
         event_stream = env.from_source(EventDataSource, delay=0.4)
@@ -227,7 +228,7 @@ class TestConnectedStreamsKeyBy:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -238,7 +239,7 @@ class TestConnectedStreamsKeyBy:
         """测试KeyBy后接CoMap操作"""
         print("\n🚀 Testing Connected Streams KeyBy + CoMap")
         
-        env = LocalStreamEnvironment("connected_keyby_comap_test")
+        env = LocalEnvironment("connected_keyby_comap_test")
         
         user_stream = env.from_source(UserDataSource, delay=0.3)
         event_stream = env.from_source(EventDataSource, delay=0.4)
@@ -257,7 +258,7 @@ class TestConnectedStreamsKeyBy:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)  # 给更多时间让join操作完成
         finally:
             env.close()
@@ -268,7 +269,7 @@ class TestConnectedStreamsKeyBy:
         """测试无效的KeyBy配置"""
         print("\n🚀 Testing Invalid KeyBy Configurations")
         
-        env = LocalStreamEnvironment("invalid_keyby_test")
+        env = LocalEnvironment("invalid_keyby_test")
         
         user_stream = env.from_source(UserDataSource, delay=0.5)
         event_stream = env.from_source(EventDataSource, delay=0.5)
diff --git a/sage_tests/core_tests/filter_test.py b/sage_tests/core_tests/filter_test.py
index a259bbd..253ed7d 100644
--- a/sage_tests/core_tests/filter_test.py
+++ b/sage_tests/core_tests/filter_test.py
@@ -1,10 +1,12 @@
+import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.filter_function import FilterFunction
 from sage_core.function.sink_function import SinkFunction
+from sage_core.function.base_function import BaseFunction
 
 
 class NumberDataSource(SourceFunction):
@@ -73,8 +75,8 @@ class FilterDebugSink(SinkFunction):
     def execute(self, data: Any):
         if self.runtime_context:
             self.parallel_index = self.runtime_context.parallel_index
-        ctx
-        with self._lock:ctx
+        
+        with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
             
@@ -187,7 +189,7 @@ class TestFilterFunctionality:
         """测试基本的正数过滤"""
         print("\n🚀 Testing Basic Positive Number Filter")
         
-        env = LocalStreamEnvironment("positive_filter_test")
+        env = LocalEnvironment("positive_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -200,7 +202,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -211,7 +213,7 @@ class TestFilterFunctionality:
         """测试链式过滤器"""
         print("\n🚀 Testing Chained Filters")
         
-        env = LocalStreamEnvironment("chained_filter_test")
+        env = LocalEnvironment("chained_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -225,7 +227,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -236,7 +238,7 @@ class TestFilterFunctionality:
         """测试用户数据过滤"""
         print("\n🚀 Testing User Data Filters")
         
-        env = LocalStreamEnvironment("user_filter_test")
+        env = LocalEnvironment("user_filter_test")
         
         result_stream = (
             env.from_source(UserDataSource, delay=0.3)
@@ -250,7 +252,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
@@ -261,7 +263,7 @@ class TestFilterFunctionality:
         """测试Lambda函数过滤"""
         print("\n🚀 Testing Lambda Function Filter")
         
-        env = LocalStreamEnvironment("lambda_filter_test")
+        env = LocalEnvironment("lambda_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -274,7 +276,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -285,7 +287,7 @@ class TestFilterFunctionality:
         """测试极端情况的过滤器"""
         print("\n🚀 Testing Extreme Filter Cases")
         
-        env = LocalStreamEnvironment("extreme_filter_test")
+        env = LocalEnvironment("extreme_filter_test")
         
         # 测试1：所有数据都通过
         always_true_stream = (
@@ -298,7 +300,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(2)
         finally:
             env.close()
@@ -307,7 +309,7 @@ class TestFilterFunctionality:
         FilterDebugSink.clear_data()
         
         # 测试2：所有数据都被过滤
-        env2 = LocalStreamEnvironment("always_false_filter_test")
+        env2 = LocalEnvironment("always_false_filter_test")
         
         always_false_stream = (
             env2.from_source(NumberDataSource, delay=0.2)
@@ -332,7 +334,7 @@ class TestFilterFunctionality:
         """测试Filter与Map的集成"""
         print("\n🚀 Testing Filter + Map Integration")
         
-        env = LocalStreamEnvironment("filter_map_integration_test")
+        env = LocalEnvironment("filter_map_integration_test")
         
         result_stream = (
             env.from_source(UserDataSource, delay=0.3)
@@ -351,7 +353,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
@@ -362,7 +364,7 @@ class TestFilterFunctionality:
         """测试Filter的错误处理"""
         print("\n🚀 Testing Filter Error Handling")
         
-        env = LocalStreamEnvironment("filter_error_test")
+        env = LocalEnvironment("filter_error_test")
         
         # 注意：这个测试可能会产生错误日志，这是预期的
         result_stream = (
@@ -376,7 +378,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
diff --git a/sage_tests/core_tests/flatmap_test.py b/sage_tests/core_tests/flatmap_test.py
index a259bbd..253ed7d 100644
--- a/sage_tests/core_tests/flatmap_test.py
+++ b/sage_tests/core_tests/flatmap_test.py
@@ -1,10 +1,12 @@
+import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.filter_function import FilterFunction
 from sage_core.function.sink_function import SinkFunction
+from sage_core.function.base_function import BaseFunction
 
 
 class NumberDataSource(SourceFunction):
@@ -73,8 +75,8 @@ class FilterDebugSink(SinkFunction):
     def execute(self, data: Any):
         if self.runtime_context:
             self.parallel_index = self.runtime_context.parallel_index
-        ctx
-        with self._lock:ctx
+        
+        with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
             
@@ -187,7 +189,7 @@ class TestFilterFunctionality:
         """测试基本的正数过滤"""
         print("\n🚀 Testing Basic Positive Number Filter")
         
-        env = LocalStreamEnvironment("positive_filter_test")
+        env = LocalEnvironment("positive_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -200,7 +202,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -211,7 +213,7 @@ class TestFilterFunctionality:
         """测试链式过滤器"""
         print("\n🚀 Testing Chained Filters")
         
-        env = LocalStreamEnvironment("chained_filter_test")
+        env = LocalEnvironment("chained_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -225,7 +227,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -236,7 +238,7 @@ class TestFilterFunctionality:
         """测试用户数据过滤"""
         print("\n🚀 Testing User Data Filters")
         
-        env = LocalStreamEnvironment("user_filter_test")
+        env = LocalEnvironment("user_filter_test")
         
         result_stream = (
             env.from_source(UserDataSource, delay=0.3)
@@ -250,7 +252,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
@@ -261,7 +263,7 @@ class TestFilterFunctionality:
         """测试Lambda函数过滤"""
         print("\n🚀 Testing Lambda Function Filter")
         
-        env = LocalStreamEnvironment("lambda_filter_test")
+        env = LocalEnvironment("lambda_filter_test")
         
         result_stream = (
             env.from_source(NumberDataSource, delay=0.2)
@@ -274,7 +276,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
@@ -285,7 +287,7 @@ class TestFilterFunctionality:
         """测试极端情况的过滤器"""
         print("\n🚀 Testing Extreme Filter Cases")
         
-        env = LocalStreamEnvironment("extreme_filter_test")
+        env = LocalEnvironment("extreme_filter_test")
         
         # 测试1：所有数据都通过
         always_true_stream = (
@@ -298,7 +300,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(2)
         finally:
             env.close()
@@ -307,7 +309,7 @@ class TestFilterFunctionality:
         FilterDebugSink.clear_data()
         
         # 测试2：所有数据都被过滤
-        env2 = LocalStreamEnvironment("always_false_filter_test")
+        env2 = LocalEnvironment("always_false_filter_test")
         
         always_false_stream = (
             env2.from_source(NumberDataSource, delay=0.2)
@@ -332,7 +334,7 @@ class TestFilterFunctionality:
         """测试Filter与Map的集成"""
         print("\n🚀 Testing Filter + Map Integration")
         
-        env = LocalStreamEnvironment("filter_map_integration_test")
+        env = LocalEnvironment("filter_map_integration_test")
         
         result_stream = (
             env.from_source(UserDataSource, delay=0.3)
@@ -351,7 +353,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
@@ -362,7 +364,7 @@ class TestFilterFunctionality:
         """测试Filter的错误处理"""
         print("\n🚀 Testing Filter Error Handling")
         
-        env = LocalStreamEnvironment("filter_error_test")
+        env = LocalEnvironment("filter_error_test")
         
         # 注意：这个测试可能会产生错误日志，这是预期的
         result_stream = (
@@ -376,7 +378,7 @@ class TestFilterFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
diff --git a/sage_tests/core_tests/join_test.py b/sage_tests/core_tests/join_test.py
index 3ae3a15..10909de 100644
--- a/sage_tests/core_tests/join_test.py
+++ b/sage_tests/core_tests/join_test.py
@@ -1,7 +1,8 @@
+import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.flatmap_function import FlatMapFunction
 from sage_core.function.filter_function import FilterFunction
@@ -489,8 +490,8 @@ class JoinResultSink(SinkFunction):
     
     def execute(self, data: Any):
         if self.runtime_context:
-            selfctx = self.runtime_context.parallel_index
-        ctx
+            self.parallel_index = self.runtime_context.parallel_index
+        
         with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
@@ -538,7 +539,7 @@ class TestJoinFunctionality:
         """测试完整的FlatMap -> Filter -> Join管道"""
         print("\n🚀 Testing Complete FlatMap -> Filter -> Join Pipeline")
         
-        env = LocalStreamEnvironment("flatmap_filter_join_test")
+        env = LocalEnvironment("flatmap_filter_join_test")
         
         # 1. 创建源数据流
         order_source = env.from_source(OrderEventSource, delay=0.2)
@@ -571,7 +572,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(6)
         finally:
             env.close()
@@ -582,7 +583,7 @@ class TestJoinFunctionality:
         """测试多阶段Join管道"""
         print("\n🚀 Testing Multi-Stage Join Pipeline")
         
-        env = LocalStreamEnvironment("multi_stage_join_test")
+        env = LocalEnvironment("multi_stage_join_test")
         
         # 第一阶段：订单事件流处理
         order_source = env.from_source(OrderEventSource, delay=0.2)
@@ -628,7 +629,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(6)
         finally:
             env.close()
@@ -639,7 +640,7 @@ class TestJoinFunctionality:
         """测试基于时间窗口的Join"""
         print("\n🚀 Testing Windowed Join Pipeline")
         
-        env = LocalStreamEnvironment("windowed_join_test")
+        env = LocalEnvironment("windowed_join_test")
         
         order_source = env.from_source(OrderEventSource, delay=0.15)
         
@@ -671,7 +672,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(5)
         finally:
             env.close()
@@ -682,7 +683,7 @@ class TestJoinFunctionality:
         """测试包含多个Join的复杂管道"""
         print("\n🚀 Testing Complex Pipeline with Multiple Joins")
         
-        env = LocalStreamEnvironment("complex_multi_join_test")
+        env = LocalEnvironment("complex_multi_join_test")
         
         # 数据源
         order_source = env.from_source(OrderEventSource, delay=0.2)
@@ -735,7 +736,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(7)
         finally:
             env.close()
@@ -746,7 +747,7 @@ class TestJoinFunctionality:
         """测试空流的Join处理"""
         print("\n🚀 Testing Join with Empty/Filtered Streams")
         
-        env = LocalStreamEnvironment("empty_stream_join_test")
+        env = LocalEnvironment("empty_stream_join_test")
         
         order_source = env.from_source(OrderEventSource, delay=0.2)
         user_source = env.from_source(UserProfileSource, delay=0.3)
@@ -779,7 +780,7 @@ class TestJoinFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(4)
         finally:
             env.close()
diff --git a/sage_tests/core_tests/kafka_test.py b/sage_tests/core_tests/kafka_test.py
index a26ebfa..404317f 100644
--- a/sage_tests/core_tests/kafka_test.py
+++ b/sage_tests/core_tests/kafka_test.py
@@ -1,8 +1,9 @@
 import pytest
 import json
 import time
-from unittest.mock import Mock, patch
-from sage_core.api.local_environment import LocalStreamEnvironment, RemoteEnvironment
+import threading
+from unittest.mock import Mock, patch, MagicMock
+from sage_core.api.env import LocalEnvironment, RemoteEnvironment
 from sage_core.function.kafka_source import KafkaSourceFunction
 from sage_core.function.base_function import BaseFunction
 
@@ -308,7 +309,7 @@ class TestKafkaSourceIntegration:
     
     def test_environment_kafka_source_creation(self):
         """测试Environment创建Kafka源"""
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="localhost:9092",
@@ -320,10 +321,10 @@ class TestKafkaSourceIntegration:
         
         # 验证DataStream创建
         assert kafka_stream is not None
-        assert len(env.pipeline) == 1
+        assert len(env._pipeline) == 1
         
         # 验证Transformation类型
-        transformation = env.pipeline[0]
+        transformation = env._pipeline[0]
         assert transformation.function_class == KafkaSourceFunction
         
         # 验证参数传递
@@ -396,7 +397,7 @@ class TestKafkaSourcePipeline:
         mock_kafka_consumer.return_value = mock_consumer_instance
         
         # 创建测试pipeline
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="localhost:9092",
@@ -412,15 +413,15 @@ class TestKafkaSourcePipeline:
         processed_stream = kafka_stream.map(ProcessTestEvent)
         
         # 验证pipeline构建
-        assert len(env.pipeline) == 2  # source + map
+        assert len(env._pipeline) == 2  # source + map
         
         # 验证source配置
-        source_transformation = env.pipeline[0]
+        source_transformation = env._pipeline[0]
         assert source_transformation.function_class == KafkaSourceFunction
         assert source_transformation.function_kwargs['topic'] == "test_topic"
         
         # 验证map配置
-        map_transformation = env.pipeline[1]
+        map_transformation = env._pipeline[1]
         assert map_transformation.function_class == ProcessTestEvent
     
     def test_kafka_source_with_custom_deserializer(self):
@@ -431,7 +432,7 @@ class TestKafkaSourcePipeline:
             json_data['custom_prefix'] = 'CUSTOM_'
             return json_data
         
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="localhost:9092",
@@ -441,7 +442,7 @@ class TestKafkaSourcePipeline:
         )
         
         # 验证自定义反序列化器传递
-        transformation = env.pipeline[0]
+        transformation = env._pipeline[0]
         assert transformation.function_kwargs['value_deserializer'] == custom_deserializer
 
 
@@ -450,7 +451,7 @@ class TestKafkaSourceConfiguration:
     
     def test_default_configuration(self):
         """测试默认配置"""
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="localhost:9092",
@@ -458,7 +459,7 @@ class TestKafkaSourceConfiguration:
             group_id="test_group"
         )
         
-        transformation = env.pipeline[0]
+        transformation = env._pipeline[0]
         
         # 验证默认值
         assert transformation.function_kwargs['auto_offset_reset'] == 'latest'
@@ -468,7 +469,7 @@ class TestKafkaSourceConfiguration:
     
     def test_custom_configuration(self):
         """测试自定义配置"""
-        env = LocalStreamEnvironment()
+        env = LocalEnvironment()
         
         kafka_stream = env.from_kafka_source(
             bootstrap_servers="custom:9092",
@@ -482,7 +483,7 @@ class TestKafkaSourceConfiguration:
             security_protocol="SSL"
         )
         
-        transformation = env.pipeline[0]
+        transformation = env._pipeline[0]
         
         # 验证自定义值
         assert transformation.function_kwargs['bootstrap_servers'] == "custom:9092"
diff --git a/sage_tests/core_tests/keyby_test.py b/sage_tests/core_tests/keyby_test.py
index 999a87d..e380cc4 100644
--- a/sage_tests/core_tests/keyby_test.py
+++ b/sage_tests/core_tests/keyby_test.py
@@ -1,9 +1,11 @@
+import pytest
 import time
 import threading
 from typing import List, Dict, Any
-from sage_core.api.local_environment import LocalStreamEnvironment
+from sage_core.api.env import LocalEnvironment
 from sage_core.function.source_function import SourceFunction
 from sage_core.function.keyby_function import KeyByFunction
+from sage_core.function.base_function import BaseFunction
 from sage_core.function.sink_function import SinkFunction
 
 
@@ -54,8 +56,8 @@ class ParallelDebugSink(SinkFunction):
         # 从runtime_context获取parallel_index
         if self.runtime_context:
             self.parallel_index = self.runtime_context.parallel_index
-        ctx
-        with self._lock:ctx
+        
+        with self._lock:
             if self.parallel_index not in self._received_data:
                 self._received_data[self.parallel_index] = []
             
@@ -99,7 +101,7 @@ class TestKeyByFunctionality:
         print("\n🚀 Testing KeyBy Hash Partitioning")
         
         # 创建环境
-        env = LocalStreamEnvironment("keyby_test")
+        env = LocalEnvironment("keyby_test")
         
         # 构建数据流：source -> keyby -> parallel sink
         result_stream = (
@@ -114,7 +116,7 @@ class TestKeyByFunctionality:
         try:
             # 提交并运行
             env.submit()
-            
+            env.run_streaming()
             
             # 运行一段时间让数据流过
             time.sleep(3)
@@ -132,7 +134,7 @@ class TestKeyByFunctionality:
         """测试广播策略"""
         print("\n🚀 Testing KeyBy Broadcast Strategy")
         
-        env = LocalStreamEnvironment("keyby_broadcast_test")
+        env = LocalEnvironment("keyby_broadcast_test")
         
         result_stream = (
             env.from_source(TestDataSource, delay=0.3)
@@ -145,7 +147,7 @@ class TestKeyByFunctionality:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(2)
         finally:
             env.close()
@@ -241,7 +243,7 @@ class TestAdvancedKeyBy:
         """测试复杂的key提取逻辑"""
         print("\n🚀 Testing Advanced Key Extraction")
         
-        env = LocalStreamEnvironment("advanced_keyby_test")
+        env = LocalEnvironment("advanced_keyby_test")
         
         result_stream = (
             env.from_source(TestDataSource, delay=0.4)
@@ -254,7 +256,7 @@ class TestAdvancedKeyBy:
         
         try:
             env.submit()
-            
+            env.run_streaming()
             time.sleep(3)
         finally:
             env.close()
diff --git a/sage_tests/example_tests/pipeline_test.py b/sage_tests/example_tests/pipeline_test.py
index 6f614ad..d18dbaa 100644
--- a/sage_tests/example_tests/pipeline_test.py
+++ b/sage_tests/example_tests/pipeline_test.py
@@ -6,9 +6,9 @@ import logging
 import time
 from typing import TYPE_CHECKING
 
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_libs.io.sink import FileSink
-from sage_libs.io.source import FileSource
+from sage_core.api.env import LocalEnvironment
+from sage_common_funs.io.sink import FileSink
+from sage_common_funs.io.source import FileSource
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.refiner import AbstractiveRecompRefiner
@@ -21,7 +21,7 @@ if TYPE_CHECKING:
 
 def init_memory_and_pipeline():
     # 创建一个新的管道实例
-    pipeline = LocalStreamEnvironment()
+    pipeline = LocalEnvironment()
 
     # 步骤 1: 定义数据源（例如，来自用户的查询）
     query_stream: DataStream = pipeline.from_source(FileSource,config["source"])  # 从文件源读取数据
diff --git a/sage_tests/function_tests/io_tests/__init__.py b/sage_tests/function_tests/io_tests/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/sage_tests/function_tests/io_tests/sink_test.py b/sage_tests/function_tests/io_tests/sink_test.py
index ad5c24e..5bde7e1 100644
--- a/sage_tests/function_tests/io_tests/sink_test.py
+++ b/sage_tests/function_tests/io_tests/sink_test.py
@@ -1,10 +1,10 @@
 import pytest
 
+from sage_common_funs.io.sink import (
+    TerminalSink, RetriveSink, FileSink, MemWriteSink
+)
 import os
 
-from sage_libs.io.sink import TerminalSink, RetriveSink, FileSink, MemWriteSink
-
-
 @pytest.fixture
 def sample_qa_data():
     return ("What is AI?", "Artificial Intelligence")
diff --git a/sage_tests/function_tests/io_tests/source_test.py b/sage_tests/function_tests/io_tests/source_test.py
index 9b2b23e..8ca547a 100644
--- a/sage_tests/function_tests/io_tests/source_test.py
+++ b/sage_tests/function_tests/io_tests/source_test.py
@@ -1,7 +1,9 @@
 import pytest
 
-from sage_libs.io.source import FileSource
-
+from sage_common_funs.io.source import (
+    FileSource
+)
+import os
 
 @pytest.fixture
 def sample_file(tmp_path):
diff --git a/sage_tests/function_tests/rag_tests/retriever_test.py b/sage_tests/function_tests/rag_tests/retriever_test.py
index 1196c28..501c9f4 100644
--- a/sage_tests/function_tests/rag_tests/retriever_test.py
+++ b/sage_tests/function_tests/rag_tests/retriever_test.py
@@ -21,7 +21,7 @@ def test_dense_retriever_execute(dense_retriever_config):
 
     # mock runtime_context
     mock_runtime_context = MagicMock()
-    retriever.ctx = mock_runtime_context
+    retriever.runtime_context = mock_runtime_context
     mock_runtime_context.retrieve.return_value = ["doc1", "doc2"]
 
     input_query = "test query"
@@ -62,7 +62,7 @@ def test_bm25s_retriever_execute(bm25s_retriever_config, caplog):
     # 注入 runtime_context mock（不需要 memory 了）
     mock_runtime_context = MagicMock()
     mock_runtime_context.retrieve.return_value = ["doc1", "doc2"]
-    retriever.ctx = mock_runtime_context
+    retriever.runtime_context = mock_runtime_context
 
     data = "some query"
 
@@ -82,7 +82,7 @@ def test_bm25s_retriever_execute_no_collection(bm25s_retriever_config):
     mock_runtime_context = MagicMock()
     mock_memory = MagicMock()
     mock_runtime_context.memory = mock_memory
-    retriever.ctx = mock_runtime_context
+    retriever.runtime_context = mock_runtime_context
 
     data = "some query"
 
diff --git a/sage_tests/runtime_tests/test_message_queue.py b/sage_tests/runtime_tests/test_message_queue.py
index f18b588..083843d 100644
--- a/sage_tests/runtime_tests/test_message_queue.py
+++ b/sage_tests/runtime_tests/test_message_queue.py
@@ -3,7 +3,7 @@ import threading
 import random
 import time
 import queue
-from sage_runtime.utils.local_message_queue import LocalMessageQueue
+from sage_runtime.io.local_message_queue import LocalMessageQueue
 
 
 class TestMessageQueue(unittest.TestCase):
diff --git a/sage_tests/runtime_tests/test_qa_oneshot.py b/sage_tests/runtime_tests/test_qa_oneshot.py
index 0aae8b5..a5823ed 100644
--- a/sage_tests/runtime_tests/test_qa_oneshot.py
+++ b/sage_tests/runtime_tests/test_qa_oneshot.py
@@ -1,11 +1,73 @@
+# import unittest
+# import logging
+# import time
+# import io
+# from contextlib import redirect_stdout
+# from pathlib import Path
+# import os
+# import yaml
+# from dotenv import load_dotenv
+#
+# from sage_core.api.env import LocalEnvironment
+# from sage_common_funs.io.sink import TerminalSink
+# from sage_common_funs.io.source import FileSource
+# from sage_common_funs.rag.generator import OpenAIGenerator
+# from sage_common_funs.rag.promptor import QAPromptor
+# from sage_common_funs.rag.retriever import DenseRetriever
+# from sage_utils.config_loader import load_config
+# from sage_utils.logging_utils import configure_logging
+#
+#
+# class TestFullLocalPipeline(unittest.TestCase):
+#     def setUp(self):
+#         configure_logging(level=logging.INFO)
+#         load_dotenv(override=False)
+#         self.config = load_config('config_mixed.yaml')
+#         api_key = os.environ.get("ALIBABA_API_KEY")
+#         if api_key:
+#             self.config.setdefault("generator", {})["api_key"] = api_key
+#
+#     def test_local_pipeline_run(self):
+#         """测试 Remote pipeline 执行，并验证是否输出至少 5 次 Q/A"""
+#         env = LocalEnvironment()
+#         env.set_memory(config=None)
+#
+#         query_stream = (env
+#             .from_source(FileSource, self.config["source"])
+#             .map(DenseRetriever, self.config["retriever"])
+#             .map(QAPromptor, self.config["promptor"])
+#             .map(OpenAIGenerator, self.config["generator"])
+#             .sink(TerminalSink, self.config["sink"])
+#         )
+#
+#         env.submit()
+#
+#         with io.StringIO() as buf, redirect_stdout(buf):
+#             env.run_once()
+#             env.run_once()
+#             env.run_once()
+#             time.sleep(15)
+#             output = buf.getvalue()
+#
+#         q_count = output.count("[Q] Question :")
+#         a_count = output.count("[A] Answer :")
+#
+#         self.assertGreaterEqual(q_count, 3, f"Question 输出不足 3 次，实际为 {q_count}")
+#         self.assertGreaterEqual(a_count, 3, f"Answer 输出不足 3 次，实际为 {a_count}")
+#
+#
+#
+# if __name__ == '__main__':
+#     unittest.main()
+
 import logging
 import pytest
 from dotenv import load_dotenv
 import os
 
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_libs.io.sink import TerminalSink
-from sage_libs.io.source import FileSource
+from sage_core.api.env import LocalEnvironment
+from sage_common_funs.io.sink import TerminalSink
+from sage_common_funs.io.source import FileSource
 from sage_libs.rag.generator import OpenAIGenerator
 from sage_libs.rag.promptor import QAPromptor
 from sage_libs.rag.retriever import DenseRetriever
@@ -26,7 +88,7 @@ def config():
 
 @pytest.fixture(scope="function")
 def env():
-    env = LocalStreamEnvironment()
+    env = LocalEnvironment()
     env.set_memory(config=None)
     yield env
     # teardown: 主动清理资源
diff --git a/sage_tests/utils_tests/test_name_server.py b/sage_tests/utils_tests/test_name_server.py
index f18dba0..684fba4 100644
--- a/sage_tests/utils_tests/test_name_server.py
+++ b/sage_tests/utils_tests/test_name_server.py
@@ -1,6 +1,6 @@
 import unittest
 import threading
-from sage_jobmanager.utils.name_server import NameServer
+from sage_utils.name_server import NameServer
 
 class TestNameServer(unittest.TestCase):
     
diff --git a/sage_tests/utils_tests/test_ray_object_trimmer.py b/sage_tests/utils_tests/test_ray_object_trimmer.py
deleted file mode 100644
index 68c2cc1..0000000
--- a/sage_tests/utils_tests/test_ray_object_trimmer.py
+++ /dev/null
@@ -1,389 +0,0 @@
-"""
-测试Ray对象预处理器的效果
-验证trim_object_for_ray函数能否正确清理对象以供Ray序列化
-"""
-import sys
-import os
-import time
-import threading
-import pickle
-from typing import Any, Dict, List, Optional
-from pathlib import Path
-
-# 添加项目根路径
-SAGE_ROOT = Path(__file__).parent.parent
-sys.path.insert(0, str(SAGE_ROOT))
-
-from sage_utils.serialization.dill_serializer import (
-    trim_object_for_ray,
-    RayObjectTrimmer,
-    SerializationError
-)
-
-# 尝试导入Ray
-try:
-    import ray
-    RAY_AVAILABLE = True
-    print("Ray is available for testing")
-except ImportError:
-    RAY_AVAILABLE = False
-    print("Ray not available, will only test object trimming")
-
-
-class ProblematicClass:
-    """包含不可序列化内容的测试类"""
-    
-    def __init__(self, name: str):
-        self.name = name
-        self.data = {"key": "value", "number": 42}
-        
-        # 不可序列化的属性
-        self.logger = self._create_mock_logger()
-        self.thread = threading.Thread(target=lambda: None)
-        self.lock = threading.Lock()
-        self.file_handle = open(__file__, 'r')  # 文件句柄
-        self.socket_mock = self._create_socket_mock()
-        
-        # 循环引用
-        self.self_ref = self
-        
-        # 可序列化的属性
-        self.config = {
-            "setting1": "value1",
-            "setting2": 100,
-            "nested": {"inner": "data"}
-        }
-        self.items = [1, 2, 3, "test"]
-        
-        # 定义序列化排除列表
-        self.__state_exclude__ = [
-            'logger', 'thread', 'lock', 'file_handle', 'socket_mock', 'self_ref'
-        ]
-    
-    def _create_mock_logger(self):
-        """创建模拟日志对象"""
-        class MockLogger:
-            def info(self, msg): pass
-            def error(self, msg): pass
-        return MockLogger()
-    
-    def _create_socket_mock(self):
-        """创建模拟socket对象"""
-        class MockSocket:
-            def __init__(self):
-                self.connected = False
-        return MockSocket()
-    
-    def get_safe_data(self):
-        """返回可以安全序列化的数据"""
-        return {
-            "name": self.name,
-            "data": self.data,
-            "config": self.config,
-            "items": self.items
-        }
-    
-    def __del__(self):
-        # 清理文件句柄
-        try:
-            if hasattr(self, 'file_handle') and self.file_handle:
-                self.file_handle.close()
-        except:
-            pass
-
-
-class NestedProblematicClass:
-    """包含嵌套问题的测试类"""
-    
-    def __init__(self):
-        self.name = "nested_test"
-        self.problematic_child = ProblematicClass("child")
-        self.safe_data = {"nested": "safe"}
-        self.thread_list = [threading.Thread(target=lambda: None) for _ in range(3)]
-
-
-def test_basic_trimming():
-    """测试基本的对象trim功能"""
-    print("\n=== 测试基本Trim功能 ===")
-    
-    # 创建有问题的对象
-    obj = ProblematicClass("test_object")
-    
-    print(f"原始对象属性数量: {len(obj.__dict__)}")
-    print("原始对象属性:", list(obj.__dict__.keys()))
-    
-    # 执行trim
-    try:
-        trimmed_obj = trim_object_for_ray(obj)
-        
-        print(f"清理后对象属性数量: {len(trimmed_obj.__dict__)}")
-        print("清理后对象属性:", list(trimmed_obj.__dict__.keys()))
-        
-        # 验证清理效果
-        assert hasattr(trimmed_obj, 'name'), "name属性应该被保留"
-        assert hasattr(trimmed_obj, 'data'), "data属性应该被保留"
-        assert hasattr(trimmed_obj, 'config'), "config属性应该被保留"
-        assert not hasattr(trimmed_obj, 'logger'), "logger属性应该被移除"
-        assert not hasattr(trimmed_obj, 'thread'), "thread属性应该被移除"
-        assert not hasattr(trimmed_obj, 'lock'), "lock属性应该被移除"
-        
-        print("✓ 基本trim测试通过")
-        return trimmed_obj
-        
-    except Exception as e:
-        print(f"✗ 基本trim测试失败: {e}")
-        raise
-
-
-def test_custom_include_exclude():
-    """测试自定义include/exclude功能"""
-    print("\n=== 测试自定义Include/Exclude ===")
-    
-    obj = ProblematicClass("test_custom")
-    
-    # 测试exclude
-    trimmed_obj = trim_object_for_ray(obj, exclude=['data', 'config'])
-    assert hasattr(trimmed_obj, 'name'), "name应该保留"
-    assert not hasattr(trimmed_obj, 'data'), "data应该被排除"
-    assert not hasattr(trimmed_obj, 'config'), "config应该被排除"
-    print("✓ Exclude测试通过")
-    
-    # 测试include
-    trimmed_obj = trim_object_for_ray(obj, include=['name', 'items'])
-    assert hasattr(trimmed_obj, 'name'), "name应该包含"
-    assert hasattr(trimmed_obj, 'items'), "items应该包含"
-    assert not hasattr(trimmed_obj, 'data'), "data不在include中应该被排除"
-    print("✓ Include测试通过")
-
-
-def test_ray_object_trimmer():
-    """测试RayObjectTrimmer类的功能"""
-    print("\n=== 测试RayObjectTrimmer类 ===")
-    
-    obj = ProblematicClass("trimmer_test")
-    
-    # 测试浅层清理
-    shallow_cleaned = RayObjectTrimmer.trim_for_remote_call(obj, deep_clean=False)
-    print(f"浅层清理后属性: {list(shallow_cleaned.__dict__.keys())}")
-    
-    # 测试深度清理
-    deep_cleaned = RayObjectTrimmer.trim_for_remote_call(obj, deep_clean=True)
-    print(f"深度清理后属性: {list(deep_cleaned.__dict__.keys())}")
-    
-    # 验证清理效果
-    assert not hasattr(deep_cleaned, 'logger'), "logger应该被移除"
-    assert hasattr(deep_cleaned, 'name'), "name应该保留"
-    
-    print("✓ RayObjectTrimmer测试通过")
-
-
-def test_nested_object_trimming():
-    """测试嵌套对象的清理"""
-    print("\n=== 测试嵌套对象清理 ===")
-    
-    nested_obj = NestedProblematicClass()
-    
-    print("原始嵌套对象结构:")
-    print(f"  - 顶层属性: {list(nested_obj.__dict__.keys())}")
-    if hasattr(nested_obj, 'problematic_child'):
-        print(f"  - 子对象属性: {list(nested_obj.problematic_child.__dict__.keys())}")
-    
-    # 执行嵌套清理
-    trimmed_nested = trim_object_for_ray(nested_obj)
-    
-    print("清理后嵌套对象结构:")
-    print(f"  - 顶层属性: {list(trimmed_nested.__dict__.keys())}")
-    if hasattr(trimmed_nested, 'problematic_child'):
-        print(f"  - 子对象属性: {list(trimmed_nested.problematic_child.__dict__.keys())}")
-    
-    # 验证嵌套清理
-    assert hasattr(trimmed_nested, 'name'), "顶层name应该保留"
-    assert hasattr(trimmed_nested, 'safe_data'), "safe_data应该保留"
-    assert not hasattr(trimmed_nested, 'thread_list'), "thread_list应该被移除"
-    
-    # 验证子对象也被清理
-    if hasattr(trimmed_nested, 'problematic_child'):
-        child = trimmed_nested.problematic_child
-        assert hasattr(child, 'name'), "子对象name应该保留"
-        assert not hasattr(child, 'logger'), "子对象logger应该被移除"
-    
-    print("✓ 嵌套对象清理测试通过")
-
-
-def test_ray_serialization_validation():
-    """测试Ray序列化验证功能"""
-    print("\n=== 测试Ray序列化验证 ===")
-    
-    if not RAY_AVAILABLE:
-        print("⚠ Ray不可用，跳过序列化验证测试")
-        return
-    
-    obj = ProblematicClass("validation_test")
-    
-    # 验证原始对象
-    print("验证原始对象...")
-    original_result = RayObjectTrimmer.validate_ray_serializable(obj)
-    print(f"原始对象可序列化: {original_result['is_serializable']}")
-    if original_result['issues']:
-        print("问题列表:")
-        for issue in original_result['issues']:
-            print(f"  - {issue}")
-    
-    # 验证清理后的对象
-    print("验证清理后对象...")
-    trimmed_obj = trim_object_for_ray(obj)
-    trimmed_result = RayObjectTrimmer.validate_ray_serializable(trimmed_obj)
-    print(f"清理后对象可序列化: {trimmed_result['is_serializable']}")
-    if trimmed_result['issues']:
-        print("问题列表:")
-        for issue in trimmed_result['issues']:
-            print(f"  - {issue}")
-    else:
-        print(f"序列化大小估计: {trimmed_result['size_estimate']} 字节")
-    
-    # 验证改进效果
-    if not original_result['is_serializable'] and trimmed_result['is_serializable']:
-        print("✓ 清理成功解决了序列化问题")
-    elif original_result['is_serializable'] and trimmed_result['is_serializable']:
-        print("✓ 对象清理后仍然可序列化")
-    else:
-        print("⚠ 清理可能没有完全解决序列化问题")
-
-
-def test_ray_remote_call_simulation():
-    """模拟Ray远程调用测试"""
-    print("\n=== 模拟Ray远程调用测试 ===")
-    
-    if not RAY_AVAILABLE:
-        print("⚠ Ray不可用，跳过远程调用测试")
-        return
-    
-    # 初始化Ray
-    if not ray.is_initialized():
-        ray.init(address="auto", _temp_dir="/var/lib/ray_shared")
-    
-    @ray.remote
-    class TestActor:
-        def process_object(self, obj):
-            """处理接收到的对象"""
-            return {
-                "received_type": type(obj).__name__,
-                "attributes": list(obj.__dict__.keys()) if hasattr(obj, '__dict__') else [],
-                "name": getattr(obj, 'name', 'unknown'),
-                "data": getattr(obj, 'data', None)
-            }
-    
-    # 创建测试对象和Actor
-    obj = ProblematicClass("ray_test")
-    actor = TestActor.remote()
-    
-    # 测试原始对象（可能失败）
-    print("测试原始对象传输...")
-    try:
-        original_result = ray.get(actor.process_object.remote(obj))
-        print(f"原始对象传输成功: {original_result}")
-    except Exception as e:
-        print(f"原始对象传输失败: {e}")
-    
-    # 测试清理后的对象
-    print("测试清理后对象传输...")
-    try:
-        trimmed_obj = trim_object_for_ray(obj)
-        trimmed_result = ray.get(actor.process_object.remote(trimmed_obj))
-        print(f"清理后对象传输成功: {trimmed_result}")
-        print("✓ Ray远程调用测试通过")
-    except Exception as e:
-        print(f"清理后对象传输失败: {e}")
-        print("✗ Ray远程调用测试失败")
-    
-    # 清理Ray
-    try:
-        ray.shutdown()
-    except:
-        pass
-
-
-def test_performance_comparison():
-    """测试性能对比"""
-    print("\n=== 性能对比测试 ===")
-    
-    # 创建大量测试对象
-    objects = [ProblematicClass(f"perf_test_{i}") for i in range(100)]
-    
-    # 测试trim性能
-    start_time = time.time()
-    trimmed_objects = []
-    for obj in objects:
-        try:
-            trimmed = trim_object_for_ray(obj)
-            trimmed_objects.append(trimmed)
-        except Exception as e:
-            print(f"Trim失败: {e}")
-    
-    trim_time = time.time() - start_time
-    print(f"清理100个对象耗时: {trim_time:.4f}秒")
-    print(f"平均每个对象: {trim_time/100:.4f}秒")
-    print(f"成功清理对象数量: {len(trimmed_objects)}")
-    
-    if RAY_AVAILABLE:
-        # 测试Ray序列化性能
-        import ray.cloudpickle as cloudpickle
-        
-        start_time = time.time()
-        serialized_count = 0
-        for obj in trimmed_objects[:10]:  # 只测试前10个
-            try:
-                cloudpickle.dumps(obj)
-                serialized_count += 1
-            except:
-                pass
-        
-        serialize_time = time.time() - start_time
-        print(f"Ray序列化10个清理对象耗时: {serialize_time:.4f}秒")
-        print(f"成功序列化对象数量: {serialized_count}")
-
-
-def run_all_tests():
-    """运行所有测试"""
-    print("开始Ray对象预处理器测试...")
-    print("=" * 50)
-    
-    try:
-        # 基本测试
-        test_basic_trimming()
-        test_custom_include_exclude()
-        test_ray_object_trimmer()
-        test_nested_object_trimming()
-        
-        # Ray相关测试
-        test_ray_serialization_validation()
-        test_ray_remote_call_simulation()
-        
-        # 性能测试
-        test_performance_comparison()
-        
-        print("\n" + "=" * 50)
-        print("✓ 所有测试完成！")
-        
-    except Exception as e:
-        print(f"\n✗ 测试过程中出现错误: {e}")
-        import traceback
-        traceback.print_exc()
-        return False
-    
-    finally:
-        # 清理资源
-        import gc
-        gc.collect()
-        
-    return True
-
-
-if __name__ == "__main__":
-    success = run_all_tests()
-    
-    if success:
-        print("\n🎉 所有测试通过！trim_object_for_ray函数工作正常")
-    else:
-        print("\n❌ 部分测试失败，请检查实现")
-        sys.exit(1)
diff --git a/sage_tests/utils_tests/test_serializer.py b/sage_tests/utils_tests/test_serializer.py
deleted file mode 100644
index 58ad9a8..0000000
--- a/sage_tests/utils_tests/test_serializer.py
+++ /dev/null
@@ -1,325 +0,0 @@
-import logging
-import pytest
-from dotenv import load_dotenv
-import os
-import tempfile
-
-from sage_core.api.local_environment import LocalStreamEnvironment
-from sage_libs.io.sink import TerminalSink
-from sage_libs.io.source import FileSource
-from sage_libs.rag.generator import OpenAIGenerator
-from sage_libs.rag.promptor import QAPromptor
-from sage_libs.rag.retriever import DenseRetriever
-from sage_utils.config_loader import load_config
-from sage_utils.logging_utils import configure_logging
-from sage_utils.serialization.dill_serializer import (
-    serialize_object, deserialize_object, pack_object, unpack_object,
-    save_object_state, load_object_state
-)
-
-
-@pytest.fixture(scope="function")
-def config():
-    configure_logging(level=logging.INFO)
-    load_dotenv(override=False)
-    cfg = load_config("config_mixed.yaml")
-    api_key = os.environ.get("VLLM_API_KEY")
-    if api_key:
-        cfg.setdefault("generator", {})["api_key"] = api_key
-    return cfg
-
-
-@pytest.fixture(scope="function")
-def env():
-    env = LocalStreamEnvironment()
-    env.set_memory(config=None)
-    # return env
-    yield env
-    # teardown: 主动清理资源
-    try:
-        if hasattr(env, "executor"):
-            env.executor.shutdown(wait=False)
-        if hasattr(env, "actors"):
-            for a in env.actors:
-                a.kill()
-    except Exception as e:
-        logging.warning(f"env teardown failed: {e}")
-
-
-def test_env_serialization_and_reconstruction(env, config):
-    """测试环境的序列化和重建"""
-    
-    # 1. 创建一个完整的pipeline
-    query_stream = (env
-        .from_source(FileSource, config["source"])
-        .map(DenseRetriever, config["retriever"])
-        .map(QAPromptor, config["promptor"])
-        .map(OpenAIGenerator, config["generator"])
-        .sink(TerminalSink, config["sink"])
-    )
-    
-    # 调试信息：检查pipeline是否正确构建
-    logging.info(f"Original pipeline length: {len(env._pipeline)}")
-    for i, trans in enumerate(env._pipeline):
-        logging.info(f"Transformation {i}: {type(trans).__name__} - {trans.basename}")
-    
-    # 添加一些自定义属性来测试序列化
-    env.custom_metadata = {
-        "created_at": "2025-01-01",
-        "version": "1.0.0",
-        "description": "Test environment for serialization"
-    }
-    env.processing_stats = {
-        "total_processed": 0,
-        "errors": 0,
-        # "last_run": None
-    }
-    
-    # 2. 序列化环境
-    logging.info("Serializing environment...")
-    serialized_data = serialize_object(env)
-    
-    # # 验证序列化数据包含必要信息
-    # assert '__class_path__' in serialized_data
-    # assert '__attributes__' in serialized_data
-    # assert serialized_data['__class_path__'] == 'sage_core.api.env.LocalEnvironment'
-    
-    # attributes = serialized_data['__attributes__']
-    # assert 'name' in attributes
-    # assert 'config' in attributes
-    # assert 'platform' in attributes
-    # assert '_pipeline' in attributes
-    # assert 'custom_metadata' in attributes
-    # assert 'processing_stats' in attributes
-    
-    # # 调试信息：检查序列化后的pipeline
-    # pipeline_data = attributes['_pipeline']
-    # logging.info(f"Serialized pipeline length: {len(pipeline_data)}")
-    # for i, trans_data in enumerate(pipeline_data):
-    #     if isinstance(trans_data, dict) and '__class_path__' in trans_data:
-    #         logging.info(f"Serialized transformation {i}: {trans_data['__class_path__']}")
-    
-    # # 验证pipeline中的transformations也被序列化
-    # assert len(pipeline_data) == 5, f"Expected 5 transformations, got {len(pipeline_data)}"
-    
-    # logging.info(f"Environment serialized successfully with {len(attributes)} attributes")
-    # logging.info(f"Pipeline contains {len(pipeline_data)} transformations")
-    
-    # 3. 重建环境
-    logging.info("Reconstructing environment from serialized data...")
-    restored_env = deserialize_object(serialized_data)
-    print(f"Restored environment name: {restored_env.name}")
-    # 调试信息：检查恢复后的pipeline
-    logging.info(f"Restored pipeline length: {len(restored_env._pipeline)}")
-    for i, trans in enumerate(restored_env._pipeline):
-        logging.info(f"Restored transformation {i}: {type(trans).__name__} - {trans.basename}")
-    
-    # 验证环境基本信息
-    assert restored_env.name == "local_environment"
-    assert restored_env.platform == env.platform
-    assert restored_env.config == env.config
-    assert restored_env.custom_metadata == env.custom_metadata
-    print( restored_env.processing_stats)
-    assert restored_env.processing_stats == env.processing_stats
-    
-    # 验证pipeline被正确重建
-    assert len(restored_env._pipeline) == len(env._pipeline)
-    
-    # 验证每个transformation的基本信息
-    for i, (original, restored) in enumerate(zip(env._pipeline, restored_env._pipeline)):
-        assert type(original) == type(restored)
-        assert original.basename == restored.basename
-        assert original.parallelism == restored.parallelism
-        assert original.function_class == restored.function_class
-        logging.info(f"Transformation {i}: {original.basename} restored successfully")
-    
-    # ... 其余测试代码保持不变
-
-
-def test_env_binary_serialization(env, config):
-    """测试环境的二进制序列化和重建"""
-    
-    # 创建pipeline
-    query_stream = (env
-        .from_source(FileSource, config["source"])
-        .map(DenseRetriever, config["retriever"])
-        .print()
-    )
-    
-    # 添加测试数据
-    env.test_data = {
-        "numbers": [1, 2, 3, 4, 5],
-        "settings": {"batch_size": 100, "timeout": 30}
-    }
-    
-    # 二进制序列化
-    logging.info("Binary serializing environment...")
-    packed_data = pack_object(env)
-    
-    assert isinstance(packed_data, bytes)
-    assert len(packed_data) > 0
-    
-    logging.info(f"Environment packed to {len(packed_data)} bytes")
-    
-    # 二进制反序列化
-    logging.info("Unpacking environment from binary data...")
-    restored_env = unpack_object(packed_data)
-    
-    # 验证重建结果
-    assert restored_env.test_data == env.test_data
-    assert len(restored_env._pipeline) == len(env._pipeline)
-    
-    # 测试运行
-    restored_env.set_memory(config=None)
-    restored_env.submit()
-    
-    try:
-        restored_env.run_once()
-        logging.info("Binary restored environment executed successfully")
-        restored_env.stop()
-    except Exception as e:
-        pytest.fail(f"Binary restored environment execution failed: {e}")
-    
-    restored_env.close()
-
-
-def test_env_file_persistence(env, config):
-    """测试环境的文件持久化"""
-    
-    # 创建简单的pipeline
-    query_stream = (env
-        .from_source(FileSource, config["source"])
-        .map(QAPromptor, config["promptor"])
-        .print()
-    )
-    
-    # 添加持久化数据
-    env.persistent_data = {
-        "session_id": "test_session_123",
-        "user_preferences": {"theme": "dark", "language": "en"},
-        "cache_settings": {"enabled": True, "size": 1000}
-    }
-    
-    # 保存到临时文件
-    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as tmp_file:
-        tmp_path = tmp_file.name
-    
-    try:
-        logging.info(f"Saving environment state to {tmp_path}")
-        save_object_state(env, tmp_path)
-        
-        # 验证文件存在
-        assert os.path.exists(tmp_path)
-        assert os.path.getsize(tmp_path) > 0
-        
-        # 创建新环境并加载状态
-        new_env = LocalStreamEnvironment("new_env")
-        new_env.set_memory(config=None)
-        
-        logging.info(f"Loading environment state from {tmp_path}")
-        load_success = load_object_state(new_env, tmp_path)
-        
-        assert load_success == True
-        assert new_env.persistent_data == env.persistent_data
-        assert len(new_env.pipeline) == len(env._pipeline)
-        
-        # 测试加载状态后的运行
-        new_env.submit()
-        
-        try:
-            new_env.run_once()
-            logging.info("Environment with loaded state executed successfully")
-            new_env.stop()
-        except Exception as e:
-            pytest.fail(f"Environment with loaded state execution failed: {e}")
-        
-        new_env.close()
-        
-    finally:
-        # 清理临时文件
-        if os.path.exists(tmp_path):
-            os.unlink(tmp_path)
-
-# 创建一个自定义的环境类，带有序列化配置
-class CustomTestEnvironment(LocalStreamEnvironment):
-    # __state_include__ = ['name', 'config', 'platform', '_pipeline', 'important_data']
-    
-    def __init__(self, name: str = "custom_test_env", config: dict = None):
-        super().__init__(name, config)
-        self.important_data = "must be serialized"
-        self.temp_data = "should be ignored"
-        self.secret_key = "confidential"
-
-
-def test_env_serialization_with_custom_attributes(env, config):
-    """测试带有自定义序列化配置的环境"""
-    
-
-    
-    # 创建自定义环境
-    custom_env = CustomTestEnvironment("custom_test")
-    custom_env.set_memory(config=None)
-    
-    # 创建pipeline
-    query_stream = (custom_env
-        .from_source(FileSource, config["source"])
-        .print()
-    )
-    
-    # 序列化
-    serialized_data = serialize_object(custom_env)
-    # attributes = serialized_data['__attributes__']
-    
-    # # 验证只有include的属性被序列化
-    # assert 'important_data' in attributes
-    # assert 'temp_data' not in attributes
-    # assert 'secret_key' not in attributes
-    
-    # 重建
-    restored_env = deserialize_object(serialized_data)
-    
-    assert restored_env.important_data == "must be serialized"
-    # assert not hasattr(restored_env, 'temp_data')
-    # assert not hasattr(restored_env, 'secret_key')
-    assert hasattr(restored_env, 'is_running')
-    
-    # 测试运行
-    restored_env.set_memory(config=None)
-    restored_env.submit()
-    
-    try:
-        restored_env.run_once()
-        logging.info("Custom environment executed successfully")
-        restored_env.stop()
-    except Exception as e:
-        pytest.fail(f"Custom environment execution failed: {e}")
-    
-    restored_env.close()
-    custom_env.close()
-
-
-def test_env_serialization_error_handling():
-    """测试序列化过程中的错误处理"""
-    
-    # 测试无效数据的反序列化
-    invalid_data = {'invalid': 'data'}
-    
-    with pytest.raises(Exception):
-        deserialize_object(invalid_data)
-    
-    # 测试不存在的类路径
-    invalid_class_data = {
-        '__class_path__': 'non.existent.Class',
-        '__attributes__': {}
-    }
-    
-    with pytest.raises(Exception):
-        deserialize_object(invalid_class_data)
-    
-    logging.info("Error handling tests completed successfully")
-
-
-if __name__ == "__main__":
-    # test_env_serialization_with_custom_attributes(env(), config())
-    # 运行测试
-    pytest.main([__file__, "-v"])
\ No newline at end of file
diff --git a/sage_utils/actor_wrapper.py b/sage_utils/actor_wrapper.py
deleted file mode 100644
index 2b96691..0000000
--- a/sage_utils/actor_wrapper.py
+++ /dev/null
@@ -1,73 +0,0 @@
-import ray
-import asyncio
-import concurrent.futures
-from typing import Any, Union
-import logging
-from ray.actor import ActorHandle
-
-class ActorWrapper:
-    """万能包装器，可以将任意对象包装成本地对象或Ray Actor"""
-    
-    def __init__(self, 
-                 obj: Union[Any, ActorHandle]):
-        # 使用 __dict__ 直接设置，避免触发 __setattr__
-        object.__setattr__(self, '_obj', obj)
-        object.__setattr__(self, '_execution_mode', self._detect_execution_mode())
-    
-    def _detect_execution_mode(self) -> str:
-        """检测执行模式"""
-        try:
-            if isinstance(self._obj, ray.actor.ActorHandle):
-                return "ray_actor"
-        except (ImportError, AttributeError):
-            pass
-        return "local"
-    
-    def __getattr__(self, name: str):
-        """透明代理属性访问"""
-        if name.startswith('_'):
-            raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
-        
-        # 获取原始属性/方法
-        try:
-            original_attr = getattr(self._obj, name)
-        except AttributeError:
-            raise AttributeError(f"'{type(self._obj).__name__}' object has no attribute '{name}'")
-        
-        # 如果是方法，需要包装
-        if callable(original_attr):
-            if self._execution_mode == "ray_actor":
-                # Ray Actor方法：返回同步调用包装器
-                def ray_method_wrapper(*args, **kwargs):
-                    future = original_attr.remote(*args, **kwargs)
-                    result = ray.get(future)
-                    return result
-                return ray_method_wrapper
-            else:
-                # 本地方法：直接返回
-                return original_attr
-        else:
-            # 普通属性：直接返回
-            return original_attr
-    
-    def __setattr__(self, name: str, value: Any):
-        """代理属性设置"""
-        if name.startswith('_'):
-            object.__setattr__(self, name, value)
-        else:
-            setattr(self._obj, name, value)
-    
-    def __repr__(self):
-        return f"ActorWrapper[{self._execution_mode}]({repr(self._obj)})"
-    
-    def get_wrapped_object(self):
-        """获取被包装的原始对象"""
-        return self._obj
-    
-    def is_ray_actor(self) -> bool:
-        """检查是否为Ray Actor"""
-        return self._execution_mode == "ray_actor"
-    
-    def is_local(self) -> bool:
-        """检查是否为本地对象"""
-        return self._execution_mode == "local"
\ No newline at end of file
diff --git a/sage_utils/clients/__init__.py b/sage_utils/clients/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/sage_utils/custom_formatter.py b/sage_utils/custom_formatter.py
deleted file mode 100644
index d9dbe2e..0000000
--- a/sage_utils/custom_formatter.py
+++ /dev/null
@@ -1,64 +0,0 @@
-import logging
-import os
-import sys
-from datetime import datetime
-from pathlib import Path
-from typing import Union, Optional
-import threading
-import inspect
-
-
-
-class CustomFormatter(logging.Formatter):
-    """
-    自定义格式化器，合并IDE格式和两行格式：
-    第一行：时间 | 级别 | 对象名 | 文件路径:行号
-    第二行：	→ 日志消息
-    第三行： 留空
-    """
-    COLOR_RESET = "\033[0m"
-    COLOR_DEBUG = "\033[36m"  # 青色
-    COLOR_INFO = "\033[32m"   # 绿色
-    COLOR_WARNING = "\033[33m" # 黄色
-    COLOR_ERROR = "\033[31m"  # 红色
-    COLOR_CRITICAL = "\033[35m" # 紫色
-
-
-    def format(self, record):
-
-        if record.levelno == logging.DEBUG:
-            color = self.COLOR_DEBUG
-        elif record.levelno == logging.INFO:
-            color = self.COLOR_INFO
-        elif record.levelno == logging.WARNING:
-            color = self.COLOR_WARNING
-        elif record.levelno == logging.ERROR:
-            color = self.COLOR_ERROR
-        elif record.levelno == logging.CRITICAL:
-            color = self.COLOR_CRITICAL
-        else:
-            color = self.COLOR_RESET
-
-        # 第一行：时间 | 级别 | 对象名 | 文件路径:行号
-        timestamp = self.formatTime(record, '%Y-%m-%d %H:%M:%S')
-        level = record.levelname
-        name = record.name
-        pathname = record.pathname
-        lineno = record.lineno
-
-        # 第二行：→ 消息内容
-        message = record.getMessage()
-
-        # 如果有异常信息，添加到消息后面
-        if record.exc_info:
-            if not record.exc_text:
-                record.exc_text = self.formatException(record.exc_info)
-        if record.exc_text:
-            message = message + '\n' + record.exc_text
-        if record.stack_info:
-            message = message + '\n' + self.formatStack(record.stack_info)
-
-        # 组合格式：既美观又支持IDE点击
-        formatted_message = f"{timestamp} | {level:<5} | {name} | {pathname}:{lineno} →\n\t {color}{message}{self.COLOR_RESET}\n"
-
-        return formatted_message
\ No newline at end of file
diff --git a/sage_utils/custom_logger.py b/sage_utils/custom_logger.py
index 7f2c7fa..0951605 100644
--- a/sage_utils/custom_logger.py
+++ b/sage_utils/custom_logger.py
@@ -3,27 +3,81 @@ import os
 import sys
 from datetime import datetime
 from pathlib import Path
-from typing import List, Union, Optional, Tuple
+from typing import Union, Optional
 import threading
 import inspect
-from .custom_formatter import CustomFormatter  # 假设有一个自定义格式化器
+from .name_server import get_name
 
+class CustomFormatter(logging.Formatter):
+    """
+    自定义格式化器，合并IDE格式和两行格式：
+    第一行：时间 | 级别 | 对象名 | 文件路径:行号
+    第二行：	→ 日志消息
+    第三行： 留空
+    """
+    COLOR_RESET = "\033[0m"
+    COLOR_DEBUG = "\033[36m"  # 青色
+    COLOR_INFO = "\033[32m"   # 绿色
+    COLOR_WARNING = "\033[33m" # 黄色
+    COLOR_ERROR = "\033[31m"  # 红色
+    COLOR_CRITICAL = "\033[35m" # 紫色
+
+
+    def format(self, record):
+
+        if record.levelno == logging.DEBUG:
+            color = self.COLOR_DEBUG
+        elif record.levelno == logging.INFO:
+            color = self.COLOR_INFO
+        elif record.levelno == logging.WARNING:
+            color = self.COLOR_WARNING
+        elif record.levelno == logging.ERROR:
+            color = self.COLOR_ERROR
+        elif record.levelno == logging.CRITICAL:
+            color = self.COLOR_CRITICAL
+        else:
+            color = self.COLOR_RESET
+
+        # 第一行：时间 | 级别 | 对象名 | 文件路径:行号
+        timestamp = self.formatTime(record, '%Y-%m-%d %H:%M:%S')
+        level = record.levelname
+        name = record.name
+        pathname = record.pathname
+        lineno = record.lineno
+
+        # 第二行：→ 消息内容
+        message = record.getMessage()
 
+        # 如果有异常信息，添加到消息后面
+        if record.exc_info:
+            if not record.exc_text:
+                record.exc_text = self.formatException(record.exc_info)
+        if record.exc_text:
+            message = message + '\n' + record.exc_text
+        if record.stack_info:
+            message = message + '\n' + self.formatStack(record.stack_info)
+
+        # 组合格式：既美观又支持IDE点击
+        formatted_message = f"{timestamp} | {level:<5} | {name} | {pathname}:{lineno} →\n\t {color}{message}{self.COLOR_RESET}\n"
+
+        return formatted_message
 
 
 
 class CustomLogger:
     """
     简化的自定义Logger类
-    支持多种输出目标配置：
-    - "console": 控制台输出
-    - 相对路径: 相对于log_base_folder的路径
-    - 绝对路径: 完整路径的文件输出
+    每个Logger产生三份输出：
+    1. 对象专用文件：{object_name}.log
+    2. 全局时间顺序文件：all_logs.log
+    3. 控制台输出
+    支持动态调整各输出渠道的日志等级
     """
+    # 类级别的默认session管理
+    _default_session_folder: Optional[str] = None
+    _lock = threading.Lock()
     # 全局console debug开关
     _global_console_debug_enabled: bool = True
-    _lock = threading.Lock()
-    
     # 日志级别映射
     _LEVEL_MAPPING = {
         'DEBUG': logging.DEBUG,
@@ -36,296 +90,319 @@ class CustomLogger:
     }
 
     def __init__(self,
-                 outputs: List[Tuple[str, Union[str, int]]] = [("console", "INFO")],
-                 name: str = None,
-                 log_base_folder: str = None):
+                 filename: str,
+                 session_folder: str = None,
+                 env_name: Optional[str] = None,
+                 console_output: Union[bool, str, int] = False,
+                 file_output: Union[bool, str, int] = True,
+                 global_output: Union[bool, str, int] = True,
+                 name: str = None):
         """
         初始化自定义Logger
         
         Args:
-            outputs: 输出配置列表，每个元素为 (output_target, level) 元组
-                    - output_target 可以是:
-                      - "console": 控制台输出
-                      - 相对路径: 相对于log_base_folder的路径，如 "app.log", "logs/error.log"
-                      - 绝对路径: 完整路径，如 "/tmp/app.log"
-                    - level 可以是字符串("DEBUG", "INFO"等) 或数字
-            name: logger名称，默认使用 "Logger"
-            log_base_folder: 日志基础文件夹，用于解析相对路径
-        
-        Examples:
-            # JobManager示例
-            logger = CustomLogger([
-                ("console", "INFO"),
-                ("jobmanager.log", "DEBUG"),           # 相对路径
-                ("error.log", "ERROR"),               # 相对路径
-            ], name="JobManager", log_base_folder="/tmp/sage/logs")
-            
-            # 混合路径示例
-            logger = CustomLogger([
-                ("console", "INFO"),
-                ("app.log", "DEBUG"),                 # 相对于log_base_folder
-                ("/var/log/system.log", "ERROR")      # 绝对路径
-            ], name="MyApp", log_base_folder="./logs")
+            filename: 对象名称，用作logger名称和文件名
+            session_folder: session文件夹路径
+            console_output: 控制台输出设置
+                          - False: 不输出到控制台
+                          - True: 输出所有级别到控制台 (相当于 DEBUG)
+                          - str/int: 指定日志级别，只输出 >= 该级别的日志
+            file_output: 对象专用文件输出设置
+                        - False: 不输出到专用文件
+                        - True: 输出所有级别到专用文件 (相当于 DEBUG)
+                        - str/int: 指定日志级别，只输出 >= 该级别的日志
+            global_output: 全局汇总文件输出设置
+                          - False: 不输出到全局文件
+                          - True: 输出所有级别到全局文件 (相当于 DEBUG)
+                          - str/int: 指定日志级别，只输出 >= 该级别的日志
+            name: 自定义logger名称，默认使用filename
         """
-        self.name = name or "Logger"
-        self.log_base_folder = log_base_folder or os.getcwd()
-        
-        # 确保log_base_folder存在
-        Path(self.log_base_folder).mkdir(parents=True, exist_ok=True)
+        self.object_name = filename if name is None else name
+        self.filename = filename
+        self.env_name = env_name or None
+        # 处理session_folder：空字符串检查和默认值处理
+        if not session_folder:  # None 或空字符串
+            if self._default_session_folder is None:
+                # 如果没有默认session，创建一个新的
+                with self._lock:
+                    if self._default_session_folder is None:
+                        self._default_session_folder = self.create_session_folder()
+            self.session_folder = self._default_session_folder
+        else:
+            self.session_folder = session_folder
+            # 如果这是第一次设置session_folder，将其设为默认值
+            if self._default_session_folder is None:
+                self.set_default_session_folder(session_folder)
+        if self.env_name:
+            self.session_folder = os.path.join(self.session_folder, self.env_name)
 
-        self.logger = logging.getLogger(self.name)
+        self.logger = logging.getLogger(f"{self.object_name}")
 
         # 避免重复初始化同一个logger
         if self.logger.handlers:
             return
 
-        # 解析输出配置
-        self.output_configs = []
-        enabled_levels = []
+        # 提取各输出渠道的日志级别
+        self.console_level = self._extract_log_level(console_output, default_level=logging.DEBUG)
+        self.file_level = self._extract_log_level(file_output, default_level=logging.DEBUG)
+        self.global_level = self._extract_log_level(global_output, default_level=logging.DEBUG)
+        
+        # 记录输出开关状态
+        self.console_enabled = console_output is not False
+        self.file_enabled = file_output is not False
+        self.global_enabled = global_output is not False
         
-        for output_target, level in outputs:
-            level_int = self._extract_log_level(level)
-            self.output_configs.append({
-                'target': output_target,
-                'level': level_int,
-                'level_str': logging.getLevelName(level_int),
-                'handler': None,
-                'resolved_path': self._resolve_path(output_target)
-            })
-            enabled_levels.append(level_int)
-
-        # 设置logger的最低级别
+        # 设置logger的最低级别（取所有启用输出中的最低级别）
+        enabled_levels = []
+        if self.console_enabled:
+            enabled_levels.append(self.console_level)
+        if self.file_enabled:
+            enabled_levels.append(self.file_level)
+        if self.global_enabled:
+            enabled_levels.append(self.global_level)
+            
         min_level = min(enabled_levels) if enabled_levels else logging.INFO
         self.logger.setLevel(min_level)
 
         # 创建统一的自定义格式化器
         formatter = CustomFormatter()
 
-        # 为每个输出目标创建handler
-        for config in self.output_configs:
-            handler = self._create_handler(config, formatter)
-            if handler:
-                handler.setLevel(config['level'])
-                self.logger.addHandler(handler)
-                config['handler'] = handler
+        # 存储handler引用以便动态调整
+        self.console_handler = None
+        self.file_handler = None
+        self.global_handler = None
+
+        # 控制台输出
+        if self.console_enabled and self._global_console_debug_enabled:
+            self.console_handler = logging.StreamHandler()
+            self.console_handler.setLevel(self.console_level)
+            self.console_handler.setFormatter(formatter)
+            self.logger.addHandler(self.console_handler)
+
+        # 文件输出
+        if self.file_enabled or self.global_enabled:
+            # 确保session文件夹存在
+            Path(self.session_folder).mkdir(parents=True, exist_ok=True)
+
+            # 1. 对象专用日志文件
+            if self.file_enabled:
+                object_log_file_path = os.path.join(self.session_folder, f"{self.filename}.log")
+                log_dir = os.path.dirname(object_log_file_path)
+                os.makedirs(log_dir, exist_ok=True)
+
+                self.file_handler = logging.FileHandler(object_log_file_path, encoding='utf-8')
+                self.file_handler.setLevel(self.file_level)
+                self.file_handler.setFormatter(formatter)
+                self.logger.addHandler(self.file_handler)
+
+            # 2. 全局时间顺序日志文件
+            if self.global_enabled:
+                global_log_file_path = os.path.join(self.session_folder, "all_logs.log")
+                log_dir = os.path.dirname(global_log_file_path)
+                os.makedirs(log_dir, exist_ok=True)
+                
+                self.global_handler = logging.FileHandler(global_log_file_path, encoding='utf-8')
+                self.global_handler.setLevel(self.global_level)
+                self.global_handler.setFormatter(formatter)
+                self.logger.addHandler(self.global_handler)
 
         # 不传播到父logger
         self.logger.propagate = False
 
-    def _resolve_path(self, output_target: str) -> str:
+    def _extract_log_level(self, output_setting: Union[bool, str, int], default_level: int = logging.DEBUG) -> int:
         """
-        解析输出路径
+        从输出设置中提取日志级别
         
         Args:
-            output_target: 输出目标
+            output_setting: 输出设置
+                          - False: 返回最高级别（不会实际使用，因为不会创建handler）
+                          - True: 返回默认级别
+                          - str: 日志级别名称，转换为对应的数值
+                          - int: 直接返回该数值
+            default_level: 当 output_setting 为 True 时使用的默认级别
             
-        Returns:
-            str: 解析后的路径
-        """
-        if output_target == "console":
-            return "console"
-        
-        # 检查是否为绝对路径
-        if os.path.isabs(output_target):
-            return output_target
-        else:
-            # 相对路径，相对于log_base_folder
-            return os.path.join(self.log_base_folder, output_target)
-
-    def _extract_log_level(self, level_setting: Union[str, int]) -> int:
-        """
-        从级别设置中提取日志级别
-        
-        Args:
-            level_setting: 级别设置
-        
         Returns:
             int: 对应的日志级别数值
+            
+        Raises:
+            ValueError: 当字符串级别名称无效时
+            TypeError: 当类型不支持时
         """
-        if isinstance(level_setting, str):
-            level_str = level_setting.upper()
+        if output_setting is False:
+            # 返回最高级别，实际上不会使用因为不会创建handler
+            return logging.CRITICAL + 1
+        elif output_setting is True:
+            return default_level
+        elif isinstance(output_setting, str):
+            level_str = output_setting.upper()
             if level_str not in self._LEVEL_MAPPING:
-                raise ValueError(f"Invalid log level: {level_setting}. "
+                raise ValueError(f"Invalid log level: {output_setting}. "
                                f"Valid levels are: {list(self._LEVEL_MAPPING.keys())}")
             return self._LEVEL_MAPPING[level_str]
-        elif isinstance(level_setting, int):
-            return level_setting
+        elif isinstance(output_setting, int):
+            return output_setting
         else:
-            raise TypeError(f"level_setting must be str or int, got {type(level_setting)}")
+            raise TypeError(f"output_setting must be bool, str or int, got {type(output_setting)}")
 
-    def _create_handler(self, config: dict, formatter: CustomFormatter) -> Optional[logging.Handler]:
-        """
-        根据输出配置创建对应的handler
-        
-        Args:
-            config: 输出配置字典
-            formatter: 格式化器
-            
-        Returns:
-            logging.Handler: 创建的handler，如果创建失败返回None
-        """
-        try:
-            if config['target'] == "console":
-                # 控制台输出
-                if self._global_console_debug_enabled:
-                    handler = logging.StreamHandler()
-                    handler.setFormatter(formatter)
-                    return handler
-                else:
-                    return None
-            else:
-                # 文件输出
-                file_path = config['resolved_path']
-                log_dir = os.path.dirname(file_path)
-                if log_dir:  # 如果有目录路径
-                    os.makedirs(log_dir, exist_ok=True)
-                handler = logging.FileHandler(file_path, encoding='utf-8')
-                handler.setFormatter(formatter)
-                return handler
-                
-        except Exception as e:
-            print(f"Failed to create handler for {config['target']}: {e}")
-            return None
-
-    def get_output_configs(self) -> List[dict]:
-        """获取当前输出配置"""
-        return [
-            {
-                'target': config['target'],
-                'resolved_path': config['resolved_path'],
-                'level': config['level_str'],
-                'level_num': config['level'],
-                'handler_active': config['handler'] is not None
-            }
-            for config in self.output_configs
-        ]
-
-    def print_current_configs(self):
-        """打印当前输出配置"""
-        configs = self.get_output_configs()
-        print(f"\n=== Logger '{self.name}' Output Configurations ===")
-        print(f"Log base folder: {self.log_base_folder}")
-        for i, config in enumerate(configs, 1):
-            status = "ACTIVE" if config['handler_active'] else "INACTIVE"
-            print(f"{i}. Target: {config['target']}")
-            if config['target'] != "console":
-                print(f"   Resolved Path: {config['resolved_path']}")
-            print(f"   Level: {config['level']} ({config['level_num']}) - {status}")
-        print(f"Logger minimum level: {logging.getLevelName(self.logger.level)} ({self.logger.level})")
-        print("=" * 60)
-
-    def update_output_level(self, target_index_or_name: Union[int, str], new_level: Union[str, int]):
+    def set_console_level(self, level: Union[str, int, bool]):
         """
-        动态更新指定输出的级别
+        动态设置控制台输出级别
         
         Args:
-            target_index_or_name: 目标索引(0开始)或目标名称
-            new_level: 新的日志级别
+            level: 新的日志级别
+                  - False: 禁用控制台输出
+                  - True: 启用控制台输出，级别为DEBUG
+                  - str/int: 指定具体级别
         """
-        # 查找目标配置
-        target_config = None
-        if isinstance(target_index_or_name, int):
-            if 0 <= target_index_or_name < len(self.output_configs):
-                target_config = self.output_configs[target_index_or_name]
-        else:
-            for config in self.output_configs:
-                if config['target'] == target_index_or_name:
-                    target_config = config
-                    break
-        
-        if not target_config:
-            raise ValueError(f"Output target not found: {target_index_or_name}")
+        old_enabled = self.console_enabled
+        new_level = self._extract_log_level(level)
+        self.console_enabled = level is not False
+        self.console_level = new_level
         
-        # 更新级别
-        new_level_int = self._extract_log_level(new_level)
-        target_config['level'] = new_level_int
-        target_config['level_str'] = logging.getLevelName(new_level_int)
+        # 移除现有的控制台handler
+        if self.console_handler:
+            self.logger.removeHandler(self.console_handler)
+            self.console_handler = None
         
-        # 更新handler级别
-        if target_config['handler']:
-            target_config['handler'].setLevel(new_level_int)
+        # 如果启用控制台输出，添加新的handler
+        if self.console_enabled and self._global_console_debug_enabled:
+            self.console_handler = logging.StreamHandler()
+            self.console_handler.setLevel(self.console_level)
+            self.console_handler.setFormatter(CustomFormatter())
+            self.logger.addHandler(self.console_handler)
         
         # 更新logger的最低级别
-        enabled_levels = [config['level'] for config in self.output_configs if config['handler']]
-        min_level = min(enabled_levels) if enabled_levels else logging.INFO
-        self.logger.setLevel(min_level)
+        self._update_logger_level()
         
-        print(f"Updated {target_config['target']} level to {target_config['level_str']}")
+        status = "enabled" if self.console_enabled else "disabled"
+        level_str = logging.getLevelName(self.console_level) if self.console_enabled else "N/A"
+        print(f"Console output {status} with level: {level_str}")
 
-    def add_output(self, output_target: str, level: Union[str, int]):
+    def set_file_level(self, level: Union[str, int, bool]):
         """
-        动态添加新的输出目标
+        动态设置对象专用文件输出级别
         
         Args:
-            output_target: 输出目标
-            level: 日志级别
+            level: 新的日志级别
         """
-        level_int = self._extract_log_level(level)
+        old_enabled = self.file_enabled
+        new_level = self._extract_log_level(level)
+        self.file_enabled = level is not False
+        self.file_level = new_level
         
-        # 创建新配置
-        new_config = {
-            'target': output_target,
-            'level': level_int,
-            'level_str': logging.getLevelName(level_int),
-            'handler': None,
-            'resolved_path': self._resolve_path(output_target)
-        }
+        # 移除现有的文件handler
+        if self.file_handler:
+            self.logger.removeHandler(self.file_handler)
+            self.file_handler = None
         
-        # 创建handler
-        formatter = CustomFormatter()
-        handler = self._create_handler(new_config, formatter)
-        if handler:
-            handler.setLevel(level_int)
-            self.logger.addHandler(handler)
-            new_config['handler'] = handler
+        # 如果启用文件输出，添加新的handler
+        if self.file_enabled:
+            Path(self.session_folder).mkdir(parents=True, exist_ok=True)
+            object_log_file_path = os.path.join(self.session_folder, f"{self.filename}.log")
             
-        self.output_configs.append(new_config)
+            self.file_handler = logging.FileHandler(object_log_file_path, encoding='utf-8')
+            self.file_handler.setLevel(self.file_level)
+            self.file_handler.setFormatter(CustomFormatter())
+            self.logger.addHandler(self.file_handler)
         
-        # 更新logger最低级别
-        enabled_levels = [config['level'] for config in self.output_configs if config['handler']]
-        min_level = min(enabled_levels) if enabled_levels else logging.INFO
-        self.logger.setLevel(min_level)
+        # 更新logger的最低级别
+        self._update_logger_level()
         
-        print(f"Added output: {output_target} -> {new_config['resolved_path']} with level {new_config['level_str']}")
+        status = "enabled" if self.file_enabled else "disabled"
+        level_str = logging.getLevelName(self.file_level) if self.file_enabled else "N/A"
+        self.info(f"File output {status} with level: {level_str}")
 
-    def remove_output(self, target_index_or_name: Union[int, str]):
+    def set_global_level(self, level: Union[str, int, bool]):
         """
-        移除指定的输出目标
+        动态设置全局汇总文件输出级别
         
         Args:
-            target_index_or_name: 目标索引或名称
+            level: 新的日志级别
         """
-        # 查找并移除配置
-        target_config = None
-        target_index = None
-        
-        if isinstance(target_index_or_name, int):
-            if 0 <= target_index_or_name < len(self.output_configs):
-                target_index = target_index_or_name
-                target_config = self.output_configs[target_index]
-        else:
-            for i, config in enumerate(self.output_configs):
-                if config['target'] == target_index_or_name:
-                    target_index = i
-                    target_config = config
-                    break
+        old_enabled = self.global_enabled
+        new_level = self._extract_log_level(level)
+        self.global_enabled = level is not False
+        self.global_level = new_level
         
-        if not target_config:
-            raise ValueError(f"Output target not found: {target_index_or_name}")
+        # 移除现有的全局handler
+        if self.global_handler:
+            self.logger.removeHandler(self.global_handler)
+            self.global_handler = None
         
-        # 移除handler
-        if target_config['handler']:
-            self.logger.removeHandler(target_config['handler'])
+        # 如果启用全局输出，添加新的handler
+        if self.global_enabled:
+            Path(self.session_folder).mkdir(parents=True, exist_ok=True)
+            global_log_file_path = os.path.join(self.session_folder, "all_logs.log")
+            
+            self.global_handler = logging.FileHandler(global_log_file_path, encoding='utf-8')
+            self.global_handler.setLevel(self.global_level)
+            self.global_handler.setFormatter(CustomFormatter())
+            self.logger.addHandler(self.global_handler)
         
-        # 移除配置
-        self.output_configs.pop(target_index)
+        # 更新logger的最低级别
+        self._update_logger_level()
         
-        # 更新logger最低级别
-        enabled_levels = [config['level'] for config in self.output_configs if config['handler']]
+        status = "enabled" if self.global_enabled else "disabled"
+        level_str = logging.getLevelName(self.global_level) if self.global_enabled else "N/A"
+        self.info(f"Global output {status} with level: {level_str}")
+
+    def _update_logger_level(self):
+        """更新logger的最低级别"""
+        enabled_levels = []
+        if self.console_enabled:
+            enabled_levels.append(self.console_level)
+        if self.file_enabled:
+            enabled_levels.append(self.file_level)
+        if self.global_enabled:
+            enabled_levels.append(self.global_level)
+            
         min_level = min(enabled_levels) if enabled_levels else logging.INFO
         self.logger.setLevel(min_level)
+
+    def get_current_levels(self) -> dict:
+        """
+        获取当前各输出渠道的级别设置
         
-        print(f"Removed output: {target_config['target']}")
+        Returns:
+            dict: 包含各渠道级别信息的字典
+        """
+        return {
+            'console': {
+                'enabled': self.console_enabled,
+                'level': logging.getLevelName(self.console_level) if self.console_enabled else None,
+                'level_num': self.console_level if self.console_enabled else None
+            },
+            'file': {
+                'enabled': self.file_enabled,
+                'level': logging.getLevelName(self.file_level) if self.file_enabled else None,
+                'level_num': self.file_level if self.file_enabled else None
+            },
+            'global': {
+                'enabled': self.global_enabled,
+                'level': logging.getLevelName(self.global_level) if self.global_enabled else None,
+                'level_num': self.global_level if self.global_enabled else None
+            },
+            'logger_min_level': {
+                'level': logging.getLevelName(self.logger.level),
+                'level_num': self.logger.level
+            }
+        }
+
+    def print_current_levels(self):
+        """打印当前各输出渠道的级别设置"""
+        levels = self.get_current_levels()
+        print(f"\n=== Logger '{self.object_name}' Current Levels ===")
+        for channel, info in levels.items():
+            if channel == 'logger_min_level':
+                print(f"Logger minimum level: {info['level']} ({info['level_num']})")
+            else:
+                if info['enabled']:
+                    print(f"{channel.capitalize()} output: {info['level']} ({info['level_num']})")
+                else:
+                    print(f"{channel.capitalize()} output: DISABLED")
+        print("=" * 50)
+
+    # ...existing code...
 
     def _log_with_caller_info(self, level: int, message: str, exc_info: bool = False):
         """
@@ -380,15 +457,54 @@ class CustomLogger:
         """Critical级别日志"""
         self._log_with_caller_info(logging.CRITICAL, message)
 
-    def exception(self, message: str):
-        """异常级别日志，自动包含异常信息"""
-        self.error(message, exc_info=True)
+    def get_log_file_path(self) -> str:
+        """获取当前对象的日志文件路径"""
+        return os.path.join(self.session_folder, f"{self.filename}.log")
+
+    def get_global_log_file_path(self) -> str:
+        """获取全局日志文件路径"""
+        return os.path.join(self.session_folder, "all_logs.log")
+
+    @staticmethod
+    def create_session_folder(base_path: str = "logs") -> str:
+        """
+        创建session文件夹的工具方法, 始终在项目根目录下创建.
+        项目根目录被定义为本文件所在目录的上两级目录.
+        """
+        # 将项目根目录定义为当前文件的上两级目录
+        project_root = Path(os.getcwd())  # 获取当前工作目录
+        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+        # 将 base_path (默认为 "logs") 置于项目根目录下
+        session_folder = project_root / base_path / timestamp
+        session_folder.mkdir(parents=True, exist_ok=True)
+        return str(session_folder)
 
     @classmethod
     def get_available_levels(cls) -> list:
         """获取所有可用的日志级别"""
         return list(cls._LEVEL_MAPPING.keys())
 
+    @classmethod
+    def set_default_session_folder(cls, session_folder: str):
+        """设置默认的session文件夹"""
+        with cls._lock:
+            cls._default_session_folder = session_folder
+            Path(session_folder).mkdir(parents=True, exist_ok=True)
+
+    @classmethod
+    def get_session_folder(cls) -> Optional[str]:
+        if cls._default_session_folder is None:
+            # 如果没有设置默认session文件夹，创建一个新的
+            cls._default_session_folder = cls.create_session_folder()
+        """获取默认的session文件夹"""
+        return cls._default_session_folder
+
+    @classmethod
+    def reset_default_session(cls):
+        """重置默认session（用于测试或重新开始）"""
+        with cls._lock:
+            cls._default_session_folder = None
+
     @classmethod
     def disable_global_console_debug(cls):
         """全局禁用所有console debug输出"""
@@ -404,4 +520,7 @@ class CustomLogger:
     @classmethod
     def is_global_console_debug_enabled(cls) -> bool:
         """检查全局console debug是否启用"""
-        return cls._global_console_debug_enabled
\ No newline at end of file
+        return cls._global_console_debug_enabled
+
+    def exception(self, param):
+        self.error(param)
diff --git a/sage_libs/io/utils/data_loader.py b/sage_utils/data_loader.py
similarity index 100%
rename from sage_libs/io/utils/data_loader.py
rename to sage_utils/data_loader.py
diff --git a/sage_utils/embedding_methods/embedding_api.py b/sage_utils/embedding_methods/embedding_api.py
index f44ce8b..794e68b 100644
--- a/sage_utils/embedding_methods/embedding_api.py
+++ b/sage_utils/embedding_methods/embedding_api.py
@@ -1,4 +1,4 @@
-from sage_utils.embedding_methods.embedding_model import EmbeddingModel
+from sage_utils.embedding_model import EmbeddingModel
 
 
 def apply_embedding_model(name: str = "default",**kwargs) -> EmbeddingModel:
diff --git a/sage_utils/embedding_methods/hf.py b/sage_utils/embedding_methods/hf.py
index 33990ba..17f9cb4 100644
--- a/sage_utils/embedding_methods/hf.py
+++ b/sage_utils/embedding_methods/hf.py
@@ -18,9 +18,19 @@ if not pm.is_installed("tenacity"):
     pm.install("tenacity")
 
 from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+import torch
+import numpy as np
 
 os.environ["TOKENIZERS_PARALLELISM"] = "false"
 
+
 @lru_cache(maxsize=1)
 def initialize_hf_model(model_name):
     hf_tokenizer = AutoTokenizer.from_pretrained(
@@ -35,6 +45,26 @@ def initialize_hf_model(model_name):
     return hf_model, hf_tokenizer
 
 
+
+
+
+# async def hf_embed(text: str, tokenizer, embed_model) -> list[float]:
+#     device = next(embed_model.parameters()).device
+#     encoded_texts = tokenizer(
+#         text, return_tensors="pt", padding=True, truncation=True
+#     ).to(device)
+#     with torch.no_grad():
+#         outputs = embed_model(
+#             input_ids=encoded_texts["input_ids"],
+#             attention_mask=encoded_texts["attention_mask"],
+#         )
+#         embeddings = outputs.last_hidden_state.mean(dim=1)
+#     if embeddings.dtype == torch.bfloat16:
+#         return embeddings.detach().to(torch.float32).cpu()[0].tolist()
+#     else:
+#         return embeddings.detach().cpu()[0].tolist()
+
+
 import torch
 
 def hf_embed_sync(text: str, tokenizer, embed_model) -> list[float]:
diff --git a/sage_utils/embedding_methods/lollms.py b/sage_utils/embedding_methods/lollms.py
index a43751d..d47e1e4 100644
--- a/sage_utils/embedding_methods/lollms.py
+++ b/sage_utils/embedding_methods/lollms.py
@@ -1,7 +1,5 @@
 import sys
 
-import aiohttp
-
 if sys.version_info < (3, 9):
     from typing import AsyncIterator
 else:
diff --git a/sage_utils/embedding_methods/embedding_model.py b/sage_utils/embedding_model.py
similarity index 100%
rename from sage_utils/embedding_methods/embedding_model.py
rename to sage_utils/embedding_model.py
diff --git a/sage_utils/local_tcp_server.py b/sage_utils/local_tcp_server.py
deleted file mode 100644
index 614981a..0000000
--- a/sage_utils/local_tcp_server.py
+++ /dev/null
@@ -1,416 +0,0 @@
-import socket
-import threading
-import pickle
-from typing import Dict, Any, Callable, Optional
-import os
-
-class LocalTcpServer:
-    """
-    本地TCP服务器，用于接收Ray Actor发送的数据
-    支持基于消息类型的多个处理器
-    """
-    
-    def __init__(self, 
-                 host: str = None, 
-                 port: int = None,
-                 default_handler: Optional[Callable[[Dict[str, Any], tuple], None]] = None):
-        """
-        初始化TCP服务器
-        
-        Args:
-            host: 监听地址
-            port: 监听端口
-            default_handler: 默认消息处理回调函数，用于处理未知类型的消息
-        """
-        # ... 现有初始化 ...
-        self.server_cwd = os.getcwd()
-        self.host = host or self._get_host_ip()
-        self.port = port or self._allocate_tcp_port()
-        self.server_socket: Optional[socket.socket] = None
-        self.server_thread: Optional[threading.Thread] = None
-        self.running = False
-        
-        # 消息处理器字典：消息类型 -> 处理函数
-        self.message_handlers: Dict[str, Callable[[Dict[str, Any], tuple], None]] = {}
-        self.default_handler = default_handler
-        
-        # 添加锁保护处理器字典
-        self._handlers_lock = threading.RLock()
-        
-        # 客户端连接管理
-        self.client_connections: Dict[str, socket.socket] = {}  # client_id -> socket
-        self.client_lock = threading.Lock()
-
-    def _get_host_ip(self):
-        """自动获取本机可用于外部连接的 IP 地址"""
-        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
-        try:
-            s.connect(("8.8.8.8", 80))
-            ip = s.getsockname()[0]
-        except Exception:
-            self.logger.warning("Failed to get external IP, using localhost")
-            ip = "127.0.0.1"
-        finally:
-            s.close()
-        return ip
-    
-    def _allocate_tcp_port(self) -> int:
-        print( "Allocating TCP port..." )
-        """为 DAG 分配可用的 TCP 端口"""
-        # 尝试从预设范围分配端口
-        for port in range(19200, 20000):
-            try:
-                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
-                    s.bind((self.host, port))
-                    print("Allocated port:", port)
-                    return port
-            except OSError:
-                continue
-        
-        # 如果预设范围都被占用，使用系统分配
-        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
-            self.logger.warning("All predefined ports are occupied, using system-assigned port")
-            s.bind((self.host, 0))
-            return s.getsockname()[1]
-
-
-
-    def start(self):
-        """启动TCP服务器"""
-        if self.running:
-            self.logger.warning("TCP server is already running")
-            return
-        
-        try:
-            self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-            self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
-            self.server_socket.settimeout(5)
-            self.server_socket.bind((self.host, self.port))
-            self.server_socket.listen(10)
-            
-            self.running = True
-            self.server_thread = threading.Thread(
-                target=self._server_loop,
-                name="LocalTcpServerThread"
-            )
-            self.server_thread.daemon = True
-            self.server_thread.start()
-            
-            self.logger.info(f"TCP server started on {self.host}:{self.port}")
-            
-        except Exception as e:
-            self.logger.error(f"Failed to start TCP server: {e}")
-            self.running = False
-            raise
-    
-    def stop(self):
-        """停止TCP服务器"""
-        if not self.running:
-            return
-        
-        self.logger.info("Stopping TCP server...")
-        self.running = False
-        
-        if self.server_socket:
-            self.server_socket.close()
-        
-        if self.server_thread and self.server_thread.is_alive():
-            for _ in range(5):
-                self.server_thread.join(timeout=1.0)
-                if not self.server_thread.is_alive():
-                    break
-            else:
-                self.logger.warning("TCP server thread did not stop gracefully")
-        
-        self.logger.info("TCP server stopped")
-    
-    def _server_loop(self):
-        """TCP服务器主循环"""
-        self.logger.debug("TCP server loop started")
-        
-        while self.running:
-            try:
-                if not self.server_socket:
-                    break
-                    
-                client_socket, address = self.server_socket.accept()
-                self.logger.debug(f"New TCP client connected from {address}")
-                
-                # 在新线程中处理客户端
-                client_thread = threading.Thread(
-                    target=self._handle_client,
-                    args=(client_socket, address),
-                    name=f"TcpClient-{address[0]}:{address[1]}"
-                )
-                client_thread.daemon = True
-                client_thread.start()
-            except socket.timeout:
-                continue
-            except OSError as e:
-                if self.running:
-                    self.logger.error(f"Error accepting TCP connection: {e}")
-                break
-            except Exception as e:
-                if self.running:
-                    self.logger.error(f"Unexpected error in server loop: {e}")
-        
-        self.logger.debug("TCP server loop stopped")
-    
-    def _handle_client(self, client_socket: socket.socket, address: tuple):
-        """处理客户端连接和消息"""
-        try:
-            while self.running:
-                # 读取消息长度
-                size_data = client_socket.recv(4)
-                if not size_data:
-                    break
-                
-                message_size = int.from_bytes(size_data, byteorder='big')
-                if message_size <= 0 or message_size > 10 * 1024 * 1024:  # 10MB limit
-                    self.logger.warning(f"Invalid message size {message_size} from {address}")
-                    break
-                
-                # 读取消息内容
-                message_data = self._receive_full_message(client_socket, message_size)
-                if not message_data:
-                    break
-                
-                # 反序列化并处理消息
-                try:
-                    message = pickle.loads(message_data)
-                    response = self._process_message(message, address)
-                    
-                    # 发送响应
-                    if response:
-                        self._send_response(client_socket, response)
-                        
-                except Exception as e:
-                    self.logger.error(f"Error processing message from {address}: {e}")
-                    # 发送错误响应
-                    error_response = {
-                        "type": "error_response",
-                        "request_id": message.get("request_id") if 'message' in locals() else None,
-                        "timestamp": int(time.time()),
-                        "status": "error",
-                        "message": f"Internal server error: {str(e)}",
-                        "payload": {"error_code": "ERR_INTERNAL_ERROR"}
-                    }
-                    self._send_response(client_socket, error_response)
-                
-        except Exception as e:
-            self.logger.error(f"Error handling TCP client {address}: {e}")
-        finally:
-            try:
-                client_socket.close()
-            except:
-                pass
-            self.logger.debug(f"TCP client {address} disconnected")
-    
-    def _receive_full_message(self, client_socket: socket.socket, message_size: int) -> Optional[bytes]:
-        """接收完整的消息数据"""
-        message_data = b''
-        while len(message_data) < message_size:
-            chunk_size = min(message_size - len(message_data), 8192)
-            chunk = client_socket.recv(chunk_size)
-            if not chunk:
-                self.logger.warning("Connection closed while receiving message")
-                return None
-            message_data += chunk
-        
-        return message_data
-    
-    def _process_message(self, message: Dict[str, Any], client_address: tuple) -> Optional[Dict[str, Any]]:
-        """
-        处理接收到的消息，根据消息类型分发给对应的处理器
-        
-        Args:
-            message: 接收到的消息字典
-            client_address: 客户端地址
-            
-        Returns:
-            响应字典，如果处理器返回 None 则不发送响应
-        """
-        try:
-            # 尝试获取消息类型
-            message_type = self._extract_message_type(message)
-            if message_type is None:
-                # 无法提取消息类型，使用默认处理器
-                self.logger.warning(f"Could not extract message type from message, using default handler")
-                return self._use_default_handler(message, client_address, None)
-
-            self.logger.debug(f"Processing message type '{message_type}' from {client_address}")
-            
-            # 查找对应的处理器
-            with self._handlers_lock:
-                handler = self.message_handlers.get(message_type, None)
-            
-            if handler is None:
-                # 没有找到对应的处理器，使用默认处理器
-                self.logger.warning(f"No handler found for message type '{message_type}', using default handler")
-                return self._use_default_handler(message, client_address, message_type)
-            
-
-            try:
-                response = handler(message, client_address)
-                self.logger.debug(f"Message type '{message_type}' processed successfully")
-                return response
-            except Exception as e:
-                self.logger.error(f"Error in handler for message type '{message_type}': {e}", exc_info=True)
-                return self._create_error_response(message, "ERR_HANDLER_FAILED", str(e))
-
-                
-        except Exception as e:
-            self.logger.error(f"Error in message processing: {e}", exc_info=True)
-            return self._create_error_response(message, "ERR_PROCESSING_FAILED", str(e))
-    
-    def _extract_message_type(self, message: Dict[str, Any]) -> Optional[str]:
-        """从消息中提取消息类型"""
-        if not isinstance(message, dict):
-            self.logger.warning(f"Message is not a dictionary: {type(message)}")
-            return None
-        
-        # 尝试多种可能的类型字段名
-        type_fields = ['type', 'message_type', 'msg_type', 'event_type', 'command']
-        
-        for field in type_fields:
-            if field in message:
-                msg_type = message[field]
-                if isinstance(msg_type, str) and msg_type.strip():
-                    return msg_type.strip()
-        
-        self.logger.debug(f"No valid type field found in message keys: {list(message.keys())}")
-        return None
-    
-    def _use_default_handler(self, message: Dict[str, Any], client_address: tuple, message_type: Optional[str]) -> Optional[Dict[str, Any]]:
-        """使用默认处理器处理消息"""
-        if self.default_handler:
-            try:
-                response = self.default_handler(message, client_address)
-                self.logger.debug("Message processed by default handler")
-                return response
-            except Exception as e:
-                self.logger.error(f"Error in default handler: {e}", exc_info=True)
-                return self._create_error_response(message, "ERR_DEFAULT_HANDLER_FAILED", str(e))
-        else:
-            self.logger.warning(f"No default handler set, ignoring message from {client_address}")
-            if message_type:
-                self.logger.info(f"Consider registering a handler for message type '{message_type}'")
-            return self._create_error_response(message, "ERR_NO_HANDLER", "No handler available for this message type")
-    
-    def _create_error_response(self, original_message: Dict[str, Any], error_code: str, error_message: str) -> Dict[str, Any]:
-        """创建错误响应"""
-        import time
-        return {
-            "type": f"{original_message.get('type', 'unknown')}_response",
-            "request_id": original_message.get("request_id"),
-            "env_name": original_message.get("env_name"),
-            "env_uuid": original_message.get("env_uuid"),
-            "timestamp": int(time.time()),
-            "status": "error",
-            "message": error_message,
-            "payload": {
-                "error_code": error_code,
-                "details": {}
-            }
-        }
-
-    def _send_response(self, client_socket: socket.socket, response: Dict[str, Any]):
-        """发送响应到客户端"""
-        try:
-            # 序列化响应
-            if isinstance(response, dict):
-                response["cwd"] = self.server_cwd  # 添加服务器当前工作目录
-            serialized = pickle.dumps(response)
-            message_size = len(serialized)
-            
-            # 发送消息长度
-            client_socket.send(message_size.to_bytes(4, byteorder='big'))
-            
-            # 发送消息内容
-            client_socket.send(serialized)
-            
-            self.logger.debug(f"Sent response: {response.get('type')}")
-            
-        except Exception as e:
-            self.logger.error(f"Error sending response: {e}")
-
-    def get_server_info(self) -> Dict[str, Any]:
-        """获取服务器信息"""
-        with self._handlers_lock:
-            registered_types = list(self.message_handlers.keys())
-        
-        return {
-            "host": self.host,
-            "port": self.port,
-            "running": self.running,
-            "address": f"{self.host}:{self.port}",
-            "registered_message_types": registered_types,
-            "has_default_handler": self.default_handler is not None
-        }
-    
-    def __del__(self):
-        """析构函数，确保资源清理"""
-        try:
-            self.stop()
-        except:
-            pass
-
-    ########################################################
-    #                handler  registration                 #
-    ########################################################
-
-    # 修改 register_handler 方法的类型注解，使其返回 Dict[str, Any]
-    def register_handler(self, message_type: str, handler: Callable[[Dict[str, Any], tuple], Dict[str, Any]]):
-        with self._handlers_lock:
-            self.message_handlers[message_type] = handler
-            self.logger.info(f"Registered handler for message type: {message_type}")
-
-    def set_default_handler(self, handler: Callable[[Dict[str, Any], tuple], Dict[str, Any]]):
-        self.default_handler = handler
-        self.logger.info("Default message handler set")
-
-    def unregister_handler(self, message_type: str):
-        with self._handlers_lock:
-            if message_type in self.message_handlers:
-                del self.message_handlers[message_type]
-                self.logger.info(f"Unregistered handler for message type: {message_type}")
-            else:
-                self.logger.warning(f"No handler found for message type: {message_type}")
-
-    def get_registered_types(self) -> list[str]:
-        with self._handlers_lock:
-            return list(self.message_handlers.keys())
-
-
-
-# 使用示例
-if __name__ == "__main__":
-    def handle_status_message(message: Dict[str, Any], client_address: tuple):
-        print(f"Status message from {client_address}: {message}")
-    
-    def handle_data_message(message: Dict[str, Any], client_address: tuple):
-        print(f"Data message from {client_address}: {message}")
-    
-    def handle_unknown_message(message: Dict[str, Any], client_address: tuple):
-        print(f"Unknown message from {client_address}: {message}")
-    
-    # 创建服务器
-    server = LocalTcpServer(default_handler=handle_unknown_message)
-    
-    # 注册不同类型的处理器
-    server.register_handler("status", handle_status_message)
-    server.register_handler("data", handle_data_message)
-    
-    # 启动服务器
-    server.start()
-    
-    print(f"Server info: {server.get_server_info()}")
-    
-    try:
-        # 保持服务器运行
-        import time
-        while True:
-            time.sleep(1)
-    except KeyboardInterrupt:
-        print("Stopping server...")
-        server.stop()
\ No newline at end of file
diff --git a/sage_jobmanager/utils/name_server.py b/sage_utils/name_server.py
similarity index 100%
rename from sage_jobmanager/utils/name_server.py
rename to sage_utils/name_server.py
diff --git a/sage_utils/serialization/__init__.py b/sage_utils/serialization/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/sage_utils/serialization/dill_serializer.py b/sage_utils/serialization/dill_serializer.py
deleted file mode 100644
index 882e8b4..0000000
--- a/sage_utils/serialization/dill_serializer.py
+++ /dev/null
@@ -1,647 +0,0 @@
-import os
-import pickle
-import inspect
-import threading
-import importlib
-from typing import Any, Dict, List, Set, Type, Optional, Union
-from collections.abc import Mapping, Sequence, Set as AbstractSet
-import dill
-
-
-class SerializationError(Exception):
-    """序列化相关错误"""
-    pass
-
-
-# 不可序列化类型黑名单
-_BLACKLIST = [
-    threading.Thread,  # 线程
-    type(open),        # 文件句柄
-    type(threading.Lock),    # 锁
-    type(threading.RLock),   # 递归锁
-    threading.Event,   # 事件
-    threading.Condition,  # 条件变量
-]
-
-# 序列化时需要排除的属性名
-_ATTRIBUTE_BLACKLIST = {
-    'logger',          # 日志对象
-    '_logger',         # 私有日志对象
-    'server_socket',   # socket对象
-    'server_thread',   # 线程对象
-    '_server_thread',  # 私有线程对象
-    'client_socket',   # socket对象
-    '__weakref__',     # 弱引用
-    'runtime_context', # 运行时上下文
-    # 'memory_collection', # 内存集合（通常是Ray Actor句柄）
-    'env',             # 环境引用（避免循环引用）
-    # '_dag_node_factory',  # 工厂对象
-    # '_operator_factory',  # 工厂对象
-    # '_function_factory',  # 工厂对象
-}
-
-# 哨兵值，表示应该跳过的值
-_SKIP_VALUE = object()
-
-
-def _gather_attrs(obj):
-    """枚举实例 __dict__ 和 @property 属性。"""
-    attrs = dict(getattr(obj, "__dict__", {}))
-    for name, prop in inspect.getmembers(type(obj), lambda x: isinstance(x, property)):
-        try:
-            attrs[name] = getattr(obj, name)
-        except Exception:
-            pass
-    return attrs
-
-
-def _filter_attrs(attrs, include, exclude):
-    """根据 include/exclude 过滤字段字典。"""
-    if include:
-        return {k: attrs[k] for k in include if k in attrs}
-    
-    # 合并用户定义的exclude和系统默认的exclude
-    all_exclude = set(exclude or []) | _ATTRIBUTE_BLACKLIST
-    return {k: v for k, v in attrs.items() if k not in all_exclude}
-
-
-def _should_skip(v):
-    """判断对象是否应该跳过序列化"""
-    # 检查黑名单 - 修改为更精确的检查
-    for i, blacklisted_type in enumerate(_BLACKLIST):
-        if isinstance(v, blacklisted_type):
-            # print(f"Skipping blacklisted instance {i}: {type(v)}, {v}")
-            return True
-    
-    # 检查是否是模块（通常不应该序列化）
-    if inspect.ismodule(v):
-        # print(f"Skipping module: {v}")
-        return True
-    
-    return False
-
-
-def _preprocess_for_dill(obj, _seen=None):
-    """
-    递归预处理对象，清理不可序列化的内容，为dill序列化做准备。
-    
-    Args:
-        obj: 要预处理的对象
-        _seen: 已处理对象的集合，用于处理循环引用
-    
-    Returns:
-        预处理后的对象，可以安全地交给dill序列化
-    """
-    # print(f"_preprocess_for_dill called for object: {obj}")
-    if _seen is None:
-        _seen = set()
-    
-    # 防止循环引用
-    obj_id = id(obj)
-    if obj_id in _seen:
-        # print(f"Skipping already seen object: {obj}")
-        return _SKIP_VALUE
-    
-    # 基本类型直接返回
-    if isinstance(obj, (int, float, str, bool, type(None))):
-        return obj
-    
-    # 类对象可以直接被dill序列化，不需要预处理
-    if inspect.isclass(obj):
-        # print(f"Processing class object: {obj}")
-        return obj
-    
-    # 函数对象也可以直接被dill序列化
-    if inspect.isfunction(obj) or inspect.ismethod(obj):
-        # print(f"Processing function object: {obj}")
-        return obj
-
-    # 检查是否应该跳过
-    if _should_skip(obj):
-        return _SKIP_VALUE
-    
-
-
-    # 处理字典
-    if isinstance(obj, Mapping):
-        _seen.add(obj_id)
-        try:
-            cleaned = {}
-            for k, v in obj.items():
-                if not _should_skip(k) and not _should_skip(v):
-                    cleaned_k = _preprocess_for_dill(k, _seen)
-                    cleaned_v = _preprocess_for_dill(v, _seen)
-                    if cleaned_k is not _SKIP_VALUE and ((cleaned_v is not _SKIP_VALUE) or (cleaned_v is None)):
-                        cleaned[cleaned_k] = cleaned_v
-            return cleaned
-        finally:
-            _seen.remove(obj_id)
-    
-    # 处理序列（列表、元组等）
-    if isinstance(obj, Sequence) and not isinstance(obj, str):
-        _seen.add(obj_id)
-        try:
-            cleaned = []
-            for item in obj:
-                if not _should_skip(item):
-                    cleaned_item = _preprocess_for_dill(item, _seen)
-                    if cleaned_item is not _SKIP_VALUE:
-                        cleaned.append(cleaned_item)
-            return type(obj)(cleaned) if cleaned else []
-        finally:
-            _seen.remove(obj_id)
-    
-    # 处理集合
-    if isinstance(obj, AbstractSet):
-        _seen.add(obj_id)
-        try:
-            cleaned = set()
-            for item in obj:
-                if not _should_skip(item):
-                    cleaned_item = _preprocess_for_dill(item, _seen)
-                    if cleaned_item is not _SKIP_VALUE:
-                        cleaned.add(cleaned_item)
-            return type(obj)(cleaned) if cleaned else set()
-        finally:
-            _seen.remove(obj_id)
-    
-    # 处理复杂对象
-    if hasattr(obj, '__dict__'):
-        # print(f"Processing complex object: {obj}")
-        # print(f"dict is {obj.__dict__}")
-        _seen.add(obj_id)
-        try:
-            # 创建一个新的对象实例
-            obj_class = type(obj)
-            
-            # 尝试创建空实例
-            try:
-                cleaned_obj = obj_class.__new__(obj_class)
-            except Exception:
-                # 如果无法创建空实例，返回原对象让dill处理
-                return obj
-            
-            # 获取和过滤属性
-            custom_include = getattr(obj.__class__, "__state_include__", [])
-            custom_exclude = getattr(obj.__class__, "__state_exclude__", [])
-            # if len(custom_exclude) is not 0:
-            #     print(f"custom_exclude is {custom_exclude}")
-            # 一般不用include字段，只用exclude字段就行了    
-            
-            attrs = _gather_attrs(obj)
-            # if len(custom_exclude) is not 0:
-            #     print(f"attrs is {attrs}")
-
-            filtered_attrs = _filter_attrs(attrs, custom_include, custom_exclude)
-            # if len(custom_exclude) is not 0:
-            #     print(f"filtered_attrs is {filtered_attrs}")
-            
-            # 递归清理属性
-            for attr_name, attr_value in filtered_attrs.items():
-                # print(f"Processing attribute: {attr_name} = {attr_value}")
-                if not _should_skip(attr_value):
-                    # print(f"Cleaning attribute: {attr_name}")
-                    cleaned_value = _preprocess_for_dill(attr_value, _seen)
-                    if cleaned_value is not _SKIP_VALUE:
-                        try:
-                            setattr(cleaned_obj, attr_name, cleaned_value)
-                        except Exception:
-                            # 忽略设置失败的属性
-                            pass
-            
-            return cleaned_obj
-        finally:
-            _seen.remove(obj_id)
-    
-    # 对于其他对象，直接返回给dill处理
-    return obj
-
-
-def _postprocess_from_dill(obj, _seen=None):
-    """递归后处理从dill反序列化的对象，清理哨兵值。"""
-    # print(f"_postprocess_from_dill called for object: {obj}")
-    if _seen is None:
-        _seen = set()
-    
-    # 防止循环引用
-    obj_id = id(obj)
-    if obj_id in _seen:
-        return obj
-    
-    # 基本类型直接返回
-    if isinstance(obj, (int, float, str, bool, type(None))):
-        return obj
-    
-    # 跳过哨兵值
-    if obj is _SKIP_VALUE:
-        return None
-    
-    # 处理字典
-    if isinstance(obj, Mapping):
-        _seen.add(obj_id)
-        try:
-            cleaned = {}
-            for k, v in obj.items():
-                # print(f"Processing dict item: {k} = {v}")
-                # 修复：只过滤掉哨兵值，保留所有合法值（包括None、False、0等）
-                if k is not _SKIP_VALUE and v is not _SKIP_VALUE:
-                    cleaned_k = _postprocess_from_dill(k, _seen)
-                    cleaned_v = _postprocess_from_dill(v, _seen)
-                    # 保留所有值，包括None、False、0、空字典等
-                    cleaned[cleaned_k] = cleaned_v
-                    # print(f"Cleaned dict item: {cleaned_k} = {cleaned_v}")
-            return cleaned
-        finally:
-            _seen.remove(obj_id)
-    
-    # 处理序列
-    if isinstance(obj, Sequence) and not isinstance(obj, str):
-        _seen.add(obj_id)
-        try:
-            cleaned = []
-            for item in obj:
-                if item is not _SKIP_VALUE:
-                    cleaned_item = _postprocess_from_dill(item, _seen)
-                    # 保留所有值，包括None、False、0等
-                    cleaned.append(cleaned_item)
-            return type(obj)(cleaned)
-        finally:
-            _seen.remove(obj_id)
-    
-    # 处理集合
-    if isinstance(obj, AbstractSet):
-        _seen.add(obj_id)
-        try:
-            cleaned = set()
-            for item in obj:
-                if item is not _SKIP_VALUE:
-                    cleaned_item = _postprocess_from_dill(item, _seen)
-                    # 集合中不能包含None，但可以包含False、0等
-                    if cleaned_item is not None:
-                        cleaned.add(cleaned_item)
-            return type(obj)(cleaned)
-        finally:
-            _seen.remove(obj_id)
-    
-    # 处理复杂对象
-    if hasattr(obj, '__dict__'):
-        _seen.add(obj_id)
-        try:
-            # 递归清理属性
-            for attr_name, attr_value in list(obj.__dict__.items()):
-                if attr_value is _SKIP_VALUE:
-                    # 删除哨兵值属性
-                    try:
-                        delattr(obj, attr_name)
-                    except Exception:
-                        pass
-                else:
-                    # 递归清理属性值，保留所有合法值
-                    cleaned_value = _postprocess_from_dill(attr_value, _seen)
-                    try:
-                        setattr(obj, attr_name, cleaned_value)
-                    except Exception:
-                        pass
-            
-            return obj
-        finally:
-            _seen.remove(obj_id)
-    
-    return obj
-
-
-class UniversalSerializer:
-    """基于dill的通用序列化器，预处理清理不可序列化内容"""
-    
-    @staticmethod
-    def serialize_object(obj: Any, 
-                        include: Optional[List[str]] = None,
-                        exclude: Optional[List[str]] = None) -> bytes:
-        """
-        序列化任意对象
-        
-        Args:
-            obj: 要序列化的对象
-            include: 包含的属性列表
-            exclude: 排除的属性列表
-            
-        Returns:
-            序列化后的字节数据
-        """
-        if dill is None:
-            raise SerializationError("dill is required for serialization. Install with: pip install dill")
-        
-        try:
-            # 预处理对象，清理不可序列化的内容
-            cleaned_obj = _preprocess_for_dill(obj)
-            
-            # 使用dill序列化
-            return dill.dumps(cleaned_obj)
-            
-        except Exception as e:
-            raise SerializationError(f"Object serialization failed: {e}")
-    
-    @staticmethod
-    def deserialize_object(data: bytes) -> Any:
-        """
-        反序列化对象
-        
-        Args:
-            data: 序列化的字节数据
-            
-        Returns:
-            反序列化后的对象
-        """
-        if dill is None:
-            raise SerializationError("dill is required for deserialization. Install with: pip install dill")
-        
-        try:
-            # 使用dill反序列化
-            obj = dill.loads(data)
-            
-            # 后处理对象，清理哨兵值
-            return _postprocess_from_dill(obj)
-            
-        except Exception as e:
-            raise SerializationError(f"Object deserialization failed: {e}")
-    
-    @staticmethod
-    def save_object_state(obj: Any, path: str,
-                         include: Optional[List[str]] = None,
-                         exclude: Optional[List[str]] = None):
-        """将对象状态保存到文件"""
-        serialized_data = UniversalSerializer.serialize_object(obj, include, exclude)
-        
-        os.makedirs(os.path.dirname(path), exist_ok=True)
-        with open(path, 'wb') as f:
-            f.write(serialized_data)
-    
-    @staticmethod
-    def load_object_from_file(path: str) -> Any:
-        """从文件加载对象"""
-        if not os.path.isfile(path):
-            raise FileNotFoundError(f"File not found: {path}")
-        
-        with open(path, 'rb') as f:
-            data = f.read()
-        
-        return UniversalSerializer.deserialize_object(data)
-    
-    @staticmethod
-    def load_object_state(obj: Any, path: str) -> bool:
-        """从文件加载对象状态到现有对象"""
-        if not os.path.isfile(path):
-            return False
-        
-        try:
-            # 加载序列化的对象
-            loaded_obj = UniversalSerializer.load_object_from_file(path)
-            
-            # 检查类型是否匹配
-            if type(obj) != type(loaded_obj):
-                return False
-            
-            # 复制属性
-            if hasattr(loaded_obj, '__dict__'):
-                # 检查对象的include/exclude配置
-                include = getattr(obj, "__state_include__", [])
-                exclude = getattr(obj, "__state_exclude__", [])
-                
-                for attr_name, attr_value in loaded_obj.__dict__.items():
-                    # 应用include/exclude过滤
-                    if include and attr_name not in include:
-                        continue
-                    if attr_name in (exclude or []):
-                        continue
-                    
-                    try:
-                        setattr(obj, attr_name, attr_value)
-                    except Exception:
-                        pass
-            
-            return True
-            
-        except Exception:
-            return False
-
-
-# 便捷函数
-def serialize_object(obj: Any, 
-                    include: Optional[List[str]] = None,
-                    exclude: Optional[List[str]] = None) -> bytes:
-    """序列化对象的便捷函数"""
-    return UniversalSerializer.serialize_object(obj, include, exclude)
-
-
-def deserialize_object(data: bytes) -> Any:
-    """反序列化对象的便捷函数"""
-    return UniversalSerializer.deserialize_object(data)
-
-
-def save_object_state(obj: Any, path: str,
-                     include: Optional[List[str]] = None,
-                     exclude: Optional[List[str]] = None):
-    """保存对象状态的便捷函数"""
-    return UniversalSerializer.save_object_state(obj, path, include, exclude)
-
-
-def load_object_from_file(path: str) -> Any:
-    """从文件加载对象的便捷函数"""
-    return UniversalSerializer.load_object_from_file(path)
-
-
-def load_object_state(obj: Any, path: str) -> bool:
-    """加载对象状态的便捷函数"""
-    return UniversalSerializer.load_object_state(obj, path)
-
-
-# 向后兼容的函数
-def pack_object(obj: Any, 
-               include: Optional[List[str]] = None,
-               exclude: Optional[List[str]] = None) -> bytes:
-    """打包对象的便捷函数（向后兼容）"""
-    return serialize_object(obj, include, exclude)
-
-
-def unpack_object(data: bytes) -> Any:
-    """解包对象的便捷函数（向后兼容）"""
-    return deserialize_object(data)
-
-
-def trim_object_for_ray(obj: Any, 
-                       include: Optional[List[str]] = None,
-                       exclude: Optional[List[str]] = None) -> Any:
-    """
-    为Ray远程调用预处理对象，移除不可序列化的内容
-    
-    这个函数只做清理工作，不进行实际的序列化，让Ray自己处理序列化过程。
-    适用于在ray.remote调用前清理对象，避免序列化错误。
-    
-    Args:
-        obj: 要预处理的对象
-        include: 包含的属性列表（如果指定，只保留这些属性）
-        exclude: 排除的属性列表（这些属性将被移除）
-        
-    Returns:
-        清理后的对象，可以安全地传递给Ray进行序列化
-        
-    Example:
-        # 清理transformation对象用于Ray调用
-        cleaned_trans = trim_object_for_ray(transformation, 
-                                          exclude=['logger', 'env', '_operator_factory'])
-        
-        # 现在可以安全地传递给Ray
-        result = ray_actor.process_transformation.remote(cleaned_trans)
-    """
-    try:
-        # 使用现有的预处理函数，但不进行dill序列化
-        cleaned_obj = _preprocess_for_dill(obj)
-        
-        # 如果有额外的include/exclude需求，再次过滤
-        if cleaned_obj is not _SKIP_VALUE and hasattr(cleaned_obj, '__dict__'):
-            # 应用用户指定的include/exclude
-            if include or exclude:
-                attrs = _gather_attrs(cleaned_obj)
-                filtered_attrs = _filter_attrs(attrs, include, exclude)
-                
-                # 创建新对象并设置过滤后的属性
-                obj_class = type(cleaned_obj)
-                try:
-                    final_obj = obj_class.__new__(obj_class)
-                    for attr_name, attr_value in filtered_attrs.items():
-                        try:
-                            setattr(final_obj, attr_name, attr_value)
-                        except Exception:
-                            pass  # 忽略设置失败的属性
-                    return final_obj
-                except Exception:
-                    # 如果无法创建新实例，返回原对象
-                    return cleaned_obj
-        
-        return cleaned_obj if cleaned_obj is not _SKIP_VALUE else None
-        
-    except Exception as e:
-        # 如果预处理失败，返回None或抛出异常
-        raise SerializationError(f"Object trimming for Ray failed: {e}")
-
-
-class RayObjectTrimmer:
-    """专门用于Ray远程调用的对象预处理器"""
-    
-    @staticmethod
-    def trim_for_remote_call(obj: Any,
-                           include: Optional[List[str]] = None,
-                           exclude: Optional[List[str]] = None,
-                           deep_clean: bool = True) -> Any:
-        """
-        为Ray远程调用准备对象
-        
-        Args:
-            obj: 要清理的对象
-            include: 只保留这些属性
-            exclude: 排除这些属性
-            deep_clean: 是否进行深度清理（递归处理嵌套对象）
-        
-        Returns:
-            清理后可以传递给Ray的对象
-        """
-        if not deep_clean:
-            # 浅层清理：只处理顶层对象的属性
-            if hasattr(obj, '__dict__'):
-                attrs = _gather_attrs(obj)
-                filtered_attrs = _filter_attrs(attrs, include, exclude)
-                
-                obj_class = type(obj)
-                try:
-                    cleaned_obj = obj_class.__new__(obj_class)
-                    for attr_name, attr_value in filtered_attrs.items():
-                        if not _should_skip(attr_value):
-                            try:
-                                setattr(cleaned_obj, attr_name, attr_value)
-                            except Exception:
-                                pass
-                    return cleaned_obj
-                except Exception:
-                    return obj
-            return obj
-        else:
-            # 深度清理：使用完整的预处理流程
-            return trim_object_for_ray(obj, include, exclude)
-    
-    @staticmethod
-    def trim_transformation_for_ray(transformation_obj) -> Any:
-        """
-        专门为Transformation对象定制的清理方法
-        移除常见的不可序列化属性
-        """
-        exclude_attrs = [
-            'logger', '_logger',           # 日志对象
-            'env',                         # 环境引用（避免循环引用）
-            'runtime_context',             # 运行时上下文
-            '_dag_node_factory',           # 懒加载工厂
-            '_operator_factory',           # 懒加载工厂  
-            '_function_factory',           # 懒加载工厂
-            'server_socket',               # socket对象
-            'server_thread', '_server_thread',  # 线程对象
-        ]
-        
-        return RayObjectTrimmer.trim_for_remote_call(
-            transformation_obj, 
-            exclude=exclude_attrs
-        )
-    
-    @staticmethod
-    def trim_operator_for_ray(operator_obj) -> Any:
-        """
-        专门为Operator对象定制的清理方法
-        """
-        exclude_attrs = [
-            'logger', '_logger',
-            'runtime_context',
-            'emit_context',
-            'server_socket', 'client_socket',
-            'server_thread', '_server_thread',
-            '__weakref__',
-        ]
-        
-        return RayObjectTrimmer.trim_for_remote_call(
-            operator_obj,
-            exclude=exclude_attrs
-        )
-    
-    @staticmethod
-    def validate_ray_serializable(obj: Any, max_depth: int = 3) -> Dict[str, Any]:
-        """
-        验证对象是否可以被Ray序列化
-        
-        Args:
-            obj: 要验证的对象
-            max_depth: 最大检查深度
-        
-        Returns:
-            验证结果字典，包含是否可序列化和问题列表
-        """
-        import ray
-        
-        result = {
-            'is_serializable': False,
-            'issues': [],
-            'size_estimate': 0
-        }
-        
-        try:
-            # 尝试Ray的内部序列化
-            serialized = ray.cloudpickle.dumps(obj)
-            result['is_serializable'] = True
-            result['size_estimate'] = len(serialized)
-            
-        except Exception as e:
-            result['issues'].append(f"Ray serialization failed: {str(e)}")
-            
-            # 尝试识别具体的问题
-            if hasattr(obj, '__dict__'):
-                for attr_name, attr_value in obj.__dict__.items():
-                    if _should_skip(attr_value):
-                        result['issues'].append(f"Problematic attribute: {attr_name} = {type(attr_value)}")
-        
-        return result
\ No newline at end of file
diff --git a/sage_utils/serialization/universal_serializer.py b/sage_utils/serialization/universal_serializer.py
deleted file mode 100644
index 03177af..0000000
--- a/sage_utils/serialization/universal_serializer.py
+++ /dev/null
@@ -1,444 +0,0 @@
-import os
-import pickle
-import inspect
-import threading
-import importlib
-from typing import Any, Dict, List, Set, Type, Optional, Union
-from collections.abc import Mapping, Sequence, Set as AbstractSet
-
-
-class SerializationError(Exception):
-    """序列化相关错误"""
-    pass
-
-
-# 扩展的不可序列化类型黑名单
-_BLACKLIST = (
-    type(open),        # 文件句柄
-    type(threading.Thread),  # 线程
-    type(threading.Lock),    # 锁
-    type(threading.RLock),   # 递归锁
-    type(threading.Event),   # 事件
-    type(threading.Condition),  # 条件变量
-    type(lambda: None),   # 函数类型（除非特殊处理）
-    type(print),       # 内置函数
-    type(len),         # 内置函数
-)
-
-
-# 序列化时需要排除的属性名
-_ATTRIBUTE_BLACKLIST = {
-    'logger',          # 日志对象
-    '_logger',         # 私有日志对象
-    'server_socket',   # socket对象
-    'server_thread',   # 线程对象
-    '_server_thread',  # 私有线程对象
-    'client_socket',   # socket对象
-    '__weakref__',     # 弱引用
-    '__dict__',        # 防止递归
-    'runtime_context', # 运行时上下文
-    'memory_collection', # 内存集合（通常是Ray Actor句柄）
-    'env',             # 环境引用（避免循环引用）
-    '_dag_node_factory',  # 工厂对象
-    '_operator_factory',  # 工厂对象
-    '_function_factory',  # 工厂对象
-}
-
-
-def _gather_attrs(obj):
-    """枚举实例 __dict__ 和 @property 属性。"""
-    attrs = dict(getattr(obj, "__dict__", {}))
-    for name, prop in inspect.getmembers(type(obj), lambda x: isinstance(x, property)):
-        try:
-            attrs[name] = getattr(obj, name)
-        except Exception:
-            pass
-    return attrs
-
-
-def _filter_attrs(attrs, include, exclude):
-    """根据 include/exclude 过滤字段字典。"""
-    if include:
-        return {k: attrs[k] for k in include if k in attrs}
-    
-    # 合并用户定义的exclude和系统默认的exclude
-    all_exclude = set(exclude or []) | _ATTRIBUTE_BLACKLIST
-    return {k: v for k, v in attrs.items() if k not in all_exclude}
-
-
-def _is_serializable(v):
-    print(f"Checking serializability of value: {v} (type: {type(v)})")
-    """判断对象能否通过 pickle 序列化，且不在黑名单中。"""
-    if isinstance(v, _BLACKLIST):
-        print(f"Value {v} is in blacklist, not serializable")
-        return False
-    
-    # 检查是否是模块（模块通常不应该序列化）
-    if inspect.ismodule(v):
-        print(f"Value {v} is a module, not serializable")
-        return False
-    
-    # 检查是否是类（类定义通过类路径序列化，这里返回False让其他逻辑处理）
-    if inspect.isclass(v):
-        return False
-    
-    try:
-        pickle.dumps(v)
-        return True
-    except Exception:
-        print(f"exception not serializable")
-        return False
-
-
-def _is_complex_object(v):
-    """判断是否是需要递归序列化的复杂对象"""
-    # 基本类型不需要递归
-    if isinstance(v, (int, float, str, bool, type(None))):
-        return False
-    
-    # 容器类型需要递归处理内容，但不是复杂对象本身
-    if isinstance(v, (Mapping, Sequence, AbstractSet)) and not isinstance(v, str):
-        return False
-    
-    # 有__dict__属性的对象通常是复杂对象
-    if hasattr(v, '__dict__'):
-        return True
-    
-    return False
-def _can_be_serialized(v):
-    """判断对象是否可以被序列化（直接pickle或通过递归结构）"""
-    # 先检查是否在黑名单中
-    if isinstance(v, _BLACKLIST):
-        return False
-    
-    # 检查是否是模块
-    if inspect.ismodule(v):
-        return False
-    
-    # 检查是否是类
-    if inspect.isclass(v):
-        return True
-    
-    # 如果可以直接pickle，当然可以序列化
-    if _is_serializable(v):
-        return True
-    
-    # 如果是复杂对象，检查是否可以通过递归结构序列化
-    if _is_complex_object(v):
-        return True
-    
-    return False
-
-
-# 在文件开头添加哨兵值
-_SKIP_VALUE = object()  # 用作哨兵值，表示应该跳过的值
-
-def _prepare(v, _seen=None):
-    """递归清洗容器类型，过滤不可序列化元素。"""
-    if _seen is None:
-        _seen = set()
-    
-    # 防止循环引用
-    obj_id = id(v)
-    if obj_id in _seen:
-        return _SKIP_VALUE
-    
-    # 基本类型直接返回
-    if isinstance(v, (int, float, str, bool, type(None))):
-        return v
-
-    # 处理类对象 - 序列化为类路径
-    if inspect.isclass(v):
-        return {
-            '__class_reference__': _get_class_path(v),
-            '__serializer_version__': '1.0'
-        }
-
-    # 处理字典
-    if isinstance(v, Mapping):
-        _seen.add(obj_id)
-        try:
-            result = {}
-            for k, val in v.items():
-                print(f"Processing dict item: {k} -> {val}")
-                if _can_be_serialized(k) and _can_be_serialized(val):
-                    prepared_k = _prepare(k, _seen)
-                    prepared_v = _prepare(val, _seen)
-                    # 只过滤掉哨兵值
-                    if prepared_k is not _SKIP_VALUE and prepared_v is not _SKIP_VALUE:
-                        result[prepared_k] = prepared_v
-                        print(f"Added to result: {prepared_k} -> {prepared_v}")
-            return result
-        finally:
-            _seen.remove(obj_id)
-    
-    # 处理序列（列表、元组等）
-    if isinstance(v, Sequence) and not isinstance(v, str):
-        _seen.add(obj_id)
-        try:
-            cleaned = []
-            for x in v:
-                if _can_be_serialized(x):
-                    prepared_x = _prepare(x, _seen)
-                    # 只过滤掉哨兵值
-                    if prepared_x is not _SKIP_VALUE:
-                        cleaned.append(prepared_x)
-            return type(v)(cleaned) if cleaned else []
-        finally:
-            _seen.remove(obj_id)
-    
-    # 处理集合
-    if isinstance(v, AbstractSet):
-        _seen.add(obj_id)
-        try:
-            cleaned = set()
-            for x in v:
-                if _can_be_serialized(x):
-                    prepared_x = _prepare(x, _seen)
-                    # 只过滤掉哨兵值
-                    if prepared_x is not _SKIP_VALUE:
-                        cleaned.add(prepared_x)
-            return type(v)(cleaned) if cleaned else set()
-        finally:
-            _seen.remove(obj_id)
-    
-    # 处理复杂对象
-    if _is_complex_object(v):
-        _seen.add(obj_id)
-        try:
-            return _serialize_complex_object(v, _seen)
-        finally:
-            _seen.remove(obj_id)
-    
-    # 对于其他可直接序列化的对象，直接返回
-    if _is_serializable(v):
-        return v
-    
-    # 不可序列化的对象返回哨兵值
-    return _SKIP_VALUE
-
-
-def _serialize_complex_object(obj, _seen=None):
-    # print("serializing complex object:", obj)
-    """递归序列化复杂对象的内部结构"""
-    if _seen is None:
-        _seen = set()
-    
-    # 获取对象的类信息
-    obj_class = type(obj)
-    
-    # 检查对象是否有自定义的序列化配置
-    custom_include = getattr(obj, "__state_include__", [])
-    custom_exclude = getattr(obj, "__state_exclude__", [])
-    # print("custom_exclude:", custom_exclude)
-    # 收集所有属性
-    attrs = _gather_attrs(obj)
-    
-    # 过滤属性
-    filtered_attrs = _filter_attrs(attrs, custom_include, custom_exclude)
-    
-    # 准备序列化数据 - 递归处理每个属性
-    prepared_attrs = {}
-    for k, v in filtered_attrs.items():
-        print(f"Preparing attribute: {k} -> {v}")
-        prepared_value = _prepare(v, _seen)
-        # 只过滤掉哨兵值
-        if prepared_value is not _SKIP_VALUE:
-            prepared_attrs[k] = prepared_value
-            print(f"Prepared attribute: {k} -> {prepared_value}")
-    
-    # 构建序列化数据
-    return {
-        '__class_path__': _get_class_path(obj_class),
-        '__attributes__': prepared_attrs,
-        '__serializer_version__': '1.0'
-    }
-
-def _get_class_path(cls):
-    """获取类的完整路径"""
-    return f"{cls.__module__}.{cls.__qualname__}"
-
-
-def _load_class(class_path: str) -> Type:
-    """动态加载类"""
-    try:
-        module_name, class_name = class_path.rsplit('.', 1)
-        module = importlib.import_module(module_name)
-        return getattr(module, class_name)
-    except Exception as e:
-        raise SerializationError(f"Failed to load class {class_path}: {e}")
-
-
-def _restore_value(value):
-    """递归恢复值"""
-    if isinstance(value, dict) and '__class_path__' in value:
-        # 这是一个序列化的复杂对象
-        return _deserialize_complex_object(value)
-    elif isinstance(value, dict) and '__class_reference__' in value:
-        # 这是一个类引用
-        class_path = value['__class_reference__']
-        return _load_class(class_path)
-    elif isinstance(value, list):
-        return [_restore_value(item) for item in value]
-    elif isinstance(value, dict):
-        return {k: _restore_value(v) for k, v in value.items()}
-    elif isinstance(value, set):
-        return {_restore_value(item) for item in value}
-    else:
-        return value
-
-
-def _deserialize_complex_object(data: Dict[str, Any]) -> Any:
-    """反序列化复杂对象"""
-    # 验证数据格式
-    if not isinstance(data, dict) or '__class_path__' not in data:
-        raise SerializationError("Invalid serialized data format")
-    
-    # 加载类
-    class_path = data['__class_path__']
-    obj_class = _load_class(class_path)
-    
-    try:
-        obj = obj_class.__new__(obj_class)
-    except Exception:
-        raise SerializationError(f"Cannot create instance of {class_path}")
-    
-    # 恢复属性
-    attributes = data.get('__attributes__', {})
-    for attr_name, attr_value in attributes.items():
-        try:
-            # 递归反序列化属性值
-            restored_value = _restore_value(attr_value)
-            setattr(obj, attr_name, restored_value)
-        except Exception as e:
-            # 忽略设置失败的属性，但记录日志
-            pass
-    
-    return obj
-
-
-class UniversalSerializer:
-    """通用序列化器，基于反选机制自动处理所有可序列化对象"""
-    
-    @staticmethod
-    def serialize_object(obj: Any, 
-                        include: Optional[List[str]] = None,
-                        exclude: Optional[List[str]] = None) -> Dict[str, Any]:
-        """序列化任意对象"""
-        try:
-            # 使用_serialize_complex_object来处理顶层对象
-            return _serialize_complex_object(obj)
-            
-        except Exception as e:
-            raise SerializationError(f"Object serialization failed: {e}")
-    
-    @staticmethod
-    def deserialize_object(data: Dict[str, Any]) -> Any:
-        """反序列化对象"""
-        try:
-            return _deserialize_complex_object(data)
-            
-        except Exception as e:
-            raise SerializationError(f"Object deserialization failed: {e}")
-    
-    @staticmethod
-    def pack_object(obj: Any, 
-                   include: Optional[List[str]] = None,
-                   exclude: Optional[List[str]] = None) -> bytes:
-        """将对象打包为二进制数据"""
-        serialized_data = UniversalSerializer.serialize_object(obj, include, exclude)
-        return pickle.dumps(serialized_data)
-    
-    @staticmethod
-    def unpack_object(data: bytes, 
-                     constructor_args: Optional[tuple] = None,
-                     constructor_kwargs: Optional[dict] = None) -> Any:
-        """从二进制数据解包对象"""
-        serialized_data = pickle.loads(data)
-        return UniversalSerializer.deserialize_object(serialized_data, constructor_args, constructor_kwargs)
-    
-    @staticmethod
-    def save_object_state(obj: Any, path: str,
-                         include: Optional[List[str]] = None,
-                         exclude: Optional[List[str]] = None):
-        """将对象状态保存到文件"""
-        serialized_data = UniversalSerializer.serialize_object(obj, include, exclude)
-        
-        os.makedirs(os.path.dirname(path), exist_ok=True)
-        with open(path, 'wb') as f:
-            pickle.dump(serialized_data, f)
-    
-    @staticmethod
-    def load_object_state(obj: Any, path: str) -> bool:
-        """从文件加载对象状态"""
-        if not os.path.isfile(path):
-            return False
-        
-        try:
-            with open(path, 'rb') as f:
-                serialized_data = pickle.load(f)
-            
-            # 只恢复属性，不创建新对象
-            attributes = serialized_data.get('__attributes__', {})
-            
-            # 检查对象的include/exclude配置
-            include = getattr(obj, "__state_include__", [])
-            exclude = getattr(obj, "__state_exclude__", [])
-            
-            for attr_name, attr_value in attributes.items():
-                # 应用include/exclude过滤
-                if include and attr_name not in include:
-                    continue
-                if attr_name in (exclude or []):
-                    continue
-                
-                try:
-                    restored_value = _restore_value(attr_value)
-                    setattr(obj, attr_name, restored_value)
-                except Exception:
-                    # 忽略设置失败的属性
-                    pass
-            
-            return True
-            
-        except Exception as e:
-            return False
-
-
-# 便捷函数
-def serialize_object(obj: Any, 
-                    include: Optional[List[str]] = None,
-                    exclude: Optional[List[str]] = None) -> Dict[str, Any]:
-    """序列化对象的便捷函数"""
-    return UniversalSerializer.serialize_object(obj, include, exclude)
-
-
-def deserialize_object(data: Dict[str, Any]) -> Any:
-    """反序列化对象的便捷函数"""
-    return UniversalSerializer.deserialize_object(data)
-
-
-def pack_object(obj: Any, 
-               include: Optional[List[str]] = None,
-               exclude: Optional[List[str]] = None) -> bytes:
-    """打包对象的便捷函数"""
-    return UniversalSerializer.pack_object(obj, include, exclude)
-
-
-def unpack_object(data: bytes, 
-                 constructor_args: Optional[tuple] = None,
-                 constructor_kwargs: Optional[dict] = None) -> Any:
-    """解包对象的便捷函数"""
-    return UniversalSerializer.unpack_object(data, constructor_args, constructor_kwargs)
-
-
-def save_object_state(obj: Any, path: str,
-                     include: Optional[List[str]] = None,
-                     exclude: Optional[List[str]] = None):
-    """保存对象状态的便捷函数"""
-    return UniversalSerializer.save_object_state(obj, path, include, exclude)
-
-
-def load_object_state(obj: Any, path: str) -> bool:
-    """加载对象状态的便捷函数"""
-    return UniversalSerializer.load_object_state(obj, path)
\ No newline at end of file
diff --git a/sage_tests/function_tests/io_tests/test_print_functionality.py b/tests/test_print_functionality.py
similarity index 98%
rename from sage_tests/function_tests/io_tests/test_print_functionality.py
rename to tests/test_print_functionality.py
index f2b49e3..098abe5 100644
--- a/sage_tests/function_tests/io_tests/test_print_functionality.py
+++ b/tests/test_print_functionality.py
@@ -8,11 +8,11 @@ import os
 from io import StringIO
 from unittest.mock import patch
 
-from sage_libs.io.sink import PrintSink
-
 # 添加项目根目录到路径
 sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 
+from sage_common_funs.io.sink import PrintSink
+
 
 class TestPrintSink(unittest.TestCase):
     """测试 PrintSink 类的功能"""
diff --git a/third-party/kafka/sage-kafka-stop.sh b/third-party/kafka/sage-kafka-stop.sh
new file mode 100644
index 0000000..4591d54
--- /dev/null
+++ b/third-party/kafka/sage-kafka-stop.sh
@@ -0,0 +1,22 @@
+#!/bin/bash
+# SAGE Kafka Stop Script
+
+KAFKA_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
+
+echo "Stopping SAGE Kafka services..."
+
+# Stop Kafka
+if pgrep -f "sage-server.properties" > /dev/null; then
+    echo "Stopping Kafka..."
+    "$KAFKA_DIR/bin/kafka-server-stop.sh"
+    sleep 2
+fi
+
+# Stop Zookeeper
+if pgrep -f "sage-zookeeper.properties" > /dev/null; then
+    echo "Stopping Zookeeper..."
+    "$KAFKA_DIR/bin/zookeeper-server-stop.sh"
+    sleep 2
+fi
+
+echo "SAGE Kafka services stopped"
\ No newline at end of file
