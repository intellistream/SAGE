diff --git a/.gitignore b/.gitignore
index 95ae40d..b2b757c 100644
--- a/.gitignore
+++ b/.gitignore
@@ -23,7 +23,6 @@ CMakeUserPresets.json
 /installation/candy/
 /sage.egg-info/
 /deps/CANDY/.github/
-/archive
 
 id_llm
 auto_env_setup.log
@@ -45,12 +44,11 @@ output.txt
 /.vscode/
 /arxiv_pdfs
 /arxiv_structured_json
-run.sh
 /sage/.env
 mem_output.txt
 qa_output.txt
-run.sh
 ray_logs
 /output/
 /logs
-/logs/**
\ No newline at end of file
+/logs/**
+
diff --git a/archive/collection_wrapper.py b/archive/collection_wrapper.py
deleted file mode 100644
index bdd3541..0000000
--- a/archive/collection_wrapper.py
+++ /dev/null
@@ -1,135 +0,0 @@
-import ray
-import asyncio
-import concurrent.futures
-from typing import Any
-
-
-class CollectionWrapper:
-    """透明的集合包装器，自动适配本地执行、Ray Actor、Ray Function等多种模式"""
-    def __init__(self, collection: Any):
-        # 使用 __dict__ 直接设置，避免触发 __setattr__
-        object.__setattr__(self, '_collection', collection)
-        object.__setattr__(self, '_execution_mode', self._detect_execution_mode())
-        object.__setattr__(self, '_method_cache', {})
-        object.__setattr__(self, '_attribute_cache', {})
-
-    def _detect_execution_mode(self) -> str:
-        """检测执行模式"""
-        try:
-            # 1. 检查是否是Ray Actor
-            import ray
-            if isinstance(self._collection, ray.actor.ActorHandle):
-                return "ray_actor"
-            # 2. 检查是否有远程调用方法
-            if hasattr(self._collection, 'remote') and callable(self._collection.remote):
-                return "ray_function"
-        except ImportError:
-            # 如果Ray不可用，忽略
-            pass
-
-        # 3. 默认作为本地对象处理
-        return "local"
-
-    # 属性访问代理
-    def __getattr__(self, name: str):
-        """透明代理属性访问"""
-        # 缓存查找
-        if name in self._attribute_cache:
-            return self._attribute_cache[name]
-
-        # 获取原始属性
-        try:
-            original_attr = getattr(self._collection, name)
-        except AttributeError:
-            raise AttributeError(f"'{type(self._collection).__name__}' object has no attribute '{name}'")
-
-        # 处理方法调用
-        if callable(original_attr):
-            # 创建统一调用方式
-            wrapped_method = self._create_unified_method(name, original_attr)
-            self._attribute_cache[name] = wrapped_method
-            return wrapped_method
-
-        # 普通属性
-        self._attribute_cache[name] = original_attr
-        return original_attr
-
-    # 属性设置代理
-    def __setattr__(self, name: str, value: Any):
-        """代理属性设置"""
-        if name.startswith('_'):
-            # 内部属性直接设置
-            object.__setattr__(self, name, value)
-        else:
-            # 外部属性设置到原集合
-            setattr(self._collection, name, value)
-
-    # 辅助方法
-    def __dir__(self):
-        """代理dir()调用"""
-        return dir(self._collection)
-
-    def __repr__(self):
-        """代理repr()"""
-        return f"CollectionWrapper({repr(self._collection)})"
-
-    def __str__(self):
-        """代理str()"""
-        return str(self._collection)
-
-    def _create_unified_method(self, method_name: str, original_method):
-        """创建统一的方法包装 - 对外始终提供同步接口"""
-        if self._execution_mode == "ray_actor":
-            # Ray Actor处理
-            def ray_actor_wrapper(*args, **kwargs):
-                try:
-                    # 获取远程方法引用
-                    remote_method = getattr(self._collection, method_name)
-                    # 执行远程调用
-                    future = remote_method.remote(*args, **kwargs)
-                    # 同步获取结果
-                    return ray.get(future)
-                except Exception as e:
-                    raise RuntimeError(f"Ray Actor method '{method_name}' failed: {str(e)}")
-
-            return ray_actor_wrapper
-
-        elif self._execution_mode == "ray_function":
-            # Ray函数处理
-            def ray_function_wrapper(*args, **kwargs):
-                try:
-                    # 执行远程调用
-                    future = original_method(*args, **kwargs)
-                    # 同步获取结果
-                    return ray.get(future)
-                except Exception as e:
-                    raise RuntimeError(f"Ray function '{method_name}' failed: {str(e)}")
-
-            return ray_function_wrapper
-
-        else:
-            # 本地方法处理
-            if asyncio.iscoroutinefunction(original_method):
-                # 异步方法转同步
-                def async_wrapper(*args, **kwargs):
-                    try:
-                        # 检查事件循环
-                        try:
-                            loop = asyncio.get_running_loop()
-                            # 在独立线程中执行异步任务
-                            with concurrent.futures.ThreadPoolExecutor() as executor:
-                                result = executor.submit(
-                                    asyncio.run,
-                                    original_method(*args, **kwargs)
-                                ).result()
-                            return result
-                        except RuntimeError:
-                            # 直接运行异步方法
-                            return asyncio.run(original_method(*args, **kwargs))
-                    except Exception as e:
-                        raise RuntimeError(f"Local async method '{method_name}' failed: {str(e)}")
-
-                return async_wrapper
-            else:
-                # 同步方法直接返回
-                return original_method
\ No newline at end of file
diff --git a/archive/dag_manager.py b/archive/dag_manager.py
deleted file mode 100644
index 81649ec..0000000
--- a/archive/dag_manager.py
+++ /dev/null
@@ -1,142 +0,0 @@
-import logging
-from typing import Dict, List, Set
-
-from sympy.strategies.core import switch
-
-from sage.core.dag.local.dag import DAG  # 假设已存在DAG类
-from sage.core.dag.local.dag_node import BaseDAGNode, ContinuousDAGNode, OneShotDAGNode
-
-
-class DAGManager:
-    """
-    DAG管理系统，实现全生命周期管理
-    - 通过逻辑DAG创建实例化DAG
-    - 提交DAG执行
-    - 提供运行中DAG查询
-    - 安全删除DAG
-    """
-
-    def __init__(self):
-        self.dags: Dict[int, DAG] = {}  # 所有DAG存储 {dag_id: dag_instance} 包括代运行、运行中、运行结束的dag
-        self.running_dags: Set[int] = set()  # 待运行的dag的集合
-        self.next_id = 0  # 自增ID生成器
-        self.logger = logging.getLogger(self.__class__.__name__)
-
-    # def create_dag(self, logical_dag: DAG, operator_mapping: Dict) -> int:
-    #     """
-    #     通过逻辑DAG创建实例化DAG
-    #     :param logical_dag: 不含operator的逻辑DAG
-    #     :param operator_mapping: 算子映射 {node_name: {"config":config,"operator_class": class,"init_kwargs": kwargs}}
-    #     :return: 新DAG的ID
-    #     """
-    #     # 生成唯一ID
-    #     dag_id = self.next_id
-    #     self.next_id += 1
-
-    #     new_dag = DAG(id=dag_id,strategy=logical_dag.strategy)
-    #     node_mapping = {}  # 原节点到新节点的映射
-
-    #     # 实例化节点
-    #     for orig_node in logical_dag.nodes:
-    #         # 从映射表获取算子配置
-    #         node_info = operator_mapping.get(orig_node.name, {})
-
-    #         # 创建具体类型节点
-    #         if logical_dag.strategy == "streaming" :
-    #             new_node = ContinuousDAGNode(
-    #                 name=orig_node.name,
-    #                 operator=node_info["operator"](node_info.get("kwargs", {})),
-    #                 config=node_info.get("config", {}),
-    #                 is_spout=orig_node.is_spout
-    #             )
-    #         else:  # 默认为OneShot
-    #             new_node = OneShotDAGNode(
-    #                 name=orig_node.name,
-    #                 operator=node_info["operator"](node_info.get("kwargs", {})),
-    #                 config=node_info.get("config", {}),
-    #                 is_spout=orig_node.is_spout
-    #             )
-
-    #         new_dag.add_node(new_node)
-    #         node_mapping[orig_node] = new_node
-    #     # 重建边关系
-    #     for parent, children in logical_dag.edges.items():
-    #         for child in children:
-    #             new_dag.add_edge(node_mapping[parent], node_mapping[child])
-
-    #     self.dags[dag_id] = new_dag
-    #     self.logger.info(f"Created DAG {dag_id} with {len(new_dag.nodes)} nodes")
-    #     return dag_id
-
-    def add_dag(self,dag:DAG)->int:
-        dag_id = self.next_id
-        self.next_id += 1
-        self.dags[dag_id] = dag
-        dag.dag_id = dag_id
-        return dag_id
-
-    def submit_dag(self, dag_id: int) -> None:
-        """
-        提交DAG到运行队列
-        :param dag_id: 要执行的DAG ID
-        """
-        # 有效性检查
-        if dag_id not in self.dags:
-            raise ValueError(f"DAG {dag_id} does not exist")
-        if dag_id in self.running_dags:
-            self.logger.warning(f"DAG {dag_id} is already running")
-            return
-
-        # 获取DAG实例并启动
-        dag = self.dags[dag_id]
-
-        # 加入运行集合
-        self.running_dags.add(dag_id)
-        self.logger.info(f"DAG {dag_id} submitted for execution")
-
-    def get_running_dags(self) -> List[int]:
-        """获取所有运行中DAG ID列表"""
-        return list(self.running_dags)
-
-    def clear_running_dags(self) -> None:
-         """清除running_dags列表"""
-         self.running_dags.clear()
-
-    def delete_dag(self, dag_id: int) -> None:
-        """
-        删除非运行中的DAG
-        :param dag_id: 要删除的DAG ID
-        """
-        try :
-            if dag_id in self.running_dags:
-                raise PermissionError(f"Cannot delete running DAG {dag_id}")
-            if dag_id not in self.dags:
-                raise ValueError(f"DAG {dag_id} does not exist")
-
-            del self.dags[dag_id]
-            self.logger.info(f"DAG {dag_id} deleted")
-        except Exception as e:
-            self.logger.error(f"Failed to delete DAG {dag_id}: {e}")
-
-    def remove_from_running(self, dag_id: int) -> None:
-        """
-        将DAG移出运行队列
-        :param dag_id: 要停止的DAG ID
-        """
-        if dag_id not in self.running_dags:
-            self.logger.warning(f"DAG {dag_id} is not running")
-            return
-
-        self.running_dags.remove(dag_id)
-        self.logger.info(f"DAG {dag_id} removed from running")
-
-    def get_dag(self, dag_id: int) -> DAG:
-        #通过dag_id获取dag
-        try :
-            if not dag_id in self.dags:
-                raise ValueError(f"DAG {dag_id} does not exist")
-            return self.dags[dag_id]
-        except Exception as e:
-            self.logger.error(f"Failed to get DAG {dag_id}: {e}")
-            return None
-
diff --git a/archive/executor_manager.py b/archive/executor_manager.py
deleted file mode 100644
index a6ce42c..0000000
--- a/archive/executor_manager.py
+++ /dev/null
@@ -1,166 +0,0 @@
-import logging
-
-
-
-from sage.core.runtime.local.local_task import StreamingTask, OneshotTask, BaseTask
-from sage.core.runtime.local.local_scheduling_strategy import SchedulingStrategy, ResourceAwareStrategy, PriorityStrategy
-from sage.core.dag.local.dag import DAG
-from archive.dag_manager import DAGManager
-from sage.core.runtime.local.local_slot import Slot
-from sage.core.dag.local.dag_node import BaseDAGNode,ContinuousDAGNode,OneShotDAGNode
-from sage.core.runtime import BaseRuntime, LocalRuntime,  RayRuntime
-import time
-
-class ExecutorManager:
-    """用于管理任务，初始化slots,并负责将任务提交到slot里面运行，基于已提交的dag提取出所有的node并根据node按照一定的策略分配给不同的slot
-     Attributes:
-        available_slots (List[Slot]): 可用计算槽位列表
-        max_slots (int): 最大槽位数量
-        dag_manager (DAGManager): DAG流程管理器
-        task_to_slot (Dict[object, int]): 任务到槽位的映射关系
-        dag_to_tasks (Dict[str, List[object]]): DAG到其关联任务的映射
-        logger (logging.Logger): 日志记录器
-        scheduling_strategy (SchedulingStrategy): 任务调度策略实现
-    """
-    def __init__(self,dag_manager:DAGManager, max_slots=4,scheduling_strategy=None):
-        self.dag_manager = dag_manager
-        self.dag_to_tasks={}
-        self.task_handles = {}  # dag_id -> [task_handles]
-        self.logger=logging.getLogger(__name__)
-        self.local_backend = LocalRuntime(max_slots, scheduling_strategy)
-        # self.ray_backend:RayExecutionBackend = None
-
-        # self.available_slots = [Slot(slot_id=i) for i in range(max_slots)]
-        # self.max_slots = max_slots
-        # self.task_to_slot ={}
-        # if scheduling_strategy is None:
-        #     self.scheduling_strategy = ResourceAwareStrategy()
-        # elif scheduling_strategy.lower() =="prioritystrategy":
-        #     self.scheduling_strategy = PriorityStrategy({})
-        # else :
-        #     self.scheduling_strategy = ResourceAwareStrategy()
-
-
-    def run_dags(self):
-        # """
-        # 提交DAG并调度其节点
-
-        # 流程：
-        # 1. 从DAG管理器获取正在运行的DAG列表
-        # 2. 清空管理器的运行中DAG记录（避免重复提交）
-        # 3. 对每个DAG创建对应任务：
-        #    - 流式DAG：为每个节点创建独立任务
-        #    - 一次性DAG：为整个DAG创建单个任务
-        # 4. 将任务分配到可用槽位执行
-        # """
-        """
-        执行所有待运行的DAG
-        根据DAG配置选择不同的执行后端
-        """
-        running_dags=self.dag_manager.get_running_dags()
-        self.dag_manager.clear_running_dags()
-        for dag_id in running_dags:
-            self.task_handles[dag_id] = []
-            dag = self.dag_manager.get_dag(dag_id)
-            
-            # 根据DAG配置选择执行后端
-            execution_backend = self._get_runtime(dag)
-            
-            if dag.strategy == "streaming":
-                self._execute_streaming_dag(dag_id, dag, execution_backend)
-            else:
-                self._execute_oneshot_dag(dag_id, dag, execution_backend)
-
-    def _get_runtime(self, dag: DAG) -> BaseRuntime:
-        """根据DAG配置选择执行后端"""
-        # 检查DAG配置中的执行后端设置
-        backend_type = dag.platform
-        
-        if backend_type == "ray":
-            self.logger.info(f"Using Ray backend for DAG {dag.dag_id}")
-            if self.ray_backend is None:
-                self.ray_backend = RayRuntime()
-            return self.ray_backend
-        else:
-            self.logger.info(f"Using local backend for DAG {dag.dag_id}")
-            return self.local_backend
-    
-    def _execute_streaming_dag(self, dag_id: int, dag: DAG, backend: BaseRuntime):
-        """使用指定后端执行流式DAG"""
-        for node in dag.nodes:
-            task = StreamingTask(node, dag.working_config)
-            task_handle = backend.submit_task(task)
-            self.task_handles[dag_id].append(task_handle)
-            self.logger.debug(f"{node.name} submitted to {backend.__class__.__name__}")
-    
-    def _execute_oneshot_dag(self, dag_id: int, dag: DAG, runtime: BaseRuntime):
-        """使用指定后端执行一次性DAG"""
-        task = OneshotTask(dag)
-        
-        if isinstance(runtime, RayRuntime):
-            # Ray执行
-            task_handle = runtime.submit_task(task)
-            self.task_handles[dag_id].append(task_handle)
-        else:
-            # 本地执行，直接调用execute
-            task.execute()
-    
-    def stop_dag(self, dag_id: int):
-        """停止指定DAG的所有任务"""
-        if dag_id not in self.task_handles:
-            return
-        
-        dag = self.dag_manager.get_dag(dag_id)
-        backend = self._get_runtime(dag)
-        
-        # 停止所有任务
-        for task_handle in self.task_handles[dag_id]:
-            backend.stop_task(task_handle)
-        
-        # 清理记录
-        del self.task_handles[dag_id]
-        self.dag_manager.remove_from_running(dag_id)
-    
-    def get_dag_status(self, dag_id: int):
-        """获取DAG执行状态"""
-        if dag_id not in self.task_handles:
-            return {"status": "not_found"}
-        
-        dag = self.dag_manager.get_dag(dag_id)
-        backend = self._get_runtime(dag)
-        
-        task_statuses = []
-        for task_handle in self.task_handles[dag_id]:
-            status = backend.get_status(task_handle)
-            task_statuses.append(status)
-        
-        return {
-            "dag_id": dag_id,
-            "backend": backend.__class__.__name__,
-            "task_count": len(task_statuses),
-            "tasks": task_statuses
-        }
-
-    # def schedule_task(self, task: BaseTask) -> int :
-    #         """
-    #            调度任务到指定槽位
-
-    #            Args:
-    #                task: 需要调度的任务对象
-
-    #            Returns:
-    #                int: 分配的槽位ID
-
-    #            Raises:
-    #                RuntimeError: 当无可用槽位时抛出
-    #            """
-    #         selected_slot_id = self.scheduling_strategy.select_slot(
-    #             task, self.available_slots
-    #         )
-    #         self.logger.info(f"chosen slot id {selected_slot_id}")
-    #         if selected_slot_id > 0 :
-    #             self.available_slots[selected_slot_id].submit_task(task)
-    #             self.task_to_slot[task] = selected_slot_id
-    #         return selected_slot_id
-
-
diff --git a/archive/executor_test.py b/archive/executor_test.py
deleted file mode 100644
index f29a899..0000000
--- a/archive/executor_test.py
+++ /dev/null
@@ -1,195 +0,0 @@
-from archive.dag_manager import DAGManager
-from sage.core.dag.local.dag import DAG
-from sage.core.dag.local.dag_node import OneShotDAGNode,ContinuousDAGNode
-from sage.archive.executor_manager import ExecutorManager
-import time
-import logging
-import ray
-from ray import state
-# 用于测试的operator
-"""用于测试的operator,其中spout负责生成数据源，末节点generator负责将收到的数据处理后写进该dag对应的一个文件里面"""
-
-@ray.remote
-class Spout :
-    def __init__(self,config={}):
-        self.logger=logging.getLogger(self.__class__.__name__)
-        self.config=config
-    def execute(self,input="streaming query operator_test"):
-        dagid=self.config['id']
-        self.logger.debug(f"spout_{dagid} execute start")
-        time.sleep(0.1)
-        return input
-@ray.remote
-class Retriever:
-    def __init__(self,config={}):
-        self.logger=logging.getLogger(self.__class__.__name__)
-        self.config=config
-    def execute(self,input):
-        dagid=self.config["id"]
-        self.logger.debug(f"retriever_{dagid} execute start")
-        time.sleep(0.1)
-        return input+"retriever done"
-
-@ray.remote
-class PromptOperator:
-    def __init__(self,config={}):
-        self.logger = logging.getLogger(self.__class__.__name__)
-        self.config=config
-    def execute(self, input):
-        dagid=self.config["id"]
-        self.logger.debug(f"prompt_{dagid} execute start")
-        time.sleep(0.1)
-        return input+"prompt done"
-
-@ray.remote
-class Generator:
-    def __init__(self,config={}):
-        self.config=config
-        self.logger = logging.getLogger(self.__class__.__name__)
-    def execute(self, input):
-        dagid=self.config["id"]
-        self.logger.debug(f"generator_{dagid} execute start")
-        file_name=self.config["file_name"]
-        with open(f"test_output_{dagid}.txt", "a", encoding="utf-8") as f:
-            f.write(input +"generator done"+  "\n")  # 写入内容并换行
-
-        return input+"generator done"
-
-def create_test_streaming_dag(dag_manager: DAGManager) :
-    #用于测试的dag
-    """创建一个用于测试的流式dag，默认形态为spout->retriever->prompt->generator"""
-    dag=DAG(dag_manager.next_id,strategy="streaming")
-    dag_manager.next_id+=1
-
-    spout_node=ContinuousDAGNode(
-        name='Spout',
-        operator=Spout.remote(config={"id": dag.id}),
-        config={},
-        is_spout=True
-    )
-    retriever_node =ContinuousDAGNode(
-        name="Retriever",
-        operator=Retriever.remote(config={"id": dag.id}),
-        config={}
-    )
-    prompt_node = ContinuousDAGNode(
-        name="PromptGenerator",
-        operator=PromptOperator.remote(config={"id": dag.id})
-    )
-    generator_node =ContinuousDAGNode(
-        name="Generator",
-        operator=Generator.remote(config={"id": dag.id,"file_name": f"test_output_{dag.id}"})
-    )
-    dag.add_node(spout_node)
-    dag.add_node(retriever_node)
-    dag.add_node(prompt_node)
-    dag.add_node(generator_node)
-    dag.add_edge(spout_node, retriever_node)
-    dag.add_edge(retriever_node, prompt_node)
-    dag.add_edge(prompt_node, generator_node)
-    dag_manager.dags[dag.id] = dag
-    return dag.id
-def create_test_oneshot_dag(dag_manager: DAGManager) :
-    """创建一个用于测试的非流式dag，默认形态为spout->retriever->prompt->generator"""
-    dag=DAG(dag_manager.next_id,strategy="one_shot")
-    dag_manager.next_id+=1
-
-    spout_node=OneShotDAGNode(
-        name='Spout',
-        operator=Spout.remote(config={"id": dag.id}),
-        config={},
-        is_spout=True
-    )
-    retriever_node =OneShotDAGNode(
-        name="Retriever",
-        operator=Retriever.remote(config={"id": dag.id}),
-        config={}
-    )
-    prompt_node = OneShotDAGNode(
-        name="PromptGenerator",
-        operator=PromptOperator.remote(config={"id": dag.id})
-    )
-    generator_node = OneShotDAGNode(
-        name="Generator",
-        operator=Generator.remote(config={"id": dag.id,"file_name": f"test_output_{dag.id}"})
-    )
-    dag.add_node(spout_node)
-    dag.add_node(retriever_node)
-    dag.add_node(prompt_node)
-    dag.add_node(generator_node)
-    dag.add_edge(spout_node, retriever_node)
-    dag.add_edge(retriever_node, prompt_node)
-    dag.add_edge(prompt_node, generator_node)
-    dag_manager.dags[dag.id] = dag
-    return dag.id
-
-def streaming_dag_test():
-#测试多线程流式rag
-    ray.init(  dashboard_port=8265 )
-    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",)
-    dag_manager = DAGManager()
-    dag_ids=[]
-    for i in range(1) :   #测试的dag个数为10
-        dag_id=create_test_streaming_dag(dag_manager)
-        dag_manager.run_dag(dag_id)
-        dag_ids.append(dag_id)
-    executor_manager = ExecutorManager(dag_manager,max_slots=5)
-    executor_manager.run_dag()
-    #dag 运行10s
-    time.sleep(1000)
-
-
-    actors = state.actors()
-    for actor_id, actor_info in actors.items():
-        print(f"actor_id {actor_id}: {actor_info}")
-
-    #依次停止dag
-    for dag_id in dag_ids :
-        time.sleep(1)
-        executor_manager.stop_dag(dag_id)
-    time.sleep(1)
-    print(f"num of tasks is {len(executor_manager.dag_to_tasks)}")
-    print(f"num of dags is {len(executor_manager.dag_to_tasks)}")
-    print(f"num if running dags is {len(executor_manager.dag_manager.running_dags)}")
-    print(f"num if created dags is {len(executor_manager.dag_manager.dags)}")
-
-def oneshot_dag_test():
-    #测试多线程非流式rag
-    """测试多线程非流时rag,模拟多轮对话模式"""
-    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", )
-    dag_manager = DAGManager()
-    dag_ids = []
-    for i in range(10):  # 测试的dag个数为10
-        dag_id = create_test_oneshot_dag(dag_manager)
-        dag_manager.run_dag(dag_id)
-        dag_ids.append(dag_id)
-    executor_manager = ExecutorManager(dag_manager, max_slots=5)
-    executor_manager.run_dag()
-    # dag 运行10s
-    time.sleep(10)
-    # 查询 Actor 分配情况
-    actors = state.actors()
-    for actor_id, actor_info in actors.items():
-        print(f"Actor ID: {actor_id}")
-        print(f"类名: {actor_info['class_name']}")
-        print(f"节点 IP: {actor_info['address']['ip']}")
-        print(f"资源请求: {actor_info['required_resources']}")
-    for i in dag_ids :
-        dag_manager.run_dag(i)
-        executor_manager.run_dag()
-    time.sleep(5)
-
-    print(f"num of tasks is {len(executor_manager.task_to_slot)}")
-    print(f"num of dags is {len(executor_manager.dag_to_tasks)}")
-    print(f"num if running dags is {len(executor_manager.dag_manager.running_dags)}")
-    print(f"num if created dags is {len(executor_manager.dag_manager.dags)}")
-
-
-
-if __name__ == '__main__':
-    streaming_dag_test()
-
-
-
-
-
diff --git a/archive/operator_factory.py b/archive/operator_factory.py
deleted file mode 100644
index 916d351..0000000
--- a/archive/operator_factory.py
+++ /dev/null
@@ -1,57 +0,0 @@
-# sage/runtime/operator_factory.py
-from typing import TypeVar, Any, Type
-import ray
-import logging
-from sage.archive.operator_wrapper import OperatorWrapper
-
-T = TypeVar("T")
-
-class OperatorFactory:
-    """简化的算子工厂，全局控制本地或远程创建"""
-    
-    def __init__(self, use_ray: bool = True):
-        # self.use_ray = use_ray
-        self.use_ray = False
-        self.logger = logging.getLogger(__name__)
-        self._ray_remote_classes = {}  # 缓存Ray远程类
-        
-        if self.use_ray and not ray.is_initialized():
-            self.logger.warning("Ray not initialized, falling back to local mode")
-            self.use_ray = False
-    
-
-    def create(self, operator_class:Type[T], config:dict) -> T:
-        """
-        创建算子实例
-        
-        Args:
-            operator_class: 算子类（如 FileSource）
-            config: 算子配置
-            
-        Returns:
-            算子实例（本地对象或Ray Actor Handle）
-        """
-        if self.use_ray:
-            raw_operator = self._create_ray_operator(operator_class, config)
-        else:
-            raw_operator = self._create_local_operator(operator_class, config)
-        
-        wrapped_operator = OperatorWrapper(raw_operator)
-        return wrapped_operator
-
-
-    def _create_ray_operator(self, operator_class, config):
-        """创建Ray远程算子"""
-        print("create ray operator")
-        class_name = operator_class.__name__
-        
-        # 从缓存获取或创建Ray远程类
-        if class_name not in self._ray_remote_classes:
-            self._ray_remote_classes[class_name] = ray.remote(operator_class)
-        
-        ray_remote_class = self._ray_remote_classes[class_name]
-        return ray_remote_class.remote(config)
-    
-    def _create_local_operator(self, operator_class, config):
-        """创建本地算子"""
-        return operator_class(config)
\ No newline at end of file
diff --git a/archive/operator_wrapper.py b/archive/operator_wrapper.py
deleted file mode 100644
index 8eb79cf..0000000
--- a/archive/operator_wrapper.py
+++ /dev/null
@@ -1,135 +0,0 @@
-# sage/runtime/operator_wrapper.py
-import ray
-import asyncio
-from typing import Any, Dict, Optional, Type
-from sage.api.operator.base_operator_api import BaseOperator
-class OperatorWrapper:
-    """
-    透明的算子包装器，提供统一的同步接口
-    
-    该包装器自动处理本地对象、Ray Actor和Ray Function的差异，
-    对外提供一致的同步调用接口。
-    
-    支持的执行模式：
-    - local: 本地对象直接调用
-    - ray_actor: Ray Actor远程调用，自动处理ray.get()
-    - ray_function: Ray Function远程调用，自动处理ray.get()
-    
-    示例:
-        # 本地模式
-        local_op = FileSource(config)
-        wrapper = OperatorWrapper(local_op)
-        result = wrapper.read()  # 直接调用
-        
-        # Ray Actor模式
-        ray_actor = ray.remote(FileSource).remote(config)
-        wrapper = OperatorWrapper(ray_actor)
-        result = wrapper.read()  # 自动处理ray.get()
-    """
-    
-    def __init__(self, operator: Any):
-        # 使用 __dict__ 直接设置，避免触发 __setattr__
-        object.__setattr__(self, '_operator', operator)
-        object.__setattr__(self, '_execution_mode', self._detect_execution_mode())
-        object.__setattr__(self, '_method_cache', {})
-        object.__setattr__(self, '_attribute_cache', {})
-    
-    def _detect_execution_mode(self) -> str:
-        """检测执行模式"""
-        if isinstance(self._operator, ray.actor.ActorHandle):
-            return "ray_actor"
-        elif hasattr(self._operator, 'remote'):
-            return "ray_function"
-        else:
-            return "local"
-    
-    # 完美代理所有属性访问
-    def __getattr__(self, name: str):
-        """透明代理属性访问"""
-        if name in self._attribute_cache:
-            return self._attribute_cache[name]
-        
-        original_attr = getattr(self._operator, name)
-        
-        # 如果是方法，则包装成统一的同步调用
-        if callable(original_attr):
-            wrapped_method = self._create_unified_method(name, original_attr)
-            self._attribute_cache[name] = wrapped_method
-            return wrapped_method
-        else:
-            # 普通属性直接返回
-            self._attribute_cache[name] = original_attr
-            return original_attr
-    
-    def __setattr__(self, name: str, value: Any):
-        """代理属性设置"""
-        if name.startswith('_'):
-            # 内部属性直接设置
-            object.__setattr__(self, name, value)
-        else:
-            # 外部属性设置到原算子
-            setattr(self._operator, name, value)
-    
-    def __dir__(self):
-        """代理dir()调用"""
-        return dir(self._operator)
-    
-    def __repr__(self):
-        """代理repr()"""
-        return f"OperatorWrapper({repr(self._operator)})"
-    
-    def __str__(self):
-        """代理str()"""
-        return str(self._operator)
-    
-    def _create_unified_method(self, method_name: str, original_method):
-        """创建统一的方法包装 - 对外始终提供同步接口"""
-        
-        if self._execution_mode == "ray_actor":
-            # Ray Actor: 同步调用，内部处理Ray异步
-            def sync_ray_actor_wrapper(*args, **kwargs):
-                try:
-                    future = original_method.remote(*args, **kwargs)
-                    result = ray.get(future)
-                    return result
-                except Exception as e:
-                    raise RuntimeError(f"Ray Actor method '{method_name}' failed: {e}")
-            
-            return sync_ray_actor_wrapper
-            
-        elif self._execution_mode == "ray_function":
-            # Ray Function: 同步调用
-            def sync_ray_function_wrapper(*args, **kwargs):
-                try:
-                    future = original_method.remote(*args, **kwargs)
-                    result = ray.get(future)
-                    return result
-                except Exception as e:
-                    raise RuntimeError(f"Ray function '{method_name}' failed: {e}")
-            
-            return sync_ray_function_wrapper
-            
-        else:
-            # 本地方法: 处理异步方法，统一返回同步结果
-            if asyncio.iscoroutinefunction(original_method):
-                def sync_local_async_wrapper(*args, **kwargs):
-                    try:
-                        # 检查是否已在事件循环中
-                        try:
-                            loop = asyncio.get_running_loop()
-                            # 如果在事件循环中，需要在新线程中运行
-                            import concurrent.futures
-                            with concurrent.futures.ThreadPoolExecutor() as executor:
-                                future = executor.submit(asyncio.run, original_method(*args, **kwargs))
-                                return future.result()
-                        except RuntimeError:
-                            # 不在事件循环中，直接使用asyncio.run
-                            return asyncio.run(original_method(*args, **kwargs))
-                    except Exception as e:
-                        raise RuntimeError(f"Local async method '{method_name}' failed: {e}")
-                
-                return sync_local_async_wrapper
-            else:
-                # 本地同步方法，直接返回
-                return original_method
-    
diff --git a/archive/raydag_task.py b/archive/raydag_task.py
deleted file mode 100644
index e5a7663..0000000
--- a/archive/raydag_task.py
+++ /dev/null
@@ -1,33 +0,0 @@
-from typing import Optional, Dict, Any
-from sage.core.dag.ray.ray_dag import RayDAG
-
-class RayDAGTask:
-    """
-    Ray DAG 任务包装器，用于与执行后端接口兼容
-    """
-    
-    def __init__(self, ray_dag: RayDAG, execution_config: Optional[Dict[str, Any]] = None):
-        """
-        Initialize Ray DAG task.
-        
-        Args:
-            ray_dag: The RayDAG to execute
-            execution_config: Optional execution configuration
-        """
-        self.ray_dag = ray_dag
-        self.execution_config = execution_config or {}
-        self.task_type = "ray_dag"
-    
-    def get_dag(self) -> RayDAG:
-        """Get the underlying RayDAG."""
-        return self.ray_dag
-    
-    def get_config(self) -> Dict[str, Any]:
-        """Get execution configuration."""
-        return self.execution_config
-    
-    def __str__(self):
-        return f"RayDAGTask(dag_id={self.ray_dag.id}, strategy={self.ray_dag.strategy})"
-
-
-        
diff --git a/archive/slot_test.py b/archive/slot_test.py
deleted file mode 100644
index fae70b1..0000000
--- a/archive/slot_test.py
+++ /dev/null
@@ -1,83 +0,0 @@
-from sage.core.runtime.local.local_slot import Slot
-import time
-import threading
-import logging
-from concurrent.futures import Future
-
-
-class TestTask:
-    """模拟任务类"""
-
-    def __init__(self, name, duration=0):
-        self.name = name
-        self.duration = duration
-        self.stop_called = False
-        self.stop_event = threading.Event()
-    def run(self):
-        cnt=0
-        while not self.stop_event.is_set():
-            time.sleep(self.duration)
-            if cnt%5 == 0 :
-                print(f"{self.name} loop {cnt} times")
-            cnt+=1
-
-    def stop(self):
-        self.stop_event.set()
-
-
-class TestSlot():
-    def __init__(self):
-        self.slot = Slot("test_slot", max_threads=2)
-        self.task1 = TestTask("task1", duration=0.1)
-        self.task2 = TestTask("task2", duration=0.2)
-        self.task3 = TestTask("task3", duration=0.3)
-
-    def tearDown(self):
-        self.slot.shutdown()
-
-    def test_submit_task_within_capacity(self):
-        # 测试正常提交任务
-        self.slot.submit_task(self.task1)
-        if not self.slot.current_load == 1 :
-            raise RuntimeError("current load not equal to 1")
-        self.slot.submit_task(self.task2)
-        if not self.slot.current_load == 2 :
-            raise RuntimeError("current load not equal to 2")
-        time.sleep(3)
-        self.slot.shutdown()
-        if not self.slot.current_load == 0 :
-            raise RuntimeError("current load not equal to 0")
-
-    def test_submit_task_exceed_capacity(self):
-        # 测试超过容量时提交失败
-        self.slot.submit_task(self.task1)
-        self.slot.submit_task(self.task2)
-        self.slot.submit_task(self.task3)
-        if not self.slot.current_load == 2 :
-            raise RuntimeError("current load not equal to 2")
-        time.sleep(3)
-        self.slot.shutdown()
-
-    def test_stop_running_task(self):
-        # 测试停止正在运行的任务
-        self.slot.submit_task(self.task3)  # 0.3秒任务
-        time.sleep(3)  # 确保任务已经开始
-
-        future = self.slot.task_to_future[self.task3]
-        if future.running():
-            print("task is running")
-        self.slot.stop(self.task3)
-        future.result()
-        if future.done() :
-            print("task is done")
-
-    def test_stop_pending_task(self):
-        # 测试取消未开始的任务
-        self.slot.submit_task(self.task1)
-        self.slot.submit_task(self.task2)
-        self.slot.submit_task(self.task3)  # 会被拒绝，因为容量为2
-
-
-
-
-
diff --git a/config/config.yaml b/config/config.yaml
index 1cad202..b18912d 100644
--- a/config/config.yaml
+++ b/config/config.yaml
@@ -7,18 +7,22 @@ pipeline:
 
 
 source:
-  data_path: "sample/one_question.txt"
-  # data_path: "sample/question.txt"
+  # data_path: "sample/one_question.txt"
+  data_path: "sample/question.txt"
+  platform: "local"
 
 retriever:
+  platform: "ray"
   ltm:
     topk: 3
 
 reranker:
+  platform: "local"
   model_name: "BAAI/bge-reranker-v2-m3"
   top_k: 3
 
 refiner:
+  platform: "local"
   method: "openai"
   model_name: "qwen-turbo-0919"
   base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
@@ -26,6 +30,7 @@ refiner:
   seed: 42
 
 generator:
+  platform: "local"
   method: "openai"
   model_name: "qwen-turbo-0919"
   base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
@@ -34,6 +39,9 @@ generator:
 
 writer:
 
+promptor:
+  platform: "local"
 
 sink:
+  platform: "local"
 
diff --git a/sage/api/operator/base_operator_api.py b/sage/api/operator/base_operator_api.py
index 5423517..288e0e6 100644
--- a/sage/api/operator/base_operator_api.py
+++ b/sage/api/operator/base_operator_api.py
@@ -1,7 +1,7 @@
 
 import logging
 from typing import TypeVar,Generic, Callable, Any, List
-from sage.core.io.message_queue import MessageQueue
+
 from typing import TypeVar,Generic
 T = TypeVar('T')
 class Data(Generic[T]):
@@ -9,39 +9,6 @@ class Data(Generic[T]):
         self.data = data 
 
 
-class EmitContext:
-    """
-    Emit context that encapsulates emission logic and channels.
-    This avoids closures that reference the parent DAG node.
-    """
-    
-    def __init__(self, node_name: str):
-        self.node_name = node_name
-        self.downstream_channels: List[MessageQueue] = []
-    
-    def add_downstream_channel(self, message_queue: MessageQueue):
-        """Add a downstream channel."""
-        self.downstream_channels.append(message_queue)
-    
-    def emit(self, channel: int, data: Any) -> None:
-        """
-        Emit data to specified downstream channel.
-        
-        Args:
-            channel: The downstream channel index
-            data: Data to emit
-        """
-        if(channel == -1):
-            # Broadcast to all downstream channels
-            for downstream_channel in self.downstream_channels:
-                downstream_channel.put(data)
-            return
-        elif(0 <= channel and channel < len(self.downstream_channels)) :
-            self.downstream_channels[channel].put(data)
-        else:
-            # Note: We can't use logger here to keep the context simple and serializable
-            print(f"Warning: Channel index {channel} out of range for node {self.node_name}")
-
 
 
 
diff --git a/sage/api/operator/operator_impl/generator.py b/sage/api/operator/operator_impl/generator.py
index 71ad07c..d74306f 100644
--- a/sage/api/operator/operator_impl/generator.py
+++ b/sage/api/operator/operator_impl/generator.py
@@ -18,7 +18,7 @@ class OpenAIGenerator(GeneratorFunction):
                        the method, model name, base URL, API key, etc.
         """
         super().__init__()
-        self.config = config["generator"]
+        self.config = config
 
         # Apply the generator model with the provided configuration
         self.model = apply_generator_model(
diff --git a/sage/api/operator/operator_impl/retriever.py b/sage/api/operator/operator_impl/retriever.py
index 8e97cb4..bdf98aa 100644
--- a/sage/api/operator/operator_impl/retriever.py
+++ b/sage/api/operator/operator_impl/retriever.py
@@ -9,7 +9,7 @@ from sage.utils.custom_logger import CustomLogger
 class DenseRetriever(StateRetrieverFunction):
     def __init__(self, config: dict):
         super().__init__()
-        self.config = config["retriever"]
+        self.config = config
 
         
         if self.config.get("ltm", False):
diff --git a/sage/api/operator/operator_impl/sink.py b/sage/api/operator/operator_impl/sink.py
index 48aef17..6211def 100644
--- a/sage/api/operator/operator_impl/sink.py
+++ b/sage/api/operator/operator_impl/sink.py
@@ -7,7 +7,7 @@ class TerminalSink(SinkFunction):
 
     def __init__(self,config):
         super().__init__()
-        self.config=config["sink"]
+        self.config=config
 
     def execute(self, data:Data[Tuple[str,str]]):
         question,answer=data.data
diff --git a/sage/api/operator/operator_impl/source.py b/sage/api/operator/operator_impl/source.py
index 0c1249b..2a86a61 100644
--- a/sage/api/operator/operator_impl/source.py
+++ b/sage/api/operator/operator_impl/source.py
@@ -25,10 +25,9 @@ class FileSource(SourceFunction):
         :param config: Configuration dictionary containing source settings, including `data_path`.
         """
         super().__init__()
-        self.config = config["source"]
+        self.config = config
         # self.data_path = self.config["data_path"]
-        raw = self.config["data_path"]  # e.g. "sample/question.txt"
-        self.data_path = resolve_data_path(raw)  # → project_root/data/sample/question.txt
+        self.data_path = resolve_data_path(config["data_path"])  # → project_root/data/sample/question.txt
         self.file_pos = 0  # Track the file read position
 
     def execute(self) -> Data[str]:
diff --git a/sage/api/pipeline/datastream_api.py b/sage/api/pipeline/datastream_api.py
index 7eccba9..d45851e 100644
--- a/sage/api/pipeline/datastream_api.py
+++ b/sage/api/pipeline/datastream_api.py
@@ -32,10 +32,10 @@ class DataStream:
         self.config = config or {}
         self.node_type = node_type  # "source", "sink", "normal" or other types
 
-    def _transform(self, name: str, operator_class:Type[BaseFuction], config) -> DataStream:
-        # operator_instance = self.pipeline.operator_factory.create(operator_class, config)
+    def _transform(self, name: str, function_class:Type[BaseFuction], config) -> DataStream:
+        # operator_instance = self.pipeline.operator_factory.create(function_class, config)
         # op = next_operator_class
-        new_stream = DataStream(operator_class, self.pipeline, name=name, config = config, node_type="normal")
+        new_stream = DataStream(function_class, self.pipeline, name=name, config = config, node_type="normal")
         self.pipeline.data_streams.append(new_stream)
         # Wire dependencies
         new_stream.upstreams.append(self)
diff --git a/sage/api/pipeline/pipeline_api.py b/sage/api/pipeline/pipeline_api.py
index ba2cd61..8a1f36f 100644
--- a/sage/api/pipeline/pipeline_api.py
+++ b/sage/api/pipeline/pipeline_api.py
@@ -13,15 +13,15 @@ class Pipeline:
     operator_config: dict
     operator_cls_mapping: dict
     # operator_factory: OperatorFactory
-    use_ray: bool
+    # use_ray: bool
     # compiler: QueryCompiler
-    def __init__(self, name: str, use_ray: bool = True):
+    def __init__(self, name: str):
         self.name = name
         self.operators = []
         self.data_streams = []
         self.operator_config = {}
         self.operator_cls_mapping = {}
-        self.use_ray = use_ray
+        self.use_ray = False  # 是否使用 Ray 运行时，默认为 False
         # 创建全局算子工厂
         # self.operator_factory = OperatorFactory(self.use_ray)
 
@@ -83,6 +83,12 @@ class Pipeline:
         print(f"[Pipeline] Pipeline '{self.name}'submitted to engine.")
         engine.submit_pipeline(self, config, generate_func)
 
+    def submit_mixed(self, config=None):
+        from sage.core.engine import Engine
+        engine = Engine.get_instance()
+        print(f"[Pipeline] Pipeline '{self.name}'submitted to engine.")
+        engine.submit_mixed_pipeline(self, config)
+
     def get_graph_preview(self) -> dict:
         """
         获取 pipeline 转换为 graph 后的预览信息，不实际提交
diff --git a/sage/core/compiler/query_compiler.py b/sage/core/compiler/query_compiler.py
index e6f266b..f72ff28 100644
--- a/sage/core/compiler/query_compiler.py
+++ b/sage/core/compiler/query_compiler.py
@@ -4,7 +4,7 @@ from sage.core.compiler.query_parser import QueryParser
 from sage.core.dag.local.dag import DAG
 from sage.core.dag.local.dag_node import BaseDAGNode, OneShotDAGNode
 from sage.core.compiler.logical_graph_constructor import LogicGraphConstructor
-from sage.core.dag.local.multi_dag_node import MultiplexerDagNode
+from sage.core.dag.local.local_dag_node import LocalDAGNode
 from sage.core.io.message_queue import MessageQueue
 from sage.core.dag.ray.ray_dag import RayDAG
 if TYPE_CHECKING:
@@ -13,20 +13,19 @@ from sage.utils.custom_logger import CustomLogger
 
 class QueryCompiler:
 
-    def __init__(self,generate_func = None, session_folder: str = None):
+    def __init__(self):
         """
         Initialize the QueryCompiler with memory layers.
         :param memory_manager: Memory manager for managing memory layers.
         :param generate_func: Function for query generation
         """
-        self.session_folder = session_folder
+        self.session_folder = CustomLogger.get_session_folder()
         self.logical_graph_constructor = LogicGraphConstructor()
         self.optimizer = Optimizer()
-        self.parser = QueryParser(generate_func=generate_func)
+        self.parser = QueryParser(generate_func=None)
         self.dag_dict = {}
         self.logger = CustomLogger(
             object_name=f"QueryCompiler",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
@@ -57,10 +56,10 @@ class QueryCompiler:
         # Step 1: Create all Ray Actor DAG nodes
         for node_name, graph_node in graph.nodes.items():
             # Extract operator class and configuration instead of creating instance
-            operator_class = graph_node.operator
+            function_class = graph_node.operator
             operator_config = graph_node.config or {}
             
-            from sage.core.dag.ray.ray_multi_node import RayMultiplexerDagNode
+            from sage.core.dag.ray.ray_dag_node import RayDAGNode
             from sage.core.runtime.collection_wrapper import CollectionWrapper
             
             # Create Ray Actor with operator class, not instance
@@ -68,9 +67,9 @@ class QueryCompiler:
             
             # wrapper:CollectionWrapper = operator_config["retriever"]["ltm_collection"]
             # operator_config["retriever"]["ltm_collection"] = wrapper._collection
-            ray_actor = RayMultiplexerDagNode.remote(
+            ray_actor = RayDAGNode.remote(
                 name=graph_node.name,
-                operator_class=operator_class,
+                function_class=function_class,
                 operator_config=operator_config,
                 is_spout=(graph_node.type == "source"), 
                 session_folder = self.session_folder
@@ -89,7 +88,7 @@ class QueryCompiler:
             
             # Get channel information from edge
             upstream_output_channel = edge.upstream_channel
-            downstream_input_channel = edge.downstream_channnel
+            downstream_input_channel = edge.downstream_channel
             self.logger.info(f"Connecting actors '{edge.upstream_node.name}' "
                              f"to {edge.downstream_node.name}")
             
@@ -116,14 +115,14 @@ class QueryCompiler:
 
 
         # Step 2: Create all DAG nodes first
-        dag_nodes:Dict[str, MultiplexerDagNode] = {}
+        dag_nodes:Dict[str, LocalDAGNode] = {}
         for node_name, graph_node in graph.nodes.items():
             # Create operator instance
             # operator = operator_factory.create(graph_node.operator, graph_node.config)
             graph_node.config["session_folder"] = self.session_folder
             operator_instance = graph_node.operator(graph_node.config)
             # Create DAG node
-            dag_node = MultiplexerDagNode(
+            dag_node = LocalDAGNode(
                 graph_node.name,
                 operator_instance,
                 config=graph_node.config,
diff --git a/sage/core/dag/local/dag.py b/sage/core/dag/local/dag.py
index ad945bf..26ad83f 100644
--- a/sage/core/dag/local/dag.py
+++ b/sage/core/dag/local/dag.py
@@ -33,7 +33,7 @@ class DAG:
             file_output=True
         )
         self.working_config=None
-    def add_node(self, node):
+    def add_node(self, node:BaseDAGNode):
         """
         Add a node to the DAG.
         :param node: DAGNode instance to add.
diff --git a/sage/core/dag/local/multi_dag_node.py b/sage/core/dag/local/local_dag_node.py
similarity index 63%
rename from sage/core/dag/local/multi_dag_node.py
rename to sage/core/dag/local/local_dag_node.py
index 614aa9c..d02f993 100644
--- a/sage/core/dag/local/multi_dag_node.py
+++ b/sage/core/dag/local/local_dag_node.py
@@ -1,3 +1,4 @@
+from __future__ import annotations
 import asyncio
 import inspect
 import logging
@@ -8,11 +9,17 @@ from typing import Any, Type, TYPE_CHECKING, Union, List, Optional, Tuple
 
 #from sage.archive.operator_wrapper import OperatorWrapper
 from sage.api.operator.base_operator_api import BaseFuction
+from sage.core.graph import SageGraph, GraphEdge, GraphNode
 from sage.core.io.message_queue import MessageQueue
-from sage.api.operator.base_operator_api import EmitContext
+from sage.core.io.emit_context import  NodeType
+from sage.core.io.local_emit_context import LocalEmitContext
 from sage.utils.custom_logger import CustomLogger
+# from sage.core.dag.local.multi_dag_node import LocalDAGNode
+import ray
+from ray.actor import ActorHandle
 
-class MultiplexerDagNode:
+
+class LocalDAGNode:
     """
     Multiplexer DAG Node.
 
@@ -39,22 +46,21 @@ class MultiplexerDagNode:
         self.operator = operator
         self.config = config
         self.is_spout = is_spout
-        # self.logger = logging.getLogger(self.__class__.__name__)
-        # self.logger = None
-        self.upstream_channels: List[MessageQueue] = []
-        self.downstream_channels: List[MessageQueue] = []
-        # self.stop_event = threading.Event()
-        # self.stop_event = None
-        # self.operator.set_emit_func(self._create_emit_func())
-        # Round-robin scheduling for upstream channels
+        
+        self.input_buffer = MessageQueue()  # Local input buffer for this node
+
+
+
         self._current_channel_index = 0
         self._initialized = False
         # Create emit context
-        self.emit_context = EmitContext(self.name)
-        # Don't inject emit context in __init__ to avoid serialization issues
+        # Create emit context for mixed environment
+        self.emit_context = LocalEmitContext(self.name, session_folder=session_folder)
         self._emit_context_injected = False
+
+
         self.logger = CustomLogger(
-            object_name=f"MultiplexerDagNode_{self.name}",
+            object_name=f"LocalDAGNode_{self.name}",
             session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
@@ -88,35 +94,16 @@ class MultiplexerDagNode:
         
         self._initialized = True
     
-
-
-    def fetch_input(self) -> Optional[Tuple[int, Any]]:
+    def put(self, data_packet: Tuple[int, Any]):
         """
-        Fetch input from upstream channels using round-robin scheduling.
-        Returns a tuple of (channel_id, data) from the next available upstream channel.
+        向输入缓冲区放入数据包
         
-        Returns:
-            Tuple of (channel_id, data) or None if no data is available from any channel
+        Args:
+            data_packet: (input_channel, data) 元组
         """
-        if not self.upstream_channels:
-            return None
-        
-        num_channels = len(self.upstream_channels)
-        # Try all channels starting from current position
-        for _ in range(num_channels):
-            channel_id = self._current_channel_index
-            channel:MessageQueue = self.upstream_channels[channel_id]
-            
-            # Move to next channel for next call (round-robin)
-            self._current_channel_index = (self._current_channel_index + 1) % num_channels
-            
-            # Check if current channel has data
-            if not channel.is_empty():
-                data = channel.get()
-                return (channel_id, data)
-        
-        # No data available from any channel
-        return None
+        self.input_buffer.put(data_packet, timeout=1.0)
+        self.logger.debug(f"Put data packet into buffer: channel={data_packet[0]}")
+
     
 
 
@@ -134,10 +121,44 @@ class MultiplexerDagNode:
         else:
             self.logger.warning(f"Channel index {channel} out of range for node {self.name}")
 
-    def add_downstream_channel(self, message_queue: MessageQueue):
-        """Add downstream channel to both node and emit context."""
-        self.downstream_channels.append(message_queue)
-        self.emit_context.add_downstream_channel(message_queue)
+    def add_downstream_node(self, output_edge: GraphEdge, downstream_operator: Union['LocalDAGNode', ActorHandle]):
+        """
+        添加下游节点到emit context
+        
+        Args:
+            output_edge: 输出边
+            downstream_operator: 下游操作符（本地节点或Ray Actor）
+        """
+        try:
+            if isinstance(downstream_operator, ActorHandle):
+                # Ray Actor
+                self.emit_context.add_downstream_target(
+                    output_channel=output_edge.upstream_channel,
+                    node_type=NodeType.RAY_ACTOR,
+                    target_object=downstream_operator,
+                    target_input_channel=output_edge.downstream_channel,
+                    node_name=f"RayActor_{output_edge.downstream_node.name}"
+                )
+                self.logger.debug(f"Added Ray actor downstream: {self.name}[{output_edge.upstream_channel}] -> "
+                                f"{output_edge.downstream_node.name}[{output_edge.downstream_channel}]")
+            
+            elif isinstance(downstream_operator, LocalDAGNode):
+                # 本地节点
+                self.emit_context.add_downstream_target(
+                    output_channel=output_edge.upstream_channel,
+                    node_type=NodeType.LOCAL,
+                    target_object=downstream_operator,
+                    target_input_channel=output_edge.downstream_channel,
+                    node_name=downstream_operator.name
+                )
+                self.logger.debug(f"Added local node downstream: {self.name}[{output_edge.upstream_channel}] -> "
+                                f"{downstream_operator.name}[{output_edge.downstream_channel}]")
+            else:
+                raise TypeError(f"Unsupported downstream operator type: {type(downstream_operator)}")
+                
+        except Exception as e:
+            self.logger.error(f"Error adding downstream node: {e}", exc_info=True)
+            raise
     
 
     def _inject_emit_context_if_needed(self):
@@ -168,18 +189,18 @@ class MultiplexerDagNode:
                 if self.is_spout:
                     # For spout nodes, call operator.receive with dummy channel and data
                     self.operator.receive(0, None)
+                    time.sleep(1)  # Sleep to avoid busy loop
                 else:
                     # For non-spout nodes, fetch input and process
-                    input_result = self.fetch_input()
-                    if input_result is None:
+                    # input_result = self.fetch_input()
+                    data_packet = self.input_buffer.get(timeout=0.5)
+                    if(data_packet is None):
                         time.sleep(0.1)  # Short sleep when no data to process
                         continue
-                    
-                    # Unpack the tuple: (channel_id, data)
-                    channel_id, data = input_result
-                    
+                    (input_channel, data) = data_packet
+                    self.logger.debug(f"Processing data from buffer: channel={input_channel}")
                     # Call operator's receive method with the channel_id and data
-                    self.operator.receive(channel_id, data)
+                    self.operator.receive(input_channel, data)
                     
             except Exception as e:
                 self.logger.error(
@@ -216,17 +237,3 @@ class MultiplexerDagNode:
         self._initialized = False
 
 
-    # def add_upstream_channel(self, upstream_dagnode:MultiplexerDagNode,channel_index:int):
-    #     """
-    #     Add an upstream channel to this multiplexer node.
-    #     目前只能顺序添加，不能随机添加。
-    #     Args:
-    #         upstream_dagnode: The upstream MultiplexerDagNode instance
-    #         channel_index: The index of the channel in the upstream node
-    #     """
-    #     if channel_index < len(upstream_dagnode.downstream_channels):
-    #         self.upstream_channels.append(upstream_dagnode.downstream_channels[channel_index])
-    #     else:
-    #         self.logger.error(f"Channel index {channel_index} out of range for upstream node {upstream_dagnode.name}.")
-
-
diff --git a/sage/core/dag/mixed_dag.py b/sage/core/dag/mixed_dag.py
new file mode 100644
index 0000000..309f20c
--- /dev/null
+++ b/sage/core/dag/mixed_dag.py
@@ -0,0 +1,307 @@
+import ray
+import logging
+from typing import Dict, List, Optional, Any, Tuple, TYPE_CHECKING, Union
+from ray.actor import ActorHandle
+from sage.utils.custom_logger import CustomLogger
+from sage.core.graph import SageGraph, GraphEdge, GraphNode
+from sage.core.dag.ray.ray_dag_node import RayDAGNode
+from sage.core.dag.local.local_dag_node import LocalDAGNode
+
+
+class MixedDAG:
+    def __init__(self, graph: SageGraph):
+        self.name:str = graph.name
+        self.graph:SageGraph = graph
+        self.operators: Dict[str, Union[ActorHandle, LocalDAGNode]] = {}
+        self.nodes_metadata: Dict[str, Dict[str, Any]] = {}  # node_name -> platform
+        self.connections: List[Tuple[str, int, str, int]] = []  # (upstream_node, out_channel, downstream_node, in_channel)
+        self.session_folder = CustomLogger.get_session_folder()
+        self.logger = CustomLogger(
+            object_name=f"MixedDAG_{self.name}",
+            log_level="DEBUG",
+            console_output=True,
+            file_output=True
+        )
+        self.node_dependencies: Dict[str, List[str]] = {}  # node_name -> [upstream_node_names]
+        self.spout_nodes: List[str] = []
+        self.is_running: bool = False
+        self._compile_graph()
+    
+    def _compile_graph(self):
+        """编译图结构，创建节点并建立连接"""
+        self.logger.info(f"Compiling mixed DAG for graph: {self.name}")
+        
+        # 第一步：创建所有节点实例
+        for node_name, graph_node in self.graph.nodes.items():
+            node_instance = self.create_node_instance(graph_node)
+            upstream_nodes = self.graph.get_upstream_nodes(node_name)
+            
+            self.add_node(
+                name=node_name,
+                executor=node_instance,
+                is_spout=(graph_node.type == "source"),
+                upstream_nodes=upstream_nodes
+            )
+        
+        # 第二步：建立节点间的连接
+        for node_name, graph_node in self.graph.nodes.items():
+            self._setup_node_connections(node_name, graph_node)
+        
+        self.logger.info(f"Mixed DAG compilation completed: {len(self.operators)} nodes, "f"{len(self.spout_nodes)} spout nodes")
+
+
+    def _setup_node_connections(self, node_name: str, graph_node: GraphNode):
+        """
+        为节点设置下游连接
+        
+        Args:
+            node_name: 节点名称
+            graph_node: 图节点对象
+        """
+        node_platform = self.nodes_metadata[node_name]["platform"]
+        current_operator = self.operators[node_name]
+        
+        # 为每个输出边添加下游连接
+        for output_edge in graph_node.output_channels:
+            downstream_node_name = output_edge.downstream_node.name
+            downstream_operator = self.operators[downstream_node_name]
+            
+            try:
+                if node_platform == "ray":
+                    # Ray节点调用远程方法
+                    if(isinstance(downstream_operator, LocalDAGNode)):
+                        downstream_handle = downstream_operator.name
+                    current_operator.add_downstream_node.remote(
+                        output_edge.upstream_channel,
+                        output_edge.downstream_channel,
+                        downstream_handle
+                    )
+                    self.logger.debug(f"Setup Ray connection: {node_name} -> {downstream_node_name}")
+                else:
+                    # 本地节点直接调用
+                    current_operator.add_downstream_node(
+                        output_edge,
+                        downstream_operator
+                    )
+                    self.logger.debug(f"Setup local connection: {node_name} -> {downstream_node_name}")
+                    
+                # 记录连接信息
+                self.connections.append((
+                    node_name, 
+                    output_edge.upstream_channel,
+                    downstream_node_name, 
+                    output_edge.downstream_channel
+                ))
+                
+            except Exception as e:
+                self.logger.error(f"Error setting up connection {node_name} -> {downstream_node_name}: {e}")
+                raise        
+
+    def create_node_instance(self, graph_node: GraphNode) -> Union[RayDAGNode, LocalDAGNode]:
+        """
+        根据图节点创建对应的执行实例
+        
+        Args:
+            graph_node: 图节点对象
+            
+        Returns:
+            节点实例（Ray Actor或本地节点）
+        """
+        platform = graph_node.config.get("platform", "local")
+        
+        if platform == "ray":
+            # 创建Ray Actor
+            node = RayDAGNode.remote(
+                name=graph_node.name,
+                function_class=graph_node.operator,
+                operator_config=graph_node.config,
+                is_spout=(graph_node.type == "source"), 
+                session_folder=self.session_folder
+            )
+            self.logger.debug(f"Created Ray actor node: {graph_node.name}")
+            return node
+        else:
+            # 创建本地节点
+            operator_instance = graph_node.operator(graph_node.config)
+            node = LocalDAGNode(
+                name=graph_node.name,
+                operator=operator_instance,
+                config=graph_node.config,
+                is_spout=(graph_node.type == "source"), 
+                session_folder=self.session_folder
+            )
+            self.logger.debug(f"Created local node: {graph_node.name}")
+            return node
+
+    def add_node(self, name: str, executor: Any, is_spout: bool = False, 
+                 upstream_nodes: List[str] = None):
+        """
+        添加节点到DAG
+        
+        Args:
+            name: 节点名称
+            executor: 节点执行器（本地节点或Ray Actor）
+            is_spout: 是否为spout节点
+            upstream_nodes: 上游节点名称列表
+        """
+        # 检测平台类型
+        platform = self._detect_platform(executor)
+        
+        self.operators[name] = executor
+        self.nodes_metadata[name] = {
+            'is_spout': is_spout,
+            'upstream_nodes': upstream_nodes or [], 
+            "platform": platform
+        }
+        
+        if is_spout:
+            self.spout_nodes.append(name)
+        
+        if upstream_nodes:
+            self.node_dependencies[name] = upstream_nodes
+        
+        self.logger.debug(f"Added node '{name}' of platform '{platform}'")
+
+
+    def _detect_platform(self, executor: Any) -> str:
+        """
+        检测执行器的平台类型
+        
+        Args:
+            executor: 执行器对象
+            
+        Returns:
+            平台类型字符串
+        """
+        if isinstance(executor, ActorHandle):
+            return "ray"
+        elif hasattr(executor, 'remote'):
+            return "ray_function" 
+        elif isinstance(executor, LocalDAGNode):
+            return "local"
+        else:
+            return "unknown"
+
+
+    def start_all_nodes(self):
+        """启动所有本地节点（Ray Actor会自动启动）"""
+        self.logger.info("Starting all DAG nodes...")
+        
+        local_node_count = 0
+        ray_node_count = 0
+        
+        for node_name, node_meta in self.nodes_metadata.items():
+            if node_meta["platform"] == "local":
+                node = self.operators[node_name]
+                node.start()
+                local_node_count += 1
+                self.logger.debug(f"Started local node: {node_name}")
+            else:
+                ray_node_count += 1
+        
+        self.logger.info(f"Started {local_node_count} local nodes, {ray_node_count} Ray actors")
+
+    def stop_all_nodes(self):
+        """停止所有节点"""
+        self.logger.info("Stopping all DAG nodes...")
+        
+        for node_name, node_meta in self.nodes_metadata.items():
+            try:
+                if node_meta["platform"] == "local":
+                    node = self.operators[node_name]
+                    node.stop()
+                    self.logger.debug(f"Stopped local node: {node_name}")
+                # Ray actors会在进程结束时自动清理
+            except Exception as e:
+                self.logger.error(f"Error stopping node {node_name}: {e}")
+
+    def get_dag_info(self) -> Dict[str, Any]:
+        """获取DAG信息"""
+        local_nodes = [name for name, meta in self.nodes_metadata.items() 
+                      if meta["platform"] == "local"]
+        ray_nodes = [name for name, meta in self.nodes_metadata.items() 
+                    if meta["platform"] == "ray"]
+        
+        return {
+            "name": self.name,
+            "total_nodes": len(self.operators),
+            "local_nodes": local_nodes,
+            "ray_nodes": ray_nodes,
+            "spout_nodes": self.spout_nodes,
+            "connections": self.connections,
+            "node_dependencies": self.node_dependencies
+        }
+    
+    def run(self) -> Dict[str, List[str]]:
+        """
+        启动MixedDAG执行，将所有节点注册到对应的运行时
+        
+        Returns:
+            Dict: 包含各平台节点句柄的字典
+        """
+        if self.is_running:
+            self.logger.warning(f"MixedDAG '{self.name}' is already running")
+            return {"local_handles": self.local_handles, "ray_handles": self.ray_handles}
+        
+        self.logger.info(f"Starting MixedDAG '{self.name}' execution...")
+        
+        try:
+            # 获取运行时实例
+            from sage.core.runtime.local.local_runtime import LocalRuntime
+            from sage.core.runtime.ray.ray_runtime import RayRuntime
+            
+            local_runtime = LocalRuntime.get_instance()
+            ray_runtime = RayRuntime.get_instance()
+            
+            # 分离本地节点和Ray节点
+            local_nodes = []
+            ray_actors = []
+            ray_node_names = []
+            
+            for node_name, node_meta in self.nodes_metadata.items():
+                if node_meta["platform"] == "local":
+                    local_node = self.operators[node_name]
+                    local_nodes.append(local_node)
+                elif node_meta["platform"] == "ray":
+                    ray_actor = self.operators[node_name]
+                    ray_actors.append(ray_actor)
+                    ray_node_names.append(node_name)
+            
+            # 提交本地节点到LocalRuntime
+            if local_nodes:
+                self.logger.info(f"Submitting {len(local_nodes)} local nodes to LocalRuntime")
+                self.local_handles = local_runtime.submit_nodes(local_nodes)
+                
+                # 注册本地节点到TCP服务器（用于接收Ray Actor的数据）
+                for local_node in local_nodes:
+                    # 这里需要确保local_runtime知道节点名称映射
+                    # 实际上submit_nodes已经在running_nodes中注册了
+                    pass
+                
+                self.logger.info(f"Successfully submitted local nodes with handles: {self.local_handles}")
+            
+            # 提交Ray节点到RayRuntime
+            if ray_actors:
+                self.logger.info(f"Submitting {len(ray_actors)} Ray actors to RayRuntime")
+                self.ray_handles = ray_runtime.submit_actors(ray_actors, ray_node_names)
+                self.logger.info(f"Successfully submitted Ray actors with handles: {self.ray_handles}")
+            
+            # 启动所有节点
+            # 在submit时所有节点就都启动了
+            # self._start_all_nodes(local_runtime, ray_runtime)
+            
+            self.is_running = True
+            self.logger.info(f"MixedDAG '{self.name}' started successfully with "
+                           f"{len(self.local_handles)} local nodes and {len(self.ray_handles)} Ray actors")
+            
+            return {
+                "local_handles": self.local_handles,
+                "ray_handles": self.ray_handles,
+                "total_nodes": len(self.local_handles) + len(self.ray_handles)
+            }
+            
+        except Exception as e:
+            self.logger.error(f"Failed to start MixedDAG '{self.name}': {e}", exc_info=True)
+            # 清理已经提交的节点
+            self._cleanup_partial_submission(local_runtime, ray_runtime)
+            raise
\ No newline at end of file
diff --git a/sage/core/dag/ray/ray_dag_node.py b/sage/core/dag/ray/ray_dag_node.py
new file mode 100644
index 0000000..9f9a97b
--- /dev/null
+++ b/sage/core/dag/ray/ray_dag_node.py
@@ -0,0 +1,285 @@
+import ray
+import asyncio
+import logging
+import time
+from typing import Any, List, Optional, Dict, Tuple, TYPE_CHECKING, Type, Union
+from ray.actor import ActorHandle
+from sage.api.operator.base_operator_api import BaseFuction
+from sage.core.graph import GraphEdge, GraphNode
+from sage.core.io.emit_context import NodeType
+from sage.core.io.ray_emit_context import RayEmitContext
+from sage.utils.custom_logger import CustomLogger
+from sage.core.dag.local.local_dag_node import LocalDAGNode
+@ray.remote
+class RayDAGNode:
+    """
+    Ray Actor version of LocalDAGNode for distributed execution.
+    
+    Unlike local nodes, Ray actors don't need input buffers as Ray platform
+    maintains the request queue for actors automatically.
+    """
+    
+    def __init__(self, 
+                 name: str, 
+                 function_class: Type[BaseFuction],
+                 operator_config: Dict = None,
+                 is_spout: bool = False,
+                 session_folder: str = None) -> None:
+        """
+        Initialize Ray multiplexer DAG node.
+        
+        Args:
+            name: Node name
+            function_class: Operator class (not instance)
+            operator_config: Configuration for operator instantiation
+            is_spout: Whether this is a spout node
+            session_folder: Session folder for logging
+        """
+        self.name = name
+        self.function_class = function_class
+        self.operator_config = operator_config or {}
+        self.is_spout = is_spout
+        self._initialized = False
+        
+        # Running state management
+        self._running = False
+        self._stop_requested = False
+        
+        # Create logger first
+        self.logger = CustomLogger(
+            object_name=f"RayNode_{self.name}",
+            session_folder=session_folder,
+            log_level="DEBUG",
+            console_output=False,
+            file_output=True
+        )
+        
+        # Create emit context for mixed environment
+        self.emit_context = RayEmitContext(
+            self.name, 
+            ray_node_actor=self,
+            session_folder=session_folder
+        )
+        self._emit_context_injected = False
+        
+        self.logger.info(f"Created Ray actor node: {self.name}")
+
+    def _ensure_initialized(self):
+        """
+        Ensure that all runtime objects are initialized.
+        Called when the node actually starts running.
+        """
+        if self._initialized:
+            return
+        
+        # Create operator instance locally within the Ray actor
+        operator_config = self.operator_config.copy()
+        operator_config["session_folder"] = CustomLogger.get_session_folder()
+        
+        try:
+            self.operator = self.function_class(operator_config)
+            self.logger.debug(f"Created operator instance for {self.name}")
+        except Exception as e:
+            self.logger.error(f"Failed to create operator instance: {e}", exc_info=True)
+            raise
+        
+        # Inject emit context if operator supports it
+        if hasattr(self.operator, 'set_emit_context') and not self._emit_context_injected:
+            try:
+                self.operator.set_emit_context(self.emit_context)
+                self._emit_context_injected = True
+                self.logger.debug(f"Injected emit context for operator in node {self.name}")
+            except Exception as e:
+                self.logger.warning(f"Failed to inject emit context in node {self.name}: {e}")
+        
+        self._initialized = True
+
+    def add_downstream_node(self,output_channel:int, target_input_channel:int,   downstream_handle: Union[ActorHandle, str]):
+        """
+        添加下游节点到emit context
+        
+        Args:
+            output_edge: 输出边
+            downstream_operator: 下游操作符（Ray Actor或本地节点）
+        """
+        try:
+            if isinstance(downstream_handle, ActorHandle):
+                # 下游是Ray Actor
+                self.emit_context.add_downstream_target(
+                    output_channel=output_channel,
+                    node_type=NodeType.RAY_ACTOR,
+                    target_object=downstream_handle,
+                    target_input_channel=target_input_channel,
+                    node_name=f"RayActor_output_channel_{output_channel}"
+                )
+                self.logger.debug(f"Added Ray actor downstream: {self.name}[{output_channel}]")
+            
+            else:
+                # 下游是本地节点（通过TCP通信）
+                self.emit_context.add_downstream_target(
+                    output_channel=output_channel,
+                    node_type=NodeType.LOCAL,
+                    target_object=None,  # TCP通信不需要直接引用
+                    target_input_channel=target_input_channel,
+                    node_name=downstream_handle
+                )
+                self.logger.debug(f"Added local node downstream: {self.name}[{output_channel}] -> "
+                                f"{downstream_handle}[{target_input_channel}] (via TCP)")
+                
+        except Exception as e:
+            self.logger.error(f"Error adding downstream node: {e}", exc_info=True)
+            raise
+
+    def receive(self, input_channel: int, data: Any):
+        """
+        Receive data from upstream node and process it.
+        This method is called directly by upstream nodes (Ray actors or local nodes via TCP).
+        
+        Note: Ray platform automatically queues these method calls, so no input buffer needed.
+        
+        Args:
+            input_channel: The input channel number on this node
+            data: Data received from upstream
+        """
+        try:
+            # Ensure initialization on first call
+            self._ensure_initialized()
+            
+            if self._stop_requested:
+                self.logger.debug(f"Ignoring data on stopped node {self.name}")
+                return
+                
+            self.logger.debug(f"Received data in node {self.name}, channel {input_channel}")
+            
+            # Call operator's receive method with correct input channel
+            self.operator.receive(input_channel, data)
+            
+        except Exception as e:
+            self.logger.error(f"Error processing data in node {self.name}: {e}", exc_info=True)
+            raise
+
+    # 理论上来说，emit方法不会被调用，因为算子会直接调用emit_context.emit
+    # 但为了兼容性和未来可能的需求，这里保留emit方法
+    def emit(self, output_channel: int, data: Any):
+        """
+        Emit data to downstream nodes through the specified output channel.
+        Called by the operator through emit context.
+        
+        Args:
+            output_channel: This node's output channel number (-1 for all channels)
+            data: Data to emit
+        """
+        try:
+            self.emit_context.emit(output_channel, data)
+        except Exception as e:
+            self.logger.error(f"Error emitting data from {self.name}[out:{output_channel}]: {e}", exc_info=True)
+            raise
+
+    def start_spout(self):
+        """
+        Start the spout node execution.
+        For spout nodes, continuously call operator.receive with dummy data.
+        This runs in a loop until stop is requested.
+        """
+        if not self.is_spout:
+            self.logger.warning(f"start_spout called on non-spout node {self.name}")
+            return
+        
+        # Ensure initialization
+        self._ensure_initialized()
+            
+        self._running = True
+        self._stop_requested = False
+        
+        self.logger.info(f"Starting spout execution for node {self.name}")
+        
+        try:
+            while self._running and not self._stop_requested:
+                # For spout nodes, call operator.receive with dummy channel and data
+                self.operator.receive(0, None)
+                time.sleep(0.1)  # Small delay to prevent overwhelming
+                
+        except Exception as e:
+            self.logger.error(f"Error in spout node {self.name}: {e}", exc_info=True)
+            raise
+        finally:
+            self._running = False
+            self.logger.info(f"Spout execution stopped for node {self.name}")
+
+    def start(self):
+        """
+        Start the node. For spout nodes, this starts the generation loop.
+        For non-spout nodes, this just marks the node as ready to receive data.
+        """
+        self._ensure_initialized()
+        
+        if self.is_spout:
+            # Start spout execution asynchronously
+            self.start_spout()
+        else:
+            # For non-spout nodes, just mark as running
+            self._running = True
+            self._stop_requested = False
+            self.logger.info(f"Ray node {self.name} started and ready to receive data")
+
+    def stop(self):
+        """Stop the node execution."""
+        self._stop_requested = True
+        self._running = False
+        self.logger.info(f"Ray node {self.name} stopped")
+
+    def is_running(self):
+        """Check if the node is currently running."""
+        return self._running and not self._stop_requested
+
+    def get_name(self):
+        """Get node name."""
+        return self.name
+
+    def get_node_info(self) -> Dict[str, Any]:
+        """Get comprehensive node information for debugging."""
+        return {
+            "name": self.name,
+            "is_spout": self.is_spout,
+            "is_running": self.is_running(),
+            "stop_requested": self._stop_requested,
+            "initialized": self._initialized,
+            "operator_class": self.function_class.__name__ if self.function_class else None,
+            "downstream_targets": len(self.emit_context.downstream_channels) if hasattr(self, 'emit_context') else 0
+        }
+
+    def health_check(self) -> Dict[str, Any]:
+        """Perform health check and return status."""
+        try:
+            return {
+                "status": "healthy",
+                "node_name": self.name,
+                "is_running": self.is_running(),
+                "initialized": self._initialized,
+                "timestamp": time.time_ns()
+            }
+        except Exception as e:
+            return {
+                "status": "unhealthy",
+                "node_name": self.name,
+                "error": str(e),
+                "timestamp": time.time_ns()
+            }
+
+    def __getstate__(self):
+        """
+        Custom serialization to exclude non-serializable objects.
+        Ray handles most serialization automatically, but this helps with debugging.
+        """
+        state = self.__dict__.copy()
+        # Ray actors typically don't need custom serialization,
+        # but we can exclude logger if needed
+        return state
+
+    def __setstate__(self, state):
+        """
+        Custom deserialization to restore state.
+        """
+        self.__dict__.update(state)
+        # Mark as not initialized so runtime objects will be created when needed
+        self._initialized = False
\ No newline at end of file
diff --git a/sage/core/dag/ray/ray_multi_node.py b/sage/core/dag/ray/ray_multi_node.py
deleted file mode 100644
index 889eef2..0000000
--- a/sage/core/dag/ray/ray_multi_node.py
+++ /dev/null
@@ -1,225 +0,0 @@
-import ray
-import asyncio
-import logging
-from typing import Any, List, Optional, Dict, Tuple, TYPE_CHECKING, Type
-# from sage.archive.operator_wrapper import OperatorWrapper
-from sage.api.operator.base_operator_api import BaseFuction
-from sage.api.operator.base_operator_api import EmitContext
-from sage.utils.custom_logger import CustomLogger
-from ray.actor import ActorHandle  # 只在类型检查期间生效
-import time
-@ray.remote
-class RayMultiplexerDagNode:
-    """
-    Ray Actor version of MultiplexerDagNode for distributed execution.
-    """
-    
-    def __init__(self, 
-                 name: str, 
-                 operator_class: Type[BaseFuction],
-                 operator_config: Dict = None,
-                 is_spout: bool = False,
-                 session_folder: str = None) -> None:
-        self.name = name
-        self.operator_class = operator_class
-        self.operator_config = operator_config or {}
-        self.is_spout = is_spout
-
-        self.logger = CustomLogger(
-            object_name=f"RayNode_{self.name}",
-            session_folder=session_folder,
-            log_level="DEBUG",
-            console_output=False,
-            file_output=True
-        )
-
-        # 取消继承 root logger 的 stdout handler
-        # self.logger.propagate = False
-        """
-        Initialize Ray multiplexer DAG node.
-        
-        Args:
-            name: Node name
-            operator_class: Operator class (not instance)
-            operator_config: Configuration for operator instantiation
-            is_spout: Whether this is a spout node
-        """
-
-        
-        # Create operator instance locally within the Ray actor
-      
-
-        # Store downstream connections: output_channel -> [(downstream_actor, downstream_input_channel)]
-        self.downstream_connections: List[Tuple[ActorHandle, int]] = []
-
-        operator_config["session_folder"] = session_folder
-        self.operator = operator_class(operator_config)
-        
-        # Running state
-        self._running = False
-        self._stop_requested = False
-        
-        # Create emit context for Ray environment
-        self.emit_context = RayEmitContext(self.name, self)
-        
-        # Inject emit context if operator supports it
-        if hasattr(self.operator, 'set_emit_context'):
-            try:
-                self.operator.set_emit_context(self.emit_context)
-            except Exception as e:
-                pass
-
-
-    def add_downstream_connection(self, output_channel: int, downstream_actor:ActorHandle, 
-                                downstream_input_channel: int):
-        """
-        Add downstream connection mapping.
-        
-        Args:
-            output_channel: This node's output channel number
-            downstream_actor: Downstream Ray actor handle
-            downstream_input_channel: Downstream node's input channel number
-        """
-        if output_channel >= len(self.downstream_connections):
-            self.downstream_connections.extend([None] * (output_channel + 1 - len(self.downstream_connections)))
-        if self.downstream_connections[output_channel] is not None:
-            raise ValueError(
-                f"Output channel {output_channel} already has a downstream connection in node {self.name}"
-            )
-        
-        self.downstream_connections[output_channel] = (downstream_actor, downstream_input_channel)
-        
-        self.logger.debug(
-            f"Added downstream connection: {self.name}[out:{output_channel}] -> "
-            f"downstream_node[in:{downstream_input_channel}]"
-        )
-    
-    def receive(self, input_channel: int, data: Any):
-        """
-        Receive data from upstream node and process it.
-        This method is called directly by upstream Ray actors.
-        
-        Args:
-            input_channel: The input channel number on this node
-            data: Data received from upstream
-        """
-        try:
-            if self._stop_requested:
-                return
-                
-            # Call operator's receive method with correct input channel
-            self.logger.debug(f"Received data in node {self.name}, channel {input_channel}")
-            self.operator.receive(input_channel, data)
-            
-        except Exception as e:
-            self.logger.error(f"Error processing data in node {self.name}: {e}", exc_info=True)
-            raise
-    
-    def emit(self, output_channel: int, data: Any):
-        """
-        Emit data to downstream actors through the specified output channel.
-        Called by the operator through emit context.
-        
-        Args:
-            output_channel: This node's output channel number (-1 for all channels)
-            data: Data to emit
-        """
-        if output_channel == -1:
-            # Special case for broadcasting to all channels
-            for downstream_actor, downstream_input_channel in self.downstream_connections:
-                if downstream_actor is not None:  # Skip None entries
-                    try:
-                        # Asynchronously call downstream actor's receive method
-                        downstream_actor.receive.remote(downstream_input_channel, data)
-                        
-                        self.logger.debug(
-                            f"Emitted data from {self.name}[out:all] to "
-                            f"downstream[in:{downstream_input_channel}]"
-                        )
-                    except Exception as e:
-                        self.logger.error(
-                            f"Failed to emit data from {self.name}[out:all]: {e}"
-                        )
-        elif 0 <= output_channel < len(self.downstream_connections):
-            connection = self.downstream_connections[output_channel]
-            if connection is not None:
-                downstream_actor, downstream_input_channel = connection
-                try:
-                    # Asynchronously call downstream actor's receive method
-                    downstream_actor.receive.remote(downstream_input_channel, data)
-                    
-                    self.logger.debug(
-                        f"Emitted data from {self.name}[out:{output_channel}] to "
-                        f"downstream[in:{downstream_input_channel}]"
-                    )
-                except Exception as e:
-                    self.logger.error(
-                        f"Failed to emit data from {self.name}[out:{output_channel}]: {e}"
-                    )
-            else:
-                self.logger.warning(
-                    f"No downstream connection for output channel {output_channel} in node {self.name}"
-                )
-        else:
-            self.logger.warning(
-                f"Invalid output channel {output_channel} in node {self.name}"
-            )
-    
-    def get_downstream_connections(self) -> List[Tuple[ActorHandle, int]]:
-        """Get all downstream connections for debugging."""
-        return self.downstream_connections.copy()
-    
-    def start_spout(self):
-        """
-        Start the spout node execution.
-        For spout nodes, continuously call operator.receive with dummy data.
-        """
-        if not self.is_spout:
-            self.logger.warning(f"start_spout called on non-spout node {self.name}")
-            return
-            
-        self._running = True
-        self._stop_requested = False
-        
-        try:
-            while self._running and not self._stop_requested:
-                # For spout, we typically call with channel 0 and None data
-                self.operator.receive(0, None)
-                time.sleep(1)
-        except Exception as e:
-            self.logger.error(f"Error in spout node {self.name}: {e}", exc_info=True)
-            raise
-        finally:
-            self._running = False
-    
-    def stop(self):
-        """Stop the node execution."""
-        self._stop_requested = True
-        self._running = False
-        # self.logger.info(f"Ray node {self.name} stopped")
-    
-    def is_running(self):
-        """Check if the node is currently running."""
-        return self._running
-    
-    def get_name(self):
-        """Get node name."""
-        return self.name
-
-
-class RayEmitContext(EmitContext):
-    """
-    Ray-specific emit context that uses direct actor calls instead of message queues.
-    """
-    
-    def __init__(self, node_name: str, ray_node_actor):
-        super().__init__(node_name)
-        self.ray_node_actor = ray_node_actor
-    
-    def emit(self, channel: int, data: Any):
-        """Emit data through Ray actor's emit method."""
-        self.ray_node_actor.emit(channel, data)
-    
-    def add_downstream_channel(self, channel):
-        """For Ray actors, downstream channels are managed differently."""
-        pass  # No-op for Ray implementation
\ No newline at end of file
diff --git a/sage/core/engine.py b/sage/core/engine.py
index 19a9768..7f3595a 100644
--- a/sage/core/engine.py
+++ b/sage/core/engine.py
@@ -2,33 +2,27 @@ from typing import Type, TYPE_CHECKING, Union, Any, TYPE_CHECKING
 from sage.core.compiler.query_compiler import QueryCompiler
 from sage.core.runtime.runtime_manager import RuntimeManager
 from sage.utils.custom_logger import CustomLogger
+from sage.core.dag.mixed_dag import MixedDAG
 import threading, typing, logging
 
 class Engine:
     _instance = None
     _lock = threading.Lock()
-    def __init__(self,generate_func = None, session_folder: str = None):
-        if session_folder is None:
-            # 如果没有提供 session_folder，则创建一个新的会话文件夹
-            # 这将确保每次运行时都有独立的日志和数据存储
-            session_folder = CustomLogger.create_session_folder()
-        # 如果提供了 session_folder，则使用它
-        self.session_folder = session_folder
+    def __init__(self):
 
         # 确保只初始化一次
         if hasattr(self, "_initialized"):
             return
         self._initialized = True
         # self.dag_manager = DAGManager() # deprecated
-        self.runtime_manager = RuntimeManager(self.session_folder)
-        self.compiler= QueryCompiler(generate_func=generate_func, session_folder=self.session_folder)
+        self.runtime_manager = RuntimeManager.get_instance()
+        self.compiler= QueryCompiler()
         from sage.core.graph import SageGraph
         self.graphs:dict[str, SageGraph] = {}  # 存储 pipeline 名称到 SageGraph 的映射
         self.dags:dict = {} # 存储name到dag的映射，其中dag的类型为DAG或RayDAG
 
         self.logger = CustomLogger(
             object_name=f"SageEngine",
-            session_folder=self.session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
@@ -42,7 +36,7 @@ class Engine:
     # 用来获取类的唯一实例
     # 同一个进程中只存在唯一的实例
     @classmethod
-    def get_instance(cls,generate_func):
+    def get_instance(cls):
         # 双重检查锁确保线程安全
         if cls._instance is None:
             with cls._lock:
@@ -50,7 +44,7 @@ class Engine:
 
                     # 绕过 __new__ 的异常，直接创建实例
                     instance = super().__new__(cls)
-                    instance.__init__(generate_func)
+                    instance.__init__()
                     cls._instance = instance
         return cls._instance
 
@@ -70,6 +64,23 @@ class Engine:
             self.logger.info(f"Graph '{graph.name}' submitted to runtime manager.")
             # 通过运行时管理器获取对应平台的运行时并提交任务
             task_handle = self.runtime_manager.submit(dag) 
+        except Exception as e:
+            self.logger.info(f"Failed to submit graph '{graph.name}': {e}")
+            raise
+    
+    def submit_mixed_pipeline(self, pipeline, config=None):
+        from sage.core.graph import SageGraph
+        graph = SageGraph(pipeline, config)
+        self.graphs[graph.name] = graph
+        if config:
+            graph.config.update(config)
+        try:
+            self.logger.info(f"Received mixed graph '{graph.name}' with {len(graph.nodes)} nodes")
+            # 编译图
+            mixed_dag = MixedDAG(graph)
+            self.dags[mixed_dag.name] = mixed_dag  # 存储 DAG 到字典中
+            mixed_dag.run()
+            self.logger.info(f"Mixed graph '{graph.name}' submitted to runtime manager.")
         except Exception as e:
             self.logger.info(f"Failed to submit graph '{graph.name}': {e}")
             raise
\ No newline at end of file
diff --git a/sage/core/graph/sage_graph.py b/sage/core/graph/sage_graph.py
index 3978d5a..57e9d1f 100644
--- a/sage/core/graph/sage_graph.py
+++ b/sage/core/graph/sage_graph.py
@@ -9,13 +9,13 @@ from sage.utils.custom_logger import CustomLogger
 
 
 class GraphNode:
-    def __init__(self,name:str, operator_class: Type[BaseFuction], type:str, operator_config: Dict = None):
+    def __init__(self,name:str, function_class: Type[BaseFuction], type:str, operator_config: Dict = None):
         self.name: str = name
         self.type: str = type # "normal "or "source" or "sink"
         self.config: Dict = operator_config
         self.input_channels: list[GraphEdge] = []
         self.output_channels: list[GraphEdge] = []
-        self.operator: Type[BaseFuction] = operator_class
+        self.operator: Type[BaseFuction] = function_class
         pass
 
 class GraphEdge:
@@ -30,10 +30,10 @@ class GraphEdge:
         self.upstream_node:GraphNode = upstream_node
         self.upstream_channel: int = upstream_channel
         self.downstream_node:GraphNode = None
-        self.downstream_channnel: int = None
+        self.downstream_channel: int = None
 
 class SageGraph:
-    def __init__(self, pipeline:Pipeline, config: dict = None, session_folder: str = None):
+    def __init__(self, pipeline:Pipeline, config: dict = None):
         """
         Initialize the NodeGraph with a name and optional configuration.
         Args:
@@ -50,7 +50,6 @@ class SageGraph:
 
         self.logger = CustomLogger(
             object_name=f"SageGraph_{self.name}",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=True,
             file_output=True
@@ -118,7 +117,7 @@ class SageGraph:
                     node_name=node_name,
                     input_streams=connection_info['input_edges'],
                     output_streams=connection_info['output_edges'],
-                    operator_class=stream.operator,
+                    function_class=stream.operator,
                     operator_config=stream.config, 
                     node_type=stream.node_type
                 )
@@ -272,7 +271,7 @@ class SageGraph:
                  node_name: str,
                  input_streams: Union[str, List[str]], 
                  output_streams: Union[str, List[str]], 
-                 operator_class: Type[BaseFuction],
+                 function_class: Type[BaseFuction],
                  operator_config: Dict = None, 
                  node_type: str = "normal") -> GraphNode:
         """
@@ -296,7 +295,7 @@ class SageGraph:
             output_streams = []
 
         # 创建节点
-        node = GraphNode(node_name, operator_class, node_type, operator_config)
+        node = GraphNode(node_name, function_class, node_type, operator_config)
         # 检查节点名是否已存在
         if node.name in self.nodes:
             raise ValueError(f"Node with name '{node.name}' already exists")
@@ -313,7 +312,7 @@ class SageGraph:
             
             # 连接边到当前节点
             edge.downstream_node = node
-            edge.downstream_channnel = i
+            edge.downstream_channel = i
             node.input_channels.append(edge)
 
         # 处理输出边（创建新的空边）
diff --git a/sage/core/io/emit_context.py b/sage/core/io/emit_context.py
new file mode 100644
index 0000000..788b9e8
--- /dev/null
+++ b/sage/core/io/emit_context.py
@@ -0,0 +1,116 @@
+from typing import TypeVar, Generic, Callable, Any, List, Dict, Union, Tuple, Literal
+from abc import ABC, abstractmethod
+from enum import Enum
+import ray
+from ray.actor import ActorHandle
+import socket
+import json
+import pickle
+import threading
+from sage.utils.custom_logger import CustomLogger
+
+class NodeType(Enum):
+    LOCAL = "local"
+    RAY_ACTOR = "ray_actor"
+
+class DownstreamTarget:
+    """下游目标节点的封装"""
+    def __init__(self, 
+                 node_type: NodeType, 
+                 target_object: Any, 
+                 target_input_channel: int,
+                 node_name: str = None):
+        self.node_type = node_type
+        self.target_object = target_object
+        self.target_input_channel = target_input_channel
+        self.node_name = node_name or str(target_object)
+
+class BaseEmitContext(ABC):
+    """
+    基础Emit Context抽象类
+    支持混合环境中本地和Ray Actor之间的通信
+    """
+    
+    def __init__(self, node_name: str, session_folder: str = None):
+        self.node_name = node_name
+        self.downstream_channels: Dict[int, DownstreamTarget] = {}
+        # Dict[自身出口号, DownstreamTarget]
+        
+        self.logger = CustomLogger(
+            object_name=f"EmitContext_{node_name}",
+            session_folder=session_folder,
+            log_level="DEBUG"
+        )
+    
+    def add_downstream_target(self, 
+                            output_channel: int,
+                            node_type: NodeType, 
+                            target_object: Any, 
+                            target_input_channel: int,
+                            node_name: str = None) -> None:
+        """
+        添加下游目标节点
+        
+        Args:
+            output_channel: 自身的输出通道号
+            node_type: 下游节点类型
+            target_object: 下游节点对象
+            target_input_channel: 下游节点的输入通道号
+            node_name: 下游节点名称
+        """
+        target = DownstreamTarget(node_type, target_object, target_input_channel, node_name)
+        self.downstream_channels[output_channel] = target
+        
+        self.logger.debug(f"Added downstream target: {self.node_name}[out:{output_channel}] -> "
+                         f"{node_name}[in:{target_input_channel}] (type: {node_type.value})")
+    
+    def emit(self, channel: int, data: Any) -> None:
+        """
+        向指定下游通道发送数据
+        
+        Args:
+            channel: 下游通道索引，-1表示广播到所有通道
+            data: 要发送的数据
+        """
+        if channel == -1:
+            # 广播到所有下游通道
+            for output_channel, target in self.downstream_channels.items():
+                self._route_and_send(target, data)
+        elif channel in self.downstream_channels:
+            target = self.downstream_channels[channel]
+            self._route_and_send(target, data)
+        else:
+            self.logger.warning(f"Channel index {channel} out of range for node {self.node_name}")
+    
+    def _route_and_send(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        根据目标类型路由并发送数据
+        
+        Args:
+            target: 目标节点信息
+            data: 要发送的数据
+        """
+        try:
+            if target.node_type == NodeType.LOCAL:
+                self._send_to_local(target, data)
+            elif target.node_type == NodeType.RAY_ACTOR:
+                self._send_to_ray_actor(target, data)
+            else:
+                self.logger.error(f"Unknown target type: {target.node_type}")
+        except Exception as e:
+            self.logger.error(f"Failed to send data to {target.node_name}: {e}", exc_info=True)
+    
+    @abstractmethod
+    def _send_to_local(self, target: DownstreamTarget, data: Any) -> None:
+        """发送数据到本地节点"""
+        pass
+    
+    @abstractmethod
+    def _send_to_ray_actor(self, target: DownstreamTarget, data: Any) -> None:
+        """发送数据到Ray Actor"""
+        pass
+
+
+
+
+
diff --git a/sage/core/io/local_emit_context.py b/sage/core/io/local_emit_context.py
new file mode 100644
index 0000000..9102e91
--- /dev/null
+++ b/sage/core/io/local_emit_context.py
@@ -0,0 +1,67 @@
+from typing import TypeVar, Generic, Callable, Any, List, Dict, Union, Tuple, Literal
+from abc import ABC, abstractmethod
+from enum import Enum
+import ray
+from ray.actor import ActorHandle
+import socket
+import json
+import pickle
+import threading
+from sage.utils.custom_logger import CustomLogger
+from sage.core.io.emit_context import BaseEmitContext, DownstreamTarget, NodeType
+
+class LocalEmitContext(BaseEmitContext):
+    """
+    本地DAG节点使用的Emit Context
+    支持向本地节点的输入缓冲区写入数据，向Ray Actor发送远程调用
+    """
+    
+    def __init__(self, node_name: str, session_folder: str = None):
+        super().__init__(node_name, session_folder)
+    
+    def _send_to_local(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        向本地节点的输入缓冲区写入数据包
+        
+        Args:
+            target: 目标本地节点
+            data: 数据
+        """
+        try:
+            # 向下游本地节点的输入缓冲区写入 (输入channel, 数据) 包
+            data_packet = (target.target_input_channel, data)
+            
+            # 假设本地节点有input_buffer属性用于接收数据
+            if hasattr(target.target_object, 'input_buffer'):
+                target.target_object.input_buffer.put(data_packet)
+            elif hasattr(target.target_object, 'put'):
+                # 或者有put方法
+                target.target_object.put(data_packet)
+            else:
+                raise AttributeError(f"Local node {target.node_name} has no input_buffer or put method")
+                
+            self.logger.debug(f"Written data packet to local node {target.node_name}[in:{target.target_input_channel}] input buffer")
+            
+        except Exception as e:
+            self.logger.error(f"Error writing data to local node {target.node_name} input buffer: {e}")
+            raise
+    
+    def _send_to_ray_actor(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        向Ray Actor发送远程调用
+        
+        Args:
+            target: 目标Ray Actor
+            data: 数据
+        """
+        try:
+            if isinstance(target.target_object, ActorHandle):
+                # 直接调用Ray Actor的remote方法
+                target.target_object.receive.remote(target.target_input_channel, data)
+                self.logger.debug(f"Sent remote call to Ray actor {target.node_name}[in:{target.target_input_channel}]")
+            else:
+                raise TypeError(f"Expected ActorHandle for Ray actor, got {type(target.target_object)}")
+                
+        except Exception as e:
+            self.logger.error(f"Error sending remote call to Ray actor {target.node_name}: {e}")
+            raise
\ No newline at end of file
diff --git a/sage/core/io/ray_emit_context.py b/sage/core/io/ray_emit_context.py
new file mode 100644
index 0000000..c1b347a
--- /dev/null
+++ b/sage/core/io/ray_emit_context.py
@@ -0,0 +1,111 @@
+from typing import TypeVar, Generic, Callable, Any, List, Dict, Union, Tuple, Literal
+from abc import ABC, abstractmethod
+from enum import Enum
+import ray
+from ray.actor import ActorHandle
+import socket
+import json
+import pickle
+import threading
+from sage.utils.custom_logger import CustomLogger
+from sage.core.io.emit_context import BaseEmitContext, DownstreamTarget, NodeType
+import time
+
+
+
+class RayEmitContext(BaseEmitContext):
+    """
+    Ray Actor使用的Emit Context
+    支持向Ray Actor发送远程调用，向本地节点发送TCP包
+    """
+    
+    def __init__(self, node_name: str, ray_node_actor=None,
+                 local_tcp_host: str = "localhost", 
+                 local_tcp_port: int = 9999, session_folder: str = None):
+        super().__init__(node_name, session_folder)
+        self.ray_node_actor = ray_node_actor
+        self.local_tcp_host = local_tcp_host
+        self.local_tcp_port = local_tcp_port
+        self._tcp_socket = None
+        self._socket_lock = threading.Lock()
+    
+    def _get_tcp_connection(self) -> socket.socket:
+        """获取到本地的TCP连接（懒加载）"""
+        if self._tcp_socket is None:
+            with self._socket_lock:
+                if self._tcp_socket is None:
+                    try:
+                        self._tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+                        self._tcp_socket.connect((self.local_tcp_host, self.local_tcp_port))
+                        self.logger.info(f"Ray actor connected to local TCP server at {self.local_tcp_host}:{self.local_tcp_port}")
+                    except Exception as e:
+                        self.logger.error(f"Failed to connect to local TCP server: {e}")
+                        raise
+        return self._tcp_socket
+    
+    def _send_to_local(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        向本地节点发送TCP包
+        
+        Args:
+            target: 目标本地节点
+            data: 数据
+        """
+        try:
+            # 构造TCP消息包
+            message = {
+                "type": "ray_to_local",
+                "source_actor": self.node_name,
+                "target_node": target.node_name,
+                "target_channel": target.target_input_channel,
+                "data": data,
+                "timestamp": time.time_ns()
+            }
+            
+            # 序列化消息
+            serialized_data = pickle.dumps(message)
+            message_size = len(serialized_data)
+            
+            # 发送消息长度，然后发送消息内容
+            tcp_conn = self._get_tcp_connection()
+            tcp_conn.sendall(message_size.to_bytes(4, byteorder='big'))
+            tcp_conn.sendall(serialized_data)
+            
+            self.logger.debug(f"Sent TCP packet to local node {target.node_name}[in:{target.target_input_channel}]")
+            
+        except Exception as e:
+            self.logger.error(f"Error sending TCP packet to local node {target.node_name}: {e}")
+            # 重置连接以便下次重试
+            with self._socket_lock:
+                if self._tcp_socket:
+                    self._tcp_socket.close()
+                    self._tcp_socket = None
+            raise
+    
+    def _send_to_ray_actor(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        向其他Ray Actor发送远程调用
+        
+        Args:
+            target: 目标Ray Actor
+            data: 数据
+        """
+        try:
+            if isinstance(target.target_object, ActorHandle):
+                # 直接调用Ray Actor的remote方法
+                target.target_object.receive.remote(target.target_input_channel, data)
+                self.logger.debug(f"Sent remote call to Ray actor {target.node_name}[in:{target.target_input_channel}]")
+            else:
+                raise TypeError(f"Expected ActorHandle for Ray actor, got {type(target.target_object)}")
+                
+        except Exception as e:
+            self.logger.error(f"Error sending remote call to Ray actor {target.node_name}: {e}")
+            raise
+    
+    def close(self):
+        """关闭TCP连接"""
+        with self._socket_lock:
+            if self._tcp_socket:
+                self._tcp_socket.close()
+                self._tcp_socket = None
+                self.logger.info("Closed TCP connection to local server")
diff --git a/sage/core/runtime/base_runtime.py b/sage/core/runtime/base_runtime.py
index d1858a2..69e261e 100644
--- a/sage/core/runtime/base_runtime.py
+++ b/sage/core/runtime/base_runtime.py
@@ -3,18 +3,18 @@ from abc import ABC, abstractmethod
 class BaseRuntime(ABC):
     """执行后端抽象接口"""
     
-    @abstractmethod
-    def submit_task(self, task) -> str:
-        """提交任务执行，返回任务句柄"""
-        pass
+    # @abstractmethod
+    # def submit_task(self, task) -> str:
+    #     """提交任务执行，返回任务句柄"""
+    #     pass
     
-    @abstractmethod
-    def stop_task(self, task_handle: str):
-        """停止指定任务"""
-        pass
+    # @abstractmethod
+    # def stop_task(self, task_handle: str):
+    #     """停止指定任务"""
+    #     pass
     
-    @abstractmethod
-    def get_status(self, task_handle: str):
-        """获取任务状态"""
-        pass
+    # @abstractmethod
+    # def get_status(self, task_handle: str):
+    #     """获取任务状态"""
+    #     pass
 
diff --git a/sage/core/runtime/local/local_runtime.py b/sage/core/runtime/local/local_runtime.py
index 1be58a0..986345b 100644
--- a/sage/core/runtime/local/local_runtime.py
+++ b/sage/core/runtime/local/local_runtime.py
@@ -2,24 +2,41 @@ from sage.core.runtime import BaseRuntime
 from sage.core.runtime.local.local_scheduling_strategy import SchedulingStrategy, ResourceAwareStrategy, PriorityStrategy
 from sage.core.runtime.local.local_task import StreamingTask, OneshotTask, BaseTask
 from sage.core.runtime.local.local_slot import Slot
-from sage.core.dag.local.dag import DAG
+from sage.core.dag.local.local_dag_node import LocalDAGNode
 from sage.utils.custom_logger import CustomLogger
-import logging
+import threading
+import socket
+import pickle
+import time
+from typing import Dict, Optional, Any, List
 
 class LocalRuntime(BaseRuntime):
     """本地线程池执行后端"""
     
-    def __init__(self, max_slots=4, scheduling_strategy=None, session_folder:str = None):
-        self.session_folder = session_folder
+    _instance = None
+    _lock = threading.Lock()
+
+
+    def __init__(self, max_slots=4, scheduling_strategy=None,  tcp_host: str = "localhost", tcp_port: int = 9999):
+        # 确保只初始化一次
+        if hasattr(self, "_initialized"):
+            return
+        self._initialized = True
         self.name = "LocalRuntime"
         self.available_slots = [Slot(slot_id=i) for i in range(max_slots)]
-        self.task_to_slot = {}
-        self.task_to_handle = {}  # task -> handle映射
-        self.handle_to_task = {}  # handle -> task映射
+        self.tcp_host = tcp_host  # 添加这行
+        self.tcp_port = tcp_port  # 添加这行
+        # 节点管理
+        self.running_nodes: Dict[str, LocalDAGNode] = {}  # 正在运行的节点表
+        self.node_to_slot: Dict[LocalDAGNode, int] = {}  # 节点到slot的映射
+        self.node_to_handle: Dict[LocalDAGNode, str] = {}  # 节点到handle的映射
+        self.handle_to_node: Dict[str, LocalDAGNode] = {}  # handle到节点的映射
         self.next_handle_id = 0
+
+
+
         self.logger = CustomLogger(
             object_name=f"LocalRuntime",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
@@ -29,74 +46,336 @@ class LocalRuntime(BaseRuntime):
             self.scheduling_strategy = ResourceAwareStrategy()
         else:
             self.scheduling_strategy = scheduling_strategy
+            # 启动TCP服务器
+        self._start_tcp_server()
+
+    def __new__(cls, max_slots=4, scheduling_strategy=None, session_folder: str = None,  
+                tcp_host: str = "localhost", tcp_port: int = 9999):
+        # 禁止直接实例化
+        raise RuntimeError("请通过 get_instance() 方法获取实例")
+    
+    @classmethod
+    def get_instance(cls, max_slots=4, scheduling_strategy=None, 
+                     tcp_host: str = "localhost", tcp_port: int = 9999):
+        """获取LocalRuntime的唯一实例"""
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # 绕过 __new__ 的异常，直接创建实例
+                    instance = super().__new__(cls)
+                    instance.__init__(max_slots, scheduling_strategy, tcp_host, tcp_port)
+                    cls._instance = instance
+        return cls._instance
+    
+    @classmethod
+    def reset_instance(cls):
+        """重置实例（主要用于测试）"""
+        with cls._lock:
+            if cls._instance:
+                cls._instance.shutdown()
+                cls._instance = None
+
+
+
+
+    def _start_tcp_server(self):
+        """启动TCP服务器用于接收Ray Actor的数据"""
+        try:
+            self.tcp_server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            self.tcp_server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+            self.tcp_server_socket.bind((self.tcp_host, self.tcp_port))
+            self.tcp_server_socket.listen(10)
+            
+            self.tcp_running = True
+            self.tcp_server_thread = threading.Thread(
+                target=self._tcp_server_loop,
+                name="TCPServerThread"
+            )
+            self.tcp_server_thread.daemon = True
+            self.tcp_server_thread.start()
+            
+            self.logger.info(f"TCP server started on {self.tcp_host}:{self.tcp_port}")
+            
+        except Exception as e:
+            self.logger.error(f"Failed to start TCP server: {e}")
+            raise
+    def _tcp_server_loop(self):
+        """TCP服务器主循环"""
+        self.logger.debug("TCP server loop started")
+        
+        while self.tcp_running:
+            try:
+                client_socket, address = self.tcp_server_socket.accept()
+                self.logger.debug(f"New TCP client connected from {address}")
+                
+                # 在新线程中处理客户端
+                client_thread = threading.Thread(
+                    target=self._handle_tcp_client,
+                    args=(client_socket, address),
+                    name=f"TCPClient-{address}"
+                )
+                client_thread.daemon = True
+                client_thread.start()
+                
+            except Exception as e:
+                if self.tcp_running:
+                    self.logger.error(f"Error accepting TCP connection: {e}")
+        
+        self.logger.debug("TCP server loop stopped")
+
+    def _handle_tcp_client(self, client_socket: socket.socket, address):
+        """处理TCP客户端连接和消息"""
+        try:
+            while self.tcp_running:
+                # 读取消息长度
+                size_data = client_socket.recv(4)
+                if not size_data:
+                    break
+                
+                message_size = int.from_bytes(size_data, byteorder='big')
+                
+                # 读取消息内容
+                message_data = b''
+                while len(message_data) < message_size:
+                    chunk = client_socket.recv(message_size - len(message_data))
+                    if not chunk:
+                        break
+                    message_data += chunk
+                
+                if len(message_data) != message_size:
+                    self.logger.warning(f"Incomplete message received from {address}")
+                    continue
+                
+                # 反序列化并处理消息
+                try:
+                    message = pickle.loads(message_data)
+                    self._process_tcp_message(message, address)
+                except Exception as e:
+                    self.logger.error(f"Error processing message from {address}: {e}")
+                
+        except Exception as e:
+            self.logger.error(f"Error handling TCP client {address}: {e}")
+        finally:
+            client_socket.close()
+            self.logger.debug(f"TCP client {address} disconnected")
+    
+    def _process_tcp_message(self, message: Dict[str, Any], client_address):
+        """
+        处理来自Ray Actor的TCP消息
+        
+        Args:
+            message: 包含消息内容的字典
+            client_address: 客户端地址
+        """
+        try:
+            message_type = message.get("type")
+            
+            if message_type == "ray_to_local":
+                # Ray Actor发送给本地节点的数据
+                target_node_name = message["target_node"]
+                target_channel = message["target_channel"]
+                data = message["data"]
+                source_actor = message.get("source_actor", "unknown")
+                
+                # 查找目标节点
+                if target_node_name in self.running_nodes:
+                    target_node = self.running_nodes[target_node_name]
+                    
+                    # 将数据放入目标节点的输入缓冲区
+                    data_packet = (target_channel, data)
+                    target_node.put(data_packet)
+                    
+                    self.logger.debug(f"Delivered TCP message: {source_actor} -> "
+                                    f"{target_node_name}[in:{target_channel}]")
+                else:
+                    self.logger.warning(f"Target node '{target_node_name}' not found for TCP message from {client_address}")
+            else:
+                self.logger.warning(f"Unknown TCP message type: {message_type}")
+                
+        except Exception as e:
+            self.logger.error(f"Error processing TCP message: {e}", exc_info=True)
     
-    def submit_task(self, local_dag:DAG):
+    def submit_node(self, node: LocalDAGNode) -> str:
         """
-        提交到本地后端执行
+        提交单个MultiplexerDagNode到本地运行时
         
         Args:
-            local_dag: local_dag 实例
+            node: MultiplexerDagNode实例
             
         Returns:
             str: 任务句柄
         """
-
-        if not isinstance(local_dag, DAG):
-            raise TypeError("local_dag must be an instance of DAG")
+        if not isinstance(node, LocalDAGNode):
+            raise TypeError("Expected LocalDAGNode instance")
+        
+        self.logger.info(f"Submitting node '{node.name}' to {self.name}")
         
-        self.logger.info(f"Submitting DAG '{local_dag.name}' to {self.name}")
-
-        print(local_dag.strategy)
         try:
-            if local_dag.strategy == "oneshot":
-                task = OneshotTask(local_dag, session_folder=self.session_folder)
-                task.execute()
-                # self.logger.debug(f"OneshotTask submitted to {self.name} with handle: {task_handle}")
-            elif local_dag.strategy == "streaming":
-                for node in local_dag.nodes:
-                    task = StreamingTask(node, local_dag.working_config, session_folder=self.session_folder)
-                    task_handle = self.submit_node(task)
-                    #self.task_handles[dag_id].append(task_handle)
-                    self.logger.debug(f"DAGNode {node.name} submitted to {self.name} with handle: {task_handle}")
+            # 创建StreamingTask包装节点
+            task = StreamingTask(node, {})
+            
+            # 选择slot并提交
+            slot_id = self.scheduling_strategy.select_slot(task, self.available_slots)
+            success = self.available_slots[slot_id].submit_task(task)
+            
+            if success:
+                # 生成handle
+                handle = f"local_node_{self.next_handle_id}"
+                self.next_handle_id += 1
+                
+                # 更新映射关系
+                self.running_nodes[node.name] = node
+                self.node_to_slot[node] = slot_id
+                self.node_to_handle[node] = handle
+                self.handle_to_node[handle] = node
+                
+                self.logger.info(f"Node '{node.name}' submitted successfully with handle: {handle}")
+                return handle
             else:
-                raise ValueError(f"Unsupported strategy: {local_dag.strategy}")
+                raise RuntimeError(f"Failed to submit node '{node.name}' to slot {slot_id}")
+                
         except Exception as e:
-            self.logger.error(f"Failed to submit DAG '{local_dag.name}' to {self.name}: {e}")
-            raise RuntimeError(f"Failed to submit DAG '{local_dag.name}' to {self.name}: {e}")
+            self.logger.error(f"Failed to submit node '{node.name}': {e}")
+            raise
+    
+    def submit_nodes(self, nodes: List[LocalDAGNode]) -> List[str]:
+        """
+        批量提交多个节点
+        
+        Args:
+            nodes: MultiplexerDagNode列表
+            
+        Returns:
+            List[str]: 任务句柄列表
+        """
+        handles = []
+        for node in nodes:
+            try:
+                handle = self.submit_node(node)
+                handles.append(handle)
+            except Exception as e:
+                self.logger.error(f"Failed to submit node '{node.name}': {e}")
+                # 停止已经提交的节点
+                for h in handles:
+                    self.stop_node(h)
+                raise
+        
+        self.logger.info(f"Successfully submitted {len(handles)} nodes")
+        return handles
 
 
-    def submit_node(self, task: BaseTask) -> str:
-        """提交任务到本地线程池"""
-        slot_id = self.scheduling_strategy.select_slot(task, self.available_slots)
-        success = self.available_slots[slot_id].submit_task(task)
-        
-        if success:
-            handle = f"local_task_{self.next_handle_id}"
-            self.next_handle_id += 1
-            
-            self.task_to_slot[task] = slot_id
-            self.task_to_handle[task] = handle
-            self.handle_to_task[handle] = task
-            return handle
-        else:
-            raise RuntimeError(f"Failed to submit task to slot {slot_id}")
-    
-    def stop_task(self, task_handle: str):
-        """停止本地任务"""
-        if task_handle not in self.handle_to_task:
+    def stop_node(self, node_handle: str):
+        """
+        停止指定的节点
+        
+        Args:
+            node_handle: 节点句柄
+        """
+        if node_handle not in self.handle_to_node:
+            self.logger.warning(f"Node handle '{node_handle}' not found")
             return
+        
+        try:
+            node = self.handle_to_node[node_handle]
+            slot_id = self.node_to_slot[node]
+            
+            # 停止节点
+            node.stop()
             
-        task = self.handle_to_task[task_handle]
-        slot_id = self.task_to_slot[task]
-        self.available_slots[slot_id].stop(task)
-        
-        # 清理映射关系
-        self.task_to_slot.pop(task, None)
-        self.task_to_handle.pop(task, None)
-        self.handle_to_task.pop(task_handle, None)
+            # 从slot中移除任务
+            # 这里需要找到对应的task
+            for task in self.available_slots[slot_id].running_tasks:
+                if hasattr(task, 'node') and task.node == node:
+                    self.available_slots[slot_id].stop(task)
+                    break
+            
+            # 清理映射关系
+            self.running_nodes.pop(node.name, None)
+            self.node_to_slot.pop(node, None)
+            self.node_to_handle.pop(node, None)
+            self.handle_to_node.pop(node_handle, None)
+            
+            self.logger.info(f"Node '{node.name}' stopped successfully")
+            
+        except Exception as e:
+            self.logger.error(f"Error stopping node with handle '{node_handle}': {e}")
+    
+    def stop_all_nodes(self):
+        """停止所有运行中的节点"""
+        self.logger.info("Stopping all nodes...")
+        
+        handles_to_stop = list(self.handle_to_node.keys())
+        for handle in handles_to_stop:
+            self.stop_node(handle)
+        
+        self.logger.info(f"Stopped {len(handles_to_stop)} nodes")
     
-    def get_status(self, task_handle: str):
-        """获取本地任务状态"""
-        if task_handle not in self.handle_to_task:
+    def get_node_status(self, node_handle: str) -> Dict[str, Any]:
+        """
+        获取节点状态
+        
+        Args:
+            node_handle: 节点句柄
+            
+        Returns:
+            Dict: 节点状态信息
+        """
+        if node_handle not in self.handle_to_node:
             return {"status": "not_found"}
-        return {"status": "running", "backend": "local"}
+        
+        node = self.handle_to_node[node_handle]
+        slot_id = self.node_to_slot[node]
+        
+        return {
+            "status": "running",
+            "node_name": node.name,
+            "is_spout": node.is_spout,
+            "slot_id": slot_id,
+            "backend": "local",
+            "handle": node_handle
+        }
+    
+    def get_running_nodes(self) -> List[str]:
+        """获取所有运行中的节点名称"""
+        return list(self.running_nodes.keys())
+    
+    def get_node_by_name(self, node_name: str) -> Optional[LocalDAGNode]:
+        """根据名称获取节点"""
+        return self.running_nodes.get(node_name)
+    
+    def get_runtime_info(self) -> Dict[str, Any]:
+        """获取运行时信息"""
+        return {
+            "name": self.name,
+            "tcp_server": f"{self.tcp_host}:{self.tcp_port}",
+            "running_nodes_count": len(self.running_nodes),
+            "running_nodes": list(self.running_nodes.keys()),
+            "available_slots": len(self.available_slots),
+            "used_slots": len(self.node_to_slot),
+            "tcp_running": self.tcp_running
+        }
+    
+    def shutdown(self):
+        """关闭运行时和所有资源"""
+        self.logger.info("Shutting down LocalRuntime...")
+        
+        # 停止所有节点
+        self.stop_all_nodes()
+        
+        # 关闭TCP服务器
+        self.tcp_running = False
+        if self.tcp_server_socket:
+            self.tcp_server_socket.close()
+        
+        # 等待TCP服务器线程结束
+        if self.tcp_server_thread and self.tcp_server_thread.is_alive():
+            self.tcp_server_thread.join(timeout=2.0)
+        
+        self.logger.info("LocalRuntime shutdown completed")
+    
+    def __del__(self):
+        """析构函数，确保资源清理"""
+        try:
+            self.shutdown()
+        except:
+            pass
\ No newline at end of file
diff --git a/sage/core/runtime/local/local_task.py b/sage/core/runtime/local/local_task.py
index ffdcc3a..09bca3b 100644
--- a/sage/core/runtime/local/local_task.py
+++ b/sage/core/runtime/local/local_task.py
@@ -1,7 +1,7 @@
 import logging
 from re import M
 from sage.core.dag.local.dag_node import BaseDAGNode,OneShotDAGNode
-from sage.core.dag.local.multi_dag_node import MultiplexerDagNode
+from sage.core.dag.local.local_dag_node import LocalDAGNode
 from sage.core.dag.local.dag import DAG
 import threading
 from sage.utils.custom_logger import CustomLogger
@@ -23,7 +23,7 @@ class StreamingTask(BaseTask):
                TypeError: 当节点类型不匹配时抛出
                RuntimeError: 执行过程中出现错误时抛出
     """
-    def __init__(self,node,working_config=None, session_folder: str = None):
+    def __init__(self,node,working_config=None):
         super().__init__()
         self.long_running=True
         self.node=node
@@ -32,7 +32,6 @@ class StreamingTask(BaseTask):
         self.working_config=working_config or {}
         self.logger = CustomLogger(
             object_name=f"StreamingTask_{self.name}",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
@@ -41,10 +40,10 @@ class StreamingTask(BaseTask):
     def execute(self):
         #循环的执行算子
         try:
-            if  isinstance(self.node,MultiplexerDagNode):
+            if  isinstance(self.node,LocalDAGNode):
                 self.node.run_loop()
             else :
-                raise TypeError(f"node{self.node.name} is not a MultiplexerDagNode")
+                raise TypeError(f"node{self.node.name} is not a LocalDAGNode")
         except Exception as e:
             self.logger.error(e)
             raise TypeError(e)
diff --git a/sage/core/runtime/ray/ray_runtime.py b/sage/core/runtime/ray/ray_runtime.py
index 6ca80d0..ce46cd1 100644
--- a/sage/core/runtime/ray/ray_runtime.py
+++ b/sage/core/runtime/ray/ray_runtime.py
@@ -1,45 +1,422 @@
-from typing import Dict, List, Optional, Any
-# from sage.archive.raydag_task import RayDAGTask
+from typing import Dict, List, Optional, Any, Union
 from sage.core.dag.ray.ray_dag import RayDAG
-import logging, ray, time
+from sage.core.dag.ray.ray_dag_node import RayDAGNode
+import logging, ray, time, threading 
 from sage.core.runtime.base_runtime import BaseRuntime
 from sage.utils.custom_logger import CustomLogger
+from ray.actor import ActorHandle
 
+class RayRuntime(BaseRuntime):
+    """
+    Ray 执行后端，支持单个Ray节点和完整Ray DAG的管理
+    """
+    _instance = None
+    _lock = threading.Lock()
 
 
-class RayRuntime(BaseRuntime):
-    """Ray DAG 专用执行后端"""
-    
-    def __init__(self, monitoring_interval: float = 1.0, session_folder: str = None):
+    def __init__(self, monitoring_interval: float = 1.0):
         """
-        Initialize Ray DAG execution backend.
+        Initialize Ray execution backend.
         
         Args:
-            monitoring_interval: Interval in seconds for monitoring DAG status
+            monitoring_interval: Interval in seconds for monitoring status
+            session_folder: Session folder for logging
         """
         # 确保Ray已初始化
         if not ray.is_initialized():
             ray.init(temp_dir="./ray_tmp")
+            
         self.name = "RayRuntime"
+        self.session_folder = CustomLogger.get_session_folder()
+        
+        # Ray DAG 管理（保留向后兼容）
         self.running_dags: Dict[str, RayDAG] = {}  # handle -> RayDAG映射
         self.dag_spout_futures: Dict[str, List[ray.ObjectRef]] = {}  # handle -> spout futures
         self.dag_metadata: Dict[str, Dict[str, Any]] = {}  # handle -> metadata
+        
+        # Ray节点管理
+        self.running_nodes: Dict[str, ActorHandle] = {}  # node_name -> ActorHandle
+        self.node_metadata: Dict[str, Dict[str, Any]] = {}  # node_name -> metadata
+        self.node_handles: Dict[str, str] = {}  # handle -> node_name
+        self.handle_to_node: Dict[str, str] = {}  # handle -> node_name
+        self.node_spout_futures: Dict[str, ray.ObjectRef] = {}  # node_name -> spout future
+        
         self.next_handle_id = 0
         self.monitoring_interval = monitoring_interval
+        
         self.logger = CustomLogger(
             object_name=f"RayRuntime",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
         )
+    def __new__(cls, monitoring_interval: float = 1.0):
+        # 禁止直接实例化
+        raise RuntimeError("请通过 get_instance() 方法获取实例")
+    
+    @classmethod
+    def get_instance(cls, monitoring_interval: float = 1.0):
+        """获取RayRuntime的唯一实例"""
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # 绕过 __new__ 的异常，直接创建实例
+                    instance = super().__new__(cls)
+                    instance.__init__(monitoring_interval)
+                    cls._instance = instance
+        return cls._instance
+    
+    @classmethod
+    def reset_instance(cls):
+        """重置实例（主要用于测试）"""
+        with cls._lock:
+            if cls._instance:
+                cls._instance.shutdown()
+                cls._instance = None
+    # ==================== Ray节点管理接口 ====================
     
-    def submit_task(self, ray_dag:RayDAG) -> str:
+    def submit_node(self, node_class, node_name: str, operator_class, 
+                   operator_config: Dict = None, is_spout: bool = False) -> str:
         """
-        提交 Ray DAG 执行任务
+        提交单个Ray节点到运行时
         
         Args:
-            ray_dag: 应该是 RayDAG 实例或包含 RayDAG 的任务包装器
+            node_class: Ray节点类（通常是RayMultiplexerDagNode）
+            node_name: 节点名称
+            operator_class: 操作符类
+            operator_config: 操作符配置
+            is_spout: 是否为spout节点
+            
+        Returns:
+            str: 节点句柄
+        """
+        if node_name in self.running_nodes:
+            raise ValueError(f"Node '{node_name}' already exists")
+        
+        self.logger.info(f"Submitting Ray node '{node_name}' to {self.name}")
+        
+        try:
+            # 创建Ray Actor
+            actor_handle = node_class.remote(
+                name=node_name,
+                function_class=operator_class,
+                operator_config=operator_config or {},
+                is_spout=is_spout,
+                session_folder=self.session_folder
+            )
+            
+            # 生成handle
+            handle = f"ray_node_{self.next_handle_id}"
+            self.next_handle_id += 1
+            
+            # 存储节点信息
+            self.running_nodes[node_name] = actor_handle
+            self.node_metadata[node_name] = {
+                'is_spout': is_spout,
+                'operator_class': operator_class.__name__,
+                'start_time': time.time(),
+                'status': 'created'
+            }
+            self.node_handles[handle] = node_name
+            self.handle_to_node[handle] = node_name
+            
+            self.logger.info(f"Ray node '{node_name}' created successfully with handle: {handle}")
+            return handle
+            
+        except Exception as e:
+            self.logger.error(f"Failed to create Ray node '{node_name}': {e}")
+            raise
+    
+    def submit_actor_instance(self, node_instance: ActorHandle, node_name: str) -> str:
+        """
+        提交已创建的Ray节点实例
+        
+        Args:
+            node_instance: Ray Actor实例
+            node_name: 节点名称
+            
+        Returns:
+            str: 节点句柄
+        """
+        if node_name in self.running_nodes:
+            raise ValueError(f"Node '{node_name}' already exists")
+        
+        # 生成handle
+        handle = f"ray_node_{self.next_handle_id}"
+        self.next_handle_id += 1
+        
+        # 存储节点信息
+        self.running_nodes[node_name] = node_instance
+        self.node_metadata[node_name] = {
+            'start_time': time.time(),
+            'status': 'submitted'
+        }
+        self.node_handles[handle] = node_name
+        self.handle_to_node[handle] = node_name
+        
+        self.logger.info(f"Ray node instance '{node_name}' submitted with handle: {handle}")
+        return handle
+    
+    def submit_actors(self, nodes: List[ActorHandle], node_names: List[str]) -> List[str]:
+        """
+        批量提交Ray节点实例
+        
+        Args:
+            nodes: Ray Actor实例列表
+            node_names: 节点名称列表
+            
+        Returns:
+            List[str]: 节点句柄列表
+        """
+        if len(nodes) != len(node_names):
+            raise ValueError("nodes and node_names must have the same length")
+        
+        handles = []
+        for actor_instance, node_name in zip(nodes, node_names):
+            try:
+                handle = self.submit_actor_instance(actor_instance, node_name)
+                handles.append(handle)
+                self.start_node(handle)  # 自动启动节点
+            except Exception as e:
+                self.logger.error(f"Failed to submit node '{node_name}': {e}")
+                # 停止已经提交的节点
+                for h in handles:
+                    self.stop_node(h)
+                raise
+        
+        self.logger.info(f"Successfully submitted {len(handles)} Ray nodes")
+        return handles
+    
+    def start_node(self, node_handle: str):
+        """
+        启动Ray节点
+        
+        Args:
+            node_handle: 节点句柄
+        """
+        if node_handle not in self.handle_to_node:
+            raise ValueError(f"Node handle '{node_handle}' not found")
+        
+        node_name = self.handle_to_node[node_handle]
+        actor_handle = self.running_nodes[node_name]
+        
+        try:
+            # 检查是否为spout节点
+            is_spout = self.node_metadata[node_name].get('is_spout', False)
+            
+            if is_spout:
+                # Spout节点启动数据生成循环
+                future = actor_handle.start.remote()
+                self.node_spout_futures[node_name] = future
+                self.logger.info(f"Started Ray spout node '{node_name}'")
+            else:
+                # 非Spout节点启动就绪状态
+                future = actor_handle.start.remote()
+                self.logger.info(f"Started Ray node '{node_name}' in ready state")
+            
+            # 更新状态
+            self.node_metadata[node_name]['status'] = 'running'
+            
+        except Exception as e:
+            self.logger.error(f"Failed to start Ray node '{node_name}': {e}")
+            raise
+    
+    def start_all_nodes(self):
+        """启动所有节点"""
+        self.logger.info("Starting all Ray nodes...")
+        
+        spout_count = 0
+        ready_count = 0
+        
+        for handle in self.handle_to_node.keys():
+            try:
+                node_name = self.handle_to_node[handle]
+                is_spout = self.node_metadata[node_name].get('is_spout', False)
+                
+                self.start_node(handle)
+                
+                if is_spout:
+                    spout_count += 1
+                else:
+                    ready_count += 1
+                    
+            except Exception as e:
+                self.logger.error(f"Failed to start node with handle {handle}: {e}")
+        
+        self.logger.info(f"Started {spout_count} spout nodes and {ready_count} ready nodes")
+    
+    def stop_node(self, node_handle: str):
+        """
+        停止Ray节点
+        
+        Args:
+            node_handle: 节点句柄
+        """
+        if node_handle not in self.handle_to_node:
+            self.logger.warning(f"Node handle '{node_handle}' not found")
+            return
+        
+        node_name = self.handle_to_node[node_handle]
+        actor_handle = self.running_nodes[node_name]
+        
+        try:
+            # 停止Actor
+            actor_handle.stop.remote()
+            
+            # 清理spout future
+            self.node_spout_futures.pop(node_name, None)
+            
+            # 更新状态
+            self.node_metadata[node_name]['status'] = 'stopped'
+            self.node_metadata[node_name]['end_time'] = time.time()
+            
+            self.logger.info(f"Ray node '{node_name}' stopped")
+            
+        except Exception as e:
+            self.logger.error(f"Error stopping Ray node '{node_name}': {e}")
+    
+    def stop_all_nodes(self):
+        """停止所有节点"""
+        self.logger.info("Stopping all Ray nodes...")
+        
+        handles_to_stop = list(self.handle_to_node.keys())
+        for handle in handles_to_stop:
+            self.stop_node(handle)
+        
+        self.logger.info(f"Stopped {len(handles_to_stop)} Ray nodes")
+    
+    def remove_node(self, node_handle: str):
+        """
+        移除Ray节点并清理资源
+        
+        Args:
+            node_handle: 节点句柄
+        """
+        if node_handle not in self.handle_to_node:
+            self.logger.warning(f"Node handle '{node_handle}' not found")
+            return
+        
+        node_name = self.handle_to_node[node_handle]
+        actor_handle = self.running_nodes[node_name]
+        
+        try:
+            # 停止并杀死Actor
+            self.stop_node(node_handle)
+            ray.kill(actor_handle)
+            
+            # 清理映射关系
+            self.running_nodes.pop(node_name, None)
+            self.node_metadata.pop(node_name, None)
+            self.node_handles.pop(node_handle, None)
+            self.handle_to_node.pop(node_handle, None)
+            self.node_spout_futures.pop(node_name, None)
+            
+            self.logger.info(f"Ray node '{node_name}' removed")
+            
+        except Exception as e:
+            self.logger.error(f"Error removing Ray node '{node_name}': {e}")
+    
+    def get_node_status(self, node_handle: str) -> Dict[str, Any]:
+        """
+        获取Ray节点状态
+        
+        Args:
+            node_handle: 节点句柄
+            
+        Returns:
+            Dict: 节点状态信息
+        """
+        if node_handle not in self.handle_to_node:
+            return {"status": "not_found"}
+        
+        node_name = self.handle_to_node[node_handle]
+        actor_handle = self.running_nodes[node_name]
+        metadata = self.node_metadata[node_name]
+        
+        try:
+            # 非阻塞健康检查
+            health_future = actor_handle.health_check.remote()
+            ready, not_ready = ray.wait([health_future], timeout=0.1)
+            
+            if ready:
+                health_info = ray.get(ready[0])
+            else:
+                health_info = {"status": "busy"}
+                
+        except Exception as e:
+            health_info = {"status": "error", "error": str(e)}
+        
+        status_info = {
+            "handle": node_handle,
+            "node_name": node_name,
+            "backend": "ray_node",
+            "status": metadata.get('status', 'unknown'),
+            "is_spout": metadata.get('is_spout', False),
+            "operator_class": metadata.get('operator_class', 'unknown'),
+            "start_time": metadata.get('start_time'),
+            "health": health_info
+        }
+        
+        # 计算运行时间
+        if 'start_time' in metadata:
+            if 'end_time' in metadata:
+                status_info['duration'] = metadata['end_time'] - metadata['start_time']
+            else:
+                status_info['duration'] = time.time() - metadata['start_time']
+        
+        return status_info
+    
+    def get_running_nodes(self) -> List[str]:
+        """获取所有运行中的节点名称"""
+        return list(self.running_nodes.keys())
+    
+    def get_node_by_name(self, node_name: str) -> Optional[ActorHandle]:
+        """根据名称获取节点"""
+        return self.running_nodes.get(node_name)
+    
+    def wait_for_node_completion(self, node_handle: str, timeout: Optional[float] = None) -> bool:
+        """
+        等待spout节点完成执行
+        
+        Args:
+            node_handle: 节点句柄
+            timeout: 超时时间（秒），None表示无限等待
+            
+        Returns:
+            是否成功完成
+        """
+        if node_handle not in self.handle_to_node:
+            return False
+        
+        node_name = self.handle_to_node[node_handle]
+        
+        if node_name not in self.node_spout_futures:
+            return True  # 不是spout节点或没有运行的future
+        
+        spout_future = self.node_spout_futures[node_name]
+        
+        try:
+            ray.get(spout_future, timeout=timeout)
+            self.logger.info(f"Ray node '{node_name}' completed successfully")
+            
+            # 更新状态
+            self.node_metadata[node_name]['status'] = 'completed'
+            self.node_metadata[node_name]['end_time'] = time.time()
+            
+            return True
+            
+        except Exception as e:
+            self.logger.error(f"Ray node '{node_name}' failed or timed out: {e}")
+            return False
+    
+    # ==================== Ray DAG管理接口（向后兼容） ====================
+    
+    def submit_task(self, ray_dag: RayDAG) -> str:
+        """
+        提交 Ray DAG 执行任务（向后兼容接口）
+        
+        Args:
+            ray_dag: RayDAG 实例
         """
         if not isinstance(ray_dag, RayDAG):
             raise TypeError("Task must be RayDAG instance")
@@ -53,21 +430,13 @@ class RayRuntime(BaseRuntime):
         self.logger.info(f"Submitting Ray DAG {ray_dag.name} with handle {handle}")
         
         try:
-            self.logger.info(f"ray_dag.strategy is {ray_dag.strategy}")
-
             # 启动 DAG 执行
-            if ray_dag.strategy == "streaming":
-                spout_futures = self._start_streaming_dag(ray_dag)
-            elif ray_dag.strategy == "oneshot":
-                spout_futures = self._start_oneshot_dag(ray_dag)
-            else:
-                raise ValueError(f"Unsupported DAG strategy: {ray_dag.strategy}")
+            spout_futures = self._start_dag(ray_dag)
             
             # 存储DAG信息
             self.running_dags[handle] = ray_dag
             self.dag_spout_futures[handle] = spout_futures
             self.dag_metadata[handle] = {
-                'strategy': ray_dag.strategy,
                 'actor_count': ray_dag.get_actor_count(),
                 'start_time': time.time(),
                 'status': 'running'
@@ -80,39 +449,19 @@ class RayRuntime(BaseRuntime):
             self.logger.error(f"Failed to start Ray DAG {ray_dag.name}: {e}", exc_info=True)
             raise
     
-    def _start_streaming_dag(self, ray_dag: RayDAG) -> List[ray.ObjectRef]:
-        """启动流式 DAG"""
-        spout_actors = ray_dag.get_spout_actors()
-        
-        if not spout_actors:
-            raise RuntimeError("No spout actors found in streaming DAG")
-        
-        spout_futures = []
-        for spout_actor in spout_actors:
-            try:
-                self.logger.debug(f"Started streaming spout actor in DAG {ray_dag.name}")
-                future = spout_actor.start_spout.remote()
-                spout_futures.append(future)
-            except Exception as e:
-                self.logger.error(f"Failed to start spout actor in DAG {ray_dag.name}: {e}")
-                raise
-        
-        return spout_futures
-    
-    def _start_oneshot_dag(self, ray_dag: RayDAG) -> List[ray.ObjectRef]:
-        """启动一次性 DAG"""
+    def _start_dag(self, ray_dag: RayDAG) -> List[ray.ObjectRef]:
+        """启动DAG中的所有spout actors"""
         spout_actors = ray_dag.get_spout_actors()
-        self.logger.info(f"Started oneshot task in DAG {ray_dag.name}")
         
         if not spout_actors:
-            raise RuntimeError("No spout actors found in oneshot DAG")
+            raise RuntimeError("No spout actors found in DAG")
         
         spout_futures = []
         for spout_actor in spout_actors:
             try:
-                future = spout_actor.start_spout.remote()
+                future = spout_actor.start.remote()
                 spout_futures.append(future)
-                self.logger.info(f"Started oneshot spout actor in DAG {ray_dag.name}")
+                self.logger.debug(f"Started spout actor in DAG {ray_dag.name}")
             except Exception as e:
                 self.logger.error(f"Failed to start spout actor in DAG {ray_dag.name}: {e}")
                 raise
@@ -120,7 +469,7 @@ class RayRuntime(BaseRuntime):
         return spout_futures
     
     def stop_task(self, task_handle: str):
-        """停止 Ray DAG 执行"""
+        """停止 Ray DAG 执行（向后兼容接口）"""
         if task_handle not in self.running_dags:
             self.logger.warning(f"DAG handle {task_handle} not found")
             return
@@ -183,7 +532,7 @@ class RayRuntime(BaseRuntime):
         self.dag_metadata.pop(task_handle, None)
     
     def get_status(self, task_handle: str) -> Dict[str, Any]:
-        """获取 Ray DAG 状态"""
+        """获取 Ray DAG 状态（向后兼容接口）"""
         if task_handle not in self.running_dags:
             return {"status": "not_found"}
         
@@ -204,7 +553,6 @@ class RayRuntime(BaseRuntime):
             "status": metadata.get('status', 'unknown'),
             "backend": "ray_dag",
             "dag_id": ray_dag.name,
-            "strategy": metadata.get('strategy', 'unknown'),
             "actor_count": metadata.get('actor_count', 0),
             "spout_count": len(spout_futures),
             "completed_spouts": completed_spouts,
@@ -242,16 +590,7 @@ class RayRuntime(BaseRuntime):
         return actor_health
     
     def wait_for_completion(self, task_handle: str, timeout: Optional[float] = None) -> bool:
-        """
-        等待 DAG 完成执行（主要用于 oneshot 策略）
-        
-        Args:
-            task_handle: DAG 句柄
-            timeout: 超时时间（秒），None表示无限等待
-            
-        Returns:
-            是否成功完成
-        """
+        """等待 DAG 完成执行（向后兼容接口）"""
         if task_handle not in self.running_dags:
             return False
         
@@ -274,22 +613,35 @@ class RayRuntime(BaseRuntime):
             self.logger.error(f"DAG {task_handle} failed or timed out: {e}")
             return False
     
-    def list_running_dags(self) -> List[str]:
-        """列出所有正在运行的 DAG 句柄"""
-        return list(self.running_dags.keys())
+    # ==================== 通用管理接口 ====================
     
-    def get_dag_info(self, task_handle: str) -> Optional[Dict[str, Any]]:
-        """获取 DAG 详细信息"""
-        if task_handle not in self.running_dags:
-            return None
-        
-        ray_dag = self.running_dags[task_handle]
+    def get_runtime_info(self) -> Dict[str, Any]:
+        """获取运行时信息"""
         return {
-            'handle': task_handle,
-            'dag_id': ray_dag.name,
-            'strategy': ray_dag.strategy,
-            'actor_count': ray_dag.get_actor_count(),
-            'connections': ray_dag.get_connections(),
-            'spout_actors': ray_dag.spout_actors,
-            'metadata': self.dag_metadata.get(task_handle, {})
+            "name": self.name,
+            "running_nodes_count": len(self.running_nodes),
+            "running_nodes": list(self.running_nodes.keys()),
+            "running_dags_count": len(self.running_dags),
+            "running_dags": list(self.running_dags.keys()),
+            "ray_initialized": ray.is_initialized()
         }
+    
+    def shutdown(self):
+        """关闭运行时和所有资源"""
+        self.logger.info("Shutting down RayRuntime...")
+        
+        # 停止所有节点
+        self.stop_all_nodes()
+        
+        # 停止所有DAG
+        for handle in list(self.running_dags.keys()):
+            self.stop_task(handle)
+        
+        self.logger.info("RayRuntime shutdown completed")
+    
+    def __del__(self):
+        """析构函数，确保资源清理"""
+        try:
+            self.shutdown()
+        except:
+            pass
\ No newline at end of file
diff --git a/sage/core/runtime/runtime_manager.py b/sage/core/runtime/runtime_manager.py
index 03eca79..9b878cb 100644
--- a/sage/core/runtime/runtime_manager.py
+++ b/sage/core/runtime/runtime_manager.py
@@ -1,4 +1,5 @@
 import logging
+import threading
 from typing import Dict, Any, Union
 from sage.core.runtime.base_runtime import BaseRuntime
 from sage.core.runtime.ray.ray_runtime import RayRuntime
@@ -12,9 +13,17 @@ class RuntimeManager:
     运行时管理器，负责管理不同平台的运行时实例
     """
     
+    _instance = None
+    _lock = threading.Lock()
+    
     def __init__(self, session_folder: str = None):
+        # 确保只初始化一次
+        if hasattr(self, "_initialized"):
+            return
+        self._initialized = True
+        
         self.backends: Dict[str, Any] = {}
-        self.session_folder = session_folder
+        self.session_folder = CustomLogger.get_session_folder()
         self.logger = CustomLogger(
             object_name=f"RuntimeManager",
             session_folder=session_folder,
@@ -23,6 +32,30 @@ class RuntimeManager:
             file_output=True
         )
     
+    def __new__(cls):
+        # 禁止直接实例化
+        raise RuntimeError("请通过 get_instance() 方法获取实例")
+    
+    @classmethod
+    def get_instance(cls):
+        """获取RuntimeManager的唯一实例"""
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # 绕过 __new__ 的异常，直接创建实例
+                    instance = super().__new__(cls)
+                    instance.__init__()
+                    cls._instance = instance
+        return cls._instance
+    
+    @classmethod
+    def reset_instance(cls):
+        """重置实例（主要用于测试）"""
+        with cls._lock:
+            if cls._instance:
+                cls._instance.shutdown_all()
+                cls._instance = None
+    
     def get(self, platform: str, **kwargs) -> BaseRuntime:
         """
         获取指定平台的运行时实例，支持延迟初始化
@@ -53,12 +86,19 @@ class RuntimeManager:
         """
         if platform == "ray":
             monitoring_interval = kwargs.get('monitoring_interval', 2.0)
-            return RayRuntime(monitoring_interval=monitoring_interval, session_folder=self.session_folder)
+            return RayRuntime.get_instance(monitoring_interval=monitoring_interval)
         
         elif platform == "local":
             max_slots = kwargs.get('max_slots', 4)
             scheduling_strategy = kwargs.get('scheduling_strategy', None)
-            return LocalRuntime(max_slots=max_slots, scheduling_strategy=scheduling_strategy)
+            tcp_host = kwargs.get('tcp_host', "localhost")
+            tcp_port = kwargs.get('tcp_port', 9999)
+            return LocalRuntime.get_instance(
+                max_slots=max_slots, 
+                scheduling_strategy=scheduling_strategy,
+                tcp_host=tcp_host,
+                tcp_port=tcp_port
+            )
         
         else:
             raise ValueError(f"Unknown platform: {platform}")
diff --git a/sage/tests/config/config_mixed.yaml b/sage/tests/config/config_mixed.yaml
new file mode 100644
index 0000000..b18912d
--- /dev/null
+++ b/sage/tests/config/config_mixed.yaml
@@ -0,0 +1,47 @@
+pipeline:
+  name: "sage-api-operator-operator_test"
+  description: "Test pipeline for Sage API Operator"
+  version: "1.0.0"
+  type: "ray"
+
+
+
+source:
+  # data_path: "sample/one_question.txt"
+  data_path: "sample/question.txt"
+  platform: "local"
+
+retriever:
+  platform: "ray"
+  ltm:
+    topk: 3
+
+reranker:
+  platform: "local"
+  model_name: "BAAI/bge-reranker-v2-m3"
+  top_k: 3
+
+refiner:
+  platform: "local"
+  method: "openai"
+  model_name: "qwen-turbo-0919"
+  base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
+  api_key: "sk-b21a67cf99d14ead9d1c5bf8c2eb90ef"
+  seed: 42
+
+generator:
+  platform: "local"
+  method: "openai"
+  model_name: "qwen-turbo-0919"
+  base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
+  api_key: "sk-b21a67cf99d14ead9d1c5bf8c2eb90ef"
+  seed: 42
+
+writer:
+
+promptor:
+  platform: "local"
+
+sink:
+  platform: "local"
+
diff --git a/sage/tests/runtime/qa_dense_retrieval_mixed.py b/sage/tests/runtime/qa_dense_retrieval_mixed.py
new file mode 100644
index 0000000..1d3680c
--- /dev/null
+++ b/sage/tests/runtime/qa_dense_retrieval_mixed.py
@@ -0,0 +1,58 @@
+import logging
+import time
+from sage.api.pipeline import Pipeline
+from sage.api.operator.operator_impl.promptor import QAPromptor
+from sage.api.operator.operator_impl.generator import OpenAIGenerator
+from sage.api.operator.operator_impl.retriever import DenseRetriever
+from sage.api.operator.operator_impl.source import FileSource
+from sage.api.operator.operator_impl.sink import FileSink,TerminalSink
+from sage.core.neuromem.memory_manager import MemoryManager
+from sage.utils.config_loader import load_config
+from sage.utils.logging_utils import configure_logging
+from sage.api.model.model_api import apply_embedding_model
+def memory_init():
+    """初始化内存管理器并创建测试集合"""
+    manager = MemoryManager()
+    embedding_model = apply_embedding_model("hf", model="sentence-transformers/all-MiniLM-L6-v2")
+    col = manager.create_collection(
+        name="vdb_test",
+        backend_type="VDB",
+        embedding_model=embedding_model,
+        dim=embedding_model.get_dim(),
+        description="test vdb collection",
+        as_ray_actor=True
+    )
+    col.add_metadata_field("owner")
+    col.add_metadata_field("show_type")
+    texts = [
+        ("hello world", {"owner": "ruicheng", "show_type": "text"}),
+        ("你好，世界", {"owner": "Jun", "show_type": "text"}),
+        ("こんにちは、世界", {"owner": "Lei", "show_type": "img"}),
+    ]
+    for text, metadata in texts:
+        col.insert(text, metadata)
+    col.create_index(index_name="vdb_index")
+    config["retriever"]["ltm_collection"] = col._collection
+
+
+def pipeline_run():
+    """创建并运行数据处理管道"""
+    pipeline = Pipeline(name="example_pipeline")
+    # 构建数据处理流程
+    query_stream = pipeline.add_source(FileSource, config["source"])
+    query_and_chunks_stream = query_stream.retrieve(DenseRetriever, config["retriever"])
+    prompt_stream = query_and_chunks_stream.construct_prompt(QAPromptor, config["promptor"])
+    response_stream = prompt_stream.generate_response(OpenAIGenerator, config["generator"])
+    response_stream.sink(TerminalSink, config["sink"])
+    # 提交管道并运行
+    pipeline.submit_mixed(config={"is_long_running":True})
+    time.sleep(100)  # 等待管道运行
+
+
+if __name__ == '__main__':
+    configure_logging(level=logging.INFO)
+    # 加载配置并初始化日志
+    config = load_config('./config_mixed.yaml')
+    # 初始化内存并运行管道
+    memory_init()
+    pipeline_run()
diff --git a/sage/utils/custom_logger.py b/sage/utils/custom_logger.py
index 5a50efe..24f17db 100644
--- a/sage/utils/custom_logger.py
+++ b/sage/utils/custom_logger.py
@@ -224,7 +224,10 @@ class CustomLogger:
             Path(session_folder).mkdir(parents=True, exist_ok=True)
     
     @classmethod
-    def get_default_session_folder(cls) -> Optional[str]:
+    def get_session_folder(cls) -> Optional[str]:
+        if cls._default_session_folder is None:
+            # 如果没有设置默认session文件夹，创建一个新的
+            cls._default_session_folder = cls.create_session_folder()
         """获取默认的session文件夹"""
         return cls._default_session_folder
     
diff --git a/.gitignore b/.gitignore
index 95ae40d..b2b757c 100644
--- a/.gitignore
+++ b/.gitignore
@@ -23,7 +23,6 @@ CMakeUserPresets.json
 /installation/candy/
 /sage.egg-info/
 /deps/CANDY/.github/
-/archive
 
 id_llm
 auto_env_setup.log
@@ -45,12 +44,11 @@ output.txt
 /.vscode/
 /arxiv_pdfs
 /arxiv_structured_json
-run.sh
 /sage/.env
 mem_output.txt
 qa_output.txt
-run.sh
 ray_logs
 /output/
 /logs
-/logs/**
\ No newline at end of file
+/logs/**
+
diff --git a/archive/collection_wrapper.py b/archive/collection_wrapper.py
deleted file mode 100644
index bdd3541..0000000
--- a/archive/collection_wrapper.py
+++ /dev/null
@@ -1,135 +0,0 @@
-import ray
-import asyncio
-import concurrent.futures
-from typing import Any
-
-
-class CollectionWrapper:
-    """透明的集合包装器，自动适配本地执行、Ray Actor、Ray Function等多种模式"""
-    def __init__(self, collection: Any):
-        # 使用 __dict__ 直接设置，避免触发 __setattr__
-        object.__setattr__(self, '_collection', collection)
-        object.__setattr__(self, '_execution_mode', self._detect_execution_mode())
-        object.__setattr__(self, '_method_cache', {})
-        object.__setattr__(self, '_attribute_cache', {})
-
-    def _detect_execution_mode(self) -> str:
-        """检测执行模式"""
-        try:
-            # 1. 检查是否是Ray Actor
-            import ray
-            if isinstance(self._collection, ray.actor.ActorHandle):
-                return "ray_actor"
-            # 2. 检查是否有远程调用方法
-            if hasattr(self._collection, 'remote') and callable(self._collection.remote):
-                return "ray_function"
-        except ImportError:
-            # 如果Ray不可用，忽略
-            pass
-
-        # 3. 默认作为本地对象处理
-        return "local"
-
-    # 属性访问代理
-    def __getattr__(self, name: str):
-        """透明代理属性访问"""
-        # 缓存查找
-        if name in self._attribute_cache:
-            return self._attribute_cache[name]
-
-        # 获取原始属性
-        try:
-            original_attr = getattr(self._collection, name)
-        except AttributeError:
-            raise AttributeError(f"'{type(self._collection).__name__}' object has no attribute '{name}'")
-
-        # 处理方法调用
-        if callable(original_attr):
-            # 创建统一调用方式
-            wrapped_method = self._create_unified_method(name, original_attr)
-            self._attribute_cache[name] = wrapped_method
-            return wrapped_method
-
-        # 普通属性
-        self._attribute_cache[name] = original_attr
-        return original_attr
-
-    # 属性设置代理
-    def __setattr__(self, name: str, value: Any):
-        """代理属性设置"""
-        if name.startswith('_'):
-            # 内部属性直接设置
-            object.__setattr__(self, name, value)
-        else:
-            # 外部属性设置到原集合
-            setattr(self._collection, name, value)
-
-    # 辅助方法
-    def __dir__(self):
-        """代理dir()调用"""
-        return dir(self._collection)
-
-    def __repr__(self):
-        """代理repr()"""
-        return f"CollectionWrapper({repr(self._collection)})"
-
-    def __str__(self):
-        """代理str()"""
-        return str(self._collection)
-
-    def _create_unified_method(self, method_name: str, original_method):
-        """创建统一的方法包装 - 对外始终提供同步接口"""
-        if self._execution_mode == "ray_actor":
-            # Ray Actor处理
-            def ray_actor_wrapper(*args, **kwargs):
-                try:
-                    # 获取远程方法引用
-                    remote_method = getattr(self._collection, method_name)
-                    # 执行远程调用
-                    future = remote_method.remote(*args, **kwargs)
-                    # 同步获取结果
-                    return ray.get(future)
-                except Exception as e:
-                    raise RuntimeError(f"Ray Actor method '{method_name}' failed: {str(e)}")
-
-            return ray_actor_wrapper
-
-        elif self._execution_mode == "ray_function":
-            # Ray函数处理
-            def ray_function_wrapper(*args, **kwargs):
-                try:
-                    # 执行远程调用
-                    future = original_method(*args, **kwargs)
-                    # 同步获取结果
-                    return ray.get(future)
-                except Exception as e:
-                    raise RuntimeError(f"Ray function '{method_name}' failed: {str(e)}")
-
-            return ray_function_wrapper
-
-        else:
-            # 本地方法处理
-            if asyncio.iscoroutinefunction(original_method):
-                # 异步方法转同步
-                def async_wrapper(*args, **kwargs):
-                    try:
-                        # 检查事件循环
-                        try:
-                            loop = asyncio.get_running_loop()
-                            # 在独立线程中执行异步任务
-                            with concurrent.futures.ThreadPoolExecutor() as executor:
-                                result = executor.submit(
-                                    asyncio.run,
-                                    original_method(*args, **kwargs)
-                                ).result()
-                            return result
-                        except RuntimeError:
-                            # 直接运行异步方法
-                            return asyncio.run(original_method(*args, **kwargs))
-                    except Exception as e:
-                        raise RuntimeError(f"Local async method '{method_name}' failed: {str(e)}")
-
-                return async_wrapper
-            else:
-                # 同步方法直接返回
-                return original_method
\ No newline at end of file
diff --git a/archive/dag_manager.py b/archive/dag_manager.py
deleted file mode 100644
index 81649ec..0000000
--- a/archive/dag_manager.py
+++ /dev/null
@@ -1,142 +0,0 @@
-import logging
-from typing import Dict, List, Set
-
-from sympy.strategies.core import switch
-
-from sage.core.dag.local.dag import DAG  # 假设已存在DAG类
-from sage.core.dag.local.dag_node import BaseDAGNode, ContinuousDAGNode, OneShotDAGNode
-
-
-class DAGManager:
-    """
-    DAG管理系统，实现全生命周期管理
-    - 通过逻辑DAG创建实例化DAG
-    - 提交DAG执行
-    - 提供运行中DAG查询
-    - 安全删除DAG
-    """
-
-    def __init__(self):
-        self.dags: Dict[int, DAG] = {}  # 所有DAG存储 {dag_id: dag_instance} 包括代运行、运行中、运行结束的dag
-        self.running_dags: Set[int] = set()  # 待运行的dag的集合
-        self.next_id = 0  # 自增ID生成器
-        self.logger = logging.getLogger(self.__class__.__name__)
-
-    # def create_dag(self, logical_dag: DAG, operator_mapping: Dict) -> int:
-    #     """
-    #     通过逻辑DAG创建实例化DAG
-    #     :param logical_dag: 不含operator的逻辑DAG
-    #     :param operator_mapping: 算子映射 {node_name: {"config":config,"operator_class": class,"init_kwargs": kwargs}}
-    #     :return: 新DAG的ID
-    #     """
-    #     # 生成唯一ID
-    #     dag_id = self.next_id
-    #     self.next_id += 1
-
-    #     new_dag = DAG(id=dag_id,strategy=logical_dag.strategy)
-    #     node_mapping = {}  # 原节点到新节点的映射
-
-    #     # 实例化节点
-    #     for orig_node in logical_dag.nodes:
-    #         # 从映射表获取算子配置
-    #         node_info = operator_mapping.get(orig_node.name, {})
-
-    #         # 创建具体类型节点
-    #         if logical_dag.strategy == "streaming" :
-    #             new_node = ContinuousDAGNode(
-    #                 name=orig_node.name,
-    #                 operator=node_info["operator"](node_info.get("kwargs", {})),
-    #                 config=node_info.get("config", {}),
-    #                 is_spout=orig_node.is_spout
-    #             )
-    #         else:  # 默认为OneShot
-    #             new_node = OneShotDAGNode(
-    #                 name=orig_node.name,
-    #                 operator=node_info["operator"](node_info.get("kwargs", {})),
-    #                 config=node_info.get("config", {}),
-    #                 is_spout=orig_node.is_spout
-    #             )
-
-    #         new_dag.add_node(new_node)
-    #         node_mapping[orig_node] = new_node
-    #     # 重建边关系
-    #     for parent, children in logical_dag.edges.items():
-    #         for child in children:
-    #             new_dag.add_edge(node_mapping[parent], node_mapping[child])
-
-    #     self.dags[dag_id] = new_dag
-    #     self.logger.info(f"Created DAG {dag_id} with {len(new_dag.nodes)} nodes")
-    #     return dag_id
-
-    def add_dag(self,dag:DAG)->int:
-        dag_id = self.next_id
-        self.next_id += 1
-        self.dags[dag_id] = dag
-        dag.dag_id = dag_id
-        return dag_id
-
-    def submit_dag(self, dag_id: int) -> None:
-        """
-        提交DAG到运行队列
-        :param dag_id: 要执行的DAG ID
-        """
-        # 有效性检查
-        if dag_id not in self.dags:
-            raise ValueError(f"DAG {dag_id} does not exist")
-        if dag_id in self.running_dags:
-            self.logger.warning(f"DAG {dag_id} is already running")
-            return
-
-        # 获取DAG实例并启动
-        dag = self.dags[dag_id]
-
-        # 加入运行集合
-        self.running_dags.add(dag_id)
-        self.logger.info(f"DAG {dag_id} submitted for execution")
-
-    def get_running_dags(self) -> List[int]:
-        """获取所有运行中DAG ID列表"""
-        return list(self.running_dags)
-
-    def clear_running_dags(self) -> None:
-         """清除running_dags列表"""
-         self.running_dags.clear()
-
-    def delete_dag(self, dag_id: int) -> None:
-        """
-        删除非运行中的DAG
-        :param dag_id: 要删除的DAG ID
-        """
-        try :
-            if dag_id in self.running_dags:
-                raise PermissionError(f"Cannot delete running DAG {dag_id}")
-            if dag_id not in self.dags:
-                raise ValueError(f"DAG {dag_id} does not exist")
-
-            del self.dags[dag_id]
-            self.logger.info(f"DAG {dag_id} deleted")
-        except Exception as e:
-            self.logger.error(f"Failed to delete DAG {dag_id}: {e}")
-
-    def remove_from_running(self, dag_id: int) -> None:
-        """
-        将DAG移出运行队列
-        :param dag_id: 要停止的DAG ID
-        """
-        if dag_id not in self.running_dags:
-            self.logger.warning(f"DAG {dag_id} is not running")
-            return
-
-        self.running_dags.remove(dag_id)
-        self.logger.info(f"DAG {dag_id} removed from running")
-
-    def get_dag(self, dag_id: int) -> DAG:
-        #通过dag_id获取dag
-        try :
-            if not dag_id in self.dags:
-                raise ValueError(f"DAG {dag_id} does not exist")
-            return self.dags[dag_id]
-        except Exception as e:
-            self.logger.error(f"Failed to get DAG {dag_id}: {e}")
-            return None
-
diff --git a/archive/executor_manager.py b/archive/executor_manager.py
deleted file mode 100644
index a6ce42c..0000000
--- a/archive/executor_manager.py
+++ /dev/null
@@ -1,166 +0,0 @@
-import logging
-
-
-
-from sage.core.runtime.local.local_task import StreamingTask, OneshotTask, BaseTask
-from sage.core.runtime.local.local_scheduling_strategy import SchedulingStrategy, ResourceAwareStrategy, PriorityStrategy
-from sage.core.dag.local.dag import DAG
-from archive.dag_manager import DAGManager
-from sage.core.runtime.local.local_slot import Slot
-from sage.core.dag.local.dag_node import BaseDAGNode,ContinuousDAGNode,OneShotDAGNode
-from sage.core.runtime import BaseRuntime, LocalRuntime,  RayRuntime
-import time
-
-class ExecutorManager:
-    """用于管理任务，初始化slots,并负责将任务提交到slot里面运行，基于已提交的dag提取出所有的node并根据node按照一定的策略分配给不同的slot
-     Attributes:
-        available_slots (List[Slot]): 可用计算槽位列表
-        max_slots (int): 最大槽位数量
-        dag_manager (DAGManager): DAG流程管理器
-        task_to_slot (Dict[object, int]): 任务到槽位的映射关系
-        dag_to_tasks (Dict[str, List[object]]): DAG到其关联任务的映射
-        logger (logging.Logger): 日志记录器
-        scheduling_strategy (SchedulingStrategy): 任务调度策略实现
-    """
-    def __init__(self,dag_manager:DAGManager, max_slots=4,scheduling_strategy=None):
-        self.dag_manager = dag_manager
-        self.dag_to_tasks={}
-        self.task_handles = {}  # dag_id -> [task_handles]
-        self.logger=logging.getLogger(__name__)
-        self.local_backend = LocalRuntime(max_slots, scheduling_strategy)
-        # self.ray_backend:RayExecutionBackend = None
-
-        # self.available_slots = [Slot(slot_id=i) for i in range(max_slots)]
-        # self.max_slots = max_slots
-        # self.task_to_slot ={}
-        # if scheduling_strategy is None:
-        #     self.scheduling_strategy = ResourceAwareStrategy()
-        # elif scheduling_strategy.lower() =="prioritystrategy":
-        #     self.scheduling_strategy = PriorityStrategy({})
-        # else :
-        #     self.scheduling_strategy = ResourceAwareStrategy()
-
-
-    def run_dags(self):
-        # """
-        # 提交DAG并调度其节点
-
-        # 流程：
-        # 1. 从DAG管理器获取正在运行的DAG列表
-        # 2. 清空管理器的运行中DAG记录（避免重复提交）
-        # 3. 对每个DAG创建对应任务：
-        #    - 流式DAG：为每个节点创建独立任务
-        #    - 一次性DAG：为整个DAG创建单个任务
-        # 4. 将任务分配到可用槽位执行
-        # """
-        """
-        执行所有待运行的DAG
-        根据DAG配置选择不同的执行后端
-        """
-        running_dags=self.dag_manager.get_running_dags()
-        self.dag_manager.clear_running_dags()
-        for dag_id in running_dags:
-            self.task_handles[dag_id] = []
-            dag = self.dag_manager.get_dag(dag_id)
-            
-            # 根据DAG配置选择执行后端
-            execution_backend = self._get_runtime(dag)
-            
-            if dag.strategy == "streaming":
-                self._execute_streaming_dag(dag_id, dag, execution_backend)
-            else:
-                self._execute_oneshot_dag(dag_id, dag, execution_backend)
-
-    def _get_runtime(self, dag: DAG) -> BaseRuntime:
-        """根据DAG配置选择执行后端"""
-        # 检查DAG配置中的执行后端设置
-        backend_type = dag.platform
-        
-        if backend_type == "ray":
-            self.logger.info(f"Using Ray backend for DAG {dag.dag_id}")
-            if self.ray_backend is None:
-                self.ray_backend = RayRuntime()
-            return self.ray_backend
-        else:
-            self.logger.info(f"Using local backend for DAG {dag.dag_id}")
-            return self.local_backend
-    
-    def _execute_streaming_dag(self, dag_id: int, dag: DAG, backend: BaseRuntime):
-        """使用指定后端执行流式DAG"""
-        for node in dag.nodes:
-            task = StreamingTask(node, dag.working_config)
-            task_handle = backend.submit_task(task)
-            self.task_handles[dag_id].append(task_handle)
-            self.logger.debug(f"{node.name} submitted to {backend.__class__.__name__}")
-    
-    def _execute_oneshot_dag(self, dag_id: int, dag: DAG, runtime: BaseRuntime):
-        """使用指定后端执行一次性DAG"""
-        task = OneshotTask(dag)
-        
-        if isinstance(runtime, RayRuntime):
-            # Ray执行
-            task_handle = runtime.submit_task(task)
-            self.task_handles[dag_id].append(task_handle)
-        else:
-            # 本地执行，直接调用execute
-            task.execute()
-    
-    def stop_dag(self, dag_id: int):
-        """停止指定DAG的所有任务"""
-        if dag_id not in self.task_handles:
-            return
-        
-        dag = self.dag_manager.get_dag(dag_id)
-        backend = self._get_runtime(dag)
-        
-        # 停止所有任务
-        for task_handle in self.task_handles[dag_id]:
-            backend.stop_task(task_handle)
-        
-        # 清理记录
-        del self.task_handles[dag_id]
-        self.dag_manager.remove_from_running(dag_id)
-    
-    def get_dag_status(self, dag_id: int):
-        """获取DAG执行状态"""
-        if dag_id not in self.task_handles:
-            return {"status": "not_found"}
-        
-        dag = self.dag_manager.get_dag(dag_id)
-        backend = self._get_runtime(dag)
-        
-        task_statuses = []
-        for task_handle in self.task_handles[dag_id]:
-            status = backend.get_status(task_handle)
-            task_statuses.append(status)
-        
-        return {
-            "dag_id": dag_id,
-            "backend": backend.__class__.__name__,
-            "task_count": len(task_statuses),
-            "tasks": task_statuses
-        }
-
-    # def schedule_task(self, task: BaseTask) -> int :
-    #         """
-    #            调度任务到指定槽位
-
-    #            Args:
-    #                task: 需要调度的任务对象
-
-    #            Returns:
-    #                int: 分配的槽位ID
-
-    #            Raises:
-    #                RuntimeError: 当无可用槽位时抛出
-    #            """
-    #         selected_slot_id = self.scheduling_strategy.select_slot(
-    #             task, self.available_slots
-    #         )
-    #         self.logger.info(f"chosen slot id {selected_slot_id}")
-    #         if selected_slot_id > 0 :
-    #             self.available_slots[selected_slot_id].submit_task(task)
-    #             self.task_to_slot[task] = selected_slot_id
-    #         return selected_slot_id
-
-
diff --git a/archive/executor_test.py b/archive/executor_test.py
deleted file mode 100644
index f29a899..0000000
--- a/archive/executor_test.py
+++ /dev/null
@@ -1,195 +0,0 @@
-from archive.dag_manager import DAGManager
-from sage.core.dag.local.dag import DAG
-from sage.core.dag.local.dag_node import OneShotDAGNode,ContinuousDAGNode
-from sage.archive.executor_manager import ExecutorManager
-import time
-import logging
-import ray
-from ray import state
-# 用于测试的operator
-"""用于测试的operator,其中spout负责生成数据源，末节点generator负责将收到的数据处理后写进该dag对应的一个文件里面"""
-
-@ray.remote
-class Spout :
-    def __init__(self,config={}):
-        self.logger=logging.getLogger(self.__class__.__name__)
-        self.config=config
-    def execute(self,input="streaming query operator_test"):
-        dagid=self.config['id']
-        self.logger.debug(f"spout_{dagid} execute start")
-        time.sleep(0.1)
-        return input
-@ray.remote
-class Retriever:
-    def __init__(self,config={}):
-        self.logger=logging.getLogger(self.__class__.__name__)
-        self.config=config
-    def execute(self,input):
-        dagid=self.config["id"]
-        self.logger.debug(f"retriever_{dagid} execute start")
-        time.sleep(0.1)
-        return input+"retriever done"
-
-@ray.remote
-class PromptOperator:
-    def __init__(self,config={}):
-        self.logger = logging.getLogger(self.__class__.__name__)
-        self.config=config
-    def execute(self, input):
-        dagid=self.config["id"]
-        self.logger.debug(f"prompt_{dagid} execute start")
-        time.sleep(0.1)
-        return input+"prompt done"
-
-@ray.remote
-class Generator:
-    def __init__(self,config={}):
-        self.config=config
-        self.logger = logging.getLogger(self.__class__.__name__)
-    def execute(self, input):
-        dagid=self.config["id"]
-        self.logger.debug(f"generator_{dagid} execute start")
-        file_name=self.config["file_name"]
-        with open(f"test_output_{dagid}.txt", "a", encoding="utf-8") as f:
-            f.write(input +"generator done"+  "\n")  # 写入内容并换行
-
-        return input+"generator done"
-
-def create_test_streaming_dag(dag_manager: DAGManager) :
-    #用于测试的dag
-    """创建一个用于测试的流式dag，默认形态为spout->retriever->prompt->generator"""
-    dag=DAG(dag_manager.next_id,strategy="streaming")
-    dag_manager.next_id+=1
-
-    spout_node=ContinuousDAGNode(
-        name='Spout',
-        operator=Spout.remote(config={"id": dag.id}),
-        config={},
-        is_spout=True
-    )
-    retriever_node =ContinuousDAGNode(
-        name="Retriever",
-        operator=Retriever.remote(config={"id": dag.id}),
-        config={}
-    )
-    prompt_node = ContinuousDAGNode(
-        name="PromptGenerator",
-        operator=PromptOperator.remote(config={"id": dag.id})
-    )
-    generator_node =ContinuousDAGNode(
-        name="Generator",
-        operator=Generator.remote(config={"id": dag.id,"file_name": f"test_output_{dag.id}"})
-    )
-    dag.add_node(spout_node)
-    dag.add_node(retriever_node)
-    dag.add_node(prompt_node)
-    dag.add_node(generator_node)
-    dag.add_edge(spout_node, retriever_node)
-    dag.add_edge(retriever_node, prompt_node)
-    dag.add_edge(prompt_node, generator_node)
-    dag_manager.dags[dag.id] = dag
-    return dag.id
-def create_test_oneshot_dag(dag_manager: DAGManager) :
-    """创建一个用于测试的非流式dag，默认形态为spout->retriever->prompt->generator"""
-    dag=DAG(dag_manager.next_id,strategy="one_shot")
-    dag_manager.next_id+=1
-
-    spout_node=OneShotDAGNode(
-        name='Spout',
-        operator=Spout.remote(config={"id": dag.id}),
-        config={},
-        is_spout=True
-    )
-    retriever_node =OneShotDAGNode(
-        name="Retriever",
-        operator=Retriever.remote(config={"id": dag.id}),
-        config={}
-    )
-    prompt_node = OneShotDAGNode(
-        name="PromptGenerator",
-        operator=PromptOperator.remote(config={"id": dag.id})
-    )
-    generator_node = OneShotDAGNode(
-        name="Generator",
-        operator=Generator.remote(config={"id": dag.id,"file_name": f"test_output_{dag.id}"})
-    )
-    dag.add_node(spout_node)
-    dag.add_node(retriever_node)
-    dag.add_node(prompt_node)
-    dag.add_node(generator_node)
-    dag.add_edge(spout_node, retriever_node)
-    dag.add_edge(retriever_node, prompt_node)
-    dag.add_edge(prompt_node, generator_node)
-    dag_manager.dags[dag.id] = dag
-    return dag.id
-
-def streaming_dag_test():
-#测试多线程流式rag
-    ray.init(  dashboard_port=8265 )
-    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",)
-    dag_manager = DAGManager()
-    dag_ids=[]
-    for i in range(1) :   #测试的dag个数为10
-        dag_id=create_test_streaming_dag(dag_manager)
-        dag_manager.run_dag(dag_id)
-        dag_ids.append(dag_id)
-    executor_manager = ExecutorManager(dag_manager,max_slots=5)
-    executor_manager.run_dag()
-    #dag 运行10s
-    time.sleep(1000)
-
-
-    actors = state.actors()
-    for actor_id, actor_info in actors.items():
-        print(f"actor_id {actor_id}: {actor_info}")
-
-    #依次停止dag
-    for dag_id in dag_ids :
-        time.sleep(1)
-        executor_manager.stop_dag(dag_id)
-    time.sleep(1)
-    print(f"num of tasks is {len(executor_manager.dag_to_tasks)}")
-    print(f"num of dags is {len(executor_manager.dag_to_tasks)}")
-    print(f"num if running dags is {len(executor_manager.dag_manager.running_dags)}")
-    print(f"num if created dags is {len(executor_manager.dag_manager.dags)}")
-
-def oneshot_dag_test():
-    #测试多线程非流式rag
-    """测试多线程非流时rag,模拟多轮对话模式"""
-    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", )
-    dag_manager = DAGManager()
-    dag_ids = []
-    for i in range(10):  # 测试的dag个数为10
-        dag_id = create_test_oneshot_dag(dag_manager)
-        dag_manager.run_dag(dag_id)
-        dag_ids.append(dag_id)
-    executor_manager = ExecutorManager(dag_manager, max_slots=5)
-    executor_manager.run_dag()
-    # dag 运行10s
-    time.sleep(10)
-    # 查询 Actor 分配情况
-    actors = state.actors()
-    for actor_id, actor_info in actors.items():
-        print(f"Actor ID: {actor_id}")
-        print(f"类名: {actor_info['class_name']}")
-        print(f"节点 IP: {actor_info['address']['ip']}")
-        print(f"资源请求: {actor_info['required_resources']}")
-    for i in dag_ids :
-        dag_manager.run_dag(i)
-        executor_manager.run_dag()
-    time.sleep(5)
-
-    print(f"num of tasks is {len(executor_manager.task_to_slot)}")
-    print(f"num of dags is {len(executor_manager.dag_to_tasks)}")
-    print(f"num if running dags is {len(executor_manager.dag_manager.running_dags)}")
-    print(f"num if created dags is {len(executor_manager.dag_manager.dags)}")
-
-
-
-if __name__ == '__main__':
-    streaming_dag_test()
-
-
-
-
-
diff --git a/archive/operator_factory.py b/archive/operator_factory.py
deleted file mode 100644
index 916d351..0000000
--- a/archive/operator_factory.py
+++ /dev/null
@@ -1,57 +0,0 @@
-# sage/runtime/operator_factory.py
-from typing import TypeVar, Any, Type
-import ray
-import logging
-from sage.archive.operator_wrapper import OperatorWrapper
-
-T = TypeVar("T")
-
-class OperatorFactory:
-    """简化的算子工厂，全局控制本地或远程创建"""
-    
-    def __init__(self, use_ray: bool = True):
-        # self.use_ray = use_ray
-        self.use_ray = False
-        self.logger = logging.getLogger(__name__)
-        self._ray_remote_classes = {}  # 缓存Ray远程类
-        
-        if self.use_ray and not ray.is_initialized():
-            self.logger.warning("Ray not initialized, falling back to local mode")
-            self.use_ray = False
-    
-
-    def create(self, operator_class:Type[T], config:dict) -> T:
-        """
-        创建算子实例
-        
-        Args:
-            operator_class: 算子类（如 FileSource）
-            config: 算子配置
-            
-        Returns:
-            算子实例（本地对象或Ray Actor Handle）
-        """
-        if self.use_ray:
-            raw_operator = self._create_ray_operator(operator_class, config)
-        else:
-            raw_operator = self._create_local_operator(operator_class, config)
-        
-        wrapped_operator = OperatorWrapper(raw_operator)
-        return wrapped_operator
-
-
-    def _create_ray_operator(self, operator_class, config):
-        """创建Ray远程算子"""
-        print("create ray operator")
-        class_name = operator_class.__name__
-        
-        # 从缓存获取或创建Ray远程类
-        if class_name not in self._ray_remote_classes:
-            self._ray_remote_classes[class_name] = ray.remote(operator_class)
-        
-        ray_remote_class = self._ray_remote_classes[class_name]
-        return ray_remote_class.remote(config)
-    
-    def _create_local_operator(self, operator_class, config):
-        """创建本地算子"""
-        return operator_class(config)
\ No newline at end of file
diff --git a/archive/operator_wrapper.py b/archive/operator_wrapper.py
deleted file mode 100644
index 8eb79cf..0000000
--- a/archive/operator_wrapper.py
+++ /dev/null
@@ -1,135 +0,0 @@
-# sage/runtime/operator_wrapper.py
-import ray
-import asyncio
-from typing import Any, Dict, Optional, Type
-from sage.api.operator.base_operator_api import BaseOperator
-class OperatorWrapper:
-    """
-    透明的算子包装器，提供统一的同步接口
-    
-    该包装器自动处理本地对象、Ray Actor和Ray Function的差异，
-    对外提供一致的同步调用接口。
-    
-    支持的执行模式：
-    - local: 本地对象直接调用
-    - ray_actor: Ray Actor远程调用，自动处理ray.get()
-    - ray_function: Ray Function远程调用，自动处理ray.get()
-    
-    示例:
-        # 本地模式
-        local_op = FileSource(config)
-        wrapper = OperatorWrapper(local_op)
-        result = wrapper.read()  # 直接调用
-        
-        # Ray Actor模式
-        ray_actor = ray.remote(FileSource).remote(config)
-        wrapper = OperatorWrapper(ray_actor)
-        result = wrapper.read()  # 自动处理ray.get()
-    """
-    
-    def __init__(self, operator: Any):
-        # 使用 __dict__ 直接设置，避免触发 __setattr__
-        object.__setattr__(self, '_operator', operator)
-        object.__setattr__(self, '_execution_mode', self._detect_execution_mode())
-        object.__setattr__(self, '_method_cache', {})
-        object.__setattr__(self, '_attribute_cache', {})
-    
-    def _detect_execution_mode(self) -> str:
-        """检测执行模式"""
-        if isinstance(self._operator, ray.actor.ActorHandle):
-            return "ray_actor"
-        elif hasattr(self._operator, 'remote'):
-            return "ray_function"
-        else:
-            return "local"
-    
-    # 完美代理所有属性访问
-    def __getattr__(self, name: str):
-        """透明代理属性访问"""
-        if name in self._attribute_cache:
-            return self._attribute_cache[name]
-        
-        original_attr = getattr(self._operator, name)
-        
-        # 如果是方法，则包装成统一的同步调用
-        if callable(original_attr):
-            wrapped_method = self._create_unified_method(name, original_attr)
-            self._attribute_cache[name] = wrapped_method
-            return wrapped_method
-        else:
-            # 普通属性直接返回
-            self._attribute_cache[name] = original_attr
-            return original_attr
-    
-    def __setattr__(self, name: str, value: Any):
-        """代理属性设置"""
-        if name.startswith('_'):
-            # 内部属性直接设置
-            object.__setattr__(self, name, value)
-        else:
-            # 外部属性设置到原算子
-            setattr(self._operator, name, value)
-    
-    def __dir__(self):
-        """代理dir()调用"""
-        return dir(self._operator)
-    
-    def __repr__(self):
-        """代理repr()"""
-        return f"OperatorWrapper({repr(self._operator)})"
-    
-    def __str__(self):
-        """代理str()"""
-        return str(self._operator)
-    
-    def _create_unified_method(self, method_name: str, original_method):
-        """创建统一的方法包装 - 对外始终提供同步接口"""
-        
-        if self._execution_mode == "ray_actor":
-            # Ray Actor: 同步调用，内部处理Ray异步
-            def sync_ray_actor_wrapper(*args, **kwargs):
-                try:
-                    future = original_method.remote(*args, **kwargs)
-                    result = ray.get(future)
-                    return result
-                except Exception as e:
-                    raise RuntimeError(f"Ray Actor method '{method_name}' failed: {e}")
-            
-            return sync_ray_actor_wrapper
-            
-        elif self._execution_mode == "ray_function":
-            # Ray Function: 同步调用
-            def sync_ray_function_wrapper(*args, **kwargs):
-                try:
-                    future = original_method.remote(*args, **kwargs)
-                    result = ray.get(future)
-                    return result
-                except Exception as e:
-                    raise RuntimeError(f"Ray function '{method_name}' failed: {e}")
-            
-            return sync_ray_function_wrapper
-            
-        else:
-            # 本地方法: 处理异步方法，统一返回同步结果
-            if asyncio.iscoroutinefunction(original_method):
-                def sync_local_async_wrapper(*args, **kwargs):
-                    try:
-                        # 检查是否已在事件循环中
-                        try:
-                            loop = asyncio.get_running_loop()
-                            # 如果在事件循环中，需要在新线程中运行
-                            import concurrent.futures
-                            with concurrent.futures.ThreadPoolExecutor() as executor:
-                                future = executor.submit(asyncio.run, original_method(*args, **kwargs))
-                                return future.result()
-                        except RuntimeError:
-                            # 不在事件循环中，直接使用asyncio.run
-                            return asyncio.run(original_method(*args, **kwargs))
-                    except Exception as e:
-                        raise RuntimeError(f"Local async method '{method_name}' failed: {e}")
-                
-                return sync_local_async_wrapper
-            else:
-                # 本地同步方法，直接返回
-                return original_method
-    
diff --git a/archive/raydag_task.py b/archive/raydag_task.py
deleted file mode 100644
index e5a7663..0000000
--- a/archive/raydag_task.py
+++ /dev/null
@@ -1,33 +0,0 @@
-from typing import Optional, Dict, Any
-from sage.core.dag.ray.ray_dag import RayDAG
-
-class RayDAGTask:
-    """
-    Ray DAG 任务包装器，用于与执行后端接口兼容
-    """
-    
-    def __init__(self, ray_dag: RayDAG, execution_config: Optional[Dict[str, Any]] = None):
-        """
-        Initialize Ray DAG task.
-        
-        Args:
-            ray_dag: The RayDAG to execute
-            execution_config: Optional execution configuration
-        """
-        self.ray_dag = ray_dag
-        self.execution_config = execution_config or {}
-        self.task_type = "ray_dag"
-    
-    def get_dag(self) -> RayDAG:
-        """Get the underlying RayDAG."""
-        return self.ray_dag
-    
-    def get_config(self) -> Dict[str, Any]:
-        """Get execution configuration."""
-        return self.execution_config
-    
-    def __str__(self):
-        return f"RayDAGTask(dag_id={self.ray_dag.id}, strategy={self.ray_dag.strategy})"
-
-
-        
diff --git a/archive/slot_test.py b/archive/slot_test.py
deleted file mode 100644
index fae70b1..0000000
--- a/archive/slot_test.py
+++ /dev/null
@@ -1,83 +0,0 @@
-from sage.core.runtime.local.local_slot import Slot
-import time
-import threading
-import logging
-from concurrent.futures import Future
-
-
-class TestTask:
-    """模拟任务类"""
-
-    def __init__(self, name, duration=0):
-        self.name = name
-        self.duration = duration
-        self.stop_called = False
-        self.stop_event = threading.Event()
-    def run(self):
-        cnt=0
-        while not self.stop_event.is_set():
-            time.sleep(self.duration)
-            if cnt%5 == 0 :
-                print(f"{self.name} loop {cnt} times")
-            cnt+=1
-
-    def stop(self):
-        self.stop_event.set()
-
-
-class TestSlot():
-    def __init__(self):
-        self.slot = Slot("test_slot", max_threads=2)
-        self.task1 = TestTask("task1", duration=0.1)
-        self.task2 = TestTask("task2", duration=0.2)
-        self.task3 = TestTask("task3", duration=0.3)
-
-    def tearDown(self):
-        self.slot.shutdown()
-
-    def test_submit_task_within_capacity(self):
-        # 测试正常提交任务
-        self.slot.submit_task(self.task1)
-        if not self.slot.current_load == 1 :
-            raise RuntimeError("current load not equal to 1")
-        self.slot.submit_task(self.task2)
-        if not self.slot.current_load == 2 :
-            raise RuntimeError("current load not equal to 2")
-        time.sleep(3)
-        self.slot.shutdown()
-        if not self.slot.current_load == 0 :
-            raise RuntimeError("current load not equal to 0")
-
-    def test_submit_task_exceed_capacity(self):
-        # 测试超过容量时提交失败
-        self.slot.submit_task(self.task1)
-        self.slot.submit_task(self.task2)
-        self.slot.submit_task(self.task3)
-        if not self.slot.current_load == 2 :
-            raise RuntimeError("current load not equal to 2")
-        time.sleep(3)
-        self.slot.shutdown()
-
-    def test_stop_running_task(self):
-        # 测试停止正在运行的任务
-        self.slot.submit_task(self.task3)  # 0.3秒任务
-        time.sleep(3)  # 确保任务已经开始
-
-        future = self.slot.task_to_future[self.task3]
-        if future.running():
-            print("task is running")
-        self.slot.stop(self.task3)
-        future.result()
-        if future.done() :
-            print("task is done")
-
-    def test_stop_pending_task(self):
-        # 测试取消未开始的任务
-        self.slot.submit_task(self.task1)
-        self.slot.submit_task(self.task2)
-        self.slot.submit_task(self.task3)  # 会被拒绝，因为容量为2
-
-
-
-
-
diff --git a/config/config.yaml b/config/config.yaml
index 1cad202..b18912d 100644
--- a/config/config.yaml
+++ b/config/config.yaml
@@ -7,18 +7,22 @@ pipeline:
 
 
 source:
-  data_path: "sample/one_question.txt"
-  # data_path: "sample/question.txt"
+  # data_path: "sample/one_question.txt"
+  data_path: "sample/question.txt"
+  platform: "local"
 
 retriever:
+  platform: "ray"
   ltm:
     topk: 3
 
 reranker:
+  platform: "local"
   model_name: "BAAI/bge-reranker-v2-m3"
   top_k: 3
 
 refiner:
+  platform: "local"
   method: "openai"
   model_name: "qwen-turbo-0919"
   base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
@@ -26,6 +30,7 @@ refiner:
   seed: 42
 
 generator:
+  platform: "local"
   method: "openai"
   model_name: "qwen-turbo-0919"
   base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
@@ -34,6 +39,9 @@ generator:
 
 writer:
 
+promptor:
+  platform: "local"
 
 sink:
+  platform: "local"
 
diff --git a/sage/api/operator/base_operator_api.py b/sage/api/operator/base_operator_api.py
index 5423517..288e0e6 100644
--- a/sage/api/operator/base_operator_api.py
+++ b/sage/api/operator/base_operator_api.py
@@ -1,7 +1,7 @@
 
 import logging
 from typing import TypeVar,Generic, Callable, Any, List
-from sage.core.io.message_queue import MessageQueue
+
 from typing import TypeVar,Generic
 T = TypeVar('T')
 class Data(Generic[T]):
@@ -9,39 +9,6 @@ class Data(Generic[T]):
         self.data = data 
 
 
-class EmitContext:
-    """
-    Emit context that encapsulates emission logic and channels.
-    This avoids closures that reference the parent DAG node.
-    """
-    
-    def __init__(self, node_name: str):
-        self.node_name = node_name
-        self.downstream_channels: List[MessageQueue] = []
-    
-    def add_downstream_channel(self, message_queue: MessageQueue):
-        """Add a downstream channel."""
-        self.downstream_channels.append(message_queue)
-    
-    def emit(self, channel: int, data: Any) -> None:
-        """
-        Emit data to specified downstream channel.
-        
-        Args:
-            channel: The downstream channel index
-            data: Data to emit
-        """
-        if(channel == -1):
-            # Broadcast to all downstream channels
-            for downstream_channel in self.downstream_channels:
-                downstream_channel.put(data)
-            return
-        elif(0 <= channel and channel < len(self.downstream_channels)) :
-            self.downstream_channels[channel].put(data)
-        else:
-            # Note: We can't use logger here to keep the context simple and serializable
-            print(f"Warning: Channel index {channel} out of range for node {self.node_name}")
-
 
 
 
diff --git a/sage/api/operator/operator_impl/generator.py b/sage/api/operator/operator_impl/generator.py
index 71ad07c..d74306f 100644
--- a/sage/api/operator/operator_impl/generator.py
+++ b/sage/api/operator/operator_impl/generator.py
@@ -18,7 +18,7 @@ class OpenAIGenerator(GeneratorFunction):
                        the method, model name, base URL, API key, etc.
         """
         super().__init__()
-        self.config = config["generator"]
+        self.config = config
 
         # Apply the generator model with the provided configuration
         self.model = apply_generator_model(
diff --git a/sage/api/operator/operator_impl/retriever.py b/sage/api/operator/operator_impl/retriever.py
index 8e97cb4..bdf98aa 100644
--- a/sage/api/operator/operator_impl/retriever.py
+++ b/sage/api/operator/operator_impl/retriever.py
@@ -9,7 +9,7 @@ from sage.utils.custom_logger import CustomLogger
 class DenseRetriever(StateRetrieverFunction):
     def __init__(self, config: dict):
         super().__init__()
-        self.config = config["retriever"]
+        self.config = config
 
         
         if self.config.get("ltm", False):
diff --git a/sage/api/operator/operator_impl/sink.py b/sage/api/operator/operator_impl/sink.py
index 48aef17..6211def 100644
--- a/sage/api/operator/operator_impl/sink.py
+++ b/sage/api/operator/operator_impl/sink.py
@@ -7,7 +7,7 @@ class TerminalSink(SinkFunction):
 
     def __init__(self,config):
         super().__init__()
-        self.config=config["sink"]
+        self.config=config
 
     def execute(self, data:Data[Tuple[str,str]]):
         question,answer=data.data
diff --git a/sage/api/operator/operator_impl/source.py b/sage/api/operator/operator_impl/source.py
index 0c1249b..2a86a61 100644
--- a/sage/api/operator/operator_impl/source.py
+++ b/sage/api/operator/operator_impl/source.py
@@ -25,10 +25,9 @@ class FileSource(SourceFunction):
         :param config: Configuration dictionary containing source settings, including `data_path`.
         """
         super().__init__()
-        self.config = config["source"]
+        self.config = config
         # self.data_path = self.config["data_path"]
-        raw = self.config["data_path"]  # e.g. "sample/question.txt"
-        self.data_path = resolve_data_path(raw)  # → project_root/data/sample/question.txt
+        self.data_path = resolve_data_path(config["data_path"])  # → project_root/data/sample/question.txt
         self.file_pos = 0  # Track the file read position
 
     def execute(self) -> Data[str]:
diff --git a/sage/api/pipeline/datastream_api.py b/sage/api/pipeline/datastream_api.py
index 7eccba9..d45851e 100644
--- a/sage/api/pipeline/datastream_api.py
+++ b/sage/api/pipeline/datastream_api.py
@@ -32,10 +32,10 @@ class DataStream:
         self.config = config or {}
         self.node_type = node_type  # "source", "sink", "normal" or other types
 
-    def _transform(self, name: str, operator_class:Type[BaseFuction], config) -> DataStream:
-        # operator_instance = self.pipeline.operator_factory.create(operator_class, config)
+    def _transform(self, name: str, function_class:Type[BaseFuction], config) -> DataStream:
+        # operator_instance = self.pipeline.operator_factory.create(function_class, config)
         # op = next_operator_class
-        new_stream = DataStream(operator_class, self.pipeline, name=name, config = config, node_type="normal")
+        new_stream = DataStream(function_class, self.pipeline, name=name, config = config, node_type="normal")
         self.pipeline.data_streams.append(new_stream)
         # Wire dependencies
         new_stream.upstreams.append(self)
diff --git a/sage/api/pipeline/pipeline_api.py b/sage/api/pipeline/pipeline_api.py
index ba2cd61..8a1f36f 100644
--- a/sage/api/pipeline/pipeline_api.py
+++ b/sage/api/pipeline/pipeline_api.py
@@ -13,15 +13,15 @@ class Pipeline:
     operator_config: dict
     operator_cls_mapping: dict
     # operator_factory: OperatorFactory
-    use_ray: bool
+    # use_ray: bool
     # compiler: QueryCompiler
-    def __init__(self, name: str, use_ray: bool = True):
+    def __init__(self, name: str):
         self.name = name
         self.operators = []
         self.data_streams = []
         self.operator_config = {}
         self.operator_cls_mapping = {}
-        self.use_ray = use_ray
+        self.use_ray = False  # 是否使用 Ray 运行时，默认为 False
         # 创建全局算子工厂
         # self.operator_factory = OperatorFactory(self.use_ray)
 
@@ -83,6 +83,12 @@ class Pipeline:
         print(f"[Pipeline] Pipeline '{self.name}'submitted to engine.")
         engine.submit_pipeline(self, config, generate_func)
 
+    def submit_mixed(self, config=None):
+        from sage.core.engine import Engine
+        engine = Engine.get_instance()
+        print(f"[Pipeline] Pipeline '{self.name}'submitted to engine.")
+        engine.submit_mixed_pipeline(self, config)
+
     def get_graph_preview(self) -> dict:
         """
         获取 pipeline 转换为 graph 后的预览信息，不实际提交
diff --git a/sage/core/compiler/query_compiler.py b/sage/core/compiler/query_compiler.py
index e6f266b..f72ff28 100644
--- a/sage/core/compiler/query_compiler.py
+++ b/sage/core/compiler/query_compiler.py
@@ -4,7 +4,7 @@ from sage.core.compiler.query_parser import QueryParser
 from sage.core.dag.local.dag import DAG
 from sage.core.dag.local.dag_node import BaseDAGNode, OneShotDAGNode
 from sage.core.compiler.logical_graph_constructor import LogicGraphConstructor
-from sage.core.dag.local.multi_dag_node import MultiplexerDagNode
+from sage.core.dag.local.local_dag_node import LocalDAGNode
 from sage.core.io.message_queue import MessageQueue
 from sage.core.dag.ray.ray_dag import RayDAG
 if TYPE_CHECKING:
@@ -13,20 +13,19 @@ from sage.utils.custom_logger import CustomLogger
 
 class QueryCompiler:
 
-    def __init__(self,generate_func = None, session_folder: str = None):
+    def __init__(self):
         """
         Initialize the QueryCompiler with memory layers.
         :param memory_manager: Memory manager for managing memory layers.
         :param generate_func: Function for query generation
         """
-        self.session_folder = session_folder
+        self.session_folder = CustomLogger.get_session_folder()
         self.logical_graph_constructor = LogicGraphConstructor()
         self.optimizer = Optimizer()
-        self.parser = QueryParser(generate_func=generate_func)
+        self.parser = QueryParser(generate_func=None)
         self.dag_dict = {}
         self.logger = CustomLogger(
             object_name=f"QueryCompiler",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
@@ -57,10 +56,10 @@ class QueryCompiler:
         # Step 1: Create all Ray Actor DAG nodes
         for node_name, graph_node in graph.nodes.items():
             # Extract operator class and configuration instead of creating instance
-            operator_class = graph_node.operator
+            function_class = graph_node.operator
             operator_config = graph_node.config or {}
             
-            from sage.core.dag.ray.ray_multi_node import RayMultiplexerDagNode
+            from sage.core.dag.ray.ray_dag_node import RayDAGNode
             from sage.core.runtime.collection_wrapper import CollectionWrapper
             
             # Create Ray Actor with operator class, not instance
@@ -68,9 +67,9 @@ class QueryCompiler:
             
             # wrapper:CollectionWrapper = operator_config["retriever"]["ltm_collection"]
             # operator_config["retriever"]["ltm_collection"] = wrapper._collection
-            ray_actor = RayMultiplexerDagNode.remote(
+            ray_actor = RayDAGNode.remote(
                 name=graph_node.name,
-                operator_class=operator_class,
+                function_class=function_class,
                 operator_config=operator_config,
                 is_spout=(graph_node.type == "source"), 
                 session_folder = self.session_folder
@@ -89,7 +88,7 @@ class QueryCompiler:
             
             # Get channel information from edge
             upstream_output_channel = edge.upstream_channel
-            downstream_input_channel = edge.downstream_channnel
+            downstream_input_channel = edge.downstream_channel
             self.logger.info(f"Connecting actors '{edge.upstream_node.name}' "
                              f"to {edge.downstream_node.name}")
             
@@ -116,14 +115,14 @@ class QueryCompiler:
 
 
         # Step 2: Create all DAG nodes first
-        dag_nodes:Dict[str, MultiplexerDagNode] = {}
+        dag_nodes:Dict[str, LocalDAGNode] = {}
         for node_name, graph_node in graph.nodes.items():
             # Create operator instance
             # operator = operator_factory.create(graph_node.operator, graph_node.config)
             graph_node.config["session_folder"] = self.session_folder
             operator_instance = graph_node.operator(graph_node.config)
             # Create DAG node
-            dag_node = MultiplexerDagNode(
+            dag_node = LocalDAGNode(
                 graph_node.name,
                 operator_instance,
                 config=graph_node.config,
diff --git a/sage/core/dag/local/dag.py b/sage/core/dag/local/dag.py
index ad945bf..26ad83f 100644
--- a/sage/core/dag/local/dag.py
+++ b/sage/core/dag/local/dag.py
@@ -33,7 +33,7 @@ class DAG:
             file_output=True
         )
         self.working_config=None
-    def add_node(self, node):
+    def add_node(self, node:BaseDAGNode):
         """
         Add a node to the DAG.
         :param node: DAGNode instance to add.
diff --git a/sage/core/dag/local/multi_dag_node.py b/sage/core/dag/local/local_dag_node.py
similarity index 63%
rename from sage/core/dag/local/multi_dag_node.py
rename to sage/core/dag/local/local_dag_node.py
index 614aa9c..d02f993 100644
--- a/sage/core/dag/local/multi_dag_node.py
+++ b/sage/core/dag/local/local_dag_node.py
@@ -1,3 +1,4 @@
+from __future__ import annotations
 import asyncio
 import inspect
 import logging
@@ -8,11 +9,17 @@ from typing import Any, Type, TYPE_CHECKING, Union, List, Optional, Tuple
 
 #from sage.archive.operator_wrapper import OperatorWrapper
 from sage.api.operator.base_operator_api import BaseFuction
+from sage.core.graph import SageGraph, GraphEdge, GraphNode
 from sage.core.io.message_queue import MessageQueue
-from sage.api.operator.base_operator_api import EmitContext
+from sage.core.io.emit_context import  NodeType
+from sage.core.io.local_emit_context import LocalEmitContext
 from sage.utils.custom_logger import CustomLogger
+# from sage.core.dag.local.multi_dag_node import LocalDAGNode
+import ray
+from ray.actor import ActorHandle
 
-class MultiplexerDagNode:
+
+class LocalDAGNode:
     """
     Multiplexer DAG Node.
 
@@ -39,22 +46,21 @@ class MultiplexerDagNode:
         self.operator = operator
         self.config = config
         self.is_spout = is_spout
-        # self.logger = logging.getLogger(self.__class__.__name__)
-        # self.logger = None
-        self.upstream_channels: List[MessageQueue] = []
-        self.downstream_channels: List[MessageQueue] = []
-        # self.stop_event = threading.Event()
-        # self.stop_event = None
-        # self.operator.set_emit_func(self._create_emit_func())
-        # Round-robin scheduling for upstream channels
+        
+        self.input_buffer = MessageQueue()  # Local input buffer for this node
+
+
+
         self._current_channel_index = 0
         self._initialized = False
         # Create emit context
-        self.emit_context = EmitContext(self.name)
-        # Don't inject emit context in __init__ to avoid serialization issues
+        # Create emit context for mixed environment
+        self.emit_context = LocalEmitContext(self.name, session_folder=session_folder)
         self._emit_context_injected = False
+
+
         self.logger = CustomLogger(
-            object_name=f"MultiplexerDagNode_{self.name}",
+            object_name=f"LocalDAGNode_{self.name}",
             session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
@@ -88,35 +94,16 @@ class MultiplexerDagNode:
         
         self._initialized = True
     
-
-
-    def fetch_input(self) -> Optional[Tuple[int, Any]]:
+    def put(self, data_packet: Tuple[int, Any]):
         """
-        Fetch input from upstream channels using round-robin scheduling.
-        Returns a tuple of (channel_id, data) from the next available upstream channel.
+        向输入缓冲区放入数据包
         
-        Returns:
-            Tuple of (channel_id, data) or None if no data is available from any channel
+        Args:
+            data_packet: (input_channel, data) 元组
         """
-        if not self.upstream_channels:
-            return None
-        
-        num_channels = len(self.upstream_channels)
-        # Try all channels starting from current position
-        for _ in range(num_channels):
-            channel_id = self._current_channel_index
-            channel:MessageQueue = self.upstream_channels[channel_id]
-            
-            # Move to next channel for next call (round-robin)
-            self._current_channel_index = (self._current_channel_index + 1) % num_channels
-            
-            # Check if current channel has data
-            if not channel.is_empty():
-                data = channel.get()
-                return (channel_id, data)
-        
-        # No data available from any channel
-        return None
+        self.input_buffer.put(data_packet, timeout=1.0)
+        self.logger.debug(f"Put data packet into buffer: channel={data_packet[0]}")
+
     
 
 
@@ -134,10 +121,44 @@ class MultiplexerDagNode:
         else:
             self.logger.warning(f"Channel index {channel} out of range for node {self.name}")
 
-    def add_downstream_channel(self, message_queue: MessageQueue):
-        """Add downstream channel to both node and emit context."""
-        self.downstream_channels.append(message_queue)
-        self.emit_context.add_downstream_channel(message_queue)
+    def add_downstream_node(self, output_edge: GraphEdge, downstream_operator: Union['LocalDAGNode', ActorHandle]):
+        """
+        添加下游节点到emit context
+        
+        Args:
+            output_edge: 输出边
+            downstream_operator: 下游操作符（本地节点或Ray Actor）
+        """
+        try:
+            if isinstance(downstream_operator, ActorHandle):
+                # Ray Actor
+                self.emit_context.add_downstream_target(
+                    output_channel=output_edge.upstream_channel,
+                    node_type=NodeType.RAY_ACTOR,
+                    target_object=downstream_operator,
+                    target_input_channel=output_edge.downstream_channel,
+                    node_name=f"RayActor_{output_edge.downstream_node.name}"
+                )
+                self.logger.debug(f"Added Ray actor downstream: {self.name}[{output_edge.upstream_channel}] -> "
+                                f"{output_edge.downstream_node.name}[{output_edge.downstream_channel}]")
+            
+            elif isinstance(downstream_operator, LocalDAGNode):
+                # 本地节点
+                self.emit_context.add_downstream_target(
+                    output_channel=output_edge.upstream_channel,
+                    node_type=NodeType.LOCAL,
+                    target_object=downstream_operator,
+                    target_input_channel=output_edge.downstream_channel,
+                    node_name=downstream_operator.name
+                )
+                self.logger.debug(f"Added local node downstream: {self.name}[{output_edge.upstream_channel}] -> "
+                                f"{downstream_operator.name}[{output_edge.downstream_channel}]")
+            else:
+                raise TypeError(f"Unsupported downstream operator type: {type(downstream_operator)}")
+                
+        except Exception as e:
+            self.logger.error(f"Error adding downstream node: {e}", exc_info=True)
+            raise
     
 
     def _inject_emit_context_if_needed(self):
@@ -168,18 +189,18 @@ class MultiplexerDagNode:
                 if self.is_spout:
                     # For spout nodes, call operator.receive with dummy channel and data
                     self.operator.receive(0, None)
+                    time.sleep(1)  # Sleep to avoid busy loop
                 else:
                     # For non-spout nodes, fetch input and process
-                    input_result = self.fetch_input()
-                    if input_result is None:
+                    # input_result = self.fetch_input()
+                    data_packet = self.input_buffer.get(timeout=0.5)
+                    if(data_packet is None):
                         time.sleep(0.1)  # Short sleep when no data to process
                         continue
-                    
-                    # Unpack the tuple: (channel_id, data)
-                    channel_id, data = input_result
-                    
+                    (input_channel, data) = data_packet
+                    self.logger.debug(f"Processing data from buffer: channel={input_channel}")
                     # Call operator's receive method with the channel_id and data
-                    self.operator.receive(channel_id, data)
+                    self.operator.receive(input_channel, data)
                     
             except Exception as e:
                 self.logger.error(
@@ -216,17 +237,3 @@ class MultiplexerDagNode:
         self._initialized = False
 
 
-    # def add_upstream_channel(self, upstream_dagnode:MultiplexerDagNode,channel_index:int):
-    #     """
-    #     Add an upstream channel to this multiplexer node.
-    #     目前只能顺序添加，不能随机添加。
-    #     Args:
-    #         upstream_dagnode: The upstream MultiplexerDagNode instance
-    #         channel_index: The index of the channel in the upstream node
-    #     """
-    #     if channel_index < len(upstream_dagnode.downstream_channels):
-    #         self.upstream_channels.append(upstream_dagnode.downstream_channels[channel_index])
-    #     else:
-    #         self.logger.error(f"Channel index {channel_index} out of range for upstream node {upstream_dagnode.name}.")
-
-
diff --git a/sage/core/dag/mixed_dag.py b/sage/core/dag/mixed_dag.py
new file mode 100644
index 0000000..309f20c
--- /dev/null
+++ b/sage/core/dag/mixed_dag.py
@@ -0,0 +1,307 @@
+import ray
+import logging
+from typing import Dict, List, Optional, Any, Tuple, TYPE_CHECKING, Union
+from ray.actor import ActorHandle
+from sage.utils.custom_logger import CustomLogger
+from sage.core.graph import SageGraph, GraphEdge, GraphNode
+from sage.core.dag.ray.ray_dag_node import RayDAGNode
+from sage.core.dag.local.local_dag_node import LocalDAGNode
+
+
+class MixedDAG:
+    def __init__(self, graph: SageGraph):
+        self.name:str = graph.name
+        self.graph:SageGraph = graph
+        self.operators: Dict[str, Union[ActorHandle, LocalDAGNode]] = {}
+        self.nodes_metadata: Dict[str, Dict[str, Any]] = {}  # node_name -> platform
+        self.connections: List[Tuple[str, int, str, int]] = []  # (upstream_node, out_channel, downstream_node, in_channel)
+        self.session_folder = CustomLogger.get_session_folder()
+        self.logger = CustomLogger(
+            object_name=f"MixedDAG_{self.name}",
+            log_level="DEBUG",
+            console_output=True,
+            file_output=True
+        )
+        self.node_dependencies: Dict[str, List[str]] = {}  # node_name -> [upstream_node_names]
+        self.spout_nodes: List[str] = []
+        self.is_running: bool = False
+        self._compile_graph()
+    
+    def _compile_graph(self):
+        """编译图结构，创建节点并建立连接"""
+        self.logger.info(f"Compiling mixed DAG for graph: {self.name}")
+        
+        # 第一步：创建所有节点实例
+        for node_name, graph_node in self.graph.nodes.items():
+            node_instance = self.create_node_instance(graph_node)
+            upstream_nodes = self.graph.get_upstream_nodes(node_name)
+            
+            self.add_node(
+                name=node_name,
+                executor=node_instance,
+                is_spout=(graph_node.type == "source"),
+                upstream_nodes=upstream_nodes
+            )
+        
+        # 第二步：建立节点间的连接
+        for node_name, graph_node in self.graph.nodes.items():
+            self._setup_node_connections(node_name, graph_node)
+        
+        self.logger.info(f"Mixed DAG compilation completed: {len(self.operators)} nodes, "f"{len(self.spout_nodes)} spout nodes")
+
+
+    def _setup_node_connections(self, node_name: str, graph_node: GraphNode):
+        """
+        为节点设置下游连接
+        
+        Args:
+            node_name: 节点名称
+            graph_node: 图节点对象
+        """
+        node_platform = self.nodes_metadata[node_name]["platform"]
+        current_operator = self.operators[node_name]
+        
+        # 为每个输出边添加下游连接
+        for output_edge in graph_node.output_channels:
+            downstream_node_name = output_edge.downstream_node.name
+            downstream_operator = self.operators[downstream_node_name]
+            
+            try:
+                if node_platform == "ray":
+                    # Ray节点调用远程方法
+                    if(isinstance(downstream_operator, LocalDAGNode)):
+                        downstream_handle = downstream_operator.name
+                    current_operator.add_downstream_node.remote(
+                        output_edge.upstream_channel,
+                        output_edge.downstream_channel,
+                        downstream_handle
+                    )
+                    self.logger.debug(f"Setup Ray connection: {node_name} -> {downstream_node_name}")
+                else:
+                    # 本地节点直接调用
+                    current_operator.add_downstream_node(
+                        output_edge,
+                        downstream_operator
+                    )
+                    self.logger.debug(f"Setup local connection: {node_name} -> {downstream_node_name}")
+                    
+                # 记录连接信息
+                self.connections.append((
+                    node_name, 
+                    output_edge.upstream_channel,
+                    downstream_node_name, 
+                    output_edge.downstream_channel
+                ))
+                
+            except Exception as e:
+                self.logger.error(f"Error setting up connection {node_name} -> {downstream_node_name}: {e}")
+                raise        
+
+    def create_node_instance(self, graph_node: GraphNode) -> Union[RayDAGNode, LocalDAGNode]:
+        """
+        根据图节点创建对应的执行实例
+        
+        Args:
+            graph_node: 图节点对象
+            
+        Returns:
+            节点实例（Ray Actor或本地节点）
+        """
+        platform = graph_node.config.get("platform", "local")
+        
+        if platform == "ray":
+            # 创建Ray Actor
+            node = RayDAGNode.remote(
+                name=graph_node.name,
+                function_class=graph_node.operator,
+                operator_config=graph_node.config,
+                is_spout=(graph_node.type == "source"), 
+                session_folder=self.session_folder
+            )
+            self.logger.debug(f"Created Ray actor node: {graph_node.name}")
+            return node
+        else:
+            # 创建本地节点
+            operator_instance = graph_node.operator(graph_node.config)
+            node = LocalDAGNode(
+                name=graph_node.name,
+                operator=operator_instance,
+                config=graph_node.config,
+                is_spout=(graph_node.type == "source"), 
+                session_folder=self.session_folder
+            )
+            self.logger.debug(f"Created local node: {graph_node.name}")
+            return node
+
+    def add_node(self, name: str, executor: Any, is_spout: bool = False, 
+                 upstream_nodes: List[str] = None):
+        """
+        添加节点到DAG
+        
+        Args:
+            name: 节点名称
+            executor: 节点执行器（本地节点或Ray Actor）
+            is_spout: 是否为spout节点
+            upstream_nodes: 上游节点名称列表
+        """
+        # 检测平台类型
+        platform = self._detect_platform(executor)
+        
+        self.operators[name] = executor
+        self.nodes_metadata[name] = {
+            'is_spout': is_spout,
+            'upstream_nodes': upstream_nodes or [], 
+            "platform": platform
+        }
+        
+        if is_spout:
+            self.spout_nodes.append(name)
+        
+        if upstream_nodes:
+            self.node_dependencies[name] = upstream_nodes
+        
+        self.logger.debug(f"Added node '{name}' of platform '{platform}'")
+
+
+    def _detect_platform(self, executor: Any) -> str:
+        """
+        检测执行器的平台类型
+        
+        Args:
+            executor: 执行器对象
+            
+        Returns:
+            平台类型字符串
+        """
+        if isinstance(executor, ActorHandle):
+            return "ray"
+        elif hasattr(executor, 'remote'):
+            return "ray_function" 
+        elif isinstance(executor, LocalDAGNode):
+            return "local"
+        else:
+            return "unknown"
+
+
+    def start_all_nodes(self):
+        """启动所有本地节点（Ray Actor会自动启动）"""
+        self.logger.info("Starting all DAG nodes...")
+        
+        local_node_count = 0
+        ray_node_count = 0
+        
+        for node_name, node_meta in self.nodes_metadata.items():
+            if node_meta["platform"] == "local":
+                node = self.operators[node_name]
+                node.start()
+                local_node_count += 1
+                self.logger.debug(f"Started local node: {node_name}")
+            else:
+                ray_node_count += 1
+        
+        self.logger.info(f"Started {local_node_count} local nodes, {ray_node_count} Ray actors")
+
+    def stop_all_nodes(self):
+        """停止所有节点"""
+        self.logger.info("Stopping all DAG nodes...")
+        
+        for node_name, node_meta in self.nodes_metadata.items():
+            try:
+                if node_meta["platform"] == "local":
+                    node = self.operators[node_name]
+                    node.stop()
+                    self.logger.debug(f"Stopped local node: {node_name}")
+                # Ray actors会在进程结束时自动清理
+            except Exception as e:
+                self.logger.error(f"Error stopping node {node_name}: {e}")
+
+    def get_dag_info(self) -> Dict[str, Any]:
+        """获取DAG信息"""
+        local_nodes = [name for name, meta in self.nodes_metadata.items() 
+                      if meta["platform"] == "local"]
+        ray_nodes = [name for name, meta in self.nodes_metadata.items() 
+                    if meta["platform"] == "ray"]
+        
+        return {
+            "name": self.name,
+            "total_nodes": len(self.operators),
+            "local_nodes": local_nodes,
+            "ray_nodes": ray_nodes,
+            "spout_nodes": self.spout_nodes,
+            "connections": self.connections,
+            "node_dependencies": self.node_dependencies
+        }
+    
+    def run(self) -> Dict[str, List[str]]:
+        """
+        启动MixedDAG执行，将所有节点注册到对应的运行时
+        
+        Returns:
+            Dict: 包含各平台节点句柄的字典
+        """
+        if self.is_running:
+            self.logger.warning(f"MixedDAG '{self.name}' is already running")
+            return {"local_handles": self.local_handles, "ray_handles": self.ray_handles}
+        
+        self.logger.info(f"Starting MixedDAG '{self.name}' execution...")
+        
+        try:
+            # 获取运行时实例
+            from sage.core.runtime.local.local_runtime import LocalRuntime
+            from sage.core.runtime.ray.ray_runtime import RayRuntime
+            
+            local_runtime = LocalRuntime.get_instance()
+            ray_runtime = RayRuntime.get_instance()
+            
+            # 分离本地节点和Ray节点
+            local_nodes = []
+            ray_actors = []
+            ray_node_names = []
+            
+            for node_name, node_meta in self.nodes_metadata.items():
+                if node_meta["platform"] == "local":
+                    local_node = self.operators[node_name]
+                    local_nodes.append(local_node)
+                elif node_meta["platform"] == "ray":
+                    ray_actor = self.operators[node_name]
+                    ray_actors.append(ray_actor)
+                    ray_node_names.append(node_name)
+            
+            # 提交本地节点到LocalRuntime
+            if local_nodes:
+                self.logger.info(f"Submitting {len(local_nodes)} local nodes to LocalRuntime")
+                self.local_handles = local_runtime.submit_nodes(local_nodes)
+                
+                # 注册本地节点到TCP服务器（用于接收Ray Actor的数据）
+                for local_node in local_nodes:
+                    # 这里需要确保local_runtime知道节点名称映射
+                    # 实际上submit_nodes已经在running_nodes中注册了
+                    pass
+                
+                self.logger.info(f"Successfully submitted local nodes with handles: {self.local_handles}")
+            
+            # 提交Ray节点到RayRuntime
+            if ray_actors:
+                self.logger.info(f"Submitting {len(ray_actors)} Ray actors to RayRuntime")
+                self.ray_handles = ray_runtime.submit_actors(ray_actors, ray_node_names)
+                self.logger.info(f"Successfully submitted Ray actors with handles: {self.ray_handles}")
+            
+            # 启动所有节点
+            # 在submit时所有节点就都启动了
+            # self._start_all_nodes(local_runtime, ray_runtime)
+            
+            self.is_running = True
+            self.logger.info(f"MixedDAG '{self.name}' started successfully with "
+                           f"{len(self.local_handles)} local nodes and {len(self.ray_handles)} Ray actors")
+            
+            return {
+                "local_handles": self.local_handles,
+                "ray_handles": self.ray_handles,
+                "total_nodes": len(self.local_handles) + len(self.ray_handles)
+            }
+            
+        except Exception as e:
+            self.logger.error(f"Failed to start MixedDAG '{self.name}': {e}", exc_info=True)
+            # 清理已经提交的节点
+            self._cleanup_partial_submission(local_runtime, ray_runtime)
+            raise
\ No newline at end of file
diff --git a/sage/core/dag/ray/ray_dag_node.py b/sage/core/dag/ray/ray_dag_node.py
new file mode 100644
index 0000000..9f9a97b
--- /dev/null
+++ b/sage/core/dag/ray/ray_dag_node.py
@@ -0,0 +1,285 @@
+import ray
+import asyncio
+import logging
+import time
+from typing import Any, List, Optional, Dict, Tuple, TYPE_CHECKING, Type, Union
+from ray.actor import ActorHandle
+from sage.api.operator.base_operator_api import BaseFuction
+from sage.core.graph import GraphEdge, GraphNode
+from sage.core.io.emit_context import NodeType
+from sage.core.io.ray_emit_context import RayEmitContext
+from sage.utils.custom_logger import CustomLogger
+from sage.core.dag.local.local_dag_node import LocalDAGNode
+@ray.remote
+class RayDAGNode:
+    """
+    Ray Actor version of LocalDAGNode for distributed execution.
+    
+    Unlike local nodes, Ray actors don't need input buffers as Ray platform
+    maintains the request queue for actors automatically.
+    """
+    
+    def __init__(self, 
+                 name: str, 
+                 function_class: Type[BaseFuction],
+                 operator_config: Dict = None,
+                 is_spout: bool = False,
+                 session_folder: str = None) -> None:
+        """
+        Initialize Ray multiplexer DAG node.
+        
+        Args:
+            name: Node name
+            function_class: Operator class (not instance)
+            operator_config: Configuration for operator instantiation
+            is_spout: Whether this is a spout node
+            session_folder: Session folder for logging
+        """
+        self.name = name
+        self.function_class = function_class
+        self.operator_config = operator_config or {}
+        self.is_spout = is_spout
+        self._initialized = False
+        
+        # Running state management
+        self._running = False
+        self._stop_requested = False
+        
+        # Create logger first
+        self.logger = CustomLogger(
+            object_name=f"RayNode_{self.name}",
+            session_folder=session_folder,
+            log_level="DEBUG",
+            console_output=False,
+            file_output=True
+        )
+        
+        # Create emit context for mixed environment
+        self.emit_context = RayEmitContext(
+            self.name, 
+            ray_node_actor=self,
+            session_folder=session_folder
+        )
+        self._emit_context_injected = False
+        
+        self.logger.info(f"Created Ray actor node: {self.name}")
+
+    def _ensure_initialized(self):
+        """
+        Ensure that all runtime objects are initialized.
+        Called when the node actually starts running.
+        """
+        if self._initialized:
+            return
+        
+        # Create operator instance locally within the Ray actor
+        operator_config = self.operator_config.copy()
+        operator_config["session_folder"] = CustomLogger.get_session_folder()
+        
+        try:
+            self.operator = self.function_class(operator_config)
+            self.logger.debug(f"Created operator instance for {self.name}")
+        except Exception as e:
+            self.logger.error(f"Failed to create operator instance: {e}", exc_info=True)
+            raise
+        
+        # Inject emit context if operator supports it
+        if hasattr(self.operator, 'set_emit_context') and not self._emit_context_injected:
+            try:
+                self.operator.set_emit_context(self.emit_context)
+                self._emit_context_injected = True
+                self.logger.debug(f"Injected emit context for operator in node {self.name}")
+            except Exception as e:
+                self.logger.warning(f"Failed to inject emit context in node {self.name}: {e}")
+        
+        self._initialized = True
+
+    def add_downstream_node(self,output_channel:int, target_input_channel:int,   downstream_handle: Union[ActorHandle, str]):
+        """
+        添加下游节点到emit context
+        
+        Args:
+            output_edge: 输出边
+            downstream_operator: 下游操作符（Ray Actor或本地节点）
+        """
+        try:
+            if isinstance(downstream_handle, ActorHandle):
+                # 下游是Ray Actor
+                self.emit_context.add_downstream_target(
+                    output_channel=output_channel,
+                    node_type=NodeType.RAY_ACTOR,
+                    target_object=downstream_handle,
+                    target_input_channel=target_input_channel,
+                    node_name=f"RayActor_output_channel_{output_channel}"
+                )
+                self.logger.debug(f"Added Ray actor downstream: {self.name}[{output_channel}]")
+            
+            else:
+                # 下游是本地节点（通过TCP通信）
+                self.emit_context.add_downstream_target(
+                    output_channel=output_channel,
+                    node_type=NodeType.LOCAL,
+                    target_object=None,  # TCP通信不需要直接引用
+                    target_input_channel=target_input_channel,
+                    node_name=downstream_handle
+                )
+                self.logger.debug(f"Added local node downstream: {self.name}[{output_channel}] -> "
+                                f"{downstream_handle}[{target_input_channel}] (via TCP)")
+                
+        except Exception as e:
+            self.logger.error(f"Error adding downstream node: {e}", exc_info=True)
+            raise
+
+    def receive(self, input_channel: int, data: Any):
+        """
+        Receive data from upstream node and process it.
+        This method is called directly by upstream nodes (Ray actors or local nodes via TCP).
+        
+        Note: Ray platform automatically queues these method calls, so no input buffer needed.
+        
+        Args:
+            input_channel: The input channel number on this node
+            data: Data received from upstream
+        """
+        try:
+            # Ensure initialization on first call
+            self._ensure_initialized()
+            
+            if self._stop_requested:
+                self.logger.debug(f"Ignoring data on stopped node {self.name}")
+                return
+                
+            self.logger.debug(f"Received data in node {self.name}, channel {input_channel}")
+            
+            # Call operator's receive method with correct input channel
+            self.operator.receive(input_channel, data)
+            
+        except Exception as e:
+            self.logger.error(f"Error processing data in node {self.name}: {e}", exc_info=True)
+            raise
+
+    # 理论上来说，emit方法不会被调用，因为算子会直接调用emit_context.emit
+    # 但为了兼容性和未来可能的需求，这里保留emit方法
+    def emit(self, output_channel: int, data: Any):
+        """
+        Emit data to downstream nodes through the specified output channel.
+        Called by the operator through emit context.
+        
+        Args:
+            output_channel: This node's output channel number (-1 for all channels)
+            data: Data to emit
+        """
+        try:
+            self.emit_context.emit(output_channel, data)
+        except Exception as e:
+            self.logger.error(f"Error emitting data from {self.name}[out:{output_channel}]: {e}", exc_info=True)
+            raise
+
+    def start_spout(self):
+        """
+        Start the spout node execution.
+        For spout nodes, continuously call operator.receive with dummy data.
+        This runs in a loop until stop is requested.
+        """
+        if not self.is_spout:
+            self.logger.warning(f"start_spout called on non-spout node {self.name}")
+            return
+        
+        # Ensure initialization
+        self._ensure_initialized()
+            
+        self._running = True
+        self._stop_requested = False
+        
+        self.logger.info(f"Starting spout execution for node {self.name}")
+        
+        try:
+            while self._running and not self._stop_requested:
+                # For spout nodes, call operator.receive with dummy channel and data
+                self.operator.receive(0, None)
+                time.sleep(0.1)  # Small delay to prevent overwhelming
+                
+        except Exception as e:
+            self.logger.error(f"Error in spout node {self.name}: {e}", exc_info=True)
+            raise
+        finally:
+            self._running = False
+            self.logger.info(f"Spout execution stopped for node {self.name}")
+
+    def start(self):
+        """
+        Start the node. For spout nodes, this starts the generation loop.
+        For non-spout nodes, this just marks the node as ready to receive data.
+        """
+        self._ensure_initialized()
+        
+        if self.is_spout:
+            # Start spout execution asynchronously
+            self.start_spout()
+        else:
+            # For non-spout nodes, just mark as running
+            self._running = True
+            self._stop_requested = False
+            self.logger.info(f"Ray node {self.name} started and ready to receive data")
+
+    def stop(self):
+        """Stop the node execution."""
+        self._stop_requested = True
+        self._running = False
+        self.logger.info(f"Ray node {self.name} stopped")
+
+    def is_running(self):
+        """Check if the node is currently running."""
+        return self._running and not self._stop_requested
+
+    def get_name(self):
+        """Get node name."""
+        return self.name
+
+    def get_node_info(self) -> Dict[str, Any]:
+        """Get comprehensive node information for debugging."""
+        return {
+            "name": self.name,
+            "is_spout": self.is_spout,
+            "is_running": self.is_running(),
+            "stop_requested": self._stop_requested,
+            "initialized": self._initialized,
+            "operator_class": self.function_class.__name__ if self.function_class else None,
+            "downstream_targets": len(self.emit_context.downstream_channels) if hasattr(self, 'emit_context') else 0
+        }
+
+    def health_check(self) -> Dict[str, Any]:
+        """Perform health check and return status."""
+        try:
+            return {
+                "status": "healthy",
+                "node_name": self.name,
+                "is_running": self.is_running(),
+                "initialized": self._initialized,
+                "timestamp": time.time_ns()
+            }
+        except Exception as e:
+            return {
+                "status": "unhealthy",
+                "node_name": self.name,
+                "error": str(e),
+                "timestamp": time.time_ns()
+            }
+
+    def __getstate__(self):
+        """
+        Custom serialization to exclude non-serializable objects.
+        Ray handles most serialization automatically, but this helps with debugging.
+        """
+        state = self.__dict__.copy()
+        # Ray actors typically don't need custom serialization,
+        # but we can exclude logger if needed
+        return state
+
+    def __setstate__(self, state):
+        """
+        Custom deserialization to restore state.
+        """
+        self.__dict__.update(state)
+        # Mark as not initialized so runtime objects will be created when needed
+        self._initialized = False
\ No newline at end of file
diff --git a/sage/core/dag/ray/ray_multi_node.py b/sage/core/dag/ray/ray_multi_node.py
deleted file mode 100644
index 889eef2..0000000
--- a/sage/core/dag/ray/ray_multi_node.py
+++ /dev/null
@@ -1,225 +0,0 @@
-import ray
-import asyncio
-import logging
-from typing import Any, List, Optional, Dict, Tuple, TYPE_CHECKING, Type
-# from sage.archive.operator_wrapper import OperatorWrapper
-from sage.api.operator.base_operator_api import BaseFuction
-from sage.api.operator.base_operator_api import EmitContext
-from sage.utils.custom_logger import CustomLogger
-from ray.actor import ActorHandle  # 只在类型检查期间生效
-import time
-@ray.remote
-class RayMultiplexerDagNode:
-    """
-    Ray Actor version of MultiplexerDagNode for distributed execution.
-    """
-    
-    def __init__(self, 
-                 name: str, 
-                 operator_class: Type[BaseFuction],
-                 operator_config: Dict = None,
-                 is_spout: bool = False,
-                 session_folder: str = None) -> None:
-        self.name = name
-        self.operator_class = operator_class
-        self.operator_config = operator_config or {}
-        self.is_spout = is_spout
-
-        self.logger = CustomLogger(
-            object_name=f"RayNode_{self.name}",
-            session_folder=session_folder,
-            log_level="DEBUG",
-            console_output=False,
-            file_output=True
-        )
-
-        # 取消继承 root logger 的 stdout handler
-        # self.logger.propagate = False
-        """
-        Initialize Ray multiplexer DAG node.
-        
-        Args:
-            name: Node name
-            operator_class: Operator class (not instance)
-            operator_config: Configuration for operator instantiation
-            is_spout: Whether this is a spout node
-        """
-
-        
-        # Create operator instance locally within the Ray actor
-      
-
-        # Store downstream connections: output_channel -> [(downstream_actor, downstream_input_channel)]
-        self.downstream_connections: List[Tuple[ActorHandle, int]] = []
-
-        operator_config["session_folder"] = session_folder
-        self.operator = operator_class(operator_config)
-        
-        # Running state
-        self._running = False
-        self._stop_requested = False
-        
-        # Create emit context for Ray environment
-        self.emit_context = RayEmitContext(self.name, self)
-        
-        # Inject emit context if operator supports it
-        if hasattr(self.operator, 'set_emit_context'):
-            try:
-                self.operator.set_emit_context(self.emit_context)
-            except Exception as e:
-                pass
-
-
-    def add_downstream_connection(self, output_channel: int, downstream_actor:ActorHandle, 
-                                downstream_input_channel: int):
-        """
-        Add downstream connection mapping.
-        
-        Args:
-            output_channel: This node's output channel number
-            downstream_actor: Downstream Ray actor handle
-            downstream_input_channel: Downstream node's input channel number
-        """
-        if output_channel >= len(self.downstream_connections):
-            self.downstream_connections.extend([None] * (output_channel + 1 - len(self.downstream_connections)))
-        if self.downstream_connections[output_channel] is not None:
-            raise ValueError(
-                f"Output channel {output_channel} already has a downstream connection in node {self.name}"
-            )
-        
-        self.downstream_connections[output_channel] = (downstream_actor, downstream_input_channel)
-        
-        self.logger.debug(
-            f"Added downstream connection: {self.name}[out:{output_channel}] -> "
-            f"downstream_node[in:{downstream_input_channel}]"
-        )
-    
-    def receive(self, input_channel: int, data: Any):
-        """
-        Receive data from upstream node and process it.
-        This method is called directly by upstream Ray actors.
-        
-        Args:
-            input_channel: The input channel number on this node
-            data: Data received from upstream
-        """
-        try:
-            if self._stop_requested:
-                return
-                
-            # Call operator's receive method with correct input channel
-            self.logger.debug(f"Received data in node {self.name}, channel {input_channel}")
-            self.operator.receive(input_channel, data)
-            
-        except Exception as e:
-            self.logger.error(f"Error processing data in node {self.name}: {e}", exc_info=True)
-            raise
-    
-    def emit(self, output_channel: int, data: Any):
-        """
-        Emit data to downstream actors through the specified output channel.
-        Called by the operator through emit context.
-        
-        Args:
-            output_channel: This node's output channel number (-1 for all channels)
-            data: Data to emit
-        """
-        if output_channel == -1:
-            # Special case for broadcasting to all channels
-            for downstream_actor, downstream_input_channel in self.downstream_connections:
-                if downstream_actor is not None:  # Skip None entries
-                    try:
-                        # Asynchronously call downstream actor's receive method
-                        downstream_actor.receive.remote(downstream_input_channel, data)
-                        
-                        self.logger.debug(
-                            f"Emitted data from {self.name}[out:all] to "
-                            f"downstream[in:{downstream_input_channel}]"
-                        )
-                    except Exception as e:
-                        self.logger.error(
-                            f"Failed to emit data from {self.name}[out:all]: {e}"
-                        )
-        elif 0 <= output_channel < len(self.downstream_connections):
-            connection = self.downstream_connections[output_channel]
-            if connection is not None:
-                downstream_actor, downstream_input_channel = connection
-                try:
-                    # Asynchronously call downstream actor's receive method
-                    downstream_actor.receive.remote(downstream_input_channel, data)
-                    
-                    self.logger.debug(
-                        f"Emitted data from {self.name}[out:{output_channel}] to "
-                        f"downstream[in:{downstream_input_channel}]"
-                    )
-                except Exception as e:
-                    self.logger.error(
-                        f"Failed to emit data from {self.name}[out:{output_channel}]: {e}"
-                    )
-            else:
-                self.logger.warning(
-                    f"No downstream connection for output channel {output_channel} in node {self.name}"
-                )
-        else:
-            self.logger.warning(
-                f"Invalid output channel {output_channel} in node {self.name}"
-            )
-    
-    def get_downstream_connections(self) -> List[Tuple[ActorHandle, int]]:
-        """Get all downstream connections for debugging."""
-        return self.downstream_connections.copy()
-    
-    def start_spout(self):
-        """
-        Start the spout node execution.
-        For spout nodes, continuously call operator.receive with dummy data.
-        """
-        if not self.is_spout:
-            self.logger.warning(f"start_spout called on non-spout node {self.name}")
-            return
-            
-        self._running = True
-        self._stop_requested = False
-        
-        try:
-            while self._running and not self._stop_requested:
-                # For spout, we typically call with channel 0 and None data
-                self.operator.receive(0, None)
-                time.sleep(1)
-        except Exception as e:
-            self.logger.error(f"Error in spout node {self.name}: {e}", exc_info=True)
-            raise
-        finally:
-            self._running = False
-    
-    def stop(self):
-        """Stop the node execution."""
-        self._stop_requested = True
-        self._running = False
-        # self.logger.info(f"Ray node {self.name} stopped")
-    
-    def is_running(self):
-        """Check if the node is currently running."""
-        return self._running
-    
-    def get_name(self):
-        """Get node name."""
-        return self.name
-
-
-class RayEmitContext(EmitContext):
-    """
-    Ray-specific emit context that uses direct actor calls instead of message queues.
-    """
-    
-    def __init__(self, node_name: str, ray_node_actor):
-        super().__init__(node_name)
-        self.ray_node_actor = ray_node_actor
-    
-    def emit(self, channel: int, data: Any):
-        """Emit data through Ray actor's emit method."""
-        self.ray_node_actor.emit(channel, data)
-    
-    def add_downstream_channel(self, channel):
-        """For Ray actors, downstream channels are managed differently."""
-        pass  # No-op for Ray implementation
\ No newline at end of file
diff --git a/sage/core/engine.py b/sage/core/engine.py
index 19a9768..7f3595a 100644
--- a/sage/core/engine.py
+++ b/sage/core/engine.py
@@ -2,33 +2,27 @@ from typing import Type, TYPE_CHECKING, Union, Any, TYPE_CHECKING
 from sage.core.compiler.query_compiler import QueryCompiler
 from sage.core.runtime.runtime_manager import RuntimeManager
 from sage.utils.custom_logger import CustomLogger
+from sage.core.dag.mixed_dag import MixedDAG
 import threading, typing, logging
 
 class Engine:
     _instance = None
     _lock = threading.Lock()
-    def __init__(self,generate_func = None, session_folder: str = None):
-        if session_folder is None:
-            # 如果没有提供 session_folder，则创建一个新的会话文件夹
-            # 这将确保每次运行时都有独立的日志和数据存储
-            session_folder = CustomLogger.create_session_folder()
-        # 如果提供了 session_folder，则使用它
-        self.session_folder = session_folder
+    def __init__(self):
 
         # 确保只初始化一次
         if hasattr(self, "_initialized"):
             return
         self._initialized = True
         # self.dag_manager = DAGManager() # deprecated
-        self.runtime_manager = RuntimeManager(self.session_folder)
-        self.compiler= QueryCompiler(generate_func=generate_func, session_folder=self.session_folder)
+        self.runtime_manager = RuntimeManager.get_instance()
+        self.compiler= QueryCompiler()
         from sage.core.graph import SageGraph
         self.graphs:dict[str, SageGraph] = {}  # 存储 pipeline 名称到 SageGraph 的映射
         self.dags:dict = {} # 存储name到dag的映射，其中dag的类型为DAG或RayDAG
 
         self.logger = CustomLogger(
             object_name=f"SageEngine",
-            session_folder=self.session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
@@ -42,7 +36,7 @@ class Engine:
     # 用来获取类的唯一实例
     # 同一个进程中只存在唯一的实例
     @classmethod
-    def get_instance(cls,generate_func):
+    def get_instance(cls):
         # 双重检查锁确保线程安全
         if cls._instance is None:
             with cls._lock:
@@ -50,7 +44,7 @@ class Engine:
 
                     # 绕过 __new__ 的异常，直接创建实例
                     instance = super().__new__(cls)
-                    instance.__init__(generate_func)
+                    instance.__init__()
                     cls._instance = instance
         return cls._instance
 
@@ -70,6 +64,23 @@ class Engine:
             self.logger.info(f"Graph '{graph.name}' submitted to runtime manager.")
             # 通过运行时管理器获取对应平台的运行时并提交任务
             task_handle = self.runtime_manager.submit(dag) 
+        except Exception as e:
+            self.logger.info(f"Failed to submit graph '{graph.name}': {e}")
+            raise
+    
+    def submit_mixed_pipeline(self, pipeline, config=None):
+        from sage.core.graph import SageGraph
+        graph = SageGraph(pipeline, config)
+        self.graphs[graph.name] = graph
+        if config:
+            graph.config.update(config)
+        try:
+            self.logger.info(f"Received mixed graph '{graph.name}' with {len(graph.nodes)} nodes")
+            # 编译图
+            mixed_dag = MixedDAG(graph)
+            self.dags[mixed_dag.name] = mixed_dag  # 存储 DAG 到字典中
+            mixed_dag.run()
+            self.logger.info(f"Mixed graph '{graph.name}' submitted to runtime manager.")
         except Exception as e:
             self.logger.info(f"Failed to submit graph '{graph.name}': {e}")
             raise
\ No newline at end of file
diff --git a/sage/core/graph/sage_graph.py b/sage/core/graph/sage_graph.py
index 3978d5a..57e9d1f 100644
--- a/sage/core/graph/sage_graph.py
+++ b/sage/core/graph/sage_graph.py
@@ -9,13 +9,13 @@ from sage.utils.custom_logger import CustomLogger
 
 
 class GraphNode:
-    def __init__(self,name:str, operator_class: Type[BaseFuction], type:str, operator_config: Dict = None):
+    def __init__(self,name:str, function_class: Type[BaseFuction], type:str, operator_config: Dict = None):
         self.name: str = name
         self.type: str = type # "normal "or "source" or "sink"
         self.config: Dict = operator_config
         self.input_channels: list[GraphEdge] = []
         self.output_channels: list[GraphEdge] = []
-        self.operator: Type[BaseFuction] = operator_class
+        self.operator: Type[BaseFuction] = function_class
         pass
 
 class GraphEdge:
@@ -30,10 +30,10 @@ class GraphEdge:
         self.upstream_node:GraphNode = upstream_node
         self.upstream_channel: int = upstream_channel
         self.downstream_node:GraphNode = None
-        self.downstream_channnel: int = None
+        self.downstream_channel: int = None
 
 class SageGraph:
-    def __init__(self, pipeline:Pipeline, config: dict = None, session_folder: str = None):
+    def __init__(self, pipeline:Pipeline, config: dict = None):
         """
         Initialize the NodeGraph with a name and optional configuration.
         Args:
@@ -50,7 +50,6 @@ class SageGraph:
 
         self.logger = CustomLogger(
             object_name=f"SageGraph_{self.name}",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=True,
             file_output=True
@@ -118,7 +117,7 @@ class SageGraph:
                     node_name=node_name,
                     input_streams=connection_info['input_edges'],
                     output_streams=connection_info['output_edges'],
-                    operator_class=stream.operator,
+                    function_class=stream.operator,
                     operator_config=stream.config, 
                     node_type=stream.node_type
                 )
@@ -272,7 +271,7 @@ class SageGraph:
                  node_name: str,
                  input_streams: Union[str, List[str]], 
                  output_streams: Union[str, List[str]], 
-                 operator_class: Type[BaseFuction],
+                 function_class: Type[BaseFuction],
                  operator_config: Dict = None, 
                  node_type: str = "normal") -> GraphNode:
         """
@@ -296,7 +295,7 @@ class SageGraph:
             output_streams = []
 
         # 创建节点
-        node = GraphNode(node_name, operator_class, node_type, operator_config)
+        node = GraphNode(node_name, function_class, node_type, operator_config)
         # 检查节点名是否已存在
         if node.name in self.nodes:
             raise ValueError(f"Node with name '{node.name}' already exists")
@@ -313,7 +312,7 @@ class SageGraph:
             
             # 连接边到当前节点
             edge.downstream_node = node
-            edge.downstream_channnel = i
+            edge.downstream_channel = i
             node.input_channels.append(edge)
 
         # 处理输出边（创建新的空边）
diff --git a/sage/core/io/emit_context.py b/sage/core/io/emit_context.py
new file mode 100644
index 0000000..788b9e8
--- /dev/null
+++ b/sage/core/io/emit_context.py
@@ -0,0 +1,116 @@
+from typing import TypeVar, Generic, Callable, Any, List, Dict, Union, Tuple, Literal
+from abc import ABC, abstractmethod
+from enum import Enum
+import ray
+from ray.actor import ActorHandle
+import socket
+import json
+import pickle
+import threading
+from sage.utils.custom_logger import CustomLogger
+
+class NodeType(Enum):
+    LOCAL = "local"
+    RAY_ACTOR = "ray_actor"
+
+class DownstreamTarget:
+    """下游目标节点的封装"""
+    def __init__(self, 
+                 node_type: NodeType, 
+                 target_object: Any, 
+                 target_input_channel: int,
+                 node_name: str = None):
+        self.node_type = node_type
+        self.target_object = target_object
+        self.target_input_channel = target_input_channel
+        self.node_name = node_name or str(target_object)
+
+class BaseEmitContext(ABC):
+    """
+    基础Emit Context抽象类
+    支持混合环境中本地和Ray Actor之间的通信
+    """
+    
+    def __init__(self, node_name: str, session_folder: str = None):
+        self.node_name = node_name
+        self.downstream_channels: Dict[int, DownstreamTarget] = {}
+        # Dict[自身出口号, DownstreamTarget]
+        
+        self.logger = CustomLogger(
+            object_name=f"EmitContext_{node_name}",
+            session_folder=session_folder,
+            log_level="DEBUG"
+        )
+    
+    def add_downstream_target(self, 
+                            output_channel: int,
+                            node_type: NodeType, 
+                            target_object: Any, 
+                            target_input_channel: int,
+                            node_name: str = None) -> None:
+        """
+        添加下游目标节点
+        
+        Args:
+            output_channel: 自身的输出通道号
+            node_type: 下游节点类型
+            target_object: 下游节点对象
+            target_input_channel: 下游节点的输入通道号
+            node_name: 下游节点名称
+        """
+        target = DownstreamTarget(node_type, target_object, target_input_channel, node_name)
+        self.downstream_channels[output_channel] = target
+        
+        self.logger.debug(f"Added downstream target: {self.node_name}[out:{output_channel}] -> "
+                         f"{node_name}[in:{target_input_channel}] (type: {node_type.value})")
+    
+    def emit(self, channel: int, data: Any) -> None:
+        """
+        向指定下游通道发送数据
+        
+        Args:
+            channel: 下游通道索引，-1表示广播到所有通道
+            data: 要发送的数据
+        """
+        if channel == -1:
+            # 广播到所有下游通道
+            for output_channel, target in self.downstream_channels.items():
+                self._route_and_send(target, data)
+        elif channel in self.downstream_channels:
+            target = self.downstream_channels[channel]
+            self._route_and_send(target, data)
+        else:
+            self.logger.warning(f"Channel index {channel} out of range for node {self.node_name}")
+    
+    def _route_and_send(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        根据目标类型路由并发送数据
+        
+        Args:
+            target: 目标节点信息
+            data: 要发送的数据
+        """
+        try:
+            if target.node_type == NodeType.LOCAL:
+                self._send_to_local(target, data)
+            elif target.node_type == NodeType.RAY_ACTOR:
+                self._send_to_ray_actor(target, data)
+            else:
+                self.logger.error(f"Unknown target type: {target.node_type}")
+        except Exception as e:
+            self.logger.error(f"Failed to send data to {target.node_name}: {e}", exc_info=True)
+    
+    @abstractmethod
+    def _send_to_local(self, target: DownstreamTarget, data: Any) -> None:
+        """发送数据到本地节点"""
+        pass
+    
+    @abstractmethod
+    def _send_to_ray_actor(self, target: DownstreamTarget, data: Any) -> None:
+        """发送数据到Ray Actor"""
+        pass
+
+
+
+
+
diff --git a/sage/core/io/local_emit_context.py b/sage/core/io/local_emit_context.py
new file mode 100644
index 0000000..9102e91
--- /dev/null
+++ b/sage/core/io/local_emit_context.py
@@ -0,0 +1,67 @@
+from typing import TypeVar, Generic, Callable, Any, List, Dict, Union, Tuple, Literal
+from abc import ABC, abstractmethod
+from enum import Enum
+import ray
+from ray.actor import ActorHandle
+import socket
+import json
+import pickle
+import threading
+from sage.utils.custom_logger import CustomLogger
+from sage.core.io.emit_context import BaseEmitContext, DownstreamTarget, NodeType
+
+class LocalEmitContext(BaseEmitContext):
+    """
+    本地DAG节点使用的Emit Context
+    支持向本地节点的输入缓冲区写入数据，向Ray Actor发送远程调用
+    """
+    
+    def __init__(self, node_name: str, session_folder: str = None):
+        super().__init__(node_name, session_folder)
+    
+    def _send_to_local(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        向本地节点的输入缓冲区写入数据包
+        
+        Args:
+            target: 目标本地节点
+            data: 数据
+        """
+        try:
+            # 向下游本地节点的输入缓冲区写入 (输入channel, 数据) 包
+            data_packet = (target.target_input_channel, data)
+            
+            # 假设本地节点有input_buffer属性用于接收数据
+            if hasattr(target.target_object, 'input_buffer'):
+                target.target_object.input_buffer.put(data_packet)
+            elif hasattr(target.target_object, 'put'):
+                # 或者有put方法
+                target.target_object.put(data_packet)
+            else:
+                raise AttributeError(f"Local node {target.node_name} has no input_buffer or put method")
+                
+            self.logger.debug(f"Written data packet to local node {target.node_name}[in:{target.target_input_channel}] input buffer")
+            
+        except Exception as e:
+            self.logger.error(f"Error writing data to local node {target.node_name} input buffer: {e}")
+            raise
+    
+    def _send_to_ray_actor(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        向Ray Actor发送远程调用
+        
+        Args:
+            target: 目标Ray Actor
+            data: 数据
+        """
+        try:
+            if isinstance(target.target_object, ActorHandle):
+                # 直接调用Ray Actor的remote方法
+                target.target_object.receive.remote(target.target_input_channel, data)
+                self.logger.debug(f"Sent remote call to Ray actor {target.node_name}[in:{target.target_input_channel}]")
+            else:
+                raise TypeError(f"Expected ActorHandle for Ray actor, got {type(target.target_object)}")
+                
+        except Exception as e:
+            self.logger.error(f"Error sending remote call to Ray actor {target.node_name}: {e}")
+            raise
\ No newline at end of file
diff --git a/sage/core/io/ray_emit_context.py b/sage/core/io/ray_emit_context.py
new file mode 100644
index 0000000..c1b347a
--- /dev/null
+++ b/sage/core/io/ray_emit_context.py
@@ -0,0 +1,111 @@
+from typing import TypeVar, Generic, Callable, Any, List, Dict, Union, Tuple, Literal
+from abc import ABC, abstractmethod
+from enum import Enum
+import ray
+from ray.actor import ActorHandle
+import socket
+import json
+import pickle
+import threading
+from sage.utils.custom_logger import CustomLogger
+from sage.core.io.emit_context import BaseEmitContext, DownstreamTarget, NodeType
+import time
+
+
+
+class RayEmitContext(BaseEmitContext):
+    """
+    Ray Actor使用的Emit Context
+    支持向Ray Actor发送远程调用，向本地节点发送TCP包
+    """
+    
+    def __init__(self, node_name: str, ray_node_actor=None,
+                 local_tcp_host: str = "localhost", 
+                 local_tcp_port: int = 9999, session_folder: str = None):
+        super().__init__(node_name, session_folder)
+        self.ray_node_actor = ray_node_actor
+        self.local_tcp_host = local_tcp_host
+        self.local_tcp_port = local_tcp_port
+        self._tcp_socket = None
+        self._socket_lock = threading.Lock()
+    
+    def _get_tcp_connection(self) -> socket.socket:
+        """获取到本地的TCP连接（懒加载）"""
+        if self._tcp_socket is None:
+            with self._socket_lock:
+                if self._tcp_socket is None:
+                    try:
+                        self._tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+                        self._tcp_socket.connect((self.local_tcp_host, self.local_tcp_port))
+                        self.logger.info(f"Ray actor connected to local TCP server at {self.local_tcp_host}:{self.local_tcp_port}")
+                    except Exception as e:
+                        self.logger.error(f"Failed to connect to local TCP server: {e}")
+                        raise
+        return self._tcp_socket
+    
+    def _send_to_local(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        向本地节点发送TCP包
+        
+        Args:
+            target: 目标本地节点
+            data: 数据
+        """
+        try:
+            # 构造TCP消息包
+            message = {
+                "type": "ray_to_local",
+                "source_actor": self.node_name,
+                "target_node": target.node_name,
+                "target_channel": target.target_input_channel,
+                "data": data,
+                "timestamp": time.time_ns()
+            }
+            
+            # 序列化消息
+            serialized_data = pickle.dumps(message)
+            message_size = len(serialized_data)
+            
+            # 发送消息长度，然后发送消息内容
+            tcp_conn = self._get_tcp_connection()
+            tcp_conn.sendall(message_size.to_bytes(4, byteorder='big'))
+            tcp_conn.sendall(serialized_data)
+            
+            self.logger.debug(f"Sent TCP packet to local node {target.node_name}[in:{target.target_input_channel}]")
+            
+        except Exception as e:
+            self.logger.error(f"Error sending TCP packet to local node {target.node_name}: {e}")
+            # 重置连接以便下次重试
+            with self._socket_lock:
+                if self._tcp_socket:
+                    self._tcp_socket.close()
+                    self._tcp_socket = None
+            raise
+    
+    def _send_to_ray_actor(self, target: DownstreamTarget, data: Any) -> None:
+        """
+        向其他Ray Actor发送远程调用
+        
+        Args:
+            target: 目标Ray Actor
+            data: 数据
+        """
+        try:
+            if isinstance(target.target_object, ActorHandle):
+                # 直接调用Ray Actor的remote方法
+                target.target_object.receive.remote(target.target_input_channel, data)
+                self.logger.debug(f"Sent remote call to Ray actor {target.node_name}[in:{target.target_input_channel}]")
+            else:
+                raise TypeError(f"Expected ActorHandle for Ray actor, got {type(target.target_object)}")
+                
+        except Exception as e:
+            self.logger.error(f"Error sending remote call to Ray actor {target.node_name}: {e}")
+            raise
+    
+    def close(self):
+        """关闭TCP连接"""
+        with self._socket_lock:
+            if self._tcp_socket:
+                self._tcp_socket.close()
+                self._tcp_socket = None
+                self.logger.info("Closed TCP connection to local server")
diff --git a/sage/core/runtime/base_runtime.py b/sage/core/runtime/base_runtime.py
index d1858a2..69e261e 100644
--- a/sage/core/runtime/base_runtime.py
+++ b/sage/core/runtime/base_runtime.py
@@ -3,18 +3,18 @@ from abc import ABC, abstractmethod
 class BaseRuntime(ABC):
     """执行后端抽象接口"""
     
-    @abstractmethod
-    def submit_task(self, task) -> str:
-        """提交任务执行，返回任务句柄"""
-        pass
+    # @abstractmethod
+    # def submit_task(self, task) -> str:
+    #     """提交任务执行，返回任务句柄"""
+    #     pass
     
-    @abstractmethod
-    def stop_task(self, task_handle: str):
-        """停止指定任务"""
-        pass
+    # @abstractmethod
+    # def stop_task(self, task_handle: str):
+    #     """停止指定任务"""
+    #     pass
     
-    @abstractmethod
-    def get_status(self, task_handle: str):
-        """获取任务状态"""
-        pass
+    # @abstractmethod
+    # def get_status(self, task_handle: str):
+    #     """获取任务状态"""
+    #     pass
 
diff --git a/sage/core/runtime/local/local_runtime.py b/sage/core/runtime/local/local_runtime.py
index 1be58a0..986345b 100644
--- a/sage/core/runtime/local/local_runtime.py
+++ b/sage/core/runtime/local/local_runtime.py
@@ -2,24 +2,41 @@ from sage.core.runtime import BaseRuntime
 from sage.core.runtime.local.local_scheduling_strategy import SchedulingStrategy, ResourceAwareStrategy, PriorityStrategy
 from sage.core.runtime.local.local_task import StreamingTask, OneshotTask, BaseTask
 from sage.core.runtime.local.local_slot import Slot
-from sage.core.dag.local.dag import DAG
+from sage.core.dag.local.local_dag_node import LocalDAGNode
 from sage.utils.custom_logger import CustomLogger
-import logging
+import threading
+import socket
+import pickle
+import time
+from typing import Dict, Optional, Any, List
 
 class LocalRuntime(BaseRuntime):
     """本地线程池执行后端"""
     
-    def __init__(self, max_slots=4, scheduling_strategy=None, session_folder:str = None):
-        self.session_folder = session_folder
+    _instance = None
+    _lock = threading.Lock()
+
+
+    def __init__(self, max_slots=4, scheduling_strategy=None,  tcp_host: str = "localhost", tcp_port: int = 9999):
+        # 确保只初始化一次
+        if hasattr(self, "_initialized"):
+            return
+        self._initialized = True
         self.name = "LocalRuntime"
         self.available_slots = [Slot(slot_id=i) for i in range(max_slots)]
-        self.task_to_slot = {}
-        self.task_to_handle = {}  # task -> handle映射
-        self.handle_to_task = {}  # handle -> task映射
+        self.tcp_host = tcp_host  # 添加这行
+        self.tcp_port = tcp_port  # 添加这行
+        # 节点管理
+        self.running_nodes: Dict[str, LocalDAGNode] = {}  # 正在运行的节点表
+        self.node_to_slot: Dict[LocalDAGNode, int] = {}  # 节点到slot的映射
+        self.node_to_handle: Dict[LocalDAGNode, str] = {}  # 节点到handle的映射
+        self.handle_to_node: Dict[str, LocalDAGNode] = {}  # handle到节点的映射
         self.next_handle_id = 0
+
+
+
         self.logger = CustomLogger(
             object_name=f"LocalRuntime",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
@@ -29,74 +46,336 @@ class LocalRuntime(BaseRuntime):
             self.scheduling_strategy = ResourceAwareStrategy()
         else:
             self.scheduling_strategy = scheduling_strategy
+            # 启动TCP服务器
+        self._start_tcp_server()
+
+    def __new__(cls, max_slots=4, scheduling_strategy=None, session_folder: str = None,  
+                tcp_host: str = "localhost", tcp_port: int = 9999):
+        # 禁止直接实例化
+        raise RuntimeError("请通过 get_instance() 方法获取实例")
+    
+    @classmethod
+    def get_instance(cls, max_slots=4, scheduling_strategy=None, 
+                     tcp_host: str = "localhost", tcp_port: int = 9999):
+        """获取LocalRuntime的唯一实例"""
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # 绕过 __new__ 的异常，直接创建实例
+                    instance = super().__new__(cls)
+                    instance.__init__(max_slots, scheduling_strategy, tcp_host, tcp_port)
+                    cls._instance = instance
+        return cls._instance
+    
+    @classmethod
+    def reset_instance(cls):
+        """重置实例（主要用于测试）"""
+        with cls._lock:
+            if cls._instance:
+                cls._instance.shutdown()
+                cls._instance = None
+
+
+
+
+    def _start_tcp_server(self):
+        """启动TCP服务器用于接收Ray Actor的数据"""
+        try:
+            self.tcp_server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            self.tcp_server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+            self.tcp_server_socket.bind((self.tcp_host, self.tcp_port))
+            self.tcp_server_socket.listen(10)
+            
+            self.tcp_running = True
+            self.tcp_server_thread = threading.Thread(
+                target=self._tcp_server_loop,
+                name="TCPServerThread"
+            )
+            self.tcp_server_thread.daemon = True
+            self.tcp_server_thread.start()
+            
+            self.logger.info(f"TCP server started on {self.tcp_host}:{self.tcp_port}")
+            
+        except Exception as e:
+            self.logger.error(f"Failed to start TCP server: {e}")
+            raise
+    def _tcp_server_loop(self):
+        """TCP服务器主循环"""
+        self.logger.debug("TCP server loop started")
+        
+        while self.tcp_running:
+            try:
+                client_socket, address = self.tcp_server_socket.accept()
+                self.logger.debug(f"New TCP client connected from {address}")
+                
+                # 在新线程中处理客户端
+                client_thread = threading.Thread(
+                    target=self._handle_tcp_client,
+                    args=(client_socket, address),
+                    name=f"TCPClient-{address}"
+                )
+                client_thread.daemon = True
+                client_thread.start()
+                
+            except Exception as e:
+                if self.tcp_running:
+                    self.logger.error(f"Error accepting TCP connection: {e}")
+        
+        self.logger.debug("TCP server loop stopped")
+
+    def _handle_tcp_client(self, client_socket: socket.socket, address):
+        """处理TCP客户端连接和消息"""
+        try:
+            while self.tcp_running:
+                # 读取消息长度
+                size_data = client_socket.recv(4)
+                if not size_data:
+                    break
+                
+                message_size = int.from_bytes(size_data, byteorder='big')
+                
+                # 读取消息内容
+                message_data = b''
+                while len(message_data) < message_size:
+                    chunk = client_socket.recv(message_size - len(message_data))
+                    if not chunk:
+                        break
+                    message_data += chunk
+                
+                if len(message_data) != message_size:
+                    self.logger.warning(f"Incomplete message received from {address}")
+                    continue
+                
+                # 反序列化并处理消息
+                try:
+                    message = pickle.loads(message_data)
+                    self._process_tcp_message(message, address)
+                except Exception as e:
+                    self.logger.error(f"Error processing message from {address}: {e}")
+                
+        except Exception as e:
+            self.logger.error(f"Error handling TCP client {address}: {e}")
+        finally:
+            client_socket.close()
+            self.logger.debug(f"TCP client {address} disconnected")
+    
+    def _process_tcp_message(self, message: Dict[str, Any], client_address):
+        """
+        处理来自Ray Actor的TCP消息
+        
+        Args:
+            message: 包含消息内容的字典
+            client_address: 客户端地址
+        """
+        try:
+            message_type = message.get("type")
+            
+            if message_type == "ray_to_local":
+                # Ray Actor发送给本地节点的数据
+                target_node_name = message["target_node"]
+                target_channel = message["target_channel"]
+                data = message["data"]
+                source_actor = message.get("source_actor", "unknown")
+                
+                # 查找目标节点
+                if target_node_name in self.running_nodes:
+                    target_node = self.running_nodes[target_node_name]
+                    
+                    # 将数据放入目标节点的输入缓冲区
+                    data_packet = (target_channel, data)
+                    target_node.put(data_packet)
+                    
+                    self.logger.debug(f"Delivered TCP message: {source_actor} -> "
+                                    f"{target_node_name}[in:{target_channel}]")
+                else:
+                    self.logger.warning(f"Target node '{target_node_name}' not found for TCP message from {client_address}")
+            else:
+                self.logger.warning(f"Unknown TCP message type: {message_type}")
+                
+        except Exception as e:
+            self.logger.error(f"Error processing TCP message: {e}", exc_info=True)
     
-    def submit_task(self, local_dag:DAG):
+    def submit_node(self, node: LocalDAGNode) -> str:
         """
-        提交到本地后端执行
+        提交单个MultiplexerDagNode到本地运行时
         
         Args:
-            local_dag: local_dag 实例
+            node: MultiplexerDagNode实例
             
         Returns:
             str: 任务句柄
         """
-
-        if not isinstance(local_dag, DAG):
-            raise TypeError("local_dag must be an instance of DAG")
+        if not isinstance(node, LocalDAGNode):
+            raise TypeError("Expected LocalDAGNode instance")
+        
+        self.logger.info(f"Submitting node '{node.name}' to {self.name}")
         
-        self.logger.info(f"Submitting DAG '{local_dag.name}' to {self.name}")
-
-        print(local_dag.strategy)
         try:
-            if local_dag.strategy == "oneshot":
-                task = OneshotTask(local_dag, session_folder=self.session_folder)
-                task.execute()
-                # self.logger.debug(f"OneshotTask submitted to {self.name} with handle: {task_handle}")
-            elif local_dag.strategy == "streaming":
-                for node in local_dag.nodes:
-                    task = StreamingTask(node, local_dag.working_config, session_folder=self.session_folder)
-                    task_handle = self.submit_node(task)
-                    #self.task_handles[dag_id].append(task_handle)
-                    self.logger.debug(f"DAGNode {node.name} submitted to {self.name} with handle: {task_handle}")
+            # 创建StreamingTask包装节点
+            task = StreamingTask(node, {})
+            
+            # 选择slot并提交
+            slot_id = self.scheduling_strategy.select_slot(task, self.available_slots)
+            success = self.available_slots[slot_id].submit_task(task)
+            
+            if success:
+                # 生成handle
+                handle = f"local_node_{self.next_handle_id}"
+                self.next_handle_id += 1
+                
+                # 更新映射关系
+                self.running_nodes[node.name] = node
+                self.node_to_slot[node] = slot_id
+                self.node_to_handle[node] = handle
+                self.handle_to_node[handle] = node
+                
+                self.logger.info(f"Node '{node.name}' submitted successfully with handle: {handle}")
+                return handle
             else:
-                raise ValueError(f"Unsupported strategy: {local_dag.strategy}")
+                raise RuntimeError(f"Failed to submit node '{node.name}' to slot {slot_id}")
+                
         except Exception as e:
-            self.logger.error(f"Failed to submit DAG '{local_dag.name}' to {self.name}: {e}")
-            raise RuntimeError(f"Failed to submit DAG '{local_dag.name}' to {self.name}: {e}")
+            self.logger.error(f"Failed to submit node '{node.name}': {e}")
+            raise
+    
+    def submit_nodes(self, nodes: List[LocalDAGNode]) -> List[str]:
+        """
+        批量提交多个节点
+        
+        Args:
+            nodes: MultiplexerDagNode列表
+            
+        Returns:
+            List[str]: 任务句柄列表
+        """
+        handles = []
+        for node in nodes:
+            try:
+                handle = self.submit_node(node)
+                handles.append(handle)
+            except Exception as e:
+                self.logger.error(f"Failed to submit node '{node.name}': {e}")
+                # 停止已经提交的节点
+                for h in handles:
+                    self.stop_node(h)
+                raise
+        
+        self.logger.info(f"Successfully submitted {len(handles)} nodes")
+        return handles
 
 
-    def submit_node(self, task: BaseTask) -> str:
-        """提交任务到本地线程池"""
-        slot_id = self.scheduling_strategy.select_slot(task, self.available_slots)
-        success = self.available_slots[slot_id].submit_task(task)
-        
-        if success:
-            handle = f"local_task_{self.next_handle_id}"
-            self.next_handle_id += 1
-            
-            self.task_to_slot[task] = slot_id
-            self.task_to_handle[task] = handle
-            self.handle_to_task[handle] = task
-            return handle
-        else:
-            raise RuntimeError(f"Failed to submit task to slot {slot_id}")
-    
-    def stop_task(self, task_handle: str):
-        """停止本地任务"""
-        if task_handle not in self.handle_to_task:
+    def stop_node(self, node_handle: str):
+        """
+        停止指定的节点
+        
+        Args:
+            node_handle: 节点句柄
+        """
+        if node_handle not in self.handle_to_node:
+            self.logger.warning(f"Node handle '{node_handle}' not found")
             return
+        
+        try:
+            node = self.handle_to_node[node_handle]
+            slot_id = self.node_to_slot[node]
+            
+            # 停止节点
+            node.stop()
             
-        task = self.handle_to_task[task_handle]
-        slot_id = self.task_to_slot[task]
-        self.available_slots[slot_id].stop(task)
-        
-        # 清理映射关系
-        self.task_to_slot.pop(task, None)
-        self.task_to_handle.pop(task, None)
-        self.handle_to_task.pop(task_handle, None)
+            # 从slot中移除任务
+            # 这里需要找到对应的task
+            for task in self.available_slots[slot_id].running_tasks:
+                if hasattr(task, 'node') and task.node == node:
+                    self.available_slots[slot_id].stop(task)
+                    break
+            
+            # 清理映射关系
+            self.running_nodes.pop(node.name, None)
+            self.node_to_slot.pop(node, None)
+            self.node_to_handle.pop(node, None)
+            self.handle_to_node.pop(node_handle, None)
+            
+            self.logger.info(f"Node '{node.name}' stopped successfully")
+            
+        except Exception as e:
+            self.logger.error(f"Error stopping node with handle '{node_handle}': {e}")
+    
+    def stop_all_nodes(self):
+        """停止所有运行中的节点"""
+        self.logger.info("Stopping all nodes...")
+        
+        handles_to_stop = list(self.handle_to_node.keys())
+        for handle in handles_to_stop:
+            self.stop_node(handle)
+        
+        self.logger.info(f"Stopped {len(handles_to_stop)} nodes")
     
-    def get_status(self, task_handle: str):
-        """获取本地任务状态"""
-        if task_handle not in self.handle_to_task:
+    def get_node_status(self, node_handle: str) -> Dict[str, Any]:
+        """
+        获取节点状态
+        
+        Args:
+            node_handle: 节点句柄
+            
+        Returns:
+            Dict: 节点状态信息
+        """
+        if node_handle not in self.handle_to_node:
             return {"status": "not_found"}
-        return {"status": "running", "backend": "local"}
+        
+        node = self.handle_to_node[node_handle]
+        slot_id = self.node_to_slot[node]
+        
+        return {
+            "status": "running",
+            "node_name": node.name,
+            "is_spout": node.is_spout,
+            "slot_id": slot_id,
+            "backend": "local",
+            "handle": node_handle
+        }
+    
+    def get_running_nodes(self) -> List[str]:
+        """获取所有运行中的节点名称"""
+        return list(self.running_nodes.keys())
+    
+    def get_node_by_name(self, node_name: str) -> Optional[LocalDAGNode]:
+        """根据名称获取节点"""
+        return self.running_nodes.get(node_name)
+    
+    def get_runtime_info(self) -> Dict[str, Any]:
+        """获取运行时信息"""
+        return {
+            "name": self.name,
+            "tcp_server": f"{self.tcp_host}:{self.tcp_port}",
+            "running_nodes_count": len(self.running_nodes),
+            "running_nodes": list(self.running_nodes.keys()),
+            "available_slots": len(self.available_slots),
+            "used_slots": len(self.node_to_slot),
+            "tcp_running": self.tcp_running
+        }
+    
+    def shutdown(self):
+        """关闭运行时和所有资源"""
+        self.logger.info("Shutting down LocalRuntime...")
+        
+        # 停止所有节点
+        self.stop_all_nodes()
+        
+        # 关闭TCP服务器
+        self.tcp_running = False
+        if self.tcp_server_socket:
+            self.tcp_server_socket.close()
+        
+        # 等待TCP服务器线程结束
+        if self.tcp_server_thread and self.tcp_server_thread.is_alive():
+            self.tcp_server_thread.join(timeout=2.0)
+        
+        self.logger.info("LocalRuntime shutdown completed")
+    
+    def __del__(self):
+        """析构函数，确保资源清理"""
+        try:
+            self.shutdown()
+        except:
+            pass
\ No newline at end of file
diff --git a/sage/core/runtime/local/local_task.py b/sage/core/runtime/local/local_task.py
index ffdcc3a..09bca3b 100644
--- a/sage/core/runtime/local/local_task.py
+++ b/sage/core/runtime/local/local_task.py
@@ -1,7 +1,7 @@
 import logging
 from re import M
 from sage.core.dag.local.dag_node import BaseDAGNode,OneShotDAGNode
-from sage.core.dag.local.multi_dag_node import MultiplexerDagNode
+from sage.core.dag.local.local_dag_node import LocalDAGNode
 from sage.core.dag.local.dag import DAG
 import threading
 from sage.utils.custom_logger import CustomLogger
@@ -23,7 +23,7 @@ class StreamingTask(BaseTask):
                TypeError: 当节点类型不匹配时抛出
                RuntimeError: 执行过程中出现错误时抛出
     """
-    def __init__(self,node,working_config=None, session_folder: str = None):
+    def __init__(self,node,working_config=None):
         super().__init__()
         self.long_running=True
         self.node=node
@@ -32,7 +32,6 @@ class StreamingTask(BaseTask):
         self.working_config=working_config or {}
         self.logger = CustomLogger(
             object_name=f"StreamingTask_{self.name}",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
@@ -41,10 +40,10 @@ class StreamingTask(BaseTask):
     def execute(self):
         #循环的执行算子
         try:
-            if  isinstance(self.node,MultiplexerDagNode):
+            if  isinstance(self.node,LocalDAGNode):
                 self.node.run_loop()
             else :
-                raise TypeError(f"node{self.node.name} is not a MultiplexerDagNode")
+                raise TypeError(f"node{self.node.name} is not a LocalDAGNode")
         except Exception as e:
             self.logger.error(e)
             raise TypeError(e)
diff --git a/sage/core/runtime/ray/ray_runtime.py b/sage/core/runtime/ray/ray_runtime.py
index 6ca80d0..ce46cd1 100644
--- a/sage/core/runtime/ray/ray_runtime.py
+++ b/sage/core/runtime/ray/ray_runtime.py
@@ -1,45 +1,422 @@
-from typing import Dict, List, Optional, Any
-# from sage.archive.raydag_task import RayDAGTask
+from typing import Dict, List, Optional, Any, Union
 from sage.core.dag.ray.ray_dag import RayDAG
-import logging, ray, time
+from sage.core.dag.ray.ray_dag_node import RayDAGNode
+import logging, ray, time, threading 
 from sage.core.runtime.base_runtime import BaseRuntime
 from sage.utils.custom_logger import CustomLogger
+from ray.actor import ActorHandle
 
+class RayRuntime(BaseRuntime):
+    """
+    Ray 执行后端，支持单个Ray节点和完整Ray DAG的管理
+    """
+    _instance = None
+    _lock = threading.Lock()
 
 
-class RayRuntime(BaseRuntime):
-    """Ray DAG 专用执行后端"""
-    
-    def __init__(self, monitoring_interval: float = 1.0, session_folder: str = None):
+    def __init__(self, monitoring_interval: float = 1.0):
         """
-        Initialize Ray DAG execution backend.
+        Initialize Ray execution backend.
         
         Args:
-            monitoring_interval: Interval in seconds for monitoring DAG status
+            monitoring_interval: Interval in seconds for monitoring status
+            session_folder: Session folder for logging
         """
         # 确保Ray已初始化
         if not ray.is_initialized():
             ray.init(temp_dir="./ray_tmp")
+            
         self.name = "RayRuntime"
+        self.session_folder = CustomLogger.get_session_folder()
+        
+        # Ray DAG 管理（保留向后兼容）
         self.running_dags: Dict[str, RayDAG] = {}  # handle -> RayDAG映射
         self.dag_spout_futures: Dict[str, List[ray.ObjectRef]] = {}  # handle -> spout futures
         self.dag_metadata: Dict[str, Dict[str, Any]] = {}  # handle -> metadata
+        
+        # Ray节点管理
+        self.running_nodes: Dict[str, ActorHandle] = {}  # node_name -> ActorHandle
+        self.node_metadata: Dict[str, Dict[str, Any]] = {}  # node_name -> metadata
+        self.node_handles: Dict[str, str] = {}  # handle -> node_name
+        self.handle_to_node: Dict[str, str] = {}  # handle -> node_name
+        self.node_spout_futures: Dict[str, ray.ObjectRef] = {}  # node_name -> spout future
+        
         self.next_handle_id = 0
         self.monitoring_interval = monitoring_interval
+        
         self.logger = CustomLogger(
             object_name=f"RayRuntime",
-            session_folder=session_folder,
             log_level="DEBUG",
             console_output=False,
             file_output=True
         )
+    def __new__(cls, monitoring_interval: float = 1.0):
+        # 禁止直接实例化
+        raise RuntimeError("请通过 get_instance() 方法获取实例")
+    
+    @classmethod
+    def get_instance(cls, monitoring_interval: float = 1.0):
+        """获取RayRuntime的唯一实例"""
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # 绕过 __new__ 的异常，直接创建实例
+                    instance = super().__new__(cls)
+                    instance.__init__(monitoring_interval)
+                    cls._instance = instance
+        return cls._instance
+    
+    @classmethod
+    def reset_instance(cls):
+        """重置实例（主要用于测试）"""
+        with cls._lock:
+            if cls._instance:
+                cls._instance.shutdown()
+                cls._instance = None
+    # ==================== Ray节点管理接口 ====================
     
-    def submit_task(self, ray_dag:RayDAG) -> str:
+    def submit_node(self, node_class, node_name: str, operator_class, 
+                   operator_config: Dict = None, is_spout: bool = False) -> str:
         """
-        提交 Ray DAG 执行任务
+        提交单个Ray节点到运行时
         
         Args:
-            ray_dag: 应该是 RayDAG 实例或包含 RayDAG 的任务包装器
+            node_class: Ray节点类（通常是RayMultiplexerDagNode）
+            node_name: 节点名称
+            operator_class: 操作符类
+            operator_config: 操作符配置
+            is_spout: 是否为spout节点
+            
+        Returns:
+            str: 节点句柄
+        """
+        if node_name in self.running_nodes:
+            raise ValueError(f"Node '{node_name}' already exists")
+        
+        self.logger.info(f"Submitting Ray node '{node_name}' to {self.name}")
+        
+        try:
+            # 创建Ray Actor
+            actor_handle = node_class.remote(
+                name=node_name,
+                function_class=operator_class,
+                operator_config=operator_config or {},
+                is_spout=is_spout,
+                session_folder=self.session_folder
+            )
+            
+            # 生成handle
+            handle = f"ray_node_{self.next_handle_id}"
+            self.next_handle_id += 1
+            
+            # 存储节点信息
+            self.running_nodes[node_name] = actor_handle
+            self.node_metadata[node_name] = {
+                'is_spout': is_spout,
+                'operator_class': operator_class.__name__,
+                'start_time': time.time(),
+                'status': 'created'
+            }
+            self.node_handles[handle] = node_name
+            self.handle_to_node[handle] = node_name
+            
+            self.logger.info(f"Ray node '{node_name}' created successfully with handle: {handle}")
+            return handle
+            
+        except Exception as e:
+            self.logger.error(f"Failed to create Ray node '{node_name}': {e}")
+            raise
+    
+    def submit_actor_instance(self, node_instance: ActorHandle, node_name: str) -> str:
+        """
+        提交已创建的Ray节点实例
+        
+        Args:
+            node_instance: Ray Actor实例
+            node_name: 节点名称
+            
+        Returns:
+            str: 节点句柄
+        """
+        if node_name in self.running_nodes:
+            raise ValueError(f"Node '{node_name}' already exists")
+        
+        # 生成handle
+        handle = f"ray_node_{self.next_handle_id}"
+        self.next_handle_id += 1
+        
+        # 存储节点信息
+        self.running_nodes[node_name] = node_instance
+        self.node_metadata[node_name] = {
+            'start_time': time.time(),
+            'status': 'submitted'
+        }
+        self.node_handles[handle] = node_name
+        self.handle_to_node[handle] = node_name
+        
+        self.logger.info(f"Ray node instance '{node_name}' submitted with handle: {handle}")
+        return handle
+    
+    def submit_actors(self, nodes: List[ActorHandle], node_names: List[str]) -> List[str]:
+        """
+        批量提交Ray节点实例
+        
+        Args:
+            nodes: Ray Actor实例列表
+            node_names: 节点名称列表
+            
+        Returns:
+            List[str]: 节点句柄列表
+        """
+        if len(nodes) != len(node_names):
+            raise ValueError("nodes and node_names must have the same length")
+        
+        handles = []
+        for actor_instance, node_name in zip(nodes, node_names):
+            try:
+                handle = self.submit_actor_instance(actor_instance, node_name)
+                handles.append(handle)
+                self.start_node(handle)  # 自动启动节点
+            except Exception as e:
+                self.logger.error(f"Failed to submit node '{node_name}': {e}")
+                # 停止已经提交的节点
+                for h in handles:
+                    self.stop_node(h)
+                raise
+        
+        self.logger.info(f"Successfully submitted {len(handles)} Ray nodes")
+        return handles
+    
+    def start_node(self, node_handle: str):
+        """
+        启动Ray节点
+        
+        Args:
+            node_handle: 节点句柄
+        """
+        if node_handle not in self.handle_to_node:
+            raise ValueError(f"Node handle '{node_handle}' not found")
+        
+        node_name = self.handle_to_node[node_handle]
+        actor_handle = self.running_nodes[node_name]
+        
+        try:
+            # 检查是否为spout节点
+            is_spout = self.node_metadata[node_name].get('is_spout', False)
+            
+            if is_spout:
+                # Spout节点启动数据生成循环
+                future = actor_handle.start.remote()
+                self.node_spout_futures[node_name] = future
+                self.logger.info(f"Started Ray spout node '{node_name}'")
+            else:
+                # 非Spout节点启动就绪状态
+                future = actor_handle.start.remote()
+                self.logger.info(f"Started Ray node '{node_name}' in ready state")
+            
+            # 更新状态
+            self.node_metadata[node_name]['status'] = 'running'
+            
+        except Exception as e:
+            self.logger.error(f"Failed to start Ray node '{node_name}': {e}")
+            raise
+    
+    def start_all_nodes(self):
+        """启动所有节点"""
+        self.logger.info("Starting all Ray nodes...")
+        
+        spout_count = 0
+        ready_count = 0
+        
+        for handle in self.handle_to_node.keys():
+            try:
+                node_name = self.handle_to_node[handle]
+                is_spout = self.node_metadata[node_name].get('is_spout', False)
+                
+                self.start_node(handle)
+                
+                if is_spout:
+                    spout_count += 1
+                else:
+                    ready_count += 1
+                    
+            except Exception as e:
+                self.logger.error(f"Failed to start node with handle {handle}: {e}")
+        
+        self.logger.info(f"Started {spout_count} spout nodes and {ready_count} ready nodes")
+    
+    def stop_node(self, node_handle: str):
+        """
+        停止Ray节点
+        
+        Args:
+            node_handle: 节点句柄
+        """
+        if node_handle not in self.handle_to_node:
+            self.logger.warning(f"Node handle '{node_handle}' not found")
+            return
+        
+        node_name = self.handle_to_node[node_handle]
+        actor_handle = self.running_nodes[node_name]
+        
+        try:
+            # 停止Actor
+            actor_handle.stop.remote()
+            
+            # 清理spout future
+            self.node_spout_futures.pop(node_name, None)
+            
+            # 更新状态
+            self.node_metadata[node_name]['status'] = 'stopped'
+            self.node_metadata[node_name]['end_time'] = time.time()
+            
+            self.logger.info(f"Ray node '{node_name}' stopped")
+            
+        except Exception as e:
+            self.logger.error(f"Error stopping Ray node '{node_name}': {e}")
+    
+    def stop_all_nodes(self):
+        """停止所有节点"""
+        self.logger.info("Stopping all Ray nodes...")
+        
+        handles_to_stop = list(self.handle_to_node.keys())
+        for handle in handles_to_stop:
+            self.stop_node(handle)
+        
+        self.logger.info(f"Stopped {len(handles_to_stop)} Ray nodes")
+    
+    def remove_node(self, node_handle: str):
+        """
+        移除Ray节点并清理资源
+        
+        Args:
+            node_handle: 节点句柄
+        """
+        if node_handle not in self.handle_to_node:
+            self.logger.warning(f"Node handle '{node_handle}' not found")
+            return
+        
+        node_name = self.handle_to_node[node_handle]
+        actor_handle = self.running_nodes[node_name]
+        
+        try:
+            # 停止并杀死Actor
+            self.stop_node(node_handle)
+            ray.kill(actor_handle)
+            
+            # 清理映射关系
+            self.running_nodes.pop(node_name, None)
+            self.node_metadata.pop(node_name, None)
+            self.node_handles.pop(node_handle, None)
+            self.handle_to_node.pop(node_handle, None)
+            self.node_spout_futures.pop(node_name, None)
+            
+            self.logger.info(f"Ray node '{node_name}' removed")
+            
+        except Exception as e:
+            self.logger.error(f"Error removing Ray node '{node_name}': {e}")
+    
+    def get_node_status(self, node_handle: str) -> Dict[str, Any]:
+        """
+        获取Ray节点状态
+        
+        Args:
+            node_handle: 节点句柄
+            
+        Returns:
+            Dict: 节点状态信息
+        """
+        if node_handle not in self.handle_to_node:
+            return {"status": "not_found"}
+        
+        node_name = self.handle_to_node[node_handle]
+        actor_handle = self.running_nodes[node_name]
+        metadata = self.node_metadata[node_name]
+        
+        try:
+            # 非阻塞健康检查
+            health_future = actor_handle.health_check.remote()
+            ready, not_ready = ray.wait([health_future], timeout=0.1)
+            
+            if ready:
+                health_info = ray.get(ready[0])
+            else:
+                health_info = {"status": "busy"}
+                
+        except Exception as e:
+            health_info = {"status": "error", "error": str(e)}
+        
+        status_info = {
+            "handle": node_handle,
+            "node_name": node_name,
+            "backend": "ray_node",
+            "status": metadata.get('status', 'unknown'),
+            "is_spout": metadata.get('is_spout', False),
+            "operator_class": metadata.get('operator_class', 'unknown'),
+            "start_time": metadata.get('start_time'),
+            "health": health_info
+        }
+        
+        # 计算运行时间
+        if 'start_time' in metadata:
+            if 'end_time' in metadata:
+                status_info['duration'] = metadata['end_time'] - metadata['start_time']
+            else:
+                status_info['duration'] = time.time() - metadata['start_time']
+        
+        return status_info
+    
+    def get_running_nodes(self) -> List[str]:
+        """获取所有运行中的节点名称"""
+        return list(self.running_nodes.keys())
+    
+    def get_node_by_name(self, node_name: str) -> Optional[ActorHandle]:
+        """根据名称获取节点"""
+        return self.running_nodes.get(node_name)
+    
+    def wait_for_node_completion(self, node_handle: str, timeout: Optional[float] = None) -> bool:
+        """
+        等待spout节点完成执行
+        
+        Args:
+            node_handle: 节点句柄
+            timeout: 超时时间（秒），None表示无限等待
+            
+        Returns:
+            是否成功完成
+        """
+        if node_handle not in self.handle_to_node:
+            return False
+        
+        node_name = self.handle_to_node[node_handle]
+        
+        if node_name not in self.node_spout_futures:
+            return True  # 不是spout节点或没有运行的future
+        
+        spout_future = self.node_spout_futures[node_name]
+        
+        try:
+            ray.get(spout_future, timeout=timeout)
+            self.logger.info(f"Ray node '{node_name}' completed successfully")
+            
+            # 更新状态
+            self.node_metadata[node_name]['status'] = 'completed'
+            self.node_metadata[node_name]['end_time'] = time.time()
+            
+            return True
+            
+        except Exception as e:
+            self.logger.error(f"Ray node '{node_name}' failed or timed out: {e}")
+            return False
+    
+    # ==================== Ray DAG管理接口（向后兼容） ====================
+    
+    def submit_task(self, ray_dag: RayDAG) -> str:
+        """
+        提交 Ray DAG 执行任务（向后兼容接口）
+        
+        Args:
+            ray_dag: RayDAG 实例
         """
         if not isinstance(ray_dag, RayDAG):
             raise TypeError("Task must be RayDAG instance")
@@ -53,21 +430,13 @@ class RayRuntime(BaseRuntime):
         self.logger.info(f"Submitting Ray DAG {ray_dag.name} with handle {handle}")
         
         try:
-            self.logger.info(f"ray_dag.strategy is {ray_dag.strategy}")
-
             # 启动 DAG 执行
-            if ray_dag.strategy == "streaming":
-                spout_futures = self._start_streaming_dag(ray_dag)
-            elif ray_dag.strategy == "oneshot":
-                spout_futures = self._start_oneshot_dag(ray_dag)
-            else:
-                raise ValueError(f"Unsupported DAG strategy: {ray_dag.strategy}")
+            spout_futures = self._start_dag(ray_dag)
             
             # 存储DAG信息
             self.running_dags[handle] = ray_dag
             self.dag_spout_futures[handle] = spout_futures
             self.dag_metadata[handle] = {
-                'strategy': ray_dag.strategy,
                 'actor_count': ray_dag.get_actor_count(),
                 'start_time': time.time(),
                 'status': 'running'
@@ -80,39 +449,19 @@ class RayRuntime(BaseRuntime):
             self.logger.error(f"Failed to start Ray DAG {ray_dag.name}: {e}", exc_info=True)
             raise
     
-    def _start_streaming_dag(self, ray_dag: RayDAG) -> List[ray.ObjectRef]:
-        """启动流式 DAG"""
-        spout_actors = ray_dag.get_spout_actors()
-        
-        if not spout_actors:
-            raise RuntimeError("No spout actors found in streaming DAG")
-        
-        spout_futures = []
-        for spout_actor in spout_actors:
-            try:
-                self.logger.debug(f"Started streaming spout actor in DAG {ray_dag.name}")
-                future = spout_actor.start_spout.remote()
-                spout_futures.append(future)
-            except Exception as e:
-                self.logger.error(f"Failed to start spout actor in DAG {ray_dag.name}: {e}")
-                raise
-        
-        return spout_futures
-    
-    def _start_oneshot_dag(self, ray_dag: RayDAG) -> List[ray.ObjectRef]:
-        """启动一次性 DAG"""
+    def _start_dag(self, ray_dag: RayDAG) -> List[ray.ObjectRef]:
+        """启动DAG中的所有spout actors"""
         spout_actors = ray_dag.get_spout_actors()
-        self.logger.info(f"Started oneshot task in DAG {ray_dag.name}")
         
         if not spout_actors:
-            raise RuntimeError("No spout actors found in oneshot DAG")
+            raise RuntimeError("No spout actors found in DAG")
         
         spout_futures = []
         for spout_actor in spout_actors:
             try:
-                future = spout_actor.start_spout.remote()
+                future = spout_actor.start.remote()
                 spout_futures.append(future)
-                self.logger.info(f"Started oneshot spout actor in DAG {ray_dag.name}")
+                self.logger.debug(f"Started spout actor in DAG {ray_dag.name}")
             except Exception as e:
                 self.logger.error(f"Failed to start spout actor in DAG {ray_dag.name}: {e}")
                 raise
@@ -120,7 +469,7 @@ class RayRuntime(BaseRuntime):
         return spout_futures
     
     def stop_task(self, task_handle: str):
-        """停止 Ray DAG 执行"""
+        """停止 Ray DAG 执行（向后兼容接口）"""
         if task_handle not in self.running_dags:
             self.logger.warning(f"DAG handle {task_handle} not found")
             return
@@ -183,7 +532,7 @@ class RayRuntime(BaseRuntime):
         self.dag_metadata.pop(task_handle, None)
     
     def get_status(self, task_handle: str) -> Dict[str, Any]:
-        """获取 Ray DAG 状态"""
+        """获取 Ray DAG 状态（向后兼容接口）"""
         if task_handle not in self.running_dags:
             return {"status": "not_found"}
         
@@ -204,7 +553,6 @@ class RayRuntime(BaseRuntime):
             "status": metadata.get('status', 'unknown'),
             "backend": "ray_dag",
             "dag_id": ray_dag.name,
-            "strategy": metadata.get('strategy', 'unknown'),
             "actor_count": metadata.get('actor_count', 0),
             "spout_count": len(spout_futures),
             "completed_spouts": completed_spouts,
@@ -242,16 +590,7 @@ class RayRuntime(BaseRuntime):
         return actor_health
     
     def wait_for_completion(self, task_handle: str, timeout: Optional[float] = None) -> bool:
-        """
-        等待 DAG 完成执行（主要用于 oneshot 策略）
-        
-        Args:
-            task_handle: DAG 句柄
-            timeout: 超时时间（秒），None表示无限等待
-            
-        Returns:
-            是否成功完成
-        """
+        """等待 DAG 完成执行（向后兼容接口）"""
         if task_handle not in self.running_dags:
             return False
         
@@ -274,22 +613,35 @@ class RayRuntime(BaseRuntime):
             self.logger.error(f"DAG {task_handle} failed or timed out: {e}")
             return False
     
-    def list_running_dags(self) -> List[str]:
-        """列出所有正在运行的 DAG 句柄"""
-        return list(self.running_dags.keys())
+    # ==================== 通用管理接口 ====================
     
-    def get_dag_info(self, task_handle: str) -> Optional[Dict[str, Any]]:
-        """获取 DAG 详细信息"""
-        if task_handle not in self.running_dags:
-            return None
-        
-        ray_dag = self.running_dags[task_handle]
+    def get_runtime_info(self) -> Dict[str, Any]:
+        """获取运行时信息"""
         return {
-            'handle': task_handle,
-            'dag_id': ray_dag.name,
-            'strategy': ray_dag.strategy,
-            'actor_count': ray_dag.get_actor_count(),
-            'connections': ray_dag.get_connections(),
-            'spout_actors': ray_dag.spout_actors,
-            'metadata': self.dag_metadata.get(task_handle, {})
+            "name": self.name,
+            "running_nodes_count": len(self.running_nodes),
+            "running_nodes": list(self.running_nodes.keys()),
+            "running_dags_count": len(self.running_dags),
+            "running_dags": list(self.running_dags.keys()),
+            "ray_initialized": ray.is_initialized()
         }
+    
+    def shutdown(self):
+        """关闭运行时和所有资源"""
+        self.logger.info("Shutting down RayRuntime...")
+        
+        # 停止所有节点
+        self.stop_all_nodes()
+        
+        # 停止所有DAG
+        for handle in list(self.running_dags.keys()):
+            self.stop_task(handle)
+        
+        self.logger.info("RayRuntime shutdown completed")
+    
+    def __del__(self):
+        """析构函数，确保资源清理"""
+        try:
+            self.shutdown()
+        except:
+            pass
\ No newline at end of file
diff --git a/sage/core/runtime/runtime_manager.py b/sage/core/runtime/runtime_manager.py
index 03eca79..9b878cb 100644
--- a/sage/core/runtime/runtime_manager.py
+++ b/sage/core/runtime/runtime_manager.py
@@ -1,4 +1,5 @@
 import logging
+import threading
 from typing import Dict, Any, Union
 from sage.core.runtime.base_runtime import BaseRuntime
 from sage.core.runtime.ray.ray_runtime import RayRuntime
@@ -12,9 +13,17 @@ class RuntimeManager:
     运行时管理器，负责管理不同平台的运行时实例
     """
     
+    _instance = None
+    _lock = threading.Lock()
+    
     def __init__(self, session_folder: str = None):
+        # 确保只初始化一次
+        if hasattr(self, "_initialized"):
+            return
+        self._initialized = True
+        
         self.backends: Dict[str, Any] = {}
-        self.session_folder = session_folder
+        self.session_folder = CustomLogger.get_session_folder()
         self.logger = CustomLogger(
             object_name=f"RuntimeManager",
             session_folder=session_folder,
@@ -23,6 +32,30 @@ class RuntimeManager:
             file_output=True
         )
     
+    def __new__(cls):
+        # 禁止直接实例化
+        raise RuntimeError("请通过 get_instance() 方法获取实例")
+    
+    @classmethod
+    def get_instance(cls):
+        """获取RuntimeManager的唯一实例"""
+        if cls._instance is None:
+            with cls._lock:
+                if cls._instance is None:
+                    # 绕过 __new__ 的异常，直接创建实例
+                    instance = super().__new__(cls)
+                    instance.__init__()
+                    cls._instance = instance
+        return cls._instance
+    
+    @classmethod
+    def reset_instance(cls):
+        """重置实例（主要用于测试）"""
+        with cls._lock:
+            if cls._instance:
+                cls._instance.shutdown_all()
+                cls._instance = None
+    
     def get(self, platform: str, **kwargs) -> BaseRuntime:
         """
         获取指定平台的运行时实例，支持延迟初始化
@@ -53,12 +86,19 @@ class RuntimeManager:
         """
         if platform == "ray":
             monitoring_interval = kwargs.get('monitoring_interval', 2.0)
-            return RayRuntime(monitoring_interval=monitoring_interval, session_folder=self.session_folder)
+            return RayRuntime.get_instance(monitoring_interval=monitoring_interval)
         
         elif platform == "local":
             max_slots = kwargs.get('max_slots', 4)
             scheduling_strategy = kwargs.get('scheduling_strategy', None)
-            return LocalRuntime(max_slots=max_slots, scheduling_strategy=scheduling_strategy)
+            tcp_host = kwargs.get('tcp_host', "localhost")
+            tcp_port = kwargs.get('tcp_port', 9999)
+            return LocalRuntime.get_instance(
+                max_slots=max_slots, 
+                scheduling_strategy=scheduling_strategy,
+                tcp_host=tcp_host,
+                tcp_port=tcp_port
+            )
         
         else:
             raise ValueError(f"Unknown platform: {platform}")
diff --git a/sage/tests/config/config_mixed.yaml b/sage/tests/config/config_mixed.yaml
new file mode 100644
index 0000000..b18912d
--- /dev/null
+++ b/sage/tests/config/config_mixed.yaml
@@ -0,0 +1,47 @@
+pipeline:
+  name: "sage-api-operator-operator_test"
+  description: "Test pipeline for Sage API Operator"
+  version: "1.0.0"
+  type: "ray"
+
+
+
+source:
+  # data_path: "sample/one_question.txt"
+  data_path: "sample/question.txt"
+  platform: "local"
+
+retriever:
+  platform: "ray"
+  ltm:
+    topk: 3
+
+reranker:
+  platform: "local"
+  model_name: "BAAI/bge-reranker-v2-m3"
+  top_k: 3
+
+refiner:
+  platform: "local"
+  method: "openai"
+  model_name: "qwen-turbo-0919"
+  base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
+  api_key: "sk-b21a67cf99d14ead9d1c5bf8c2eb90ef"
+  seed: 42
+
+generator:
+  platform: "local"
+  method: "openai"
+  model_name: "qwen-turbo-0919"
+  base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
+  api_key: "sk-b21a67cf99d14ead9d1c5bf8c2eb90ef"
+  seed: 42
+
+writer:
+
+promptor:
+  platform: "local"
+
+sink:
+  platform: "local"
+
diff --git a/sage/tests/runtime/qa_dense_retrieval_mixed.py b/sage/tests/runtime/qa_dense_retrieval_mixed.py
new file mode 100644
index 0000000..1d3680c
--- /dev/null
+++ b/sage/tests/runtime/qa_dense_retrieval_mixed.py
@@ -0,0 +1,58 @@
+import logging
+import time
+from sage.api.pipeline import Pipeline
+from sage.api.operator.operator_impl.promptor import QAPromptor
+from sage.api.operator.operator_impl.generator import OpenAIGenerator
+from sage.api.operator.operator_impl.retriever import DenseRetriever
+from sage.api.operator.operator_impl.source import FileSource
+from sage.api.operator.operator_impl.sink import FileSink,TerminalSink
+from sage.core.neuromem.memory_manager import MemoryManager
+from sage.utils.config_loader import load_config
+from sage.utils.logging_utils import configure_logging
+from sage.api.model.model_api import apply_embedding_model
+def memory_init():
+    """初始化内存管理器并创建测试集合"""
+    manager = MemoryManager()
+    embedding_model = apply_embedding_model("hf", model="sentence-transformers/all-MiniLM-L6-v2")
+    col = manager.create_collection(
+        name="vdb_test",
+        backend_type="VDB",
+        embedding_model=embedding_model,
+        dim=embedding_model.get_dim(),
+        description="test vdb collection",
+        as_ray_actor=True
+    )
+    col.add_metadata_field("owner")
+    col.add_metadata_field("show_type")
+    texts = [
+        ("hello world", {"owner": "ruicheng", "show_type": "text"}),
+        ("你好，世界", {"owner": "Jun", "show_type": "text"}),
+        ("こんにちは、世界", {"owner": "Lei", "show_type": "img"}),
+    ]
+    for text, metadata in texts:
+        col.insert(text, metadata)
+    col.create_index(index_name="vdb_index")
+    config["retriever"]["ltm_collection"] = col._collection
+
+
+def pipeline_run():
+    """创建并运行数据处理管道"""
+    pipeline = Pipeline(name="example_pipeline")
+    # 构建数据处理流程
+    query_stream = pipeline.add_source(FileSource, config["source"])
+    query_and_chunks_stream = query_stream.retrieve(DenseRetriever, config["retriever"])
+    prompt_stream = query_and_chunks_stream.construct_prompt(QAPromptor, config["promptor"])
+    response_stream = prompt_stream.generate_response(OpenAIGenerator, config["generator"])
+    response_stream.sink(TerminalSink, config["sink"])
+    # 提交管道并运行
+    pipeline.submit_mixed(config={"is_long_running":True})
+    time.sleep(100)  # 等待管道运行
+
+
+if __name__ == '__main__':
+    configure_logging(level=logging.INFO)
+    # 加载配置并初始化日志
+    config = load_config('./config_mixed.yaml')
+    # 初始化内存并运行管道
+    memory_init()
+    pipeline_run()
diff --git a/sage/utils/custom_logger.py b/sage/utils/custom_logger.py
index 5a50efe..24f17db 100644
--- a/sage/utils/custom_logger.py
+++ b/sage/utils/custom_logger.py
@@ -224,7 +224,10 @@ class CustomLogger:
             Path(session_folder).mkdir(parents=True, exist_ok=True)
     
     @classmethod
-    def get_default_session_folder(cls) -> Optional[str]:
+    def get_session_folder(cls) -> Optional[str]:
+        if cls._default_session_folder is None:
+            # 如果没有设置默认session文件夹，创建一个新的
+            cls._default_session_folder = cls.create_session_folder()
         """获取默认的session文件夹"""
         return cls._default_session_folder
     
