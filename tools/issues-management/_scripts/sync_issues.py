#!/usr/bin/env python3
"""
Issues åŒæ­¥è„šæœ¬
"""
import argparse
import sys
import json
from datetime import datetime
from pathlib import Path

SCRIPT_DIR = Path(__file__).resolve().parent
sys.path.insert(0, str(SCRIPT_DIR))

from config import config
from config import github_client
import requests
import time
import glob


def graphql_request(session: requests.Session, query: str, variables: dict = None, retries: int = 2):
    payload = {"query": query}
    if variables is not None:
        payload["variables"] = variables
    attempt = 0
    while True:
        try:
            resp = session.post("https://api.github.com/graphql", json=payload, timeout=30)
            resp.raise_for_status()
            data = resp.json()
            return True, data
        except Exception as e:
            attempt += 1
            if attempt > retries:
                return False, str(e)
            time.sleep(1 * attempt)


class IssuesSyncer:
    def __init__(self):
        self.workspace_dir = Path(__file__).parent.parent / "issues_workspace"
        self.output_dir = Path(__file__).parent.parent / "output"
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def sync_all_changes(self):
        changes = self.detect_all_changes()
        if not changes:
            print("âœ… æ²¡æœ‰æ£€æµ‹åˆ°éœ€è¦åŒæ­¥çš„æ›´æ”¹")
            return True

        print(f"ï¿½ æ£€æµ‹åˆ° {len(changes)} ä¸ªæ›´æ”¹:")
        for change in changes:
            print(f"  - {change['type']}: {change['description']}")

        confirm = input("\næ˜¯å¦ç»§ç»­åŒæ­¥? (y/N): ").lower().strip()
        if confirm != 'y':
            print("âŒ åŒæ­¥å·²å–æ¶ˆ")
            return False

        success = self.execute_sync(changes)
        self.log_sync_operation(changes, success)
        return success

    def sync_label_changes(self):
        label_changes = self.detect_label_changes()
        if not label_changes:
            print("âœ… æ²¡æœ‰æ£€æµ‹åˆ°æ ‡ç­¾æ›´æ”¹")
            return True
        return self.execute_sync(label_changes)

    def sync_status_changes(self):
        status_changes = self.detect_status_changes()
        if not status_changes:
            print("âœ… æ²¡æœ‰æ£€æµ‹åˆ°çŠ¶æ€æ›´æ”¹")
            return True
        return self.execute_sync(status_changes)

    def preview_changes(self):
        all_changes = self.detect_all_changes()
        if not all_changes:
            print("âœ… æ²¡æœ‰æ£€æµ‹åˆ°éœ€è¦åŒæ­¥çš„æ›´æ”¹")
            return True

        print(f"\nï¿½ æ£€æµ‹åˆ° {len(all_changes)} ä¸ªå¾…åŒæ­¥æ›´æ”¹:\n")
        for change in all_changes:
            print(f" - [{change['type']}] {change['description']}")

        report_file = self.output_dir / f"sync_preview_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        self.save_preview_report(all_changes, report_file)
        print(f"ğŸ“„ è¯¦ç»†é¢„è§ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return True

    def find_latest_plan(self):
        plans = sorted(self.output_dir.glob('project_move_plan_*.json'), reverse=True)
        if plans:
            return plans[0]
        return None

    def load_plan(self, path=None):
        if path:
            p = Path(path)
        else:
            p = self.find_latest_plan()
        if not p or not p.exists():
            print("âŒ æœªæ‰¾åˆ° plan æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ project_manage.py --stage-local")
            return []
        try:
            data = json.loads(p.read_text(encoding='utf-8'))
            print(f"âœ… å·²åŠ è½½è®¡åˆ’: {p}ï¼Œ{len(data)} é¡¹")
            return data
        except Exception as e:
            print(f"âŒ è§£æ plan å¤±è´¥: {e}")
            return []

    def preview_plan(self, plan):
        if not plan:
            print("âœ… è®¡åˆ’ä¸ºç©º")
            return True
        print(f"\nğŸ” è®¡åˆ’é¢„è§ˆ ({len(plan)} é¡¹):")
        for i, act in enumerate(plan, 1):
            print(f" [{i}/{len(plan)}] #{act.get('issue_number')} -> project {act.get('to_project')} ({act.get('to_project_number')}) staged={act.get('staged')}")
        return True

    def apply_plan(self, plan, dry_run=True, batch_size=5):
        session = github_client.session
        logs = []
        total = len(plan)
        for idx, act in enumerate(plan, 1):
            issue_number = act.get('issue_number')
            issue_node_id = act.get('issue_node_id')
            item_id = act.get('item_id')
            project_id = act.get('to_project_id')
            project_number = act.get('to_project_number')
            entry = { 'issue_number': issue_number, 'project_number': project_number }
            print(f"[{idx}/{total}] å¤„ç† Issue #{issue_number} -> project #{project_number}")

            # Idempotency check: does project already contain this contentId?
            q_check = '''query($projectId: ID!) { node(id: $projectId) { ... on ProjectV2 { items(first:100) { nodes { content { __typename ... on Issue { id } } } pageInfo { hasNextPage endCursor } } } } }'''
            ok, resp = graphql_request(session, q_check, { 'projectId': project_id }, retries=1)
            already = False
            if ok:
                nodes = resp.get('data', {}).get('node', {}).get('items', {}).get('nodes', [])
                for n in nodes:
                    c = n.get('content') or {}
                    if c.get('id') == issue_node_id:
                        already = True
                        break

            if already:
                print(f"  â­ï¸ ç›®æ ‡ project å·²åŒ…å«æ­¤ issueï¼Œè·³è¿‡ add")
                entry['added'] = False
            else:
                if dry_run:
                    print(f"  [dry-run] ä¼šæ‰§è¡Œ addProjectV2ItemById(projectId={project_id}, contentId={issue_node_id})")
                    entry['added'] = 'dry-run'
                else:
                    mut = '''mutation($projectId: ID!, $contentId: ID!) { addProjectV2ItemById(input:{projectId:$projectId, contentId:$contentId}) { item { id } } }'''
                    ok2, resp2 = graphql_request(session, mut, { 'projectId': project_id, 'contentId': issue_node_id }, retries=2)
                    if not ok2 or 'errors' in (resp2 or {}):
                        print(f"  âŒ add å¤±è´¥: {resp2}")
                        entry['added'] = False
                        entry['add_response'] = resp2
                    else:
                        print(f"  âœ… å·²æ·»åŠ åˆ°ç›®æ ‡ project")
                        entry['added'] = True
                        entry['add_response'] = resp2

            # If we added (or existed), we should remove the original org project item
            if dry_run:
                print(f"  [dry-run] ä¼šæ‰§è¡Œ deleteProjectV2Item(itemId={item_id})")
                entry['deleted'] = 'dry-run'
            else:
                mut_del = '''mutation($itemId: ID!) { deleteProjectV2Item(input: {itemId: $itemId}) { deletedItemId } }'''
                ok3, resp3 = graphql_request(session, mut_del, { 'itemId': item_id }, retries=2)
                if not ok3 or 'errors' in (resp3 or {}):
                    print(f"  âŒ delete å¤±è´¥: {resp3}")
                    entry['deleted'] = False
                    entry['delete_response'] = resp3
                else:
                    print(f"  âœ… å·²ä»åŸç»„ç»‡ project åˆ é™¤ item")
                    entry['deleted'] = True
                    entry['delete_response'] = resp3

            logs.append(entry)
            # gentle rate limiting
            time.sleep(0.5)

        # write log
        log_path = self.output_dir / f"project_move_log_{int(time.time())}.json"
        log_path.write_text(json.dumps(logs, ensure_ascii=False, indent=2), encoding='utf-8')
        print(f"\nğŸ“ æ—¥å¿—å·²å†™å…¥: {log_path}")
        return logs

    def detect_all_changes(self):
        changes = []
        changes.extend(self.detect_label_changes())
        changes.extend(self.detect_status_changes())
        changes.extend(self.detect_content_changes())
        return changes

    def detect_label_changes(self):
        # ç®€åŒ–ï¼šç¤ºä¾‹è¿”å›ç©ºæˆ–å•æ¡å˜æ›´
        return []

    def detect_status_changes(self):
        return []

    def detect_content_changes(self, limit=None):
        # Scan local markdown files and compare with remote issues for title/body/labels
        changes = []
        issues_dir = self.workspace_dir / 'issues'
        if not issues_dir.exists():
            return changes

        files = sorted(issues_dir.glob('open_*.md'))
        total_files = len(files)
        for f in files:
            if total_files and (files.index(f) % 50 == 0):
                print(f"ğŸ” scanning files... progress: {files.index(f)+1}/{total_files}")
            name = f.name
            # extract issue number from filename
            import re
            m = re.match(r'open_(\d+)_', name)
            if not m:
                continue
            issue_number = int(m.group(1))
            text = f.read_text(encoding='utf-8')
            # strip front-matter if present
            body_text = text
            if text.startswith('---'):
                parts = text.split('---', 2)
                if len(parts) == 3:
                    body_text = parts[2]

            # title: first line starting with '# '
            local_title = None
            for line in body_text.splitlines():
                if line.strip().startswith('# '):
                    local_title = line.strip().lstrip('# ').strip()
                    break

            # labels: parse '## æ ‡ç­¾' section
            local_labels = []
            lines = body_text.splitlines()
            for i, line in enumerate(lines):
                if line.strip().startswith('## æ ‡ç­¾'):
                    # read following non-empty lines as labels until blank or next section
                    for j in range(i+1, min(i+10, len(lines))):
                        l = lines[j].strip()
                        if not l:
                            break
                        # labels may be comma-separated or single per line
                        for part in l.split(','):
                            lab = part.strip()
                            if lab:
                                local_labels.append(lab)
                    break

            # local body: everything after title and sections
            local_body = body_text
            # Get remote issue
            owner = config.GITHUB_OWNER
            repo = config.GITHUB_REPO
            url = f"https://api.github.com/repos/{owner}/{repo}/issues/{issue_number}"
            try:
                resp = github_client.session.get(url, timeout=20)
                if resp.status_code != 200:
                    # couldn't fetch remote issue; skip
                    continue
                remote = resp.json()
            except Exception:
                continue

            remote_title = remote.get('title')
            remote_body = remote.get('body') or ''
            remote_labels = [l.get('name') for l in remote.get('labels', [])]
            remote_updated_at = remote.get('updated_at')

            # normalize for comparison
            def norm(s):
                return (s or '').strip().replace('\r\n', '\n')

            title_changed = local_title and norm(local_title) != norm(remote_title)
            body_changed = local_body and norm(local_body) != norm(remote_body)
            labels_changed = set(local_labels) != set(remote_labels)

            if title_changed or body_changed or labels_changed:
                changes.append({
                    'type': 'content',
                    'issue_number': issue_number,
                    'file': str(f),
                    'local_title': local_title,
                    'local_body': local_body,
                    'local_labels': local_labels,
                    'remote_title': remote_title,
                    'remote_body': remote_body,
                    'remote_labels': remote_labels,
                    'remote_updated_at': remote_updated_at,
                })

            if limit and len(changes) >= limit:
                print(f"å·²è¾¾åˆ° content-limit={limit}ï¼Œåœæ­¢æ‰«æ")
                break

        return changes

    def execute_sync(self, changes):
        print("ğŸ”„ æ­£åœ¨æ‰§è¡ŒåŒæ­¥æ“ä½œ...")
        success_count = 0
        for i, change in enumerate(changes, 1):
            print(f"  [{i}/{len(changes)}] åŒæ­¥: {change['description']}")
            # TODO: è°ƒç”¨ GitHub API
            success_count += 1
        print(f"\nğŸ“Š åŒæ­¥å®Œæˆ: {success_count}/{len(changes)} æˆåŠŸ")
        return True

    # Content sync helpers
    def save_content_plan(self, changes):
        path = self.output_dir / f"content_sync_plan_{int(time.time())}.json"
        path.write_text(json.dumps(changes, ensure_ascii=False, indent=2), encoding='utf-8')
        print(f"ğŸ“‹ å†…å®¹åŒæ­¥è®¡åˆ’å·²å†™å…¥: {path}")
        return path

    def apply_content_plan(self, plan, dry_run=True, force=False, limit=None):
        logs = []
        count = 0
        for act in plan:
            issue_number = act['issue_number']
            entry = {'issue_number': issue_number}
            # conflict detection: compare remote updated_at with local (from file)
            local_mtime = None
            try:
                local_mtime = int(Path(act['file']).stat().st_mtime)
            except Exception:
                local_mtime = None

            # fetch remote current updated_at
            owner = config.GITHUB_OWNER
            repo = config.GITHUB_REPO
            url = f"https://api.github.com/repos/{owner}/{repo}/issues/{issue_number}"
            try:
                resp = github_client.session.get(url, timeout=20)
                resp.raise_for_status()
                remote = resp.json()
            except Exception as e:
                entry['error'] = f"fetch failed: {e}"
                logs.append(entry)
                continue

            remote_updated = remote.get('updated_at')
            # If remote was updated after local file mtime and not forced, skip
            if local_mtime and remote_updated:
                # parse remote_updated to timestamp
                from datetime import datetime
                try:
                    t_remote = int(datetime.fromisoformat(remote_updated.replace('Z', '+00:00')).timestamp())
                except Exception:
                    t_remote = None
                if t_remote and local_mtime and t_remote > local_mtime and not force:
                    entry['skipped'] = 'remote_newer'
                    print(f"  âš ï¸ Issue #{issue_number} è¿œç«¯æ¯”æœ¬åœ°æ›´æ–°ï¼Œè·³è¿‡ï¼ˆä½¿ç”¨ --force-content å¼ºåˆ¶è¦†ç›–ï¼‰")
                    logs.append(entry)
                    continue

            # prepare update payload
            payload = {}
            if act.get('local_title') and act.get('local_title') != remote.get('title'):
                payload['title'] = act.get('local_title')
            # limit body size? just send
            if act.get('local_body') and act.get('local_body') != remote.get('body'):
                payload['body'] = act.get('local_body')
            if set(act.get('local_labels', [])) != set(act.get('remote_labels', [])):
                payload['labels'] = act.get('local_labels', [])

            if not payload:
                entry['noop'] = True
                logs.append(entry)
                continue

            if dry_run:
                print(f"  [dry-run] Issue #{issue_number} å°†è¢« PATCH å­—æ®µ: {list(payload.keys())}")
                entry['planned_update'] = payload
            else:
                # perform PATCH via github_client
                res = github_client.update_issue(issue_number, **payload)
                if res is None:
                    entry['updated'] = False
                    entry['error'] = 'update failed'
                    print(f"  âŒ Issue #{issue_number} æ›´æ–°å¤±è´¥")
                else:
                    entry['updated'] = True
                    entry['update_response'] = res
                    print(f"  âœ… Issue #{issue_number} å·²æ›´æ–°")

            logs.append(entry)
            count += 1
            if limit and count >= limit:
                break
            time.sleep(0.3)

        # write log
        log_path = self.output_dir / f"content_sync_log_{int(time.time())}.json"
        log_path.write_text(json.dumps(logs, ensure_ascii=False, indent=2), encoding='utf-8')
        print(f"ğŸ“ å†…å®¹åŒæ­¥æ—¥å¿—å·²å†™å…¥: {log_path}")
        return logs

    def log_sync_operation(self, changes, success, sync_type="all"):
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'sync_type': sync_type,
            'changes_count': len(changes),
            'success': success,
            'changes': changes
        }
        log_file = self.output_dir / f"sync_log_{datetime.now().strftime('%Y%m%d')}.json"
        logs = []
        if log_file.exists():
            try:
                logs = json.loads(log_file.read_text(encoding='utf-8'))
            except Exception:
                logs = []
        logs.append(log_entry)
        log_file.write_text(json.dumps(logs, ensure_ascii=False, indent=2), encoding='utf-8')

    def save_preview_report(self, changes, report_file):
        content = f"""# IssuesåŒæ­¥é¢„è§ˆæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**å¾…åŒæ­¥æ›´æ”¹æ€»æ•°**: {len(changes)}

## æ›´æ”¹è¯¦æƒ…

"""
        for i, change in enumerate(changes, 1):
            content += f"### {i}. {change['type'].upper()} æ›´æ”¹\n- **æè¿°**: {change['description']}\n\n"
        content += "\n---\n*æ­¤æŠ¥å‘Šç”±SAGE Issuesç®¡ç†å·¥å…·ç”Ÿæˆ*\n"
        report_file.write_text(content, encoding='utf-8')


def main():
    parser = argparse.ArgumentParser(description="åŒæ­¥Issuesåˆ°è¿œç«¯")
    parser.add_argument("--all", action="store_true", help="åŒæ­¥æ‰€æœ‰æ›´æ”¹")
    parser.add_argument("--labels-only", action="store_true", help="ä»…åŒæ­¥æ ‡ç­¾æ›´æ”¹")
    parser.add_argument("--status-only", action="store_true", help="ä»…åŒæ­¥çŠ¶æ€æ›´æ”¹")
    parser.add_argument("--preview", action="store_true", help="é¢„è§ˆå¾…åŒæ­¥æ›´æ”¹")
    parser.add_argument("--plan-preview", action="store_true", help="é¢„è§ˆ project_move_plan_*.json ä¸­çš„è®¡åˆ’")
    parser.add_argument("--apply-plan", action="store_true", help="å¯¹ plan æ‰§è¡Œè¿œç«¯å˜æ›´ï¼ˆéœ€ --confirm æ‰ä¼šçœŸæ­£ applyï¼‰")
    parser.add_argument("--plan-file", type=str, help="æŒ‡å®š plan æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤å–æœ€æ–°çš„ project_move_plan_*.jsonï¼‰")
    parser.add_argument("--confirm", action="store_true", help="ç¡®è®¤æ‰§è¡Œï¼ˆä¸ --apply-plan ä¸€èµ·ä½¿ç”¨ä»¥å®é™… applyï¼‰")
    parser.add_argument("--batch-size", type=int, default=5, help="æ¯æ‰¹å¤„ç†æ•°é‡ï¼ˆé»˜è®¤ 5ï¼‰")
    parser.add_argument("--content-preview", action="store_true", help="é¢„è§ˆæœ¬åœ° content æ›´æ”¹ï¼ˆtitle/body/labelsï¼‰")
    parser.add_argument("--apply-content", action="store_true", help="å¯¹æœ¬åœ° content æ›´æ”¹æ‰§è¡Œè¿œç«¯æ›´æ–°ï¼ˆéœ€ --confirm ï¼‰")
    parser.add_argument("--force-content", action="store_true", help="å¼ºåˆ¶è¦†ç›–è¿œç«¯ï¼ˆå¿½ç•¥è¿œç«¯æ›´æ–°æ—¶é—´ï¼‰")
    parser.add_argument("--content-limit", type=int, default=None, help="åªå¤„ç†å‰ N ä¸ª content æ›´æ”¹ï¼ˆç”¨äºè¯•ç‚¹ï¼‰")
    args = parser.parse_args()

    syncer = IssuesSyncer()
    success = False
    if args.all:
        success = syncer.sync_all_changes()
    elif args.labels_only:
        success = syncer.sync_label_changes()
    elif args.status_only:
        success = syncer.sync_status_changes()
    elif args.preview:
        success = syncer.preview_changes()
    elif args.plan_preview:
        plan = syncer.load_plan(args.plan_file)
        success = syncer.preview_plan(plan)
    elif args.apply_plan:
        plan = syncer.load_plan(args.plan_file)
        if not plan:
            sys.exit(1)
        dry = not args.confirm
        print(f"ğŸ”” apply_plan dry_run={dry} batch_size={args.batch_size}")
        syncer.apply_plan(plan, dry_run=dry, batch_size=args.batch_size)
        success = True
    elif args.content_preview:
        changes = syncer.detect_content_changes(limit=args.content_limit)
        if not changes:
            print("âœ… æœªæ£€æµ‹åˆ°å†…å®¹å·®å¼‚")
        else:
            p = syncer.save_content_plan(changes)
            print(f"é¢„è§ˆ {len(changes)} é¡¹å†…å®¹å·®å¼‚ï¼Œè®¡åˆ’å·²ä¿å­˜: {p}")
        success = True
    elif args.apply_content:
        changes = syncer.detect_content_changes(limit=args.content_limit)
        if not changes:
            print("âœ… æœªæ£€æµ‹åˆ°å†…å®¹å·®å¼‚")
            sys.exit(0)
        plan_path = syncer.save_content_plan(changes)
        dry = not args.confirm
        syncer.apply_content_plan(changes, dry_run=dry, force=args.force_content, limit=args.content_limit)
        success = True
    else:
        print("âŒ è¯·æŒ‡å®šåŒæ­¥æ¨¡å¼")
        parser.print_help()
        sys.exit(1)

    if success:
        print("ğŸ‰ æ“ä½œå®Œæˆï¼")
    else:
        print("ğŸ’¥ æ“ä½œå¤±è´¥ï¼")
        sys.exit(1)


if __name__ == '__main__':
    main()
