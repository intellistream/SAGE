#!/usr/bin/env python3
"""
AIæ™ºèƒ½åˆ†æIssueså·¥å…· - é‡æ–°è®¾è®¡ç‰ˆæœ¬
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. äº¤äº’å¼Copilotæ¨¡å¼ - é€šè¿‡VS Code Copilotè¿›è¡Œåˆ†æ
2. APIæ¨¡å¼ - ä½¿ç”¨é…ç½®çš„APIå¯†é’¥è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æ

æ•´åˆå›¢é˜Ÿä¿¡æ¯å’ŒGitHubå·¥å…·è¿›è¡Œæ™ºèƒ½åˆ†æ
"""

import os
import sys
import json
import argparse
import subprocess
from datetime import datetime
from pathlib import Path

# å¯¼å…¥legacyå·¥å…·
sys.path.append(os.path.join(os.path.dirname(__file__), 'legacy'))

# å°è¯•å¯¼å…¥AIåº“
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

class AIIssuesAnalyzer:
    def __init__(self):
        self.workspace_dir = Path(__file__).parent.parent / "issues_workspace"
        self.output_dir = Path(__file__).parent.parent / "output"
        self.helpers_dir = Path(__file__).parent / "helpers"
        self.ensure_output_dir()
        
        # æ£€æµ‹åˆ†ææ¨¡å¼
        self.anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.has_api = bool(self.anthropic_api_key or self.openai_api_key)
        
        # AIå®¢æˆ·ç«¯åˆå§‹åŒ–
        self.anthropic_client = None
        self.openai_client = None
        
        if self.has_api:
            self._init_api_clients()
        else:
            print("ğŸ¤– Copilotäº¤äº’æ¨¡å¼")
            print("ğŸ’¡ å¦‚éœ€APIè‡ªåŠ¨åŒ–æ¨¡å¼ï¼Œè¯·è®¾ç½® ANTHROPIC_API_KEY æˆ– OPENAI_API_KEY")
        
        # åŠ è½½å›¢é˜Ÿä¿¡æ¯
        self.team_info = self._load_team_info()
    
    def _init_api_clients(self):
        """åˆå§‹åŒ–APIå®¢æˆ·ç«¯"""
        if ANTHROPIC_AVAILABLE and self.anthropic_api_key:
            try:
                self.anthropic_client = anthropic.Anthropic(api_key=self.anthropic_api_key)
                print("âœ… Claude AI API å·²è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ Claude AI è¿æ¥å¤±è´¥: {e}")
        
        if OPENAI_AVAILABLE and self.openai_api_key:
            try:
                self.openai_client = openai.OpenAI(api_key=self.openai_api_key)
                print("âœ… OpenAI API å·²è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ OpenAI è¿æ¥å¤±è´¥: {e}")
    
    def _load_team_info(self):
        """åŠ è½½å›¢é˜Ÿä¿¡æ¯"""
        try:
            sys.path.append(str(self.output_dir))
            from team_config import TEAMS, get_all_usernames, get_team_usernames
            print(f"âœ… å·²åŠ è½½å›¢é˜Ÿä¿¡æ¯: {len(get_all_usernames())} ä½æˆå‘˜")
            return {
                'teams': TEAMS,
                'all_usernames': get_all_usernames(),
                'team_count': len(TEAMS)
            }
        except ImportError:
            print("âš ï¸ å›¢é˜Ÿä¿¡æ¯æœªæ‰¾åˆ°")
            print("ğŸ’¡ è¿è¡Œä»¥ä¸‹å‘½ä»¤è·å–å›¢é˜Ÿä¿¡æ¯:")
            print("   python3 _scripts/helpers/get_team_members.py")
            return None
    
    def ensure_output_dir(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        self.output_dir.mkdir(exist_ok=True)
    
    def analyze_duplicates(self):
        """åˆ†æé‡å¤Issues"""
        print("ğŸ¤– AIåˆ†æé‡å¤Issues...")
        
        # åŠ è½½Issuesæ•°æ®
        issues = self.load_issues()
        if not issues:
            print("âŒ æœªæ‰¾åˆ°Issuesæ•°æ®ï¼Œè¯·å…ˆä¸‹è½½Issues")
            return False
        
        print(f"ğŸ“Š åˆ†æ {len(issues)} ä¸ªIssuesä¸­çš„é‡å¤é¡¹...")
        
        if self.has_api and (self.anthropic_client or self.openai_client):
            # APIæ¨¡å¼ï¼šè‡ªåŠ¨åŒ–åˆ†æ
            duplicates = self._api_analyze_duplicates(issues)
        else:
            # Copilotäº¤äº’æ¨¡å¼ï¼šå‡†å¤‡æ•°æ®ä¾›ç”¨æˆ·åˆ†æ
            duplicates = self._copilot_analyze_duplicates(issues)
        
        # ä¿å­˜åˆ†æç»“æœ
        report_file = self.output_dir / f"duplicates_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        self.save_duplicates_report(duplicates, report_file)
        
        print(f"âœ… é‡å¤åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
        return True
    
    def optimize_labels(self):
        """AIä¼˜åŒ–æ ‡ç­¾åˆ†ç±»"""
        print("ğŸ¤– AIä¼˜åŒ–æ ‡ç­¾åˆ†ç±»...")
        
        issues = self.load_issues()
        if not issues:
            print("âŒ æœªæ‰¾åˆ°Issuesæ•°æ®")
            return False
        
        print(f"ğŸ“Š åˆ†æ {len(issues)} ä¸ªIssuesçš„æ ‡ç­¾ä¼˜åŒ–å»ºè®®...")
        
        if self.has_api and (self.anthropic_client or self.openai_client):
            label_suggestions = self._api_optimize_labels(issues)
        else:
            label_suggestions = self._copilot_optimize_labels(issues)
        
        # ä¿å­˜å»ºè®®
        report_file = self.output_dir / f"label_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        self.save_label_report(label_suggestions, report_file)
        
        print(f"âœ… æ ‡ç­¾ä¼˜åŒ–åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
        return True
    
    def evaluate_priority(self):
        """AIè¯„ä¼°ä¼˜å…ˆçº§"""
        print("ğŸ¤– AIè¯„ä¼°Issuesä¼˜å…ˆçº§...")
        
        issues = self.load_issues()
        if not issues:
            print("âŒ æœªæ‰¾åˆ°Issuesæ•°æ®")
            return False
        
        print(f"ğŸ“Š è¯„ä¼° {len(issues)} ä¸ªIssuesçš„ä¼˜å…ˆçº§...")
        
        if self.has_api and (self.anthropic_client or self.openai_client):
            priority_analysis = self._api_evaluate_priority(issues)
        else:
            priority_analysis = self._copilot_evaluate_priority(issues)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        report_file = self.output_dir / f"priority_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        self.save_priority_report(priority_analysis, report_file)
        
        print(f"âœ… ä¼˜å…ˆçº§è¯„ä¼°å®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
        return True
    
    def comprehensive_analysis(self):
        """AIç»¼åˆåˆ†æ"""
        print("ğŸ¤– AIç»¼åˆåˆ†æIssues...")
        
        issues = self.load_issues()
        if not issues:
            print("âŒ æœªæ‰¾åˆ°Issuesæ•°æ®")
            return False
        
        print(f"ğŸ“Š å¯¹ {len(issues)} ä¸ªIssuesè¿›è¡Œç»¼åˆåˆ†æ...")
        
        # æ”¶é›†æ‰€æœ‰åˆ†ææ•°æ®
        context_data = self._prepare_analysis_context(issues)
        
        if self.has_api and (self.anthropic_client or self.openai_client):
            analysis_results = self._api_comprehensive_analysis(context_data)
        else:
            analysis_results = self._copilot_comprehensive_analysis(context_data)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report_file = self.output_dir / f"comprehensive_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        self.save_comprehensive_report(analysis_results, report_file)
        
        print(f"âœ… ç»¼åˆåˆ†æå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
        return True
    
    def _prepare_analysis_context(self, issues):
        """å‡†å¤‡åˆ†æä¸Šä¸‹æ–‡æ•°æ®"""
        print("ğŸ“‹ å‡†å¤‡åˆ†æä¸Šä¸‹æ–‡...")
        
        context = {
            'issues': issues,
            'statistics': self._get_issues_statistics(issues),
            'team_info': self.team_info,
            'github_stats': self._get_github_stats()
        }
        
        return context
    
    def _get_issues_statistics(self, issues):
        """è·å–Issuesç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total': len(issues),
            'open': len([i for i in issues if i.get('state') == 'open']),
            'closed': len([i for i in issues if i.get('state') == 'closed']),
            'labels': {},
            'assignees': {},
        }
        
        # ç»Ÿè®¡æ ‡ç­¾
        for issue in issues:
            for label in issue.get('labels', []):
                label_name = label if isinstance(label, str) else label.get('name', 'unknown')
                stats['labels'][label_name] = stats['labels'].get(label_name, 0) + 1
        
        # ç»Ÿè®¡åˆ†é…ç»™
        for issue in issues:
            assignee = issue.get('assignee')
            if assignee:
                assignee_name = assignee if isinstance(assignee, str) else assignee.get('login', 'unknown')
                stats['assignees'][assignee_name] = stats['assignees'].get(assignee_name, 0) + 1
        
        return stats
    
    def _get_github_stats(self):
        """è·å–GitHubä»“åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è°ƒç”¨legacyå·¥å…·è·å–æ›´å¤šä¿¡æ¯
            from _github_operations import GitHubManager
            github_manager = GitHubManager()
            return github_manager.get_repo_stats()
        except:
            return {"message": "GitHubç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥"}
    
    # Copilotäº¤äº’æ¨¡å¼å®ç°
    def _copilot_analyze_duplicates(self, issues):
        """Copilotäº¤äº’æ¨¡å¼ï¼šé‡å¤åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ¤– Copilotäº¤äº’æ¨¡å¼ - é‡å¤Issuesåˆ†æ")
        print("="*60)
        
        # å‡†å¤‡ä¾›åˆ†æçš„æ•°æ®æ‘˜è¦
        analysis_data = {
            'total_issues': len(issues),
            'sample_titles': [issue.get('title', 'No title')[:80] for issue in issues[:10]],
            'all_titles': [issue.get('title', 'No title') for issue in issues]
        }
        
        print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
        print(f"   æ€»Issuesæ•°: {analysis_data['total_issues']}")
        print(f"   æ ·æœ¬æ ‡é¢˜é¢„è§ˆ:")
        for i, title in enumerate(analysis_data['sample_titles'], 1):
            print(f"   {i:2d}. {title}")
        
        # ğŸ†• ç›´æ¥è¿›è¡Œé‡å¤åˆ†æ
        print(f"\nï¿½ æ­£åœ¨åˆ†æé‡å¤é¡¹...")
        duplicates_found = self._find_potential_duplicates(issues)
        
        if duplicates_found:
            print(f"\nâš ï¸ å‘ç° {len(duplicates_found)} ç»„æ½œåœ¨é‡å¤Issues:")
            for dup in duplicates_found:
                print(f"   - #{dup['issue1']['number']}: {dup['issue1']['title'][:50]}...")
                print(f"     #{dup['issue2']['number']}: {dup['issue2']['title'][:50]}...")
                print(f"     ç›¸ä¼¼åº¦: {dup['similarity']}")
                print()
        else:
            print(f"\nâœ… æœªå‘ç°æ˜æ˜¾çš„é‡å¤Issues")
        
        # ä¿å­˜å®Œæ•´æ•°æ®ä¾›è¿›ä¸€æ­¥åˆ†æ
        data_file = self.output_dir / f"duplicates_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump({**analysis_data, 'duplicates_found': duplicates_found}, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
        
        return {
            'mode': 'copilot_interactive',
            'data_file': str(data_file),
            'duplicates_found': duplicates_found,
            'analysis_complete': True
        }
    
    def _copilot_optimize_labels(self, issues):
        """Copilotäº¤äº’æ¨¡å¼ï¼šæ ‡ç­¾ä¼˜åŒ–"""
        print("\n" + "="*60)
        print("ğŸ¤– Copilotäº¤äº’æ¨¡å¼ - æ ‡ç­¾ä¼˜åŒ–åˆ†æ")
        print("="*60)
        
        # åˆ†æç°æœ‰æ ‡ç­¾
        all_labels = {}
        for issue in issues:
            for label in issue.get('labels', []):
                label_name = label if isinstance(label, str) else label.get('name', 'unknown')
                all_labels[label_name] = all_labels.get(label_name, 0) + 1
        
        # ğŸ†• ç›´æ¥åˆ†ææ ‡ç­¾ä¼˜åŒ–å»ºè®®
        print(f"\nğŸ“Š å½“å‰æ ‡ç­¾åˆ†æ:")
        print(f"   æ ‡ç­¾æ€»æ•°: {len(all_labels)}")
        print(f"   ä½¿ç”¨æœ€å¤šçš„æ ‡ç­¾:")
        most_used = sorted(all_labels.items(), key=lambda x: x[1], reverse=True)[:10]
        for label, count in most_used:
            print(f"   - {label}: {count} æ¬¡")
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_suggestions = self._generate_label_optimization_suggestions(all_labels, issues)
        
        print(f"\nğŸ’¡ æ ‡ç­¾ä¼˜åŒ–å»ºè®®:")
        for suggestion in optimization_suggestions:
            print(f"   - {suggestion}")
        
        analysis_data = {
            'current_labels': all_labels,
            'label_count': len(all_labels),
            'most_used': most_used,
            'optimization_suggestions': optimization_suggestions,
            'issues_sample': [
                {
                    'title': issue.get('title', 'No title')[:80],
                    'labels': [l if isinstance(l, str) else l.get('name', 'unknown') 
                              for l in issue.get('labels', [])]
                }
                for issue in issues[:15]
            ]
        }
        
        # ä¿å­˜æ•°æ®
        data_file = self.output_dir / f"labels_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
        
        return {
            'mode': 'copilot_interactive',
            'data_file': str(data_file),
            'current_labels': all_labels,
            'optimization_suggestions': optimization_suggestions,
            'analysis_complete': True
        }
    
    def _copilot_evaluate_priority(self, issues):
        """Copilotäº¤äº’æ¨¡å¼ï¼šä¼˜å…ˆçº§è¯„ä¼°"""
        print("\n" + "="*60)
        print("ğŸ¤– Copilotäº¤äº’æ¨¡å¼ - ä¼˜å…ˆçº§è¯„ä¼°")
        print("="*60)
        
        # ğŸ†• ç›´æ¥è¿›è¡Œä¼˜å…ˆçº§è¯„ä¼°
        print(f"\nğŸ” æ­£åœ¨è¯„ä¼° {len(issues)} ä¸ªIssuesçš„ä¼˜å…ˆçº§...")
        
        priority_assessment = []
        for issue in issues:
            title = issue.get('title', 'No title')
            labels = [l if isinstance(l, str) else l.get('name', 'unknown') 
                     for l in issue.get('labels', [])]
            
            priority = self._assess_priority(title, labels)
            priority_assessment.append({
                'number': issue.get('number', 0),
                'title': title,
                'labels': labels,
                'priority': priority,
                'reasoning': self._get_priority_reasoning(title, labels, priority)
            })
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
        high_priority = [item for item in priority_assessment if item['priority'] == 'high']
        medium_priority = [item for item in priority_assessment if item['priority'] == 'medium']
        low_priority = [item for item in priority_assessment if item['priority'] == 'low']
        
        print(f"\nğŸ“Š ä¼˜å…ˆçº§è¯„ä¼°ç»“æœ:")
        print(f"   é«˜ä¼˜å…ˆçº§: {len(high_priority)} ä¸ª")
        print(f"   ä¸­ä¼˜å…ˆçº§: {len(medium_priority)} ä¸ª")
        print(f"   ä½ä¼˜å…ˆçº§: {len(low_priority)} ä¸ª")
        
        if high_priority:
            print(f"\nğŸ”¥ é«˜ä¼˜å…ˆçº§Issues:")
            for item in high_priority:
                print(f"   - #{item['number']}: {item['title'][:60]}...")
                print(f"     ç†ç”±: {item['reasoning']}")
        
        analysis_data = {
            'priority_assessment': priority_assessment,
            'summary': {
                'high': len(high_priority),
                'medium': len(medium_priority),
                'low': len(low_priority)
            },
            'priority_factors': [
                "æ ‡ç­¾ä¸­æ˜¯å¦åŒ…å« 'critical', 'urgent', 'high-priority'",
                "Issueåˆ›å»ºæ—¶é—´ï¼ˆè¶Šä¹…ä¼˜å…ˆçº§å¯èƒ½è¶Šé«˜ï¼‰",
                "åˆ†é…çŠ¶æ€å’Œè´Ÿè´£äºº",
                "Issuesæè¿°çš„è¯¦ç»†ç¨‹åº¦",
                "ç¤¾åŒºè®¨è®ºçƒ­åº¦ï¼ˆè¯„è®ºæ•°ï¼‰"
            ],
            'team_context': self.team_info
        }
        
        # ä¿å­˜æ•°æ®
        data_file = self.output_dir / f"priority_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
        
        return {
            'mode': 'copilot_interactive',
            'data_file': str(data_file),
            'priority_assessment': priority_assessment,
            'analysis_complete': True
        }
    
    def _copilot_comprehensive_analysis(self, context_data):
        """Copilotäº¤äº’æ¨¡å¼ï¼šç»¼åˆåˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ¤– Copilotäº¤äº’æ¨¡å¼ - ç»¼åˆIssuesåˆ†æ")
        print("="*60)
        
        print(f"\nğŸ“Š ç»¼åˆåˆ†æä¸Šä¸‹æ–‡:")
        print(f"   Issuesæ€»æ•°: {context_data['statistics']['total']}")
        print(f"   å¼€æ”¾Issues: {context_data['statistics']['open']}")
        print(f"   å·²å…³é—­Issues: {context_data['statistics']['closed']}")
        if context_data['team_info']:
            print(f"   å›¢é˜Ÿæˆå‘˜æ•°: {len(context_data['team_info']['all_usernames'])}")
            print(f"   å›¢é˜Ÿæ•°é‡: {context_data['team_info']['team_count']}")
        
        # ä¿å­˜å®Œæ•´åˆ†æä¸Šä¸‹æ–‡
        data_file = self.output_dir / f"comprehensive_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(context_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“„ ç»¼åˆåˆ†ææ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
        
        # ğŸ†• ç›´æ¥è¿›è¡Œåˆ†æï¼Œè€Œä¸æ˜¯è®©ç”¨æˆ·æ‰‹åŠ¨æ“ä½œ
        print("\nğŸ” æ­£åœ¨ç›´æ¥åˆ†ææ•°æ®...")
        analysis_results = self._analyze_data_directly(context_data)
        
        # ç”Ÿæˆåˆ†ææŒ‡å¯¼
        analysis_guide = {
            'analysis_areas': [
                "Issuesåˆ†å¸ƒå’Œè¶‹åŠ¿åˆ†æ",
                "å›¢é˜Ÿå·¥ä½œè´Ÿè½½åˆ†æ", 
                "æ ‡ç­¾å’Œåˆ†ç±»ä¼˜åŒ–å»ºè®®",
                "ä¼˜å…ˆçº§å’Œç´§æ€¥ç¨‹åº¦è¯„ä¼°",
                "é‡å¤Issuesè¯†åˆ«å’Œåˆå¹¶å»ºè®®",
                "å·¥ä½œæµç¨‹æ”¹è¿›å»ºè®®"
            ],
            'data_file': str(data_file),
            'mode': 'copilot_interactive',
            'direct_analysis': analysis_results  # ğŸ†• æ·»åŠ ç›´æ¥åˆ†æç»“æœ
        }
        
        return analysis_guide
    
    def _analyze_data_directly(self, context_data):
        """ç›´æ¥åˆ†æIssuesæ•°æ®ï¼Œè€Œä¸éœ€è¦ç”¨æˆ·æ‰‹åŠ¨æ“ä½œ"""
        print("ğŸ“ˆ å¼€å§‹ç›´æ¥åˆ†æIssuesæ•°æ®...")
        
        issues = context_data.get('issues', [])
        stats = context_data.get('statistics', {})
        team_info = context_data.get('team_info', {})
        
        analysis = {
            'summary': self._generate_summary(issues, stats),
            'issues_analysis': self._analyze_issues_content(issues),
            'team_analysis': self._analyze_team_distribution(issues, team_info),
            'recommendations': self._generate_recommendations(issues, stats, team_info)
        }
        
        # æ‰“å°åˆ†æç»“æœ
        self._print_analysis_results(analysis)
        
        return analysis
    
    def _generate_summary(self, issues, stats):
        """ç”ŸæˆIssuesæ¦‚è¦åˆ†æ"""
        total = stats.get('total', 0)
        open_count = stats.get('open', 0)
        closed_count = stats.get('closed', 0)
        
        completion_rate = (closed_count / total * 100) if total > 0 else 0
        
        return {
            'total_issues': total,
            'completion_rate': f"{completion_rate:.1f}%",
            'open_issues': open_count,
            'closed_issues': closed_count,
            'health_status': 'healthy' if completion_rate > 50 else 'needs_attention'
        }
    
    def _analyze_issues_content(self, issues):
        """åˆ†æIssueså†…å®¹"""
        analysis = {
            'titles_analysis': [],
            'labels_distribution': {},
            'potential_duplicates': [],
            'priority_assessment': []
        }
        
        # åˆ†ææ ‡é¢˜
        for issue in issues:
            title = issue.get('title', '')
            labels = [label.get('name', '') for label in issue.get('labels', [])]
            
            analysis['titles_analysis'].append({
                'issue_number': issue.get('number', 'unknown'),
                'title': title,
                'labels': labels,
                'type': self._classify_issue_type(title, labels),
                'priority': self._assess_priority(title, labels)
            })
        
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        all_labels = {}
        for issue in issues:
            for label in issue.get('labels', []):
                label_name = label.get('name', 'unknown')
                all_labels[label_name] = all_labels.get(label_name, 0) + 1
        
        analysis['labels_distribution'] = all_labels
        
        # æŸ¥æ‰¾æ½œåœ¨é‡å¤
        analysis['potential_duplicates'] = self._find_potential_duplicates(issues)
        
        return analysis
    
    def _classify_issue_type(self, title, labels):
        """åˆ†ç±»Issueç±»å‹"""
        title_lower = title.lower()
        
        if any(label in ['bug', 'fix'] for label in labels) or any(word in title_lower for word in ['bug', 'ä¿®å¤', 'fix', 'é”™è¯¯']):
            return 'bug_fix'
        elif any(label in ['enhancement', 'feature'] for label in labels) or any(word in title_lower for word in ['æ·»åŠ ', 'æ–°å¢', 'add', 'feature']):
            return 'feature_enhancement'
        elif any(label in ['documentation', 'docs'] for label in labels) or any(word in title_lower for word in ['æ–‡æ¡£', 'doc', 'documentation']):
            return 'documentation'
        else:
            return 'other'
    
    def _assess_priority(self, title, labels):
        """è¯„ä¼°ä¼˜å…ˆçº§"""
        title_lower = title.lower()
        
        # é«˜ä¼˜å…ˆçº§å…³é”®è¯
        high_priority_keywords = ['critical', 'ç´§æ€¥', 'urgent', 'é‡è¦', 'crash', 'å´©æºƒ']
        if any(keyword in title_lower for keyword in high_priority_keywords):
            return 'high'
        
        # ä¸­ä¼˜å…ˆçº§å…³é”®è¯
        medium_priority_keywords = ['improve', 'ä¼˜åŒ–', 'enhance', 'æ”¹è¿›']
        if any(keyword in title_lower for keyword in medium_priority_keywords):
            return 'medium'
        
        return 'low'
    
    def _find_potential_duplicates(self, issues):
        """æŸ¥æ‰¾æ½œåœ¨çš„é‡å¤Issues"""
        duplicates = []
        
        for i, issue1 in enumerate(issues):
            for j, issue2 in enumerate(issues[i+1:], i+1):
                similarity = self._calculate_title_similarity(
                    issue1.get('title', ''), 
                    issue2.get('title', '')
                )
                
                if similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    duplicates.append({
                        'issue1': {
                            'number': issue1.get('number', 'unknown'),
                            'title': issue1.get('title', '')
                        },
                        'issue2': {
                            'number': issue2.get('number', 'unknown'),
                            'title': issue2.get('title', '')
                        },
                        'similarity': f"{similarity:.2%}"
                    })
        
        return duplicates
    
    def _calculate_title_similarity(self, title1, title2):
        """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
        words1 = set(title1.lower().split())
        words2 = set(title2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _analyze_team_distribution(self, issues, team_info):
        """åˆ†æå›¢é˜Ÿåˆ†é…æƒ…å†µ"""
        if not team_info:
            return {'message': 'æ— å›¢é˜Ÿä¿¡æ¯'}
        
        teams = team_info.get('teams', {})
        all_usernames = set(team_info.get('all_usernames', []))
        
        analysis = {
            'team_workload': {},
            'unassigned_issues': 0,
            'external_assignees': []
        }
        
        for team_name, team_data in teams.items():
            team_members = {member['username'] for member in team_data.get('members', [])}
            analysis['team_workload'][team_name] = {
                'members': len(team_members),
                'assigned_issues': 0
            }
        
        # åˆ†æIssuesåˆ†é…æƒ…å†µ
        for issue in issues:
            assignee = issue.get('assignee')
            if assignee:
                assignee_name = assignee.get('login', '') if isinstance(assignee, dict) else str(assignee)
                if assignee_name in all_usernames:
                    # æ‰¾åˆ°è¯¥ç”¨æˆ·æ‰€å±çš„å›¢é˜Ÿ
                    for team_name, team_data in teams.items():
                        team_members = {member['username'] for member in team_data.get('members', [])}
                        if assignee_name in team_members:
                            analysis['team_workload'][team_name]['assigned_issues'] += 1
                            break
                else:
                    analysis['external_assignees'].append(assignee_name)
            else:
                analysis['unassigned_issues'] += 1
        
        return analysis
    
    def _generate_recommendations(self, issues, stats, team_info):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºå®Œæˆç‡çš„å»ºè®®
        total = stats.get('total', 0)
        closed = stats.get('closed', 0)
        completion_rate = (closed / total * 100) if total > 0 else 0
        
        if completion_rate < 50:
            recommendations.append("ğŸ“ˆ Issueså®Œæˆç‡è¾ƒä½ï¼Œå»ºè®®åŠ å¼ºé¡¹ç›®ç®¡ç†å’Œä»»åŠ¡åˆ†é…")
        
        # åŸºäºæ ‡ç­¾çš„å»ºè®®
        labels = stats.get('labels', {})
        if 'bug' in labels and labels['bug'] > total * 0.3:
            recommendations.append("ğŸ› Bugç±»Issuesè¾ƒå¤šï¼Œå»ºè®®åŠ å¼ºä»£ç è´¨é‡æ§åˆ¶å’Œæµ‹è¯•")
        
        # åŸºäºå›¢é˜Ÿåˆ†é…çš„å»ºè®®
        if team_info:
            unassigned_count = sum(1 for issue in issues if not issue.get('assignee'))
            if unassigned_count > total * 0.2:
                recommendations.append("ğŸ‘¥ æœªåˆ†é…çš„Issuesè¾ƒå¤šï¼Œå»ºè®®æ˜ç¡®è´£ä»»äºº")
        
        # åŸºäºé‡å¤Issuesçš„å»ºè®®
        duplicates = self._find_potential_duplicates(issues)
        if duplicates:
            recommendations.append(f"ğŸ”„ å‘ç° {len(duplicates)} ç»„æ½œåœ¨é‡å¤Issuesï¼Œå»ºè®®åˆå¹¶å¤„ç†")
        
        return recommendations
    
    def _generate_label_optimization_suggestions(self, all_labels, issues):
        """ç”Ÿæˆæ ‡ç­¾ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # æ£€æŸ¥å¸¸è§æ ‡ç­¾æ˜¯å¦å­˜åœ¨
        recommended_labels = ['bug', 'enhancement', 'documentation', 'question', 'help wanted']
        missing_labels = [label for label in recommended_labels if label not in all_labels]
        
        if missing_labels:
            suggestions.append(f"å»ºè®®æ·»åŠ æ ‡å‡†æ ‡ç­¾: {', '.join(missing_labels)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä½¿ç”¨é¢‘ç‡å¾ˆä½çš„æ ‡ç­¾
        low_usage_labels = [label for label, count in all_labels.items() if count == 1]
        if len(low_usage_labels) > 3:
            suggestions.append(f"æœ‰ {len(low_usage_labels)} ä¸ªæ ‡ç­¾ä»…ä½¿ç”¨ä¸€æ¬¡ï¼Œè€ƒè™‘æ˜¯å¦éœ€è¦åˆå¹¶")
        
        # æ£€æŸ¥æ ‡ç­¾å‘½åä¸€è‡´æ€§
        similar_labels = self._find_similar_labels(all_labels.keys())
        if similar_labels:
            suggestions.append("å‘ç°ç›¸ä¼¼æ ‡ç­¾ï¼Œå»ºè®®ç»Ÿä¸€å‘½å: " + str(similar_labels))
        
        return suggestions
    
    def _find_similar_labels(self, label_names):
        """æŸ¥æ‰¾ç›¸ä¼¼çš„æ ‡ç­¾åç§°"""
        similar_pairs = []
        label_list = list(label_names)
        
        for i, label1 in enumerate(label_list):
            for label2 in label_list[i+1:]:
                if self._calculate_title_similarity(label1, label2) > 0.5:
                    similar_pairs.append((label1, label2))
        
        return similar_pairs
    
    def _get_priority_reasoning(self, title, labels, priority):
        """è·å–ä¼˜å…ˆçº§è¯„ä¼°ç†ç”±"""
        reasons = []
        
        if priority == 'high':
            if any(word in title.lower() for word in ['critical', 'ç´§æ€¥', 'urgent', 'é‡è¦']):
                reasons.append("æ ‡é¢˜åŒ…å«é«˜ä¼˜å…ˆçº§å…³é”®è¯")
            if 'bug' in labels:
                reasons.append("å±äºBugç±»Issue")
        elif priority == 'medium':
            if any(word in title.lower() for word in ['improve', 'ä¼˜åŒ–', 'enhance']):
                reasons.append("å±äºæ”¹è¿›ç±»Issue")
        else:
            reasons.append("å¸¸è§„ä¼˜å…ˆçº§Issue")
        
        return "; ".join(reasons) if reasons else "åŸºäºæ ‡é¢˜å’Œæ ‡ç­¾çš„ç»¼åˆè¯„ä¼°"
    
    def _print_analysis_results(self, analysis):
        """æ‰“å°åˆ†æç»“æœåˆ°æ§åˆ¶å°"""
        print("\n" + "="*60)
        print("ğŸ“Š Issuesæ•°æ®åˆ†æç»“æœ")
        print("="*60)
        
        # æ‰“å°æ¦‚è¦
        summary = analysis.get('summary', {})
        print(f"\nğŸ“ˆ æ¦‚è¦ç»Ÿè®¡:")
        print(f"   æ€»Issues: {summary.get('total_issues', 0)}")
        print(f"   å®Œæˆç‡: {summary.get('completion_rate', 'N/A')}")
        print(f"   å¥åº·çŠ¶æ€: {summary.get('health_status', 'unknown')}")
        
        # æ‰“å°Issuesåˆ†æ
        issues_analysis = analysis.get('issues_analysis', {})
        
        # æ ‡ç­¾åˆ†å¸ƒ
        labels_dist = issues_analysis.get('labels_distribution', {})
        if labels_dist:
            print(f"\nğŸ·ï¸ æ ‡ç­¾åˆ†å¸ƒ (å‰5):")
            sorted_labels = sorted(labels_dist.items(), key=lambda x: x[1], reverse=True)[:5]
            for label, count in sorted_labels:
                print(f"   - {label}: {count}")
        
        # é‡å¤Issues
        duplicates = issues_analysis.get('potential_duplicates', [])
        if duplicates:
            print(f"\nâš ï¸ å‘ç° {len(duplicates)} ç»„æ½œåœ¨é‡å¤Issues:")
            for dup in duplicates[:3]:  # åªæ˜¾ç¤ºå‰3ç»„
                print(f"   - #{dup['issue1']['number']} vs #{dup['issue2']['number']} (ç›¸ä¼¼åº¦: {dup['similarity']})")
        
        # å›¢é˜Ÿåˆ†æ
        team_analysis = analysis.get('team_analysis', {})
        if 'team_workload' in team_analysis:
            print(f"\nğŸ‘¥ å›¢é˜Ÿå·¥ä½œè´Ÿè½½:")
            for team, workload in team_analysis['team_workload'].items():
                print(f"   - {team}: {workload['assigned_issues']} issues ({workload['members']} æˆå‘˜)")
        
        # å»ºè®®
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for rec in recommendations:
                print(f"   {rec}")

    # APIæ¨¡å¼å®ç°ï¼ˆç•™ä½œæ‰©å±•ï¼‰
    def _api_analyze_duplicates(self, issues):
        """APIæ¨¡å¼ï¼šè‡ªåŠ¨åŒ–é‡å¤åˆ†æ"""
        print("ğŸ”„ APIæ¨¡å¼åˆ†æ...")
        return self._copilot_analyze_duplicates(issues)
    
    def _api_optimize_labels(self, issues):
        """APIæ¨¡å¼ï¼šè‡ªåŠ¨åŒ–æ ‡ç­¾ä¼˜åŒ–"""
        print("ğŸ”„ APIæ¨¡å¼åˆ†æ...")
        return self._copilot_optimize_labels(issues)
    
    def _api_evaluate_priority(self, issues):
        """APIæ¨¡å¼ï¼šè‡ªåŠ¨åŒ–ä¼˜å…ˆçº§è¯„ä¼°"""
        print("ğŸ”„ APIæ¨¡å¼åˆ†æ...")
        return self._copilot_evaluate_priority(issues)
    
    def _api_comprehensive_analysis(self, context_data):
        """APIæ¨¡å¼ï¼šè‡ªåŠ¨åŒ–ç»¼åˆåˆ†æ"""
        print("ğŸ”„ APIæ¨¡å¼åˆ†æ...")
        return self._copilot_comprehensive_analysis(context_data)
    
    def load_issues(self):
        """åŠ è½½Issuesæ•°æ®"""
        issues_dir = self.workspace_dir / "issues"
        if not issues_dir.exists():
            print("âš ï¸ Issuesç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–ä½ç½®...")
            # å°è¯•legacyç›®å½•ä¸­çš„æ•°æ®
            legacy_file = Path(__file__).parent.parent / "downloaded_issues" / "github_issues.json"
            if legacy_file.exists():
                print(f"âœ… æ‰¾åˆ°legacyæ•°æ®: {legacy_file}")
                with open(legacy_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return []
        
        issues = []
        for md_file in issues_dir.glob("*.md"):
            issue_data = self.parse_issue_markdown(md_file)
            if issue_data:
                issues.append(issue_data)
        
        return issues
    
    def parse_issue_markdown(self, md_file):
        """è§£æMarkdownæ ¼å¼çš„Issueæ–‡ä»¶"""
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç®€å•è§£æMarkdownæ ¼å¼
            lines = content.split('\n')
            title = lines[0].replace('# ', '') if lines else ""
            
            # æå–åŸºæœ¬ä¿¡æ¯
            issue_data = {
                'title': title,
                'filename': md_file.name,
                'content': content,
                'state': 'open' if 'open_' in md_file.name else 'closed',
                'labels': [],
                'assignee': None
            }
            
            # å°è¯•è§£ææ›´å¤šä¿¡æ¯
            for line in lines:
                if line.startswith('**ç¼–å·**:'):
                    try:
                        issue_data['number'] = int(line.split('#')[1].strip())
                    except:
                        pass
                elif line.startswith('## æ ‡ç­¾'):
                    # ä¸‹ä¸€è¡Œåº”è¯¥æ˜¯æ ‡ç­¾
                    idx = lines.index(line)
                    if idx + 1 < len(lines):
                        labels_line = lines[idx + 1].strip()
                        if labels_line:
                            issue_data['labels'] = [{'name': label.strip()} for label in labels_line.split(',')]
            
            return issue_data
        except Exception as e:
            print(f"âŒ è§£ææ–‡ä»¶å¤±è´¥ {md_file}: {e}")
            return None
    
    def save_duplicates_report(self, duplicates, report_file):
        """ä¿å­˜é‡å¤åˆ†ææŠ¥å‘Š"""
        content = f"""# Issuesé‡å¤åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†ææ¨¡å¼**: {duplicates.get('mode', 'unknown')}

## åˆ†æç»“æœ

"""
        if duplicates.get('analysis_complete'):
            duplicates_found = duplicates.get('duplicates_found', [])
            if duplicates_found:
                content += f"### å‘ç°çš„é‡å¤Issues ({len(duplicates_found)} ç»„)\n\n"
                for i, dup in enumerate(duplicates_found, 1):
                    content += f"#### {i}. é‡å¤ç»„\n"
                    content += f"- **Issue #{dup['issue1']['number']}**: {dup['issue1']['title']}\n"
                    content += f"- **Issue #{dup['issue2']['number']}**: {dup['issue2']['title']}\n"
                    content += f"- **ç›¸ä¼¼åº¦**: {dup['similarity']}\n\n"
            else:
                content += "âœ… æœªå‘ç°æ˜æ˜¾çš„é‡å¤Issues\n\n"
        
        content += f"""### æ•°æ®æ–‡ä»¶
{duplicates.get('data_file', 'N/A')}

### å»ºè®®è¡ŒåŠ¨
- æ£€æŸ¥ç›¸ä¼¼åº¦é«˜çš„Issuesæ˜¯å¦å¯ä»¥åˆå¹¶
- å»ºç«‹Issueåˆ›å»ºæ¨¡æ¿ä»¥å‡å°‘é‡å¤
- æ”¹è¿›æœç´¢åŠŸèƒ½å¸®åŠ©ç”¨æˆ·æ‰¾åˆ°å·²æœ‰Issues

"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def save_label_report(self, suggestions, report_file):
        """ä¿å­˜æ ‡ç­¾ä¼˜åŒ–æŠ¥å‘Š"""
        content = f"""# æ ‡ç­¾ä¼˜åŒ–å»ºè®®æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†ææ¨¡å¼**: {suggestions.get('mode', 'unknown')}

## å½“å‰æ ‡ç­¾çŠ¶æ€

"""
        if suggestions.get('current_labels'):
            content += "### ç°æœ‰æ ‡ç­¾ç»Ÿè®¡\n"
            sorted_labels = sorted(suggestions['current_labels'].items(), key=lambda x: x[1], reverse=True)
            for label, count in sorted_labels:
                content += f"- **{label}**: {count} æ¬¡ä½¿ç”¨\n"
        
        if suggestions.get('analysis_complete'):
            opt_suggestions = suggestions.get('optimization_suggestions', [])
            if opt_suggestions:
                content += f"\n## ä¼˜åŒ–å»ºè®®\n\n"
                for suggestion in opt_suggestions:
                    content += f"- {suggestion}\n"
        
        content += f"""
### æ•°æ®æ–‡ä»¶
{suggestions.get('data_file', 'N/A')}

### å®æ–½å»ºè®®
1. ç»Ÿä¸€æ ‡ç­¾å‘½åè§„èŒƒ
2. å®šæœŸæ¸…ç†ä½ä½¿ç”¨é¢‘ç‡çš„æ ‡ç­¾
3. å»ºç«‹æ ‡ç­¾ä½¿ç”¨æŒ‡å—
4. è‡ªåŠ¨åŒ–æ ‡ç­¾å»ºè®®ç³»ç»Ÿ

"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def save_priority_report(self, analysis, report_file):
        """ä¿å­˜ä¼˜å…ˆçº§åˆ†ææŠ¥å‘Š"""
        content = f"""# ä¼˜å…ˆçº§åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†ææ¨¡å¼**: {analysis.get('mode', 'unknown')}

## ä¼˜å…ˆçº§åˆ†å¸ƒ

"""
        if analysis.get('analysis_complete'):
            priority_assessment = analysis.get('priority_assessment', [])
            
            # ç»Ÿè®¡å„ä¼˜å…ˆçº§æ•°é‡
            high_count = len([item for item in priority_assessment if item.get('priority') == 'high'])
            medium_count = len([item for item in priority_assessment if item.get('priority') == 'medium'])
            low_count = len([item for item in priority_assessment if item.get('priority') == 'low'])
            
            content += f"- **é«˜ä¼˜å…ˆçº§**: {high_count} ä¸ª\n"
            content += f"- **ä¸­ä¼˜å…ˆçº§**: {medium_count} ä¸ª\n"
            content += f"- **ä½ä¼˜å…ˆçº§**: {low_count} ä¸ª\n\n"
            
            # è¯¦ç»†åˆ—å‡ºé«˜ä¼˜å…ˆçº§Issues
            high_priority_issues = [item for item in priority_assessment if item.get('priority') == 'high']
            if high_priority_issues:
                content += "## é«˜ä¼˜å…ˆçº§Issuesè¯¦æƒ…\n\n"
                for item in high_priority_issues:
                    content += f"### Issue #{item['number']}\n"
                    content += f"- **æ ‡é¢˜**: {item['title']}\n"
                    content += f"- **æ ‡ç­¾**: {', '.join(item['labels'])}\n"
                    content += f"- **è¯„ä¼°ç†ç”±**: {item['reasoning']}\n\n"
        
        content += f"""
### æ•°æ®æ–‡ä»¶
{analysis.get('data_file', 'N/A')}

### è¡ŒåŠ¨å»ºè®®
1. ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§Issues
2. å»ºç«‹ä¼˜å…ˆçº§è¯„ä¼°æ ‡å‡†
3. å®šæœŸå›é¡¾å’Œè°ƒæ•´ä¼˜å…ˆçº§
4. å¹³è¡¡å¼€å‘èµ„æºåˆ†é…

"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def save_comprehensive_report(self, results, report_file):
        """ä¿å­˜ç»¼åˆåˆ†ææŠ¥å‘Š"""
        content = f"""# Issuesç»¼åˆåˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†ææ¨¡å¼**: {results.get('mode', 'unknown')}

## æ‰§è¡Œæ‘˜è¦

"""
        
        if results.get('direct_analysis'):
            analysis = results['direct_analysis']
            summary = analysis.get('summary', {})
            
            content += f"- **æ€»Issuesæ•°**: {summary.get('total_issues', 0)}\n"
            content += f"- **å®Œæˆç‡**: {summary.get('completion_rate', 'N/A')}\n"
            content += f"- **å¥åº·çŠ¶æ€**: {summary.get('health_status', 'unknown')}\n\n"
            
            # Issuesç±»å‹åˆ†å¸ƒ
            issues_analysis = analysis.get('issues_analysis', {})
            if 'titles_analysis' in issues_analysis:
                type_counts = {}
                for item in issues_analysis['titles_analysis']:
                    issue_type = item.get('type', 'other')
                    type_counts[issue_type] = type_counts.get(issue_type, 0) + 1
                
                content += "## Issuesç±»å‹åˆ†å¸ƒ\n\n"
                for issue_type, count in type_counts.items():
                    content += f"- **{issue_type}**: {count} ä¸ª\n"
                content += "\n"
            
            # é‡å¤Issues
            duplicates = issues_analysis.get('potential_duplicates', [])
            if duplicates:
                content += f"## é‡å¤Issuesæ£€æµ‹\n\nå‘ç° {len(duplicates)} ç»„æ½œåœ¨é‡å¤Issues:\n\n"
                for dup in duplicates:
                    content += f"- Issue #{dup['issue1']['number']} vs #{dup['issue2']['number']} (ç›¸ä¼¼åº¦: {dup['similarity']})\n"
                content += "\n"
            
            # æ”¹è¿›å»ºè®®
            recommendations = analysis.get('recommendations', [])
            if recommendations:
                content += "## æ”¹è¿›å»ºè®®\n\n"
                for rec in recommendations:
                    content += f"- {rec}\n"
        
        content += f"""
### æ•°æ®æ–‡ä»¶
{results.get('data_file', 'N/A')}

### è¯¦ç»†åˆ†æé¢†åŸŸ
"""
        for area in results.get('analysis_areas', []):
            content += f"- {area}\n"
        
        content += """
## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³è¡ŒåŠ¨**: å¤„ç†é«˜ä¼˜å…ˆçº§Issueså’Œé‡å¤Issues
2. **çŸ­æœŸç›®æ ‡**: æ”¹è¿›æ ‡ç­¾ä½“ç³»å’Œå·¥ä½œæµç¨‹
3. **é•¿æœŸè§„åˆ’**: å»ºç«‹Issuesç®¡ç†æœ€ä½³å®è·µ

---
*æœ¬æŠ¥å‘Šç”±SAGE Issuesç®¡ç†å·¥å…·è‡ªåŠ¨ç”Ÿæˆ*
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(content)

def main():
    parser = argparse.ArgumentParser(description="AIæ™ºèƒ½åˆ†æIssues")
    parser.add_argument("--mode", choices=["duplicates", "labels", "priority", "comprehensive"], 
                       required=True, help="åˆ†ææ¨¡å¼")
    
    args = parser.parse_args()
    
    analyzer = AIIssuesAnalyzer()
    
    success = False
    if args.mode == "duplicates":
        success = analyzer.analyze_duplicates()
    elif args.mode == "labels":
        success = analyzer.optimize_labels()
    elif args.mode == "priority":
        success = analyzer.evaluate_priority()
    elif args.mode == "comprehensive":
        success = analyzer.comprehensive_analysis()
    
    if success:
        print("\nğŸ‰ AIåˆ†æå®Œæˆï¼")
        print("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šäº†è§£è¯¦ç»†ç»“æœ")
        print("ğŸ’¡ æ‰€æœ‰åˆ†æéƒ½å·²è‡ªåŠ¨å®Œæˆï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ")
    else:
        print("ğŸ’¥ AIåˆ†æå¤±è´¥ï¼")
        sys.exit(1)

if __name__ == "__main__":
    main()
