#!/usr/bin/env python3
"""
CICDç¯å¢ƒä¸­çš„embeddingæ¨¡å‹é¢„ç¼“å­˜è„šæœ¬
"""

import os
import sys
from pathlib import Path


# è®¾ç½®ç½‘ç»œå’Œç¼“å­˜ç›¸å…³çš„ç¯å¢ƒå˜é‡
def setup_environment():
    """è®¾ç½®ä¼˜åŒ–çš„ç¯å¢ƒå˜é‡"""
    # ä½¿ç”¨HuggingFaceé•œåƒï¼ˆå¦‚æœåœ¨ä¸­å›½ï¼‰
    if not os.environ.get("HF_ENDPOINT"):
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

    # ç¦ç”¨è¿›åº¦æ¡ä»¥å‡å°‘è¾“å‡ºå™ªéŸ³
    os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"

    # è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
    os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"

    # å¯ç”¨ç¦»çº¿æ¨¡å¼æ£€æŸ¥
    os.environ["TRANSFORMERS_OFFLINE"] = "0"

    print("ğŸ”§ ç¯å¢ƒå˜é‡å·²é…ç½®:")
    print(f"  - HF_ENDPOINT: {os.environ.get('HF_ENDPOINT', 'default')}")
    print(
        f"  - HF_HUB_DOWNLOAD_TIMEOUT: {os.environ.get('HF_HUB_DOWNLOAD_TIMEOUT', 'default')}"
    )


# åœ¨å¯¼å…¥transformersä¹‹å‰è®¾ç½®ç¯å¢ƒ
setup_environment()


def clear_model_cache():
    """æ¸…é™¤æ¨¡å‹ç¼“å­˜"""
    import shutil

    from transformers import TRANSFORMERS_CACHE

    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    print(f"ğŸ—‘ï¸  æ¸…é™¤æ¨¡å‹ç¼“å­˜: {model_name}")

    try:
        cache_dir = Path(TRANSFORMERS_CACHE)
        if cache_dir.exists():
            # æŸ¥æ‰¾è¯¥æ¨¡å‹çš„ç¼“å­˜ç›®å½•
            model_dirs = list(cache_dir.glob("**/models--sentence-transformers--*"))

            for model_dir in model_dirs:
                if "all-MiniLM-L6-v2" in str(model_dir):
                    print(f"  åˆ é™¤: {model_dir}")
                    shutil.rmtree(model_dir, ignore_errors=True)

            print("âœ… ç¼“å­˜æ¸…é™¤å®Œæˆ!")
            return True
        else:
            print("â„¹ï¸  ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
            return True

    except Exception as e:
        print(f"âŒ ç¼“å­˜æ¸…é™¤å¤±è´¥: {e}")
        return False


def cache_embedding_models():
    """ç¼“å­˜CICDç¯å¢ƒéœ€è¦çš„embeddingæ¨¡å‹"""
    import time

    from transformers import AutoModel, AutoTokenizer

    print("ğŸ”„ å¼€å§‹ç¼“å­˜embeddingæ¨¡å‹...")

    # é»˜è®¤ä½¿ç”¨çš„æ¨¡å‹
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    print(f"ğŸ“¥ ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹: {model_name}")

    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æé«˜ä¸‹è½½ç¨³å®šæ€§
    os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")
    os.environ.setdefault(
        "TRANSFORMERS_CACHE", os.path.expanduser("~/.cache/huggingface/transformers")
    )

    # é…ç½®ç½‘ç»œé‡è¯•å‚æ•°
    try:
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        # è®¾ç½®å…¨å±€çš„requests session with retry
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET", "OPTIONS"],
            backoff_factor=1,
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # åº”ç”¨åˆ°huggingface_hub
        try:
            import huggingface_hub

            huggingface_hub.constants.DEFAULT_REQUEST_TIMEOUT = 60
        except Exception:
            pass

    except ImportError:
        print("â„¹ï¸  requestsæœªå®‰è£…ï¼Œè·³è¿‡ç½‘ç»œé‡è¯•é…ç½®")

    max_retries = 3

    # ä¸‹è½½tokenizerï¼ˆå¸¦é‡è¯•ï¼‰
    tokenizer = None
    for attempt in range(max_retries):
        try:
            print(f"  - ä¸‹è½½tokenizer (å°è¯• {attempt + 1}/{max_retries})...")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            print("  âœ… Tokenizerä¸‹è½½æˆåŠŸ!")
            break
        except Exception as e:
            print(f"  âŒ Tokenizerä¸‹è½½å¤±è´¥: {type(e).__name__}: {str(e)[:100]}...")
            if attempt < max_retries - 1:
                wait_time = 2**attempt
                print(f"  â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print("âŒ Tokenizerä¸‹è½½æœ€ç»ˆå¤±è´¥")
                return False

    # ä¸‹è½½æ¨¡å‹ï¼ˆå¸¦é‡è¯•ï¼‰
    model = None
    for attempt in range(max_retries):
        try:
            print(f"  - ä¸‹è½½æ¨¡å‹ (å°è¯• {attempt + 1}/{max_retries})...")
            model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
            print("  âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ!")
            break
        except Exception as e:
            print(f"  âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {type(e).__name__}: {str(e)[:100]}...")
            if attempt < max_retries - 1:
                wait_time = 2**attempt
                print(f"  â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print("âŒ æ¨¡å‹ä¸‹è½½æœ€ç»ˆå¤±è´¥")
                return False

    print("âœ… æ¨¡å‹ç¼“å­˜å®Œæˆ!")

    # éªŒè¯æ¨¡å‹å¯ç”¨æ€§
    try:
        print("ğŸ§ª éªŒè¯æ¨¡å‹å¯ç”¨æ€§...")
        test_text = "æµ‹è¯•æ–‡æœ¬"
        inputs = tokenizer(
            test_text, return_tensors="pt", padding=True, truncation=True
        )
        outputs = model(**inputs)

        print(f"  - æ¨¡å‹è¾“å‡ºç»´åº¦: {outputs.last_hidden_state.shape}")
        print("âœ… æ¨¡å‹éªŒè¯é€šè¿‡!")

        # æ˜¾ç¤ºç¼“å­˜ä½ç½®
        cache_dir = os.environ.get(
            "TRANSFORMERS_CACHE", "~/.cache/huggingface/transformers"
        )
        print(f"  - ç¼“å­˜ä½ç½®: {cache_dir}")

        return True

    except Exception as e:
        print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        return False


def check_model_availability():
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»å¯ç”¨"""
    import time

    from transformers import AutoModel, AutoTokenizer

    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    print(f"ğŸ” æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§: {model_name}")

    # é¦–å…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜
    try:
        # å°è¯•ä½¿ç”¨local_files_onlyå‚æ•°æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)
        model = AutoModel.from_pretrained(
            model_name, local_files_only=True, trust_remote_code=True
        )
        print("âœ… æ¨¡å‹å·²åœ¨æœ¬åœ°ç¼“å­˜ä¸­å¯ç”¨!")
        return True
    except Exception:
        print("â„¹ï¸  æœ¬åœ°ç¼“å­˜ä¸­æœªæ‰¾åˆ°æ¨¡å‹ï¼Œå°è¯•ä»è¿œç¨‹ä¸‹è½½...")

    # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå°è¯•ä»è¿œç¨‹ä¸‹è½½ï¼ˆå¸¦é‡è¯•ï¼‰
    max_retries = 3
    for attempt in range(max_retries):
        try:
            print(f"  å°è¯• {attempt + 1}/{max_retries}...")

            # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name, trust_remote_code=True)

            print("âœ… æ¨¡å‹ä¸‹è½½å¹¶å¯ç”¨!")
            return True

        except Exception as e:
            print(
                f"  âŒ å°è¯• {attempt + 1} å¤±è´¥: {type(e).__name__}: {str(e)[:100]}..."
            )
            if attempt < max_retries - 1:
                wait_time = 2**attempt  # æŒ‡æ•°é€€é¿
                print(f"  â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)

    print(f"âŒ ç»è¿‡ {max_retries} æ¬¡å°è¯•åï¼Œæ¨¡å‹ä»ä¸å¯ç”¨")
    return False


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="CICD embeddingæ¨¡å‹ç®¡ç†")
    parser.add_argument("--check", action="store_true", help="æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§")
    parser.add_argument("--cache", action="store_true", help="ç¼“å­˜æ¨¡å‹")
    parser.add_argument("--clear-cache", action="store_true", help="æ¸…é™¤æ¨¡å‹ç¼“å­˜")

    args = parser.parse_args()

    if args.clear_cache:
        success = clear_model_cache()
        sys.exit(0 if success else 1)
    elif args.check:
        available = check_model_availability()
        sys.exit(0 if available else 1)
    elif args.cache:
        success = cache_embedding_models()
        sys.exit(0 if success else 1)
    else:
        # é»˜è®¤å…ˆæ£€æŸ¥ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ç¼“å­˜
        if not check_model_availability():
            print("æ¨¡å‹ä¸å¯ç”¨ï¼Œå¼€å§‹ç¼“å­˜...")
            success = cache_embedding_models()
            sys.exit(0 if success else 1)
        else:
            print("æ¨¡å‹å·²å¯ç”¨ï¼Œæ— éœ€ç¼“å­˜")
            sys.exit(0)
