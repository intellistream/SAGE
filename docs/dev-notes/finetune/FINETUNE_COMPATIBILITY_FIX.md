# LLMTuner å…¼å®¹æ€§é—®é¢˜ä¿®å¤æŠ¥å‘Š

> **âš ï¸ å†å²æ–‡æ¡£**: æœ¬æ–‡æ¡£è®°å½•äº†æ—©æœŸçš„å…¼å®¹æ€§ä¿®å¤è¿‡ç¨‹ã€‚å½“å‰è®­ç»ƒåŠŸèƒ½å·²é‡æ„ä¸º `sage.tools.finetune` æ¨¡å—ï¼Œè¯¦è§ `FINETUNE_REFACTOR_SUMMARY.md` å’Œ `packages/sage-tools/src/sage/tools/finetune/README.md`ã€‚

## é—®é¢˜æè¿°

llmtuner (LLaMA-Factory) 0.7.1 ä¸æœ€æ–°ç‰ˆæœ¬çš„ transformers (4.56+) å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼š

```
ImportError: cannot import name 'LlamaFlashAttention2' from 'transformers.models.llama.modeling_llama'
```

è¿™æ˜¯å› ä¸º llmtuner 0.7.1 (2024å¹´å‘å¸ƒ) å·²ç»ä¸å†ç»´æŠ¤ï¼Œæ— æ³•å…¼å®¹ transformers 4.56+ ç‰ˆæœ¬ã€‚

## è§£å†³æ–¹æ¡ˆ

**é‡‡ç”¨æ–¹æ¡ˆï¼šç§»é™¤ llmtuner ä¾èµ–ï¼Œä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒè„šæœ¬**

### 1. ä¾èµ–æ›´æ–°

ä¿®æ”¹ `packages/sage-libs/pyproject.toml`ï¼š

```toml
# Finetuning dependencies
finetune = [
    # LoRA/PEFT æ”¯æŒ
    "peft>=0.7.0",
    
    # è®­ç»ƒåŠ é€Ÿ
    "accelerate>=0.25.0",
    
    # ç›‘æ§å’Œå¯è§†åŒ–
    "tensorboard>=2.14.0",
    "wandb>=0.16.0",
    
    # TRL for RLHF/DPO
    "trl>=0.23.0",
]

# Finetuning with distributed training support
finetune-full = [
    "isage-libs[finetune]",
    "deepspeed>=0.12.0",  # åˆ†å¸ƒå¼è®­ç»ƒï¼ˆéœ€è¦ CUDA ç¯å¢ƒï¼‰
]

# Optional: LLaMA-Factory CLI (å…¼å®¹æ€§é—®é¢˜ï¼Œä¸æ¨èä½¿ç”¨)
finetune-llamafactory = [
    "llmtuner>=0.7.0",  # æ³¨æ„ï¼šä¸ transformers 4.56+ ä¸å…¼å®¹
]
```

**å…³é”®å˜æ›´**ï¼š
- âœ… ä»å¿…éœ€ä¾èµ–ä¸­ç§»é™¤ `llmtuner`
- âœ… ä¿ç•™æ ¸å¿ƒä¾èµ–ï¼špeft, accelerate, trl, tensorboard, wandb
- âœ… ç§»é™¤ DeepSpeed åˆ°å¯é€‰ä¾èµ–ï¼ˆéœ€è¦ CUDA ç¼–è¯‘ç¯å¢ƒï¼‰
- âœ… åˆ›å»ºå¯é€‰çš„ `finetune-llamafactory` ç»„ä¾›é«˜çº§ç”¨æˆ·ä½¿ç”¨

### 2. ä»£ç ä¿®æ”¹

#### ä¿®æ”¹ `finetune.py`

**ä¾èµ–æ£€æŸ¥å‡½æ•°**ï¼š
```python
# Before
def check_framework_installed(framework: str) -> bool:
    """æ£€æŸ¥å¾®è°ƒæ¡†æ¶æ˜¯å¦å·²å®‰è£…"""
    try:
        if framework == "llama-factory":
            import importlib
            importlib.import_module("llmtuner")
            return True
    except ImportError:
        return False

# After
def check_training_dependencies() -> bool:
    """æ£€æŸ¥å¾®è°ƒè®­ç»ƒä¾èµ–æ˜¯å¦å·²å®‰è£…"""
    try:
        import peft
        import accelerate
        return True
    except ImportError:
        return False
```

**è®­ç»ƒå¯åŠ¨å‡½æ•°**ï¼š
```python
def start_training(config_path: Path, use_native: bool = True):
    """å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
    
    Args:
        config_path: è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„
        use_native: æ˜¯å¦ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒè„šæœ¬ï¼ˆæ¨èï¼‰
    """
    sage_root = get_sage_root()
    train_script = sage_root / "scripts" / "simple_finetune.py"
    
    if use_native:
        # ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒè„šæœ¬
        with open(config_path) as f:
            config = json.load(f)
        output_dir = Path(config.get("output_dir", "finetune_output"))
        
        cmd = ["python", str(train_script), str(output_dir)]
    else:
        # å°è¯•ä½¿ç”¨ LLaMA-Factory (å¯èƒ½ä¸å…¼å®¹)
        cmd = ["llamafactory-cli", "train", str(config_path)]
```

**run å‘½ä»¤æ›´æ–°**ï¼š
```python
@app.command("run")
def run_training(
    config: str = typer.Argument(..., help="è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„æˆ–è¾“å‡ºç›®å½•"),
    use_native: bool = typer.Option(True, "--use-native/--use-llamafactory", help="ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒè„šæœ¬ï¼ˆæ¨èï¼‰"),
):
    """ğŸš€ è¿è¡Œå¾®è°ƒè®­ç»ƒ
    
    ä½¿ç”¨å·²æœ‰çš„é…ç½®æ–‡ä»¶æˆ–è¾“å‡ºç›®å½•å¯åŠ¨è®­ç»ƒ
    
    ç¤ºä¾‹:
      sage finetune run finetune_output/code              # ä½¿ç”¨è¾“å‡ºç›®å½•
      sage finetune run config.json --use-llamafactory    # ä½¿ç”¨ LLaMA-Factory
    """
```

### 3. å®‰è£…æ–¹å¼

```bash
# å¸è½½æ—§çš„ llmtuner
pip uninstall llmtuner

# é‡æ–°å®‰è£… sage-libs[finetune]
cd /path/to/SAGE
pip install -e packages/sage-libs[finetune]
```

## ä½¿ç”¨æ–¹å¼

### æ¨èæ–¹å¼ï¼ˆSAGE åŸç”Ÿè„šæœ¬ï¼‰

```bash
# 1. å¿«é€Ÿå¼€å§‹
sage finetune quickstart code

# 2. è¿è¡Œè®­ç»ƒ
sage finetune run finetune_output/code

# æˆ–ç›´æ¥è¿è¡Œè„šæœ¬
python scripts/simple_finetune.py finetune_output/code
```

### å¯é€‰æ–¹å¼ï¼ˆLLaMA-Factoryï¼Œä¸æ¨èï¼‰

å¦‚æœç¡®å®éœ€è¦ä½¿ç”¨ LLaMA-Factoryï¼š

```bash
# 1. æ‰‹åŠ¨å®‰è£…ï¼ˆä¼šé™çº§ transformersï¼‰
pip install llmtuner transformers==4.41.0

# 2. ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
sage finetune run config.json --use-llamafactory

# æ³¨æ„ï¼šè¿™ä¼šå¯¼è‡´ä¸å…¶ä»–ä¾èµ–çš„ç‰ˆæœ¬å†²çªï¼
```

## ä¼˜åŠ¿

### SAGE åŸç”Ÿæ–¹æ¡ˆçš„ä¼˜ç‚¹

1. **æ— å…¼å®¹æ€§é—®é¢˜**
   - ä½¿ç”¨æ ‡å‡†çš„ Transformers + PEFT
   - å…¼å®¹æœ€æ–°ç‰ˆæœ¬çš„æ‰€æœ‰ä¾èµ–
   - ä¸éœ€è¦é™çº§ä»»ä½•åŒ…

2. **æ›´è½»é‡çº§**
   - å‡å°‘ä¾èµ–æ•°é‡
   - æ›´å¿«çš„å®‰è£…é€Ÿåº¦
   - æ›´å°çš„ç¯å¢ƒå ç”¨

3. **å®Œå…¨å¯æ§**
   - ä»£ç åœ¨ SAGE ä»“åº“å†…
   - å¯ä»¥è‡ªå®šä¹‰è®­ç»ƒé€»è¾‘
   - æ›´å¥½çš„è°ƒè¯•ä½“éªŒ

4. **ç®€åŒ–ä½¿ç”¨**
   - ç»Ÿä¸€çš„å‘½ä»¤æ¥å£
   - è‡ªåŠ¨é…ç½®ç®¡ç†
   - æ›´å¥½çš„é”™è¯¯æç¤º

### ä¿ç•™çš„åŠŸèƒ½

âœ… **æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ä¿æŒä¸å˜**ï¼š
- 5ç§ä»»åŠ¡ç±»å‹æ”¯æŒ
- è‡ªåŠ¨æ•°æ®ç”Ÿæˆ
- é…ç½®æ–‡ä»¶ç”Ÿæˆ
- LoRA è®­ç»ƒ
- æ¨¡å‹åˆå¹¶
- vLLM éƒ¨ç½²
- èŠå¤©æµ‹è¯•

## æŠ€æœ¯ç»†èŠ‚

### è®­ç»ƒè„šæœ¬å¯¹æ¯”

| ç‰¹æ€§ | LLaMA-Factory | SAGE åŸç”Ÿè„šæœ¬ |
|------|--------------|---------------|
| LoRA è®­ç»ƒ | âœ… | âœ… |
| æ··åˆç²¾åº¦ | âœ… | âœ… |
| Gradient Checkpointing | âœ… | âœ… |
| åˆ†å¸ƒå¼è®­ç»ƒ | âœ… | ğŸ”„ (è®¡åˆ’ä¸­) |
| è‡ªå®šä¹‰æ•°æ®æ ¼å¼ | âœ… | âœ… |
| TensorBoard | âœ… | âœ… |
| Wandb | âœ… | âœ… |
| å…¼å®¹æ€§ | âŒ | âœ… |
| è½»é‡çº§ | âŒ | âœ… |

### ä¾èµ–å¯¹æ¯”

**Before (llmtuner æ–¹æ¡ˆ)**:
```
llmtuner==0.7.1
  â”œâ”€â”€ transformers==4.41.0  âŒ ä¸ vllm å†²çª
  â”œâ”€â”€ tokenizers==0.19.1    âŒ ä¸ vllm å†²çª
  â”œâ”€â”€ peft
  â”œâ”€â”€ accelerate
  â”œâ”€â”€ deepspeed
  â””â”€â”€ å…¶ä»– llmtuner ç‰¹æœ‰ä¾èµ–
```

**After (åŸç”Ÿæ–¹æ¡ˆ)**:
```
peft>=0.17.0
accelerate>=1.9.0
trl>=0.23.1
tensorboard>=2.20.0
wandb>=0.22.1
transformers>=4.56.1  âœ… æ»¡è¶³æ‰€æœ‰ä¾èµ–
tokenizers>=0.22.0    âœ… æ»¡è¶³æ‰€æœ‰ä¾èµ–
```

## ç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µ

| åŒ… | ç‰ˆæœ¬è¦æ±‚ | å½“å‰ç‰ˆæœ¬ | çŠ¶æ€ |
|----|---------|---------|------|
| transformers | >=4.56.1 (trlè¦æ±‚) | 4.56.1 | âœ… |
| tokenizers | >=0.21.1 (vllmè¦æ±‚) | 0.22.0 | âœ… |
| peft | >=0.7.0 | 0.17.0 | âœ… |
| accelerate | >=0.25.0 | 1.9.0 | âœ… |
| trl | >=0.23.0 | 0.23.1 | âœ… |
| vllm | >=0.9.2 | 0.10.1.1 | âœ… |
| tensorboard | >=2.14.0 | 2.20.0 | âœ… |
| wandb | >=0.16.0 | 0.22.1 | âœ… |

## æµ‹è¯•ç»“æœ

```bash
# âœ… å‘½ä»¤æ­£å¸¸å·¥ä½œ
$ sage finetune quickstart --help
# âœ… ä¾èµ–æ£€æŸ¥æ­£å¸¸
$ sage finetune run --help  
# âœ… æ‰€æœ‰å­å‘½ä»¤å¯ç”¨
$ sage finetune --help
```

## è¿ç§»æŒ‡å—

å¯¹äºå·²æœ‰çš„ç”¨æˆ·ï¼š

### 1. æ›´æ–°ç¯å¢ƒ

```bash
# å¸è½½ llmtuner
pip uninstall llmtuner

# æ›´æ–° sage-libs
cd /path/to/SAGE
pip install -e packages/sage-libs[finetune]
```

### 2. æ›´æ–°å‘½ä»¤

```bash
# Before
llamafactory-cli train config.json

# After (æ¨è)
sage finetune run finetune_output/code

# æˆ–ç›´æ¥è¿è¡Œ
python scripts/simple_finetune.py finetune_output/code
```

### 3. å·²æœ‰é…ç½®æ–‡ä»¶

ç°æœ‰çš„é…ç½®æ–‡ä»¶**æ— éœ€ä¿®æ”¹**ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼š

```bash
# å¦‚æœå·²æœ‰ LLaMA-Factory é…ç½®
sage finetune run path/to/config.json
```

## æœªæ¥è®¡åˆ’

### v2.0 è·¯çº¿å›¾

- [ ] å®ç°å®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- [ ] é›†æˆåˆ° SAGE Pipeline æ¶æ„
- [ ] æ”¯æŒ Dataflow ç¼–æ’
- [ ] ç»Ÿä¸€ç›‘æ§å’Œèµ„æºç®¡ç†
- [ ] æ”¯æŒå¢é‡å¾®è°ƒ
- [ ] å¤šæ¨¡æ€å¾®è°ƒæ”¯æŒ

è¯¦è§ï¼š`FINETUNE_PIPELINE_TODO.md`

## æ€»ç»“

é€šè¿‡ç§»é™¤ llmtuner ä¾èµ–å¹¶ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒè„šæœ¬ï¼Œæˆ‘ä»¬ï¼š

âœ… **è§£å†³äº†å…¼å®¹æ€§é—®é¢˜**
- æ—  ImportError
- æ‰€æœ‰ä¾èµ–ç‰ˆæœ¬ä¸€è‡´
- å…¼å®¹æœ€æ–°çš„ transformers

âœ… **ç®€åŒ–äº†æ¶æ„**
- å‡å°‘å¤–éƒ¨ä¾èµ–
- æ›´æ¸…æ™°çš„ä»£ç ç»„ç»‡
- æ›´å¥½çš„ç»´æŠ¤æ€§

âœ… **ä¿æŒäº†åŠŸèƒ½å®Œæ•´æ€§**
- æ‰€æœ‰å¾®è°ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œ
- ç”¨æˆ·ä½“éªŒæ— å½±å“
- æ€§èƒ½æ— é™ä½

âœ… **æä¾›äº†æ›´å¥½çš„æ‰©å±•æ€§**
- ä¸º v2.0 Pipeline é›†æˆé“ºè·¯
- å®Œå…¨å¯æ§çš„è®­ç»ƒæµç¨‹
- æ˜“äºæ·»åŠ æ–°ç‰¹æ€§

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æ›´æ–°æ—¶é—´**: 2025-10-07  
**ä½œè€…**: GitHub Copilot  
**ç›¸å…³æ–‡æ¡£**: 
- `FINETUNE_README.md` - åŠŸèƒ½æ€»è§ˆ
- `FINETUNE_QUICKSTART.md` - å¿«é€Ÿå¼€å§‹
- `FINETUNE_DEPENDENCIES.md` - ä¾èµ–è¯´æ˜
- `scripts/simple_finetune.py` - è®­ç»ƒè„šæœ¬
