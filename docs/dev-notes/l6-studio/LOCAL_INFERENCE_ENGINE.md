# Studio æœ¬åœ°æ¨ç†å¼•æ“é›†æˆæ–¹æ¡ˆ

## æ¦‚è¿°

åœ¨ SAGE Studio ä¸­é›†æˆæœ¬åœ°æ¨ç†å¼•æ“ç®¡ç†åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·ï¼š
1. ä¸€é”®å¯åŠ¨/åœæ­¢æœ¬åœ° vLLM æœåŠ¡
2. åœ¨ Chat é¡µé¢é€‰æ‹©ä½¿ç”¨æœ¬åœ°æœåŠ¡æˆ–äº‘ç«¯ API
3. æŸ¥çœ‹æœ¬åœ°æœåŠ¡çŠ¶æ€å’Œèµ„æºä½¿ç”¨æƒ…å†µ

## æ¶æ„è®¾è®¡

### 1. åç«¯ API ç«¯ç‚¹

**ä½ç½®**: `packages/sage-studio/src/sage/studio/config/backend/api.py`

```python
# ==================== Local Inference Engine Management ====================

class LocalEngineStatus(BaseModel):
    """æœ¬åœ°æ¨ç†å¼•æ“çŠ¶æ€"""
    running: bool
    port: int | None = None
    model: str | None = None
    pid: int | None = None
    gpu_memory_usage: float | None = None  # GB
    uptime: str | None = None

class LocalEngineStartRequest(BaseModel):
    """å¯åŠ¨æœ¬åœ°æ¨ç†å¼•æ“è¯·æ±‚"""
    model: str  # ä¾‹å¦‚: "Qwen/Qwen2.5-7B-Instruct"
    port: int = 8001  # é»˜è®¤ 8001 é¿å…ä¸ Gateway å†²çª
    gpu_memory_utilization: float = 0.9
    quantization: str | None = None  # "awq", "gptq", None

@app.get("/api/local-engine/status")
async def get_local_engine_status() -> LocalEngineStatus:
    """è·å–æœ¬åœ°æ¨ç†å¼•æ“çŠ¶æ€"""
    # æ£€æµ‹ vLLM è¿›ç¨‹
    # æŸ¥è¯¢ GPU ä½¿ç”¨æƒ…å†µ
    pass

@app.post("/api/local-engine/start")
async def start_local_engine(request: LocalEngineStartRequest):
    """å¯åŠ¨æœ¬åœ°æ¨ç†å¼•æ“"""
    # ä½¿ç”¨ subprocess å¯åŠ¨ vLLM server
    # åå°è¿è¡Œï¼Œä¿å­˜ PID
    pass

@app.post("/api/local-engine/stop")
async def stop_local_engine():
    """åœæ­¢æœ¬åœ°æ¨ç†å¼•æ“"""
    # æ ¹æ® PID åœæ­¢è¿›ç¨‹
    pass

@app.get("/api/local-engine/models")
async def list_available_models():
    """åˆ—å‡ºå¯ç”¨çš„æœ¬åœ°æ¨¡å‹"""
    # æ‰«æ ~/.cache/huggingface/hub/
    # è¿”å›å·²ä¸‹è½½çš„æ¨¡å‹åˆ—è¡¨
    pass
```

### 2. å‰ç«¯ UI ç»„ä»¶

**ä½ç½®**: `packages/sage-studio/src/sage/studio/frontend/src/components/LocalEnginePanel.tsx`

#### 2.1 ä¸»é¢æ¿å¸ƒå±€

```tsx
/**
 * æœ¬åœ°æ¨ç†å¼•æ“ç®¡ç†é¢æ¿
 * ä½ç½®ï¼šStudio é¡¶éƒ¨å¯¼èˆªæˆ–ä¾§è¾¹æ 
 */
export default function LocalEnginePanel() {
  const [status, setStatus] = useState<LocalEngineStatus>()
  const [isStarting, setIsStarting] = useState(false)

  return (
    <Card title="æœ¬åœ°æ¨ç†å¼•æ“">
      {/* çŠ¶æ€æŒ‡ç¤ºå™¨ */}
      <StatusIndicator status={status} />

      {/* æ§åˆ¶æŒ‰é’® */}
      {!status?.running ? (
        <StartEngineForm onStart={handleStart} />
      ) : (
        <RunningEngineInfo status={status} onStop={handleStop} />
      )}
    </Card>
  )
}
```

#### 2.2 å¯åŠ¨è¡¨å•

```tsx
function StartEngineForm({ onStart }) {
  return (
    <Form onFinish={onStart}>
      <Form.Item label="é€‰æ‹©æ¨¡å‹">
        <Select placeholder="é€‰æ‹©æœ¬åœ°æ¨¡å‹">
          <Option value="Qwen/Qwen2.5-7B-Instruct">
            Qwen 2.5 7B (æ¨è)
          </Option>
          <Option value="Qwen/Qwen2.5-1.5B-Instruct">
            Qwen 2.5 1.5B (ä½æ˜¾å­˜)
          </Option>
          {/* åŠ¨æ€åŠ è½½ç”¨æˆ·å·²ä¸‹è½½çš„æ¨¡å‹ */}
        </Select>
      </Form.Item>

      <Form.Item label="GPU æ˜¾å­˜å ç”¨">
        <Slider min={0.5} max={0.95} step={0.05} defaultValue={0.9} />
      </Form.Item>

      <Form.Item label="ç«¯å£">
        <InputNumber defaultValue={8001} disabled />
        <Text type="secondary">æ¨èä½¿ç”¨ 8001ï¼ˆé¿å…ä¸ Gateway å†²çªï¼‰</Text>
      </Form.Item>

      <Button type="primary" htmlType="submit" loading={isStarting}>
        ğŸš€ å¯åŠ¨æ¨ç†å¼•æ“
      </Button>
    </Form>
  )
}
```

#### 2.3 è¿è¡ŒçŠ¶æ€æ˜¾ç¤º

```tsx
function RunningEngineInfo({ status, onStop }) {
  return (
    <Space direction="vertical" style={{ width: '100%' }}>
      <Statistic title="æ¨¡å‹" value={status.model} />
      <Statistic title="ç«¯å£" value={status.port} />
      <Statistic
        title="GPU æ˜¾å­˜"
        value={status.gpu_memory_usage}
        suffix="GB"
      />
      <Statistic title="è¿è¡Œæ—¶é—´" value={status.uptime} />

      <Button danger onClick={onStop}>
        ğŸ›‘ åœæ­¢æ¨ç†å¼•æ“
      </Button>
    </Space>
  )
}
```

### 3. Chat é¡µé¢ LLM æºé€‰æ‹©

**ä½ç½®**: `packages/sage-studio/src/sage/studio/frontend/src/components/ChatPanel.tsx`

```tsx
function ChatPanel() {
  const [llmSource, setLlmSource] = useState<'local' | 'cloud'>('auto')
  const [localEngineStatus, setLocalEngineStatus] = useState()

  return (
    <div>
      {/* LLM æºé€‰æ‹©å™¨ */}
      <Card size="small" style={{ marginBottom: 16 }}>
        <Space>
          <Text>LLM æ¥æºï¼š</Text>
          <Radio.Group
            value={llmSource}
            onChange={(e) => setLlmSource(e.target.value)}
          >
            <Radio value="auto">
              <Tooltip title="ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ï¼Œä¸å¯ç”¨æ—¶é™çº§åˆ°äº‘ç«¯">
                ğŸ¤– è‡ªåŠ¨ï¼ˆæ¨èï¼‰
              </Tooltip>
            </Radio>
            <Radio value="local" disabled={!localEngineStatus?.running}>
              <Tooltip title="ä»…ä½¿ç”¨æœ¬åœ°æ¨ç†å¼•æ“">
                ğŸ’» æœ¬åœ°
              </Tooltip>
            </Radio>
            <Radio value="cloud">
              <Tooltip title="ä½¿ç”¨äº‘ç«¯ API (éœ€é…ç½® API Key)">
                â˜ï¸ äº‘ç«¯
              </Tooltip>
            </Radio>
          </Radio.Group>

          {/* çŠ¶æ€æŒ‡ç¤º */}
          {localEngineStatus?.running && (
            <Tag color="success">æœ¬åœ°å¼•æ“è¿è¡Œä¸­</Tag>
          )}
        </Space>
      </Card>

      {/* èŠå¤©ç•Œé¢ */}
      <ChatInterface llmSource={llmSource} />
    </div>
  )
}
```

### 4. å®ç°ä¼˜å…ˆçº§

#### Phase 1: åŸºç¡€åŠŸèƒ½ï¼ˆ1-2 å¤©ï¼‰
- [ ] åç«¯ API: çŠ¶æ€æ£€æµ‹ã€å¯åŠ¨ã€åœæ­¢
- [ ] å‰ç«¯: åŸºç¡€çŠ¶æ€æ˜¾ç¤ºå’Œæ§åˆ¶æŒ‰é’®
- [ ] Chat é¡µé¢: LLM æºé€‰æ‹©å™¨

#### Phase 2: å¢å¼ºåŠŸèƒ½ï¼ˆ2-3 å¤©ï¼‰
- [ ] æ¨¡å‹åˆ—è¡¨åŠ¨æ€åŠ è½½
- [ ] GPU ä½¿ç”¨æƒ…å†µç›‘æ§
- [ ] å¯åŠ¨è¿›åº¦æ˜¾ç¤ºï¼ˆæ¨¡å‹åŠ è½½ï¼‰
- [ ] æ—¥å¿—æŸ¥çœ‹

#### Phase 3: é«˜çº§åŠŸèƒ½ï¼ˆ3-5 å¤©ï¼‰
- [ ] æ¨¡å‹ä¸‹è½½ç®¡ç†
- [ ] å¤šæ¨¡å‹åˆ‡æ¢
- [ ] æ€§èƒ½ç›‘æ§å›¾è¡¨
- [ ] æ¨ç†å‚æ•°è°ƒä¼˜

## æŠ€æœ¯ç»†èŠ‚

### å¯åŠ¨ vLLM Server

```python
# backend/api.py
import subprocess
from pathlib import Path

async def start_vllm_server(model: str, port: int = 8001, **kwargs):
    """å¯åŠ¨ vLLM æœåŠ¡å™¨"""

    # æ„å»ºå¯åŠ¨å‘½ä»¤
    cmd = [
        "python", "-m", "vllm.entrypoints.openai.api_server",
        "--model", model,
        "--host", "0.0.0.0",
        "--port", str(port),
        "--gpu-memory-utilization", str(kwargs.get("gpu_memory_utilization", 0.9)),
    ]

    if kwargs.get("quantization"):
        cmd.extend(["--quantization", kwargs["quantization"]])

    # åå°å¯åŠ¨
    log_file = Path.home() / ".sage" / "studio" / f"vllm_{port}.log"
    log_handle = open(log_file, "w")

    process = subprocess.Popen(
        cmd,
        stdout=log_handle,
        stderr=subprocess.STDOUT,
        start_new_session=True,
    )

    # ä¿å­˜ PID
    pid_file = Path.home() / ".sage" / "studio" / f"vllm_{port}.pid"
    pid_file.write_text(str(process.pid))

    return {
        "pid": process.pid,
        "port": port,
        "log_file": str(log_file),
    }
```

### çŠ¶æ€æ£€æµ‹

```python
import psutil
import requests

def get_vllm_status(port: int = 8001):
    """æ£€æµ‹ vLLM æœåŠ¡çŠ¶æ€"""

    # æ£€æŸ¥ç«¯å£
    try:
        response = requests.get(f"http://localhost:{port}/health", timeout=1)
        if response.status_code == 200:
            # è·å–æ¨¡å‹ä¿¡æ¯
            models_response = requests.get(f"http://localhost:{port}/v1/models")
            models = models_response.json().get("data", [])

            return {
                "running": True,
                "port": port,
                "model": models[0]["id"] if models else None,
            }
    except:
        pass

    return {"running": False}
```

### GPU ç›‘æ§

```python
try:
    import pynvml

    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    info = pynvml.nvmlDeviceGetMemoryInfo(handle)

    gpu_memory_usage = info.used / 1024**3  # GB
except:
    gpu_memory_usage = None
```

## ç¯å¢ƒå˜é‡æ”¯æŒ

```bash
# .env
# æœ¬åœ°æ¨ç†å¼•æ“é»˜è®¤è®¾ç½®
SAGE_LOCAL_ENGINE_PORT=8001
SAGE_LOCAL_ENGINE_DEFAULT_MODEL=Qwen/Qwen2.5-7B-Instruct
SAGE_LOCAL_ENGINE_GPU_UTIL=0.9
```

## ç”¨æˆ·ä½“éªŒæµç¨‹

### åœºæ™¯ 1: é¦–æ¬¡ä½¿ç”¨
1. ç”¨æˆ·æ‰“å¼€ Studio Chat é¡µé¢
2. çœ‹åˆ°æç¤ºï¼š"æœ¬åœ°æ¨ç†å¼•æ“æœªå¯åŠ¨ï¼Œæ­£åœ¨ä½¿ç”¨äº‘ç«¯ API"
3. ç‚¹å‡»"å¯åŠ¨æœ¬åœ°å¼•æ“"æŒ‰é’®
4. é€‰æ‹©æ¨¡å‹ï¼ˆå¦‚æœå·²ä¸‹è½½ï¼‰æˆ–ä¸‹è½½æ–°æ¨¡å‹
5. ç‚¹å‡»"å¯åŠ¨"ï¼Œç­‰å¾…åŠ è½½ï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
6. å¯åŠ¨æˆåŠŸï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æœ¬åœ°æœåŠ¡
7. å¼€å§‹å¯¹è¯ï¼Œå“åº”é€Ÿåº¦æ›´å¿«

### åœºæ™¯ 2: å·²æœ‰æœ¬åœ°æœåŠ¡
1. ç”¨æˆ·åœ¨ç»ˆç«¯å¯åŠ¨äº† vLLM: `sage llm run Qwen/Qwen2.5-7B-Instruct --port 8001`
2. æ‰“å¼€ Studio Chat é¡µé¢
3. è‡ªåŠ¨æ£€æµ‹åˆ°æœ¬åœ°æœåŠ¡ï¼Œæ˜¾ç¤ºï¼š"âœ… æ­£åœ¨ä½¿ç”¨æœ¬åœ°æ¨ç†å¼•æ“"
4. å¯é€‰æ‹©åˆ‡æ¢åˆ°äº‘ç«¯ API

## å®‰å…¨è€ƒè™‘

1. **è¿›ç¨‹éš”ç¦»**: vLLM è¿›ç¨‹ç‹¬ç«‹è¿è¡Œï¼Œå´©æºƒä¸å½±å“ Studio
2. **èµ„æºé™åˆ¶**: GPU æ˜¾å­˜å ç”¨å¯é…ç½®
3. **æƒé™æ§åˆ¶**: ä»…æœ¬åœ°è®¿é—®ï¼ˆ127.0.0.1ï¼‰
4. **æ—¥å¿—è®°å½•**: æ‰€æœ‰æ“ä½œè®°å½•åˆ°æ—¥å¿—

## æµ‹è¯•è®¡åˆ’

1. **å•å…ƒæµ‹è¯•**: API ç«¯ç‚¹åŠŸèƒ½
2. **é›†æˆæµ‹è¯•**: å¯åŠ¨/åœæ­¢æµç¨‹
3. **æ€§èƒ½æµ‹è¯•**: å¤šå¹¶å‘è¯·æ±‚
4. **ç”¨æˆ·æµ‹è¯•**: UI/UX æµç•…åº¦

## æ–‡æ¡£æ›´æ–°

- [ ] ç”¨æˆ·æ‰‹å†Œï¼šå¦‚ä½•ä½¿ç”¨æœ¬åœ°æ¨ç†å¼•æ“
- [ ] å¼€å‘æ–‡æ¡£ï¼šAPI æ¥å£è¯´æ˜
- [ ] æ•…éšœæ’é™¤ï¼šå¸¸è§é—®é¢˜è§£å†³

## æœªæ¥æ‰©å±•

1. **å¤š GPU æ”¯æŒ**: åœ¨ä¸åŒ GPU ä¸Šè¿è¡Œä¸åŒæ¨¡å‹
2. **æ¨¡å‹çƒ­åˆ‡æ¢**: æ— éœ€é‡å¯æœåŠ¡åˆ‡æ¢æ¨¡å‹
3. **é‡åŒ–æ”¯æŒ**: AWQ, GPTQ è‡ªåŠ¨æ£€æµ‹å’Œä½¿ç”¨
4. **æ€§èƒ½è°ƒä¼˜**: è‡ªåŠ¨è°ƒæ•´ batch size, max_tokens ç­‰å‚æ•°
