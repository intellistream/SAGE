# Templates å‚ä¸ LLM äº¤äº’ - è¯¦ç»†è¯´æ˜

## ğŸ¯ ç›´æ¥å›ç­”ä½ çš„é—®é¢˜

### Q1: å¤§æ¨¡å‹æ˜¯åœ¨å“ªä¸€æ­¥å‚è€ƒäº†æˆ‘ä»¬æå‰æ„å»ºçš„ templatesï¼Ÿ

**ç­”**: åœ¨ **æ­¥éª¤ 5** - æ„å»º LLM æç¤ºè¯æ—¶

### å®Œæ•´æµç¨‹ï¼ˆ5ä¸ªæ­¥éª¤ï¼‰

```
æ­¥éª¤ 1: æ”¶é›†ç”¨æˆ·éœ€æ±‚
  â†“
æ­¥éª¤ 2: Template è‡ªåŠ¨åŒ¹é… â­
  ä»£ç : templates.match_templates(requirements, top_k=3)
  ä½ç½®: pipeline.py:598-601
  â†“
æ­¥éª¤ 3: è½¬æ¢ä¸º LLM æç¤ºè¯ â­
  ä»£ç : match.template.render_prompt(score)
  ä½ç½®: templates/catalog.py:51-82
  â†“
æ­¥éª¤ 4: ç»„è£…æ‰€æœ‰ä¸Šä¸‹æ–‡
  ä»£ç : _template_contexts(self._template_matches)
  ä½ç½®: pipeline.py:226-228
  â†“
æ­¥éª¤ 5: æ³¨å…¥åˆ° User Prompt â­â­â­
  ä»£ç : _build_prompt(..., template_contexts, ...)
  ä½ç½®: pipeline.py:654-657
  
  if template_contexts:
      blocks.append("ä»¥ä¸‹åº”ç”¨æ¨¡æ¿ä»…ä½œçµæ„Ÿå‚è€ƒï¼Œè¯·ç»“åˆéœ€æ±‚è‡ªè¡Œè®¾è®¡ï¼š")
      for idx, snippet in enumerate(template_contexts, start=1):
          blocks.append(f"æ¨¡æ¿[{idx}]:\n{snippet.strip()}")
  
  â†“
æ­¥éª¤ 6: å‘é€ç»™ LLM ğŸ¤–
  messages = [
      {"role": "system", "content": SYSTEM_PROMPT},
      {"role": "user", "content": user_prompt}  # â† åŒ…å« templates!
  ]
  response = self._client.generate(messages, ...)
  ä½ç½®: pipeline.py:632-637
```

### å…³é”®ä»£ç ç‰‡æ®µ

#### 1. Template åŒ¹é…ï¼ˆpipeline.py:598-601ï¼‰
```python
self._template_matches = tuple(
    templates.match_templates(requirements, top_k=3)
)
self._last_template_contexts = _template_contexts(self._template_matches)
```

#### 2. Template è½¬æ¢ä¸ºæç¤ºè¯ï¼ˆcatalog.py:51-82ï¼‰
```python
def render_prompt(self, score: Optional[float] = None) -> str:
    """Render a prompt snippet describing the template for LLM guidance."""
    
    plan = self.pipeline_plan()
    stages = plan.get("stages", [])
    stage_lines = [
        f"              â€¢ {stage['id']}: {stage['class']} ({stage.get('summary', '')})"
        for stage in stages
    ]
    # ... æ ¼å¼åŒ–ä¸ºå¯è¯»çš„æ–‡æœ¬
    prompt = textwrap.dedent(f"""
        æ¨¡æ¿: {self.title} ({self.id}) {score_line}
        ç¤ºä¾‹è·¯å¾„: {self.example_path}
        æ ‡ç­¾: {', '.join(self.tags) or 'é€šç”¨'}
        æè¿°: {self.description}
        
        é»˜è®¤Pipeline:
          Source: {source_class}
{stage_text}
          Sink: {sink_class}
        
        é¢å¤–æŒ‡å¯¼:
        {self.guidance.strip()}
    """).strip()
    return prompt
```

#### 3. æ³¨å…¥åˆ° User Promptï¼ˆpipeline.py:654-657ï¼‰
```python
def _build_prompt(self, ..., template_contexts, ...):
    blocks = [
        "è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚ç”Ÿæˆç¬¦åˆ SAGE æ¡†æ¶çš„ pipeline é…ç½® JSONï¼š",
        json.dumps(requirements, ensure_ascii=False, indent=2),
    ]
    
    # â­ è¿™é‡Œï¼Templates è¢«æ³¨å…¥
    if template_contexts:
        blocks.append("ä»¥ä¸‹åº”ç”¨æ¨¡æ¿ä»…ä½œçµæ„Ÿå‚è€ƒï¼Œè¯·ç»“åˆéœ€æ±‚è‡ªè¡Œè®¾è®¡ï¼š")
        for idx, snippet in enumerate(template_contexts, start=1):
            blocks.append(f"æ¨¡æ¿[{idx}]:\n{snippet.strip()}")
    
    # ... å…¶ä»–ä¸Šä¸‹æ–‡
    
    return "\n\n".join(blocks)
```

#### 4. è°ƒç”¨ LLMï¼ˆpipeline.py:632-637ï¼‰
```python
user_prompt = self._build_prompt(
    requirements,
    previous_plan,
    feedback,
    knowledge_contexts,
    self._last_template_contexts,  # â† Templates åœ¨è¿™é‡Œï¼
    self._last_blueprint_contexts,
)

messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": user_prompt},  # â† åŒ…å« Templates
]

response = self._client.generate(messages, max_tokens=1200, temperature=0.2)
```

## ğŸ“ å®é™…å‘é€ç»™ LLM çš„å†…å®¹ç¤ºä¾‹

```
System Prompt:
-------------
You are SAGE Pipeline Builder, an expert in configuring SAGE pipelines...
(è§„èŒƒè¯´æ˜)

User Prompt:
-----------
è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚ç”Ÿæˆç¬¦åˆ SAGE æ¡†æ¶çš„ pipeline é…ç½® JSONï¼š

{
  "name": "æ™ºèƒ½é—®ç­”åŠ©æ‰‹",
  "goal": "æ„å»ºåŸºäºæ–‡æ¡£æ£€ç´¢çš„é—®ç­”ç³»ç»Ÿ",
  "data_sources": ["æ–‡æ¡£çŸ¥è¯†åº“"],
  "latency_budget": "å®æ—¶å“åº”ä¼˜å…ˆ"
}

ä»¥ä¸‹åº”ç”¨æ¨¡æ¿ä»…ä½œçµæ„Ÿå‚è€ƒï¼Œè¯·ç»“åˆéœ€æ±‚è‡ªè¡Œè®¾è®¡ï¼š

æ¨¡æ¿[1]:
æ¨¡æ¿: å®¢æœçŸ¥è¯†åŠ©æ‰‹ (RAG Simple) (rag-simple-demo) åŒ¹é…åº¦: 0.17
ç¤ºä¾‹è·¯å¾„: examples/rag/rag_simple.py
æ ‡ç­¾: rag, qa, support, é—®ç­”, å®¢æˆ·æ”¯æŒ, çŸ¥è¯†åŠ©æ‰‹
æè¿°: é¢å‘å®¢æœé—®ç­”çš„ç®€åŒ–RAGå·¥ä½œæµï¼Œä½¿ç”¨å†…ç½®ç¤ºä¾‹ç®—å­å³å¯ç¦»çº¿æ¼”ç¤ºã€‚

é»˜è®¤Pipeline:
  Source: examples.rag.rag_simple.SimpleQuestionSource
  â€¢ retriever: examples.rag.rag_simple.SimpleRetriever (Lookup canned snippets)
  â€¢ prompt-builder: examples.rag.rag_simple.SimplePromptor (Combine context)
  â€¢ generator: examples.rag.rag_simple.SimpleGenerator (Create answer)
  Sink: examples.rag.rag_simple.SimpleTerminalSink

æ³¨æ„äº‹é¡¹:
- åŸºäº examples.rag.rag_simple æ¨¡å—æ„å»ºï¼Œé€‚åˆç¦»çº¿æ¼”ç¤º
- æ— éœ€å¤–éƒ¨æœåŠ¡æˆ–å¤§æ¨¡å‹ä¾èµ–å³å¯è¿è¡Œ

é¢å¤–æŒ‡å¯¼:
é€‚åˆå®¢æœåœºæ™¯çš„FAQè‡ªåŠ¨ç­”å¤ã€‚å¯ç›´æ¥è¿è¡Œï¼Œæ— éœ€è¿œç¨‹æœåŠ¡...

æ¨¡æ¿[2]:
... (å…¶ä»–æ¨¡æ¿)

ä»¥ä¸‹æ˜¯ä» SAGE çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„å‚è€ƒä¿¡æ¯ï¼š
çŸ¥è¯†[1]:
... (æ–‡æ¡£å†…å®¹)

ä¸¥æ ¼è¾“å‡ºå•ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å« markdownã€æ³¨é‡Šæˆ–å¤šä½™æ–‡å­—ã€‚
```

**è¿™å°±æ˜¯ LLM çœ‹åˆ°çš„å®Œæ•´æç¤ºè¯ï¼Templates åœ¨é‡Œé¢ï¼**

---

## ğŸ§ª Q2: æµ‹è¯•çš„æ—¶å€™ç”¨çš„æ˜¯çœŸå®çš„ OpenAI å¤§æ¨¡å‹å—ï¼Ÿ

**ç­”**: **å¦ï¼Œæµ‹è¯•ä½¿ç”¨ Mockï¼ˆå‡çš„ç”Ÿæˆå™¨ï¼‰**

### æµ‹è¯•ç¯å¢ƒ (test_chat_pipeline.py)

```python
@pytest.fixture()
def fake_generator(monkeypatch):
    # é¢„å®šä¹‰çš„è¿”å›é…ç½®
    plan = {
        "pipeline": {"name": "demo", ...},
        "source": {...},
        "stages": [...],
        "sink": {...}
    }
    
    # Mock ç”Ÿæˆå™¨
    class DummyGenerator:
        def generate(self, requirements, previous_plan=None, feedback=None):
            self.calls.append((requirements, previous_plan, feedback))
            return plan  # ç›´æ¥è¿”å›é¢„å®šä¹‰é…ç½®ï¼Œä¸è°ƒç”¨ LLM
    
    # æ›¿æ¢çœŸå®çš„ PipelinePlanGenerator
    monkeypatch.setattr(pipeline_builder, "PipelinePlanGenerator", DummyGenerator)
```

**ä¸ºä»€ä¹ˆç”¨ Mockï¼Ÿ**

1. âŒ **é¿å…ä¾èµ–å¤–éƒ¨æœåŠ¡**: ä¸éœ€è¦ API Key
2. âš¡ **æé«˜æµ‹è¯•é€Ÿåº¦**: ä¸éœ€è¦ç½‘ç»œè¯·æ±‚
3. âœ… **æé«˜æµ‹è¯•ç¨³å®šæ€§**: ä¸å— API é™åˆ¶ã€ç½‘ç»œæ³¢åŠ¨å½±å“
4. ğŸ’° **èŠ‚çœæˆæœ¬**: ä¸æ¶ˆè€— API è°ƒç”¨é¢åº¦
5. ğŸ¯ **æµ‹è¯•é‡ç‚¹**: æµ‹è¯•æµç¨‹é€»è¾‘ï¼Œè€Œé LLM è´¨é‡

### ç”Ÿäº§ç¯å¢ƒï¼ˆçœŸå®ä½¿ç”¨ï¼‰

```python
# åœ¨ PipelinePlanGenerator.__init__ ä¸­
if self.config.backend != "mock":
    # ä½¿ç”¨çœŸå®çš„ OpenAI Client
    self._client = OpenAIClient(
        model_name=self.config.model,
        base_url=self.config.base_url,
        api_key=self.config.api_key,
        seed=42,
    )
```

**çœŸå®ä½¿ç”¨ç¤ºä¾‹**:
```bash
# é…ç½® API Key
export SAGE_CHAT_API_KEY="sk-xxx"

# ä½¿ç”¨çœŸå® LLM
sage chat --backend openai --model qwen-max

# æˆ–
sage pipeline build \
  --backend openai \
  --model qwen-turbo \
  --api-key "sk-xxx" \
  --name "MyApp" \
  --goal "æ„å»ºé—®ç­”åº”ç”¨"
```

### Mock vs Real å¯¹æ¯”

| ç‰¹æ€§ | Mock (æµ‹è¯•) | Real (ç”Ÿäº§) |
|------|------------|------------|
| LLM è°ƒç”¨ | âŒ ä¸è°ƒç”¨ | âœ… çœŸå®è°ƒç”¨ |
| API Key | âŒ ä¸éœ€è¦ | âœ… å¿…éœ€ |
| è¿”å›å†…å®¹ | é¢„å®šä¹‰é…ç½® | LLM ç”Ÿæˆ |
| Templates ä½¿ç”¨ | âŒ ä¸ä¼ é€’ç»™ LLM | âœ… ä¼ é€’ç»™ LLM |
| é€Ÿåº¦ | å¿«ï¼ˆæ¯«ç§’çº§ï¼‰ | æ…¢ï¼ˆç§’çº§ï¼‰ |
| æˆæœ¬ | å…è´¹ | æŒ‰è°ƒç”¨æ”¶è´¹ |
| é€‚ç”¨åœºæ™¯ | è‡ªåŠ¨åŒ–æµ‹è¯• | å®é™…ä½¿ç”¨ |

---

## ğŸ” å¦‚ä½•éªŒè¯ Templates çœŸçš„è¢« LLM ä½¿ç”¨ï¼Ÿ

### æ–¹æ³• 1: å¯ç”¨è°ƒè¯•è¾“å‡º

```bash
sage pipeline build \
  --name "TestApp" \
  --goal "æ„å»ºé—®ç­”åº”ç”¨" \
  --show-knowledge  # â† æ˜¾ç¤ºåŒ¹é…çš„æ¨¡æ¿
```

è¾“å‡ºä¼šåŒ…å«ï¼š
```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Matched Templates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [1] å®¢æœçŸ¥è¯†åŠ©æ‰‹ (RAG Simple)                    â”‚
â”‚     Score: 0.17                                 â”‚
â”‚     Tags: rag, qa, support                      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

### æ–¹æ³• 2: æ·»åŠ è°ƒè¯•ä»£ç 

åœ¨ `pipeline.py:638` åæ·»åŠ ï¼š

```python
# è°ƒè¯•ï¼šæ‰“å°å‘é€ç»™ LLM çš„å®Œæ•´æç¤ºè¯
print("=" * 80)
print("USER PROMPT:")
print("=" * 80)
print(user_prompt)
print("=" * 80)
```

è¿™æ ·ä½ å°±èƒ½çœ‹åˆ° Templates ç¡®å®åœ¨æç¤ºè¯ä¸­äº†ï¼

### æ–¹æ³• 3: è¿è¡Œæ¼”ç¤ºè„šæœ¬

```bash
python examples/tutorials/templates_to_llm_demo.py
```

è¿™ä¸ªè„šæœ¬è¯¦ç»†å±•ç¤ºäº†ä» Template åŒ¹é…åˆ°ä¼ é€’ç»™ LLM çš„å®Œæ•´è¿‡ç¨‹ã€‚

### æ–¹æ³• 4: æŸ¥çœ‹çœŸå® LLM ç”Ÿæˆçš„é…ç½®

ä½¿ç”¨çœŸå® APIï¼š
```bash
export SAGE_CHAT_API_KEY="your-key"
sage chat --backend openai --model qwen-max --show-knowledge

# è¾“å…¥ï¼šè¯·å¸®æˆ‘æ„å»ºä¸€ä¸ªé—®ç­”åº”ç”¨
```

è§‚å¯Ÿç”Ÿæˆçš„é…ç½®æ˜¯å¦å‚è€ƒäº†åŒ¹é…çš„æ¨¡æ¿ç»“æ„ã€‚

---

## ğŸ“Š Templates çš„ä»·å€¼

### ä¸ºä»€ä¹ˆéœ€è¦ Templatesï¼Ÿ

1. **ç»“æ„æŒ‡å¯¼**: æä¾›å®Œæ•´çš„ Pipeline ç»“æ„ç¤ºä¾‹
2. **ç»„ä»¶æ¨è**: å±•ç¤ºåˆé€‚çš„ç»„ä»¶ç»„åˆ
3. **æœ€ä½³å®è·µ**: åŒ…å«ä½¿ç”¨å»ºè®®å’Œæ³¨æ„äº‹é¡¹
4. **é™ä½å¹»è§‰**: å‡å°‘ LLM ç”Ÿæˆä¸å­˜åœ¨çš„ç»„ä»¶
5. **åŠ é€Ÿç”Ÿæˆ**: ç»™ LLM ä¸€ä¸ªå¥½çš„èµ·ç‚¹

### Templates åŒ…å«ä»€ä¹ˆä¿¡æ¯ï¼Ÿ

```python
ApplicationTemplate(
    id="rag-simple-demo",
    title="å®¢æœçŸ¥è¯†åŠ©æ‰‹ (RAG Simple)",
    tags=("rag", "qa", "support"),  # â† ç”¨äºåŒ¹é…
    description="é¢å‘å®¢æœé—®ç­”çš„ç®€åŒ–RAGå·¥ä½œæµ...",
    example_path="examples/rag/rag_simple.py",
    blueprint_id="rag-simple-demo",
    guidance="é€‚åˆå®¢æœåœºæ™¯çš„FAQè‡ªåŠ¨ç­”å¤...",  # â† ç»™ LLM çš„æŒ‡å¯¼
)
```

### å®é™…æ•ˆæœ

**æ²¡æœ‰ Templates**:
- LLM å¯èƒ½ç”Ÿæˆä¸å­˜åœ¨çš„ç±»å
- ç»“æ„å¯èƒ½ä¸ç¬¦åˆ SAGE è§„èŒƒ
- éœ€è¦æ›´å¤šè½®è¿­ä»£

**æœ‰ Templates**:
- LLM å‚è€ƒçœŸå®å­˜åœ¨çš„ç»„ä»¶
- ç”Ÿæˆçš„é…ç½®ç»“æ„æ­£ç¡®
- é¦–æ¬¡ç”ŸæˆæˆåŠŸç‡æ›´é«˜

---

## ğŸ¯ æ€»ç»“

### å›ç­”ä½ çš„é—®é¢˜

1. **Templates åœ¨å“ªä¸€æ­¥è¢«å‚è€ƒï¼Ÿ**
   - âœ… åœ¨æ„å»º User Prompt æ—¶ï¼ˆpipeline.py:654-657ï¼‰
   - âœ… ä½œä¸º"åº”ç”¨æ¨¡æ¿å‚è€ƒ"ä¼ é€’ç»™ LLM
   - âœ… LLM ä¼šåŸºäºè¿™äº›æ¨¡æ¿ç”Ÿæˆç¬¦åˆè§„èŒƒçš„é…ç½®

2. **æµ‹è¯•ç”¨çš„æ˜¯çœŸå® OpenAI å—ï¼Ÿ**
   - âŒ æµ‹è¯•ä½¿ç”¨ Mockï¼ˆDummyGeneratorï¼‰
   - âœ… ç”Ÿäº§ç¯å¢ƒä½¿ç”¨çœŸå® LLMï¼ˆOpenAIClientï¼‰
   - âœ… é€šè¿‡ `backend` å‚æ•°æ§åˆ¶ï¼ˆmock / openaiï¼‰

### éªŒè¯æ–¹æ³•

```bash
# æŸ¥çœ‹å®Œæ•´æ¼”ç¤º
python examples/tutorials/templates_to_llm_demo.py

# çœŸå®ä½¿ç”¨ï¼ˆéœ€è¦ API Keyï¼‰
export SAGE_CHAT_API_KEY="your-key"
sage chat --backend openai --model qwen-max --show-knowledge
```

**Templates ç¡®å®è¢« LLM ä½¿ç”¨äº†ï¼Œè¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒç‰¹æ€§ï¼** ğŸ‰
