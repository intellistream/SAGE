# å¦‚ä½•æ·»åŠ æ–°çš„ SOTA è®­ç»ƒæ–¹æ³•

## ğŸ“ æ·»åŠ ä½ç½®

æ·»åŠ æ–°æ–¹æ³•æ¶‰åŠ **3ä¸ªå±‚çº§**ï¼Œè¯·æŒ‰éœ€ä¿®æ”¹ï¼š

### 1ï¸âƒ£ ç®—æ³•å®ç° (sage-libs/finetune/agent/)

å¦‚æœæ–° SOTA æ–¹æ³•éœ€è¦æ–°çš„ç®—æ³•ç»„ä»¶ï¼ˆå¦‚æ–°çš„æ ·æœ¬é€‰æ‹©ç­–ç•¥ï¼‰ï¼Œåœ¨è¿™é‡Œæ·»åŠ ï¼š

```
packages/sage-libs/src/sage/libs/finetune/agent/
â”œâ”€â”€ continual.py        # CoresetSelector, OnlineContinualLearner
â”œâ”€â”€ trainer.py          # AgentSFTTrainer - è®­ç»ƒå¾ªç¯
â””â”€â”€ YOUR_NEW_ALGO.py    # æ–°å¢: å¦‚ curriculum_learning.py
```

**ç¤ºä¾‹ï¼šæ·»åŠ  Curriculum Learning**

```python
# packages/sage-libs/src/sage/libs/finetune/agent/curriculum.py

class CurriculumScheduler:
    """æŒ‰éš¾åº¦æ¸è¿›å¼å®‰æ’è®­ç»ƒæ ·æœ¬"""

    def __init__(self, strategy: str = "loss_based"):
        self.strategy = strategy

    def sort_samples(self, samples: list, metrics: dict) -> list:
        """æŒ‰éš¾åº¦æ’åºï¼Œå…ˆç®€å•åå¤æ‚"""
        if self.strategy == "loss_based":
            # ä½ loss = ç®€å•ï¼Œå…ˆå­¦
            return sorted(samples, key=lambda s: metrics.get(s.id, 0))
        ...
```

### 2ï¸âƒ£ æ–¹æ³•é…ç½® (method_comparison.py)

åœ¨ `MethodRegistry` ä¸­æ³¨å†Œæ–°æ–¹æ³•é…ç½®ï¼š

```python
# packages/sage-benchmark/src/sage/benchmark/benchmark_agent/experiments/method_comparison.py

class MethodRegistry:
    @staticmethod
    def get_all_methods() -> dict[str, MethodConfig]:
        return {
            # ... ç°æœ‰æ–¹æ³• A-D ...

            # ğŸ†• æ·»åŠ æ–°çš„ SOTA æ–¹æ³•
            "E_curriculum": MethodConfig(
                name="E: Curriculum Learning",
                description="Easy-to-hard progressive training",
                use_coreset=False,
                use_continual=False,
                use_curriculum=True,  # éœ€è¦å…ˆåœ¨ MethodConfig æ·»åŠ æ­¤å­—æ®µ
                curriculum_strategy="loss_based",
            ),

            "F_data_augmentation": MethodConfig(
                name="F: Data Augmentation",
                description="Augment training data with paraphrases",
                use_augmentation=True,
                augmentation_factor=2.0,
            ),

            "G_knowledge_distillation": MethodConfig(
                name="G: Knowledge Distillation",
                description="Distill from larger teacher model",
                use_distillation=True,
                teacher_model="Qwen/Qwen2.5-72B-Instruct",
            ),
        }
```

### 3ï¸âƒ£ è®­ç»ƒå™¨é›†æˆ (trainer.py)

åœ¨ `AgentSFTTrainer` ä¸­é›†æˆæ–°ç®—æ³•ï¼š

```python
# packages/sage-libs/src/sage/libs/finetune/agent/trainer.py

class AgentSFTTrainer:
    def __init__(self, config: AgentSFTConfig, ...):
        ...
        self.curriculum_scheduler = self._build_curriculum_scheduler()  # æ–°å¢

    def prepare_datasets(self) -> None:
        ...
        # ğŸ†• æ·»åŠ  curriculum learning æ”¯æŒ
        if self.curriculum_scheduler and self.config.use_curriculum:
            self._train_samples = self.curriculum_scheduler.sort_samples(
                self._train_samples,
                metrics=self._collect_metrics(...)
            )
```

---

## ğŸ”¬ å½“å‰ SOTA æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | è®ºæ–‡/æ¥æº | å½“å‰å®ç°çŠ¶æ€ |
|------|----------|-------------|
| **Coreset Selection** | "Selection Via Proxy" (2019) | âœ… å·²å®ç° (B1-B4) |
| **Continual Learning** | "Experience Replay" (2017) | âœ… å·²å®ç° (C) |
| **Curriculum Learning** | "Self-Paced Learning" (2010) | â¬œ å¾…æ·»åŠ  |
| **Data Augmentation** | "Back Translation" (2016) | â¬œ å¾…æ·»åŠ  |
| **Knowledge Distillation** | "DistilBERT" (2019) | â¬œ å¾…æ·»åŠ  |
| **Active Learning** | "Uncertainty Sampling" | â¬œ å¾…æ·»åŠ  |
| **LoRA+** | "LoRA+: Efficient Low Rank" (2024) | â¬œ å¾…æ·»åŠ  |
| **DoRA** | "Weight-Decomposed LoRA" (2024) | â¬œ å¾…æ·»åŠ  |

---

## âœ… å½“å‰å¾®è°ƒæ˜¯å¦å¯ç”¨ï¼Ÿ

**æ˜¯çš„ï¼** å½“å‰ä»£ç å·²ç»å¯ä»¥çœŸå®å¾®è°ƒå¤§æ¨¡å‹ï¼š

### å·²å®ç°åŠŸèƒ½

1. **å®Œæ•´è®­ç»ƒæµç¨‹** (`AgentSFTTrainer.train()`)
   - åŠ è½½æ¨¡å‹ + Tokenizer
   - åº”ç”¨ LoRA é€‚é…å™¨
   - æ•°æ®é¢„å¤„ç†å’Œ tokenization
   - HuggingFace Trainer è®­ç»ƒå¾ªç¯
   - ä¿å­˜ LoRA æƒé‡

2. **ä¼˜åŒ–æŠ€æœ¯**
   - LoRA (r=64, alpha=128)
   - 8-bit/4-bit é‡åŒ–
   - Gradient Checkpointing
   - bf16/fp16 æ··åˆç²¾åº¦

3. **æ ·æœ¬é€‰æ‹©** (Coreset)
   - Loss Top-K
   - Diversity-based
   - Hybrid (60/40)
   - Random baseline

4. **æŒç»­å­¦ä¹ ** (Continual)
   - Experience Replay Buffer
   - Replay Ratio æ§åˆ¶

### è¿è¡Œå¾®è°ƒ

```bash
# å¿«é€Ÿæµ‹è¯• (~15åˆ†é’Ÿ)
cd packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts
python run_full_training_comparison.py --method D_combined --quick

# å®Œæ•´è®­ç»ƒ (~3-4å°æ—¶)
python run_full_training_comparison.py --full

# å•ç‹¬è¿è¡Œ Baseline
python run_full_training_comparison.py --method A_baseline --full
```

### è¾“å‡ºä½ç½®

```
~/.sage/agent_training/
â”œâ”€â”€ checkpoints/     # è®­ç»ƒæ£€æŸ¥ç‚¹
â”œâ”€â”€ logs/            # TensorBoard æ—¥å¿—
â””â”€â”€ lora_weights/    # æœ€ç»ˆ LoRA æƒé‡
```

---

## ğŸš€ å»ºè®®çš„ä¸‹ä¸€æ­¥

1. **è¿è¡Œä¸€æ¬¡çœŸå®è®­ç»ƒ**éªŒè¯æµç¨‹æ­£å¸¸
2. **æ·»åŠ  Curriculum Learning** ä½œä¸º Method E
3. **æ·»åŠ è¯„ä¼°æµç¨‹**ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¯„ä¼° benchmark
4. **é›†æˆ DoRA/LoRA+** æœ€æ–°çš„é«˜æ•ˆå¾®è°ƒæ–¹æ³•
