# Paper 1 (SAGE-Bench) å‰©ä½™ä»»åŠ¡ - å®Œæ•´æŒ‡å—

> æœ¬æ–‡æ¡£å®šä¹‰äº†å®Œæˆ SAGE-Bench Benchmark è®ºæ–‡æ‰€éœ€çš„æ‰€æœ‰ä»»åŠ¡
> æ¯ä¸ªä»»åŠ¡éƒ½å¯ä»¥åˆ†é…ç»™ä¸åŒçš„ Copilot Agent å¹¶è¡Œæ‰§è¡Œ
>
> **ç”Ÿæˆæ—¥æœŸ**: 2025-11-27

---

## ğŸš€ ç»Ÿä¸€å…¥å£ CLI (é‡è¦)

**åœ¨æ‰§è¡Œä»»ä½•å®éªŒä¹‹å‰ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„äº¤äº’å¼ CLI å…¥å£ï¼š**

```bash
# è¿›å…¥è„šæœ¬ç›®å½•
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

# æ–¹å¼ 1: äº¤äº’å¼è¿è¡Œ (æ¨è)
python sage_benchmark_cli.py

# æ–¹å¼ 2: ç›´æ¥æŒ‡å®šå®éªŒ (è·³è¿‡ç¡®è®¤)
python sage_benchmark_cli.py --paper 1 --experiment tool_selection --yes
python sage_benchmark_cli.py --paper 1 --experiment all_challenges --yes

# æ–¹å¼ 3: åˆ—å‡ºæ‰€æœ‰å¯ç”¨å®éªŒ
python sage_benchmark_cli.py --list
```

**âš ï¸ æ³¨æ„**: ä½¿ç”¨ `--yes` æˆ– `-y` å‚æ•°å¯ä»¥è·³è¿‡ç¡®è®¤æç¤ºï¼Œç›´æ¥è¿è¡Œå®éªŒã€‚

**CLI æ”¯æŒçš„ Paper 1 å®éªŒï¼š**

| ID | åç§° | æè¿° | é¢„ä¼°æ—¶é—´ |
|----|------|------|----------|
| `timing` | Challenge 1 | è¯„æµ‹ä½•æ—¶è°ƒç”¨å·¥å…· | ~10 min |
| `planning` | Challenge 2 | è¯„æµ‹ä»»åŠ¡åˆ†è§£ä¸è§„åˆ’ | ~15 min |
| `tool_selection` | Challenge 3 | è¯„æµ‹å·¥å…·æ£€ç´¢ä¸é€‰æ‹© | ~20 min |
| `all_challenges` | å®Œæ•´è¯„æµ‹ | è¿è¡Œæ‰€æœ‰ 3 ä¸ª Challenge | ~2 hours |
| `cross_dataset` | è·¨æ•°æ®é›† | SAGE + ACEBench + ToolBench | ~30 min |
| `quick_benchmark` | å¿«é€Ÿè¯„æµ‹ | è·³è¿‡ LLM æ–¹æ³• | ~30 min |

---

## ğŸ“Š å½“å‰çŠ¶æ€æ¦‚è§ˆ

| ç»„ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| æ•°æ®é›† (SAGE-Bench) | âœ… å®Œæˆ | 1,200 æ ·æœ¬ + 1,200 å·¥å…· |
| ACEBench é›†æˆ | âœ… å®Œæˆ | HuggingFace åŠ è½½æ­£å¸¸ |
| Keyword/Embedding/Hybrid | âœ… å·¥ä½œ | åŸºç¡€æ–¹æ³•æ­£å¸¸ |
| **Gorilla** | âŒ Bug | å·¥å…·ç´¢å¼• ID ä¸åŒ¹é… |
| **DFSDT** | âŒ Bug | å·¥å…·ç´¢å¼• ID ä¸åŒ¹é… |
| **Timing Decider** | âŒ Bug | æ¥å£ä¸å…¼å®¹ (dict vs object) |
| LLM-based æ–¹æ³• | âš ï¸ æœªå®Œæ•´æµ‹è¯• | éœ€è¦éªŒè¯ |
| è®ºæ–‡å›¾è¡¨ | âš ï¸ éœ€å®Œå–„ | éœ€è¦æœ€ç»ˆç»“æœ |

### å½“å‰æ€§èƒ½åŸºå‡†

| Challenge | Best Method | Current | Target | Gap |
|-----------|-------------|---------|--------|-----|
| Timing | Rule-based | 76% | 95% | -19% |
| Planning | Hierarchical | 27% | 90% | -63% |
| Tool Selection | BM25 | 82% | 95% | -13% |

---

## ğŸ”§ Task 1: ä¿®å¤ Timing Decider æ¥å£ä¸å…¼å®¹

### é—®é¢˜æè¿°

`timing.rule_based`, `timing.llm_based`, `timing.hybrid` æœŸæœ›çš„æ˜¯å¸¦ `.message` å±æ€§çš„å¯¹è±¡ï¼Œä½† benchmark ä¼ å…¥çš„æ˜¯ dictã€‚

### æç¤ºè¯

```
è¯·å¸®æˆ‘ä¿®å¤ SAGE benchmark ä¸­ Timing Decider çš„æ¥å£ä¸å…¼å®¹é—®é¢˜ã€‚

## é—®é¢˜æè¿°

Timing Decider æ–¹æ³• (`timing.rule_based`, `timing.llm_based`, `timing.hybrid`)
æœŸæœ›è¾“å…¥æ˜¯å¸¦ `.message` å±æ€§çš„å¯¹è±¡ï¼Œä½† benchmark ä¼ å…¥çš„æ˜¯ dict æ ¼å¼ã€‚

## å…³é”®æ–‡ä»¶ä½ç½®

- Adapter Registry: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
- Timing Decider: `packages/sage-libs/src/sage/libs/agentic/agents/planning/timing_decider.py`
- Schemas: `packages/sage-libs/src/sage/libs/agentic/agents/planning/schemas.py`

## ä¿®å¤æ–¹æ¡ˆ

åœ¨ `TimingAdapter.decide()` ä¸­æ·»åŠ è¾“å…¥è½¬æ¢ï¼š

```python
def decide(self, message: Any, **kwargs) -> Any:
    # å¦‚æœæ˜¯ dictï¼Œè½¬æ¢ä¸º TimingMessage
    if isinstance(message, dict):
        from sage.libs.agentic.agents.planning.schemas import TimingMessage
        message = TimingMessage(
            message=message.get('instruction', ''),
            context=message.get('context', {})
        )
    return self.decider.decide(message)
```

## ç»Ÿä¸€å…¥å£ CLI

ä¿®å¤åï¼Œä½¿ç”¨ç»Ÿä¸€ CLI éªŒè¯ï¼š
```bash
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts
python sage_benchmark_cli.py --paper 1 --experiment timing
```

## æˆåŠŸæ ‡å‡†

- `timing.rule_based` è¿”å›æœ‰æ•ˆçš„ true/false åˆ¤æ–­
- `timing.llm_based` æ­£å¸¸è°ƒç”¨ LLM
- `timing.hybrid` ç»„åˆåˆ¤æ–­æ­£å¸¸
- æ—  AttributeError æˆ– KeyError
```

---

## ğŸ”§ Task 2: ä¿®å¤ Gorilla/DFSDT å·¥å…·ç´¢å¼•é—®é¢˜

### é—®é¢˜æè¿°

å½“å‰ Gorilla å’Œ DFSDT é€‰æ‹©å™¨åœ¨è¯„æµ‹æ—¶è¿”å› 0% å‡†ç¡®ç‡ï¼ŒåŸå› æ˜¯ï¼š
- é€‰æ‹©å™¨æ„å»ºç´¢å¼•æ—¶ä½¿ç”¨çš„æ˜¯ mock å·¥å…· ID (`tool_000`, `tool_001`...)
- ä½†æ•°æ®é›†ä¸­çš„ `candidate_tools` ä½¿ç”¨çš„æ˜¯å®é™…å·¥å…· ID (`environment_weather_001`, `finance_payment_001`...)

### æç¤ºè¯

```
è¯·å¸®æˆ‘ä¿®å¤ SAGE benchmark ä¸­ Gorilla å’Œ DFSDT é€‰æ‹©å™¨çš„å·¥å…·ç´¢å¼•é—®é¢˜ã€‚

## é—®é¢˜æè¿°

å½“å‰ Gorilla å’Œ DFSDT é€‰æ‹©å™¨åœ¨è¯„æµ‹æ—¶è¿”å› 0% å‡†ç¡®ç‡ï¼ŒåŸå› æ˜¯ï¼š
- é€‰æ‹©å™¨æ„å»ºç´¢å¼•æ—¶ä½¿ç”¨çš„æ˜¯ mock å·¥å…· ID (`tool_000`, `tool_001`...)
- ä½†æ•°æ®é›†ä¸­çš„ `candidate_tools` ä½¿ç”¨çš„æ˜¯å®é™…å·¥å…· ID (`environment_weather_001`, `finance_payment_001`...)

## å…³é”®æ–‡ä»¶ä½ç½®

- å·¥å…·ç›®å½•: `packages/sage-benchmark/src/sage/data/sources/agent_tools/data/tool_catalog.jsonl` (1,200 ä¸ªå·¥å…·)
- è¯„æµ‹æ•°æ®: `packages/sage-benchmark/src/sage/data/sources/agent_benchmark/splits/tool_selection.jsonl`
- Adapter Registry: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
- Gorilla å®ç°: `packages/sage-libs/src/sage/libs/agentic/agents/action/tool_selection/gorilla_selector.py`
- DFSDT å®ç°: `packages/sage-libs/src/sage/libs/agentic/agents/action/tool_selection/dfsdt_selector.py`

## å·¥å…·ç›®å½•æ ¼å¼ (tool_catalog.jsonl)

æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼š
{
  "tool_id": "environment_weather_001",
  "name": "Weather Fetch 1",
  "category": "environment/weather",
  "capabilities": ["forecast", "radar"],
  "inputs": [...],
  "outputs": [...],
  ...
}

## æ•°æ®æ ·æœ¬æ ¼å¼ (tool_selection.jsonl)

{
  "sample_id": "ts_000002",
  "instruction": "What's the weather in Paris?",
  "candidate_tools": ["environment_weather_001", "finance_payment_024", ...],
  "ground_truth": {"top_k": ["environment_weather_001"]}
}

## éœ€è¦å®Œæˆçš„å·¥ä½œ

1. **åˆ›å»º SageToolsLoader ç±»**
   - ä½ç½®: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/tools_loader.py` (æ–°æ–‡ä»¶)
   - åŠŸèƒ½: åŠ è½½ `tool_catalog.jsonl` ä¸­çš„ 1,200 ä¸ªå·¥å…·
   - å®ç° `iter_all()` æ–¹æ³•ï¼Œè¿”å›å·¥å…·å¯¹è±¡

2. **ä¿®æ”¹ adapter_registry.py**
   - åœ¨åˆ›å»º Gorilla/DFSDT æ—¶ï¼Œä½¿ç”¨ SageToolsLoader è€Œé mock tools
   - ç¡®ä¿ `SelectorResources.tools_loader` ä½¿ç”¨æ­£ç¡®çš„åŠ è½½å™¨

3. **éªŒè¯ä¿®å¤**
   - ä½¿ç”¨ç»Ÿä¸€ CLI: `python sage_benchmark_cli.py --paper 1 --experiment tool_selection`
   - æˆ–ç›´æ¥è¿è¡Œ: `python run_unified_eval.py --dataset sage --methods gorilla,dfsdt --samples 20`
   - é¢„æœŸ: å‡†ç¡®ç‡åº”è¯¥ > 0%

## ç»Ÿä¸€å…¥å£ CLI

```bash
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts
python sage_benchmark_cli.py --paper 1 --experiment tool_selection
```

## æˆåŠŸæ ‡å‡†

- Gorilla Top-5 å‡†ç¡®ç‡ > 60%
- DFSDT Top-5 å‡†ç¡®ç‡ > 60%
- æ—  "No tools retrieved" è­¦å‘Š
```

---

## ğŸ”§ Task 3: å®Œå–„ LLM-based æ–¹æ³•æµ‹è¯•

### æç¤ºè¯

```
è¯·å¸®æˆ‘å®Œå–„ SAGE benchmark ä¸­ LLM-based æ–¹æ³•çš„æµ‹è¯•å’ŒéªŒè¯ã€‚

## èƒŒæ™¯

SAGE-Bench éœ€è¦è¯„æµ‹ä»¥ä¸‹ LLM-based æ–¹æ³•ï¼š
- Tool Selection: `llm_direct` (ç›´æ¥ç”¨ LLM é€‰æ‹©å·¥å…·)
- Timing: `timing.llm_based` (ç”¨ LLM åˆ¤æ–­æ˜¯å¦éœ€è¦å·¥å…·)
- Planning: `planner.llm_based`, `planner.react`, `planner.tot`

è¿™äº›æ–¹æ³•éœ€è¦ LLM æœåŠ¡æ”¯æŒï¼Œå¯ä»¥ä½¿ç”¨ï¼š
1. æœ¬åœ° vLLM æœåŠ¡ (localhost:8001)
2. Embedded vLLM (è¿›ç¨‹å†…åŠ è½½)
3. äº‘ç«¯ API (DashScope)

## å…³é”®æ–‡ä»¶ä½ç½®

- **ç»Ÿä¸€å…¥å£ CLI**: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/sage_benchmark_cli.py`
- Unified Eval: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_unified_eval.py`
- All Experiments: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py`
- Adapter Registry: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
- LLM Client: `packages/sage-common/src/sage/common/components/sage_llm/client.py`

## éœ€è¦å®Œæˆçš„å·¥ä½œ

1. **éªŒè¯ LLM-based Selector**
   - æµ‹è¯• `llm_direct` æ–¹æ³•åœ¨ SAGE æ•°æ®é›†ä¸Šçš„è¡¨ç°
   - ç¡®ä¿æ­£ç¡®è°ƒç”¨ IntelligentLLMClient

2. **éªŒè¯ LLM-based Timing**
   - æµ‹è¯• `timing.llm_based` æ–¹æ³•
   - æ£€æŸ¥ prompt æ¨¡æ¿æ˜¯å¦åˆç†

3. **éªŒè¯ LLM-based Planning**
   - æµ‹è¯• `planner.llm_based`, `planner.react`, `planner.tot`
   - ç¡®ä¿è§„åˆ’ç»“æœæ ¼å¼æ­£ç¡®

4. **æ·»åŠ  --use-embedded æ¨¡å¼æ”¯æŒ**
   - ç¡®ä¿ `run_unified_eval.py --use-embedded` æ­£å¸¸å·¥ä½œ
   - é»˜è®¤ä½¿ç”¨ Qwen/Qwen2.5-0.5B-Instruct è¿›è¡Œæµ‹è¯•

## ç»Ÿä¸€å…¥å£ CLI (æ¨è)

```bash
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

# äº¤äº’å¼è¿è¡Œ
python sage_benchmark_cli.py

# æˆ–ç›´æ¥æŒ‡å®š
python sage_benchmark_cli.py --paper 1 --experiment all_challenges
```

## æµ‹è¯•å‘½ä»¤ (å¤‡ç”¨)

```bash
# æµ‹è¯• LLM-based selector
python run_unified_eval.py --dataset sage --methods llm_direct --samples 10 -v

# ä½¿ç”¨ embedded vLLM
python run_unified_eval.py --dataset sage --methods llm_direct --samples 10 --use-embedded -v

# æµ‹è¯•æ‰€æœ‰ LLM æ–¹æ³•
python run_all_experiments.py --quick --max-samples 10
```

## æˆåŠŸæ ‡å‡†

- `llm_direct` è¿”å›æœ‰æ•ˆçš„å·¥å…·é€‰æ‹©ç»“æœ
- `timing.llm_based` è¿”å›åˆç†çš„ true/false åˆ¤æ–­
- `planner.react` å’Œ `planner.tot` è¿”å›æœ‰æ•ˆçš„è§„åˆ’æ­¥éª¤
- æ—  API è°ƒç”¨é”™è¯¯æˆ–è¶…æ—¶

## æ³¨æ„äº‹é¡¹

- å¦‚æœæœ¬åœ°æ²¡æœ‰ LLM æœåŠ¡ï¼Œä½¿ç”¨ `IntelligentLLMClient.create_auto()` ä¼šè‡ªåŠ¨å›é€€åˆ°äº‘ç«¯
- äº‘ç«¯ API éœ€è¦ `SAGE_CHAT_API_KEY` ç¯å¢ƒå˜é‡
- Embedded æ¨¡å¼éœ€è¦è¶³å¤Ÿçš„ GPU å†…å­˜
```

---

## ğŸ”§ Task 4: è·¨æ•°æ®é›†éªŒè¯å®Œå–„

### æç¤ºè¯

```
è¯·å¸®æˆ‘å®Œå–„ SAGE benchmark çš„è·¨æ•°æ®é›†éªŒè¯åŠŸèƒ½ã€‚

## èƒŒæ™¯

SAGE-Bench è®ºæ–‡éœ€è¦åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯å·¥å…·é€‰æ‹©æ–¹æ³•ï¼š
1. **SAGE-Bench** (è‡ªæœ‰æ•°æ®é›†): 600 tool selection æ ·æœ¬
2. **ACEBench/ToolACE** (å¤–éƒ¨æ•°æ®é›†): HuggingFace Team-ACE/ToolACE
3. **API-Bank** (å¯é€‰): Microsoft å¤šè½® API å¯¹è¯
4. **ToolAlpaca** (å¯é€‰): Microsoft å·¥å…·å­¦ä¹ å¯¹è¯

## å…³é”®æ–‡ä»¶ä½ç½®

- **ç»Ÿä¸€å…¥å£ CLI**: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/sage_benchmark_cli.py`
- Unified Eval: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_unified_eval.py`
- ACEBench Loader: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/acebench_loader.py`
- External Benchmarks: `packages/sage-benchmark/src/sage/data/sources/agent_benchmark/external_benchmarks/`

## å½“å‰çŠ¶æ€

- SAGE æ•°æ®é›†: âœ… æ­£å¸¸åŠ è½½
- ACEBench: âœ… å¯ä» HuggingFace åŠ è½½ï¼Œä½†éœ€è¦éªŒè¯æ‰€æœ‰æ–¹æ³•
- API-Bank: âš ï¸ æœ‰åŸå§‹æ•°æ®ï¼Œæœªé›†æˆ
- ToolAlpaca: âš ï¸ æœ‰åŸå§‹æ•°æ®ï¼Œæœªé›†æˆ

## éœ€è¦å®Œæˆçš„å·¥ä½œ

1. **éªŒè¯ ACEBench åœ¨æ‰€æœ‰æ–¹æ³•ä¸Šçš„è¯„æµ‹**
   - ä½¿ç”¨ç»Ÿä¸€ CLI: `python sage_benchmark_cli.py --paper 1 --experiment cross_dataset`
   - ç¡®ä¿ç»“æœæ ¼å¼ä¸ SAGE ä¸€è‡´

2. **é›†æˆ API-Bank æ•°æ®é›†** (å¯é€‰)
   - åˆ›å»º `apibank_loader.py`
   - è½¬æ¢ä¸ºç»Ÿä¸€çš„ ToolSelectionSample æ ¼å¼

3. **ç”Ÿæˆè·¨æ•°æ®é›†å¯¹æ¯”è¡¨æ ¼**
   - æ ¼å¼: Dataset Ã— Method Ã— Metric

## ç»Ÿä¸€å…¥å£ CLI

```bash
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

# è·¨æ•°æ®é›†å¯¹æ¯”
python sage_benchmark_cli.py --paper 1 --experiment cross_dataset

# æˆ–æ‰‹åŠ¨æŒ‡å®š
python run_unified_eval.py --dataset acebench --methods keyword,embedding,hybrid --samples 100
```

## æˆåŠŸæ ‡å‡†

- ACEBench ä¸Šæ‰€æœ‰æ–¹æ³•æ­£å¸¸è¿è¡Œ
- ç»“æœå¯ä»¥ä¸ SAGE æ•°æ®é›†ç»“æœå¯¹æ¯”
- ç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„ JSON ç»“æœæ–‡ä»¶
```

---

## ğŸ”§ Task 5: Scaling åˆ†æå®éªŒ

### æç¤ºè¯

```
è¯·å¸®æˆ‘å®Œæˆ SAGE benchmark çš„ Scaling åˆ†æå®éªŒã€‚

## èƒŒæ™¯

è®ºæ–‡éœ€è¦åˆ†æå·¥å…·æ•°é‡å¯¹é€‰æ‹©å‡†ç¡®ç‡çš„å½±å“ï¼Œç”Ÿæˆ Tool Count vs Accuracy æ›²çº¿ã€‚

## å®éªŒé…ç½®

- å·¥å…·æ•°é‡: 100, 500, 1000, 1200 (full)
- æ–¹æ³•: keyword, embedding, hybrid
- æ ·æœ¬æ•°: 200 per configuration

## ç»Ÿä¸€å…¥å£ CLI

```bash
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

# å…ˆç¡®è®¤ CLI å¯ç”¨
python sage_benchmark_cli.py --list
```

## éœ€è¦å®Œæˆçš„å·¥ä½œ

1. **åœ¨ run_unified_eval.py ä¸­æ·»åŠ  --num-candidate-tools å‚æ•°**

2. **è¿è¡Œ Scaling å®éªŒ**
```bash
for num_tools in 100 500 1000 1200; do
    python run_unified_eval.py --dataset sage --num-candidate-tools $num_tools --samples 200
done
```

3. **ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨**
   - X è½´: Tool Count
   - Y è½´: Top-5 Accuracy
   - å¤šæ¡çº¿: ä¸åŒæ–¹æ³•

## æˆåŠŸæ ‡å‡†

- ç”Ÿæˆ scaling_analysis.png å›¾è¡¨
- æ•°æ®ä¿å­˜åˆ° JSON æ–‡ä»¶
```

---

## ğŸ”§ Task 6: æ¶ˆèå®éªŒ

### æç¤ºè¯

```
è¯·å¸®æˆ‘å®Œæˆ SAGE benchmark çš„æ¶ˆèå®éªŒã€‚

## Hybrid Selector æ¶ˆè

æµ‹è¯•ä¸åŒ keyword_weight å¯¹ hybrid é€‰æ‹©å™¨çš„å½±å“ï¼š
- keyword_weight = 0.0 (çº¯ embedding)
- keyword_weight = 0.5 (å¹³è¡¡)
- keyword_weight = 1.0 (çº¯ keyword)

## Timing Hybrid æ¶ˆè

æµ‹è¯•ä¸åŒç»„åˆç­–ç•¥ï¼š
- rule_only
- llm_only
- hybrid (rule + llm)

## ç»Ÿä¸€å…¥å£ CLI

```bash
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts
python sage_benchmark_cli.py --paper 1 --experiment all_challenges
```

## æˆåŠŸæ ‡å‡†

- ç”Ÿæˆæ¶ˆèå®éªŒç»“æœè¡¨æ ¼
- ç»“æœä¿å­˜åˆ° ablation_results.json
```

---

## ğŸ”§ Task 7: è®ºæ–‡å›¾è¡¨ç”Ÿæˆ

### æç¤ºè¯

```
è¯·å¸®æˆ‘ç”Ÿæˆ SAGE-Bench è®ºæ–‡æ‰€éœ€çš„å›¾è¡¨ã€‚

## éœ€è¦ç”Ÿæˆçš„å›¾è¡¨

1. **Main Results Table**: æ‰€æœ‰æ–¹æ³•åœ¨ 3 ä¸ª Challenge ä¸Šçš„è¡¨ç°
2. **Cross-Dataset Comparison**: SAGE vs ACEBench vs API-Bank
3. **Scaling Analysis**: Tool Count vs Accuracy
4. **Ablation Study**: Hybrid selector æ¶ˆè

## æ•°æ®æ¥æº

- ä¸»ç»“æœ: `.sage/benchmark/results/all_results.json`
- è·¨æ•°æ®é›†: `.sage/benchmark/results/cross_dataset/`
- Scaling: `.sage/benchmark/results/scaling/`

## ç»Ÿä¸€å…¥å£ CLI

å…ˆè¿è¡Œå®Œæ•´å®éªŒï¼š
```bash
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts
python sage_benchmark_cli.py --paper 1 --experiment all_challenges
```

## è¾“å‡ºä½ç½®

- å›¾è¡¨: `experiment_results/figures/`
- è¡¨æ ¼: `experiment_results/tables/`
```

---

## ğŸ“‹ ä¼˜å…ˆçº§æ’åº

### P0 - å¿…é¡»å®Œæˆ (é˜»å¡è®ºæ–‡)

| # | ä»»åŠ¡ | çŠ¶æ€ | è¯´æ˜ |
|---|------|------|------|
| 1 | ä¿®å¤ Timing Decider æ¥å£ | âŒ | æ¥å£ä¸å…¼å®¹ |
| 2 | ä¿®å¤ Gorilla/DFSDT ç´¢å¼• | âŒ | è¿”å› 0% å‡†ç¡®ç‡ |
| 3 | å®Œæˆ SAGE å®Œæ•´å®éªŒ | âš ï¸ | éœ€è¦ä¿®å¤ Bug åè¿è¡Œ |
| 4 | å®Œæˆ ACEBench å®éªŒ | âš ï¸ | éœ€è¦éªŒè¯æ‰€æœ‰æ–¹æ³• |

### P1 - é‡è¦ (è®ºæ–‡å®Œå–„)

| # | ä»»åŠ¡ | çŠ¶æ€ | è¯´æ˜ |
|---|------|------|------|
| 5 | LLM-based æ–¹æ³•æµ‹è¯• | âš ï¸ | éœ€è¦éªŒè¯ |
| 6 | Scaling åˆ†æå®éªŒ | âŒ | æœªå¼€å§‹ |
| 7 | æ¶ˆèå®éªŒ | âŒ | æœªå¼€å§‹ |
| 8 | è®ºæ–‡å›¾è¡¨ç”Ÿæˆ | âŒ | ä¾èµ–å®éªŒç»“æœ |

### P2 - å¯é€‰å¢å¼º

| # | ä»»åŠ¡ | çŠ¶æ€ | è¯´æ˜ |
|---|------|------|------|
| 9 | API-Bank é›†æˆ | âŒ | å¯é€‰ |
| 10 | ä¸åŒ LLM æ¨¡å‹å¯¹æ¯” | âŒ | å¯é€‰ |

---

## ğŸ“‹ ä»»åŠ¡ä¾èµ–å’Œæ‰§è¡Œé¡ºåº

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¯å¹¶è¡Œæ‰§è¡Œçš„ä»»åŠ¡                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Task 1              Task 2              Task 3             â”‚
â”‚  (Timing æ¥å£)       (Gorilla/DFSDT)     (LLM-based)        â”‚
â”‚       â”‚                  â”‚                   â”‚              â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Task 4: è·¨æ•°æ®é›†éªŒè¯                           â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Task 5-6: Scaling + æ¶ˆèå®éªŒ                   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Task 7: è®ºæ–‡å›¾è¡¨ç”Ÿæˆ                           â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ‰§è¡Œå»ºè®®**:
- Task 1, 2, 3 å¯ä»¥åˆ†é…ç»™ 3 ä¸ªä¸åŒçš„ Copilot Agent å¹¶è¡Œæ‰§è¡Œ
- Task 4-7 éœ€è¦ç­‰ Task 1-3 åŸºæœ¬å®Œæˆåæ‰§è¡Œ
- æ¯ä¸ª Task é¢„è®¡ 2-3 å°æ—¶

---

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

```bash
# 1. è¿›å…¥è„šæœ¬ç›®å½•
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

# 2. ä½¿ç”¨ç»Ÿä¸€ CLI (æ¨è)
python sage_benchmark_cli.py

# 3. æˆ–ç›´æ¥è¿è¡Œç‰¹å®šå®éªŒ
python sage_benchmark_cli.py --paper 1 --experiment tool_selection

# 4. åˆ—å‡ºæ‰€æœ‰å¯ç”¨å®éªŒ
python sage_benchmark_cli.py --list
```

---

## ğŸ“ æ–‡ä»¶ä½ç½®å‚è€ƒ

```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ sage_benchmark_cli.py      # ğŸŒŸ ç»Ÿä¸€äº¤äº’å¼å…¥å£ (æ¨è)
â”‚   â”œâ”€â”€ run_all_experiments.py     # ä¸‰ Challenge å®Œæ•´å®éªŒ
â”‚   â”œâ”€â”€ run_unified_eval.py        # Tool Selection è¯„ä¼°
â”‚   â””â”€â”€ README.md                  # è„šæœ¬æ–‡æ¡£
â”œâ”€â”€ adapter_registry.py            # éœ€è¦ä¿®å¤: æ¥å£é€‚é…
â”œâ”€â”€ acebench_loader.py             # ACEBench æ•°æ®åŠ è½½
â””â”€â”€ experiments/                   # å®éªŒå®šä¹‰

packages/sage-benchmark/src/sage/data/sources/
â”œâ”€â”€ agent_tools/data/
â”‚   â””â”€â”€ tool_catalog.jsonl         # 1,200 ä¸ªå·¥å…·å®šä¹‰
â””â”€â”€ agent_benchmark/splits/
    â””â”€â”€ tool_selection.jsonl       # è¯„æµ‹æ•°æ®

packages/sage-libs/src/sage/libs/agentic/agents/
â”œâ”€â”€ action/tool_selection/
â”‚   â”œâ”€â”€ gorilla_selector.py        # éœ€è¦ä¿®å¤: å·¥å…·ç´¢å¼•
â”‚   â””â”€â”€ dfsdt_selector.py          # éœ€è¦ä¿®å¤: å·¥å…·ç´¢å¼•
â””â”€â”€ planning/
    â”œâ”€â”€ timing_decider.py          # éœ€è¦ä¿®å¤: æ¥å£
    â””â”€â”€ schemas.py                 # TimingMessage å®šä¹‰
```

---

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

```bash
# ç¡®ä¿åœ¨æ­£ç¡®çš„åˆ†æ”¯
cd /home/shuhao/SAGE
git checkout feature/agent_tools_plan

# æ¿€æ´»ç¯å¢ƒ
conda activate sage

# éªŒè¯å®‰è£…
python -c "from sage.benchmark.benchmark_agent.adapter_registry import get_adapter_registry; print('OK')"
```

---

## ğŸ“ æäº¤è§„èŒƒ

å®Œæˆä»»åŠ¡åï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ commit æ ¼å¼ï¼š

```bash
# Task 1
git commit -m "fix(benchmark): resolve Timing Decider interface compatibility issue"

# Task 2
git commit -m "fix(benchmark): resolve Gorilla/DFSDT tool index mismatch issue"

# Task 3
git commit -m "feat(benchmark): complete LLM-based methods testing and validation"

# Task 4
git commit -m "feat(benchmark): enhance cross-dataset validation with ACEBench"

# Task 5-6
git commit -m "feat(benchmark): add scaling analysis and ablation experiments"

# Task 7
git commit -m "docs(benchmark): generate ICML paper figures and LaTeX tables"
```

---

## âš ï¸ é‡è¦æé†’

1. **ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€ CLI**: `sage_benchmark_cli.py` æ˜¯æ‰€æœ‰å®éªŒçš„ç»Ÿä¸€å…¥å£
2. **ä¿®å¤ Bug ä¼˜å…ˆ**: Task 1-2 æ˜¯ P0 ä¼˜å…ˆçº§ï¼Œé˜»å¡åç»­å®éªŒ
3. **LLM æœåŠ¡**: LLM-based æ–¹æ³•éœ€è¦ API Key æˆ–æœ¬åœ°æœåŠ¡
4. **GPU å†…å­˜**: Embedded vLLM æ¨¡å¼éœ€è¦è¶³å¤Ÿçš„ GPU å†…å­˜
