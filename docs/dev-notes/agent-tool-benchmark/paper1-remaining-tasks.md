# Paper 1 (SAGE-Bench) å‰©ä½™ä»»åŠ¡ - å¹¶è¡Œæ‰§è¡ŒæŒ‡å—

> æœ¬æ–‡æ¡£å®šä¹‰äº†å®Œæˆ SAGE-Bench Benchmark è®ºæ–‡æ‰€éœ€çš„ 4 ä¸ªç‹¬ç«‹ä»»åŠ¡
> æ¯ä¸ªä»»åŠ¡éƒ½å¯ä»¥åˆ†é…ç»™ä¸åŒçš„ Copilot Agent å¹¶è¡Œæ‰§è¡Œ

---

## ğŸ“Š å½“å‰çŠ¶æ€æ¦‚è§ˆ

| ç»„ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| æ•°æ®é›† (SAGE-Bench) | âœ… å®Œæˆ | 1,200 æ ·æœ¬ + 1,200 å·¥å…· |
| ACEBench é›†æˆ | âœ… å®Œæˆ | HuggingFace åŠ è½½æ­£å¸¸ |
| Keyword/Embedding/Hybrid | âœ… å·¥ä½œ | åŸºç¡€æ–¹æ³•æ­£å¸¸ |
| **Gorilla** | âŒ Bug | å·¥å…·ç´¢å¼• ID ä¸åŒ¹é… |
| **DFSDT** | âŒ Bug | å·¥å…·ç´¢å¼• ID ä¸åŒ¹é… |
| LLM-based æ–¹æ³• | âš ï¸ æœªå®Œæ•´æµ‹è¯• | éœ€è¦éªŒè¯ |
| è®ºæ–‡å›¾è¡¨ | âš ï¸ éœ€å®Œå–„ | éœ€è¦æœ€ç»ˆç»“æœ |

---

## Task 1: ä¿®å¤ Gorilla/DFSDT å·¥å…·ç´¢å¼•é—®é¢˜

### æç¤ºè¯

```
è¯·å¸®æˆ‘ä¿®å¤ SAGE benchmark ä¸­ Gorilla å’Œ DFSDT é€‰æ‹©å™¨çš„å·¥å…·ç´¢å¼•é—®é¢˜ã€‚

## é—®é¢˜æè¿°

å½“å‰ Gorilla å’Œ DFSDT é€‰æ‹©å™¨åœ¨è¯„æµ‹æ—¶è¿”å› 0% å‡†ç¡®ç‡ï¼ŒåŸå› æ˜¯ï¼š
- é€‰æ‹©å™¨æ„å»ºç´¢å¼•æ—¶ä½¿ç”¨çš„æ˜¯ mock å·¥å…· ID (`tool_000`, `tool_001`...)
- ä½†æ•°æ®é›†ä¸­çš„ `candidate_tools` ä½¿ç”¨çš„æ˜¯å®é™…å·¥å…· ID (`environment_weather_001`, `finance_payment_001`...)

## å…³é”®æ–‡ä»¶ä½ç½®

- å·¥å…·ç›®å½•: `packages/sage-benchmark/src/sage/data/sources/agent_tools/data/tool_catalog.jsonl` (1,200 ä¸ªå·¥å…·)
- è¯„æµ‹æ•°æ®: `packages/sage-benchmark/src/sage/data/sources/agent_benchmark/splits/tool_selection.jsonl`
- Adapter Registry: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
- Gorilla å®ç°: `packages/sage-libs/src/sage/libs/agentic/agents/action/tool_selection/gorilla_selector.py`
- DFSDT å®ç°: `packages/sage-libs/src/sage/libs/agentic/agents/action/tool_selection/dfsdt_selector.py`

## å·¥å…·ç›®å½•æ ¼å¼ (tool_catalog.jsonl)

æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼š
{
  "tool_id": "environment_weather_001",
  "name": "Weather Fetch 1",
  "category": "environment/weather",
  "capabilities": ["forecast", "radar"],
  "inputs": [...],
  "outputs": [...],
  ...
}

## æ•°æ®æ ·æœ¬æ ¼å¼ (tool_selection.jsonl)

{
  "sample_id": "ts_000002",
  "instruction": "What's the weather in Paris?",
  "candidate_tools": ["environment_weather_001", "finance_payment_024", ...],
  "ground_truth": {"top_k": ["environment_weather_001"]}
}

## éœ€è¦å®Œæˆçš„å·¥ä½œ

1. **åˆ›å»º SageToolsLoader ç±»**
   - ä½ç½®: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/tools_loader.py` (æ–°æ–‡ä»¶)
   - åŠŸèƒ½: åŠ è½½ `tool_catalog.jsonl` ä¸­çš„ 1,200 ä¸ªå·¥å…·
   - å®ç° `iter_all()` æ–¹æ³•ï¼Œè¿”å›å·¥å…·å¯¹è±¡

2. **ä¿®æ”¹ adapter_registry.py**
   - åœ¨åˆ›å»º Gorilla/DFSDT æ—¶ï¼Œä½¿ç”¨ SageToolsLoader è€Œé mock tools
   - ç¡®ä¿ `SelectorResources.tools_loader` ä½¿ç”¨æ­£ç¡®çš„åŠ è½½å™¨

3. **éªŒè¯ä¿®å¤**
   - è¿è¡Œ: `python run_unified_eval.py --dataset sage --methods gorilla,dfsdt --samples 20`
   - é¢„æœŸ: å‡†ç¡®ç‡åº”è¯¥ > 0%

## éªŒè¯å‘½ä»¤

cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts
python run_unified_eval.py --dataset sage --methods keyword,gorilla,dfsdt --samples 50 -v

## æˆåŠŸæ ‡å‡†

- Gorilla Top-5 å‡†ç¡®ç‡ > 60%
- DFSDT Top-5 å‡†ç¡®ç‡ > 60%
- æ—  "No tools retrieved" è­¦å‘Š
```

---

## Task 2: å®Œå–„ LLM-based æ–¹æ³•æµ‹è¯•

### æç¤ºè¯

```
è¯·å¸®æˆ‘å®Œå–„ SAGE benchmark ä¸­ LLM-based æ–¹æ³•çš„æµ‹è¯•å’ŒéªŒè¯ã€‚

## èƒŒæ™¯

SAGE-Bench éœ€è¦è¯„æµ‹ä»¥ä¸‹ LLM-based æ–¹æ³•ï¼š
- Tool Selection: `llm_direct` (ç›´æ¥ç”¨ LLM é€‰æ‹©å·¥å…·)
- Timing: `timing.llm_based` (ç”¨ LLM åˆ¤æ–­æ˜¯å¦éœ€è¦å·¥å…·)
- Planning: `planner.llm_based`, `planner.react`, `planner.tot`

è¿™äº›æ–¹æ³•éœ€è¦ LLM æœåŠ¡æ”¯æŒï¼Œå¯ä»¥ä½¿ç”¨ï¼š
1. æœ¬åœ° vLLM æœåŠ¡ (localhost:8001)
2. Embedded vLLM (è¿›ç¨‹å†…åŠ è½½)
3. äº‘ç«¯ API (DashScope)

## å…³é”®æ–‡ä»¶ä½ç½®

- Unified Eval: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_unified_eval.py`
- All Experiments: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py`
- Adapter Registry: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
- LLM Client: `packages/sage-common/src/sage/common/components/sage_llm/client.py`

## éœ€è¦å®Œæˆçš„å·¥ä½œ

1. **éªŒè¯ LLM-based Selector**
   - æµ‹è¯• `llm_direct` æ–¹æ³•åœ¨ SAGE æ•°æ®é›†ä¸Šçš„è¡¨ç°
   - ç¡®ä¿æ­£ç¡®è°ƒç”¨ IntelligentLLMClient

2. **éªŒè¯ LLM-based Timing**
   - æµ‹è¯• `timing.llm_based` æ–¹æ³•
   - æ£€æŸ¥ prompt æ¨¡æ¿æ˜¯å¦åˆç†

3. **éªŒè¯ LLM-based Planning**
   - æµ‹è¯• `planner.llm_based`, `planner.react`, `planner.tot`
   - ç¡®ä¿è§„åˆ’ç»“æœæ ¼å¼æ­£ç¡®

4. **æ·»åŠ  --use-embedded æ¨¡å¼æ”¯æŒ**
   - ç¡®ä¿ `run_unified_eval.py --use-embedded` æ­£å¸¸å·¥ä½œ
   - é»˜è®¤ä½¿ç”¨ Qwen/Qwen2.5-0.5B-Instruct è¿›è¡Œæµ‹è¯•

## æµ‹è¯•å‘½ä»¤

# æµ‹è¯• LLM-based selector (éœ€è¦ LLM æœåŠ¡)
cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

# ä½¿ç”¨äº‘ç«¯ API
python run_unified_eval.py --dataset sage --methods llm_direct --samples 10 -v

# ä½¿ç”¨ embedded vLLM
python run_unified_eval.py --dataset sage --methods llm_direct --samples 10 --use-embedded --model Qwen/Qwen2.5-0.5B-Instruct -v

# æµ‹è¯•æ‰€æœ‰ LLM æ–¹æ³•
python run_all_experiments.py --quick --max-samples 10

## æˆåŠŸæ ‡å‡†

- `llm_direct` è¿”å›æœ‰æ•ˆçš„å·¥å…·é€‰æ‹©ç»“æœ
- `timing.llm_based` è¿”å›åˆç†çš„ true/false åˆ¤æ–­
- `planner.react` å’Œ `planner.tot` è¿”å›æœ‰æ•ˆçš„è§„åˆ’æ­¥éª¤
- æ—  API è°ƒç”¨é”™è¯¯æˆ–è¶…æ—¶

## æ³¨æ„äº‹é¡¹

- å¦‚æœæœ¬åœ°æ²¡æœ‰ LLM æœåŠ¡ï¼Œä½¿ç”¨ `IntelligentLLMClient.create_auto()` ä¼šè‡ªåŠ¨å›é€€åˆ°äº‘ç«¯
- äº‘ç«¯ API éœ€è¦ `SAGE_CHAT_API_KEY` ç¯å¢ƒå˜é‡
- Embedded æ¨¡å¼éœ€è¦è¶³å¤Ÿçš„ GPU å†…å­˜
```

---

## Task 3: è·¨æ•°æ®é›†éªŒè¯å®Œå–„

### æç¤ºè¯

```
è¯·å¸®æˆ‘å®Œå–„ SAGE benchmark çš„è·¨æ•°æ®é›†éªŒè¯åŠŸèƒ½ã€‚

## èƒŒæ™¯

SAGE-Bench è®ºæ–‡éœ€è¦åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯å·¥å…·é€‰æ‹©æ–¹æ³•ï¼š
1. **SAGE-Bench** (è‡ªæœ‰æ•°æ®é›†): 600 tool selection æ ·æœ¬
2. **ACEBench/ToolACE** (å¤–éƒ¨æ•°æ®é›†): HuggingFace Team-ACE/ToolACE
3. **API-Bank** (å¯é€‰): Microsoft å¤šè½® API å¯¹è¯
4. **ToolAlpaca** (å¯é€‰): Microsoft å·¥å…·å­¦ä¹ å¯¹è¯

## å…³é”®æ–‡ä»¶ä½ç½®

- Unified Eval: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_unified_eval.py`
- ACEBench Loader: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/acebench_loader.py`
- External Benchmarks: `packages/sage-benchmark/src/sage/data/sources/agent_benchmark/external_benchmarks/`

## å½“å‰çŠ¶æ€

- SAGE æ•°æ®é›†: âœ… æ­£å¸¸åŠ è½½
- ACEBench: âœ… å¯ä» HuggingFace åŠ è½½ï¼Œä½†éœ€è¦éªŒè¯æ‰€æœ‰æ–¹æ³•
- API-Bank: âš ï¸ æœ‰åŸå§‹æ•°æ®ï¼Œæœªé›†æˆ
- ToolAlpaca: âš ï¸ æœ‰åŸå§‹æ•°æ®ï¼Œæœªé›†æˆ

## éœ€è¦å®Œæˆçš„å·¥ä½œ

1. **éªŒè¯ ACEBench åœ¨æ‰€æœ‰æ–¹æ³•ä¸Šçš„è¯„æµ‹**
   - è¿è¡Œ: `python run_unified_eval.py --dataset acebench --methods keyword,embedding,hybrid --samples 100`
   - ç¡®ä¿ç»“æœæ ¼å¼ä¸ SAGE ä¸€è‡´

2. **å®Œå–„ ACEBench æ•°æ®åŠ è½½**
   - æ£€æŸ¥ `acebench_loader.py` ä¸­çš„æ ¼å¼è½¬æ¢æ˜¯å¦æ­£ç¡®
   - ç¡®ä¿ candidate_tools å’Œ ground_truth æ­£ç¡®æ˜ å°„

3. **ç”Ÿæˆè·¨æ•°æ®é›†å¯¹æ¯”è¡¨æ ¼**
   - ä¿®æ”¹ `run_unified_eval.py`ï¼Œæ”¯æŒ `--dataset all` åŒæ—¶è¯„æµ‹ SAGE å’Œ ACEBench
   - ç”Ÿæˆ LaTeX æ ¼å¼çš„å¯¹æ¯”è¡¨æ ¼

4. **(å¯é€‰) æ·»åŠ  API-Bank æ”¯æŒ**
   - æ•°æ®ä½ç½®: `external_benchmarks/converted/raw/apibank/`
   - åˆ›å»º `apibank_loader.py` åŠ è½½æ•°æ®

## æµ‹è¯•å‘½ä»¤

cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

# æµ‹è¯• SAGE æ•°æ®é›†
python run_unified_eval.py --dataset sage --methods keyword,embedding,hybrid --samples 100 -v

# æµ‹è¯• ACEBench æ•°æ®é›†
python run_unified_eval.py --dataset acebench --methods keyword,embedding,hybrid --samples 100 -v

# è·¨æ•°æ®é›†å¯¹æ¯”
python run_unified_eval.py --dataset all --methods keyword,embedding,hybrid --samples 100 -v

## é¢„æœŸè¾“å‡ºæ ¼å¼

================================================================================
Cross-Dataset Tool Selection Comparison
================================================================================
Method          | SAGE Top-5 | ACEBench Top-5 | Avg
----------------+------------+----------------+------
keyword         |    82.0%   |     78.0%      | 80.0%
embedding       |    82.0%   |     76.0%      | 79.0%
hybrid          |    84.0%   |     80.0%      | 82.0%
================================================================================

## æˆåŠŸæ ‡å‡†

- ACEBench æ•°æ®æ­£ç¡®åŠ è½½ï¼Œæ— æ ¼å¼é”™è¯¯
- æ‰€æœ‰æ–¹æ³•åœ¨ ACEBench ä¸Šè¿”å›æœ‰æ•ˆç»“æœ
- ç”Ÿæˆè·¨æ•°æ®é›†å¯¹æ¯”è¡¨æ ¼ï¼ˆMarkdown å’Œ LaTeXï¼‰
```

---

## Task 4: å®éªŒç»“æœæ•´ç†å’Œè®ºæ–‡å›¾è¡¨ç”Ÿæˆ

### æç¤ºè¯

```
è¯·å¸®æˆ‘å®Œå–„ SAGE benchmark çš„å®éªŒç»“æœæ•´ç†å’Œè®ºæ–‡å›¾è¡¨ç”Ÿæˆã€‚

## èƒŒæ™¯

SAGE-Bench è®ºæ–‡éœ€è¦ä»¥ä¸‹å›¾è¡¨å’Œè¡¨æ ¼ï¼š
1. **Table 1**: ä¸‰ä¸ª Challenge çš„ä¸»è¦ç»“æœå¯¹æ¯”
2. **Table 2**: è·¨æ•°æ®é›†éªŒè¯ç»“æœ
3. **Figure 1**: Timing Detection æ–¹æ³•å¯¹æ¯”
4. **Figure 2**: Task Planning æ–¹æ³•å¯¹æ¯”
5. **Figure 3**: Tool Selection æ–¹æ³•å¯¹æ¯”
6. **Figure 4**: å·¥å…·æ•°é‡ vs å‡†ç¡®ç‡ (Scaling Analysis)
7. **Figure 5**: é”™è¯¯ç±»å‹åˆ†æ

## å…³é”®æ–‡ä»¶ä½ç½®

- All Experiments: `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py`
- ç»“æœè¾“å‡ºç›®å½•: `.sage/benchmark/results/`
- å›¾è¡¨è¾“å‡º: `.sage/benchmark/results/figures/`
- è¡¨æ ¼è¾“å‡º: `.sage/benchmark/results/tables/`

## å½“å‰ç”Ÿæˆçš„æ–‡ä»¶

è¿è¡Œ `python run_all_experiments.py --quick` åç”Ÿæˆï¼š
- `figures/fig1_timing_comparison.pdf`
- `figures/fig2_planning_comparison.pdf`
- `figures/fig3_tool_selection_comparison.pdf`
- `figures/fig4_overall_comparison.pdf`
- `tables/table1_projected_performance.tex`
- `tables/table2_observed_benchmark.tex`

## éœ€è¦å®Œæˆçš„å·¥ä½œ

1. **å®Œå–„å›¾è¡¨æ ·å¼**
   - ä½¿ç”¨ ICML 2026 è®ºæ–‡æ ¼å¼
   - å­—ä½“å¤§å°ã€é¢œè‰²æ–¹æ¡ˆç¬¦åˆå­¦æœ¯è§„èŒƒ
   - æ·»åŠ å›¾ä¾‹å’Œè½´æ ‡ç­¾

2. **ç”Ÿæˆ Scaling Analysis å›¾è¡¨**
   - X è½´: å€™é€‰å·¥å…·æ•°é‡ (10, 50, 100, 500, 1000)
   - Y è½´: Top-5 å‡†ç¡®ç‡
   - å¯¹æ¯”: keyword, embedding, hybrid, gorilla, dfsdt

3. **ç”Ÿæˆ Error Analysis å›¾è¡¨**
   - é”™è¯¯ç±»å‹åˆ†å¸ƒ: æ¼é€‰ã€é”™é€‰ã€æ’åºé”™è¯¯
   - æŒ‰éš¾åº¦çº§åˆ«åˆ†æ: easy, medium, hard

4. **ç”Ÿæˆ LaTeX è¡¨æ ¼**
   - ä½¿ç”¨ booktabs æ ·å¼
   - åŒ…å«ç½®ä¿¡åŒºé—´æˆ–æ ‡å‡†å·®
   - æœ€ä½³ç»“æœåŠ ç²—

5. **æ•´åˆæ‰€æœ‰ç»“æœåˆ° JSON**
   - ç»“æ„åŒ–çš„å®éªŒç»“æœæ±‡æ€»
   - ä¾¿äºåç»­å¼•ç”¨å’Œæ›´æ–°

## å›¾è¡¨ä»£ç ä½ç½®

`run_all_experiments.py` ä¸­çš„ `generate_paper_materials()` å‡½æ•°

## è¿è¡Œå‘½ä»¤

cd /home/shuhao/SAGE/packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

# å¿«é€Ÿæµ‹è¯•å›¾è¡¨ç”Ÿæˆ
python run_all_experiments.py --quick --max-samples 50

# å®Œæ•´è¯„æµ‹ + å›¾è¡¨ç”Ÿæˆ (éœ€è¦ Task 1-3 å®Œæˆ)
python run_all_experiments.py --eval-only --max-samples 200

# ä»…ç”Ÿæˆå›¾è¡¨ (ä½¿ç”¨å·²æœ‰ç»“æœ)
python run_all_experiments.py --paper-only --results-dir .sage/benchmark/results

## é¢„æœŸè¾“å‡º

figures/
â”œâ”€â”€ fig1_timing_comparison.pdf
â”œâ”€â”€ fig2_planning_comparison.pdf
â”œâ”€â”€ fig3_tool_selection_comparison.pdf
â”œâ”€â”€ fig4_scaling_analysis.pdf
â”œâ”€â”€ fig5_error_analysis.pdf
â””â”€â”€ fig6_cross_dataset.pdf

tables/
â”œâ”€â”€ table1_main_results.tex
â”œâ”€â”€ table2_cross_dataset.tex
â”œâ”€â”€ table3_ablation.tex
â””â”€â”€ table4_challenge_details.tex

## æˆåŠŸæ ‡å‡†

- æ‰€æœ‰å›¾è¡¨ä½¿ç”¨ç»Ÿä¸€çš„å­¦æœ¯é£æ ¼
- LaTeX è¡¨æ ¼å¯ç›´æ¥å¤åˆ¶åˆ°è®ºæ–‡ä¸­
- å›¾è¡¨æ¸…æ™°ã€å¯è¯»æ€§å¥½
- åŒ…å«æ‰€æœ‰ Paper éœ€è¦çš„æ•°æ®å¯è§†åŒ–
```

---

## ğŸ“‹ ä»»åŠ¡ä¾èµ–å’Œæ‰§è¡Œé¡ºåº

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¯å¹¶è¡Œæ‰§è¡Œçš„ä»»åŠ¡                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Task 1                Task 2               Task 3          â”‚
â”‚  (Gorilla/DFSDT)       (LLM-based)          (è·¨æ•°æ®é›†)       â”‚
â”‚       â”‚                    â”‚                    â”‚           â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚                       Task 4                                â”‚
â”‚                   (è®ºæ–‡å›¾è¡¨ç”Ÿæˆ)                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ‰§è¡Œå»ºè®®**:
- Task 1, 2, 3 å¯ä»¥åˆ†é…ç»™ 3 ä¸ªä¸åŒçš„ Copilot Agent å¹¶è¡Œæ‰§è¡Œ
- Task 4 éœ€è¦ç­‰ Task 1-3 åŸºæœ¬å®Œæˆåæ‰§è¡Œ
- æ¯ä¸ª Task é¢„è®¡ 2-3 å°æ—¶

---

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

```bash
# ç¡®ä¿åœ¨æ­£ç¡®çš„åˆ†æ”¯
cd /home/shuhao/SAGE
git checkout feature/agent_tools_plan

# æ¿€æ´»ç¯å¢ƒ
conda activate sage

# éªŒè¯å®‰è£…
python -c "from sage.benchmark.benchmark_agent.adapter_registry import get_adapter_registry; print('OK')"
```

---

## ğŸ“ æäº¤è§„èŒƒ

å®Œæˆä»»åŠ¡åï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ commit æ ¼å¼ï¼š

```bash
# Task 1
git commit -m "fix(benchmark): resolve Gorilla/DFSDT tool index mismatch issue"

# Task 2
git commit -m "feat(benchmark): complete LLM-based methods testing and validation"

# Task 3
git commit -m "feat(benchmark): enhance cross-dataset validation with ACEBench"

# Task 4
git commit -m "docs(benchmark): generate ICML paper figures and LaTeX tables"
```
