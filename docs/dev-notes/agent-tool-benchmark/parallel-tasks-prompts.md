# Agent Benchmark å¹¶è¡Œä»»åŠ¡æç¤ºè¯

**ç›®æ ‡**: éš¾é¢˜4 - Agentå¹³å°æµ·é‡å·¥å…·ä¸šåŠ¡ä¸‹çš„è§„åˆ’å’Œå·¥å…·è°ƒç”¨å‡†ç¡®ç‡æå‡ (95%+)

**åˆ›å»ºæ—¥æœŸ**: 2025-11-26  
**æ›´æ–°æ—¥æœŸ**: 2025-11-26

---

## ğŸ”´ ç´§æ€¥ Bug ä¿®å¤ä»»åŠ¡ (ä¼˜å…ˆæ‰§è¡Œ)

> è¿™äº›ä»»åŠ¡éœ€è¦åœ¨å…¶ä»–ä»»åŠ¡ä¹‹å‰å®Œæˆï¼Œå› ä¸ºå®ƒä»¬é˜»å¡äº†åŸºç¡€è¯„æµ‹åŠŸèƒ½ã€‚

| ä»»åŠ¡ | é—®é¢˜ | ä¼˜å…ˆçº§ | é¢„ä¼°æ—¶é—´ |
|------|------|--------|---------|
| **Task X1** | Tool Selection è¯„ä¼°è¿”å› 0% å‡†ç¡®ç‡ | ğŸ”´ P0 | 2-3h |
| **Task X2** | Hybrid Timing åœ¨ --skip-llm æ¨¡å¼ä»åŠ è½½ LLM | ğŸŸ¡ P1 | 1h |
| **Task X3** | æ•°æ®æ–‡ä»¶ä½ç½®æ··ä¹±éœ€è¦æ¢³ç† | ğŸŸ¡ P1 | 1-2h |

---

## ä»»åŠ¡ä¾èµ–å›¾

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      ç´§æ€¥ä»»åŠ¡ç»„ X (Bug ä¿®å¤)              â”‚
                    â”‚  X1: Tool Selection Bug                 â”‚
                    â”‚  X2: Hybrid Timing LLM                  â”‚
                    â”‚  X3: æ•°æ®æ–‡ä»¶ä½ç½®                         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         å¹¶è¡Œä»»åŠ¡ç»„ A (SOTA ç­–ç•¥å®ç°)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Task A1    â”‚  Task A2    â”‚  Task A3    â”‚  Task A4    â”‚  Task A5        â”‚
â”‚  ToolLLM    â”‚  ReAct      â”‚  ToT        â”‚  Gorilla    â”‚  API-Bank       â”‚
â”‚  (å·¥å…·é€‰æ‹©)  â”‚  (è§„åˆ’)     â”‚  (è§„åˆ’)     â”‚  (å·¥å…·é€‰æ‹©)  â”‚  (è¯„æµ‹æ•°æ®)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         å¹¶è¡Œä»»åŠ¡ç»„ B (SOTA å¾®è°ƒæ–¹æ³•)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Task B1    â”‚  Task B2    â”‚  Task B3    â”‚  Task B4                        â”‚
â”‚  FireAct    â”‚ AgentTuning â”‚  ToolAlpaca â”‚  DoRA/LoRA+                     â”‚
â”‚  (è½¨è¿¹å¾®è°ƒ)  â”‚ (Agentèƒ½åŠ›) â”‚  (å·¥å…·æ•°æ®)  â”‚  (é«˜æ•ˆå¾®è°ƒ)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         å¹¶è¡Œä»»åŠ¡ç»„ C (åŸºç¡€è®¾æ–½ + ä¼˜åŒ–)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Task C1  â”‚  Task C2  â”‚  Task C3  â”‚  Task C4  â”‚  Task C5                â”‚
â”‚  ç»Ÿä¸€å®éªŒ  â”‚  å•å…ƒæµ‹è¯•  â”‚  æ–‡æ¡£å®Œå–„  â”‚ åŸºå‡†çº¿ä¼˜åŒ– â”‚  LLM ç¼“å­˜              â”‚
â”‚  è„šæœ¬æ•´åˆ  â”‚  è¦†ç›–è¡¥å……  â”‚  APIæ–‡æ¡£  â”‚ (Timing)  â”‚  æœºåˆ¶                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         æœ€ç»ˆä»»åŠ¡ D (ä¾èµ– X+A+B+C)                         â”‚
â”‚                   Task D1: å®Œæ•´å®éªŒè¿è¡Œ + è®ºæ–‡å›¾è¡¨ç”Ÿæˆ                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”´ ç´§æ€¥ä»»åŠ¡ç»„ X: Bug ä¿®å¤ (æœ€é«˜ä¼˜å…ˆçº§ï¼Œå¯å¹¶è¡Œ)

### Task X1: ä¿®å¤ Tool Selection è¯„ä¼°è¿”å› 0% å‡†ç¡®ç‡

**ä¼˜å…ˆçº§**: ğŸ”´ P0 | **é¢„ä¼°æ—¶é—´**: 2-3å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task X1: ä¿®å¤ Tool Selection è¯„ä¼° Bug

## é—®é¢˜æè¿°
`run_all_experiments.py` ä¸­çš„ Tool Selection è¯„ä¼°è¿”å› 0% å‡†ç¡®ç‡ï¼Œæ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯ 0ã€‚

## é—®é¢˜å®šä½

å½“å‰ä»£ç  (`run_all_experiments.py` ~line 408):
```python
result = selector.select(query, candidate_tools, top_k=top_k)
```

## å¯èƒ½åŸå› 
1. `SelectorAdapter.select()` æ–¹æ³•ç­¾åä¸è°ƒç”¨ä¸åŒ¹é…
2. `candidate_tools` æ ¼å¼ä¸æ­£ç¡®ï¼ˆåº”è¯¥æ˜¯ tool å¯¹è±¡åˆ—è¡¨è¿˜æ˜¯ ID åˆ—è¡¨ï¼Ÿï¼‰
3. è¿”å›å€¼è§£æé€»è¾‘æœ‰é—®é¢˜

## ä»»åŠ¡æ¸…å•

### 1. æ£€æŸ¥ SelectorAdapter æ¥å£
æŸ¥çœ‹ `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`:

```python
class SelectorAdapter:
    def predict(self, query: Any, top_k: Optional[int] = None, **kwargs) -> list:
        # æ£€æŸ¥è¿™ä¸ªæ–¹æ³•çš„å®é™…å®ç°
        pass
```

### 2. æ£€æŸ¥ run_all_experiments.py ä¸­çš„è°ƒç”¨
æŸ¥çœ‹ `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py`:
- æ‰¾åˆ° `run_tool_selection_evaluation()` æ–¹æ³•
- æ£€æŸ¥å¦‚ä½•æ„å»º query å¯¹è±¡
- æ£€æŸ¥å¦‚ä½•ä¼ é€’ candidate_tools
- æ£€æŸ¥è¿”å›å€¼å¦‚ä½•è§£æ

### 3. æ·»åŠ è°ƒè¯•æ—¥å¿—
```python
# åœ¨è¯„ä¼°å¾ªç¯ä¸­æ·»åŠ 
print(f"Query: {query}")
print(f"Candidate tools count: {len(candidate_tools)}")
result = selector.select(query, candidate_tools, top_k=top_k)
print(f"Result: {result}")
print(f"Result type: {type(result)}")
```

### 4. ä¿®å¤æ¥å£ä¸åŒ¹é…
æ ¹æ®è°ƒè¯•ç»“æœä¿®å¤:
- å¦‚æœæ˜¯å‚æ•°é¡ºåºé—®é¢˜ï¼Œè°ƒæ•´è°ƒç”¨
- å¦‚æœæ˜¯æ•°æ®æ ¼å¼é—®é¢˜ï¼Œæ·»åŠ è½¬æ¢é€»è¾‘
- å¦‚æœæ˜¯è¿”å›å€¼è§£æé—®é¢˜ï¼Œä¿®å¤è§£æä»£ç 

### 5. éªŒè¯ä¿®å¤
```bash
cd /home/shuhao/SAGE
python packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py \
    --quick --skip-llm
```

ç¡®è®¤ Tool Selection è¾“å‡ºéé›¶ç»“æœã€‚

## éªŒæ”¶æ ‡å‡†
- [ ] Tool Selection è¯„ä¼°è¿”å›éé›¶å‡†ç¡®ç‡
- [ ] keyword/embedding/hybrid ä¸‰ä¸ªç­–ç•¥éƒ½èƒ½è¾“å‡ºç»“æœ
- [ ] ç»“æœä¸ Timing/Planning æ ¼å¼ä¸€è‡´

## å…³é”®æ–‡ä»¶
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py`
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
```

---

### Task X2: ä¿®å¤ Hybrid Timing åœ¨ --skip-llm æ¨¡å¼ä»åŠ è½½ LLM

**ä¼˜å…ˆçº§**: ğŸŸ¡ P1 | **é¢„ä¼°æ—¶é—´**: 1å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task X2: ä¿®å¤ Hybrid Timing LLM åŠ è½½é—®é¢˜

## é—®é¢˜æè¿°
å³ä½¿ä½¿ç”¨ `--skip-llm` å‚æ•°ï¼Œ`timing.hybrid` ç­–ç•¥ä»ç„¶ä¼šå°è¯•åŠ è½½ vLLM æ¨¡å‹ã€‚

## åŸå› åˆ†æ
`timing.hybrid` å†…éƒ¨ä½¿ç”¨äº† LLM ä½œä¸ºåå¤‡ç­–ç•¥ï¼Œåœ¨åˆå§‹åŒ–æ—¶å°±ä¼šåŠ è½½æ¨¡å‹ã€‚

## ä»»åŠ¡æ¸…å•

### 1. æ£€æŸ¥ Hybrid Timing Decider å®ç°
æŸ¥çœ‹ `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`:

```python
def _create_hybrid_timing_decider(self, resources: Optional[Any] = None) -> TimingAdapter:
    # æ£€æŸ¥æ˜¯å¦åœ¨åˆå§‹åŒ–æ—¶å°±åŠ è½½äº† LLM
    pass
```

### 2. æ–¹æ¡ˆ A: å»¶è¿ŸåŠ è½½ LLM
ä¿®æ”¹ Hybrid ç­–ç•¥ï¼Œåªåœ¨å®é™…éœ€è¦æ—¶æ‰åŠ è½½ LLM:

```python
class HybridTimingDecider:
    def __init__(self, ...):
        self._llm_client = None  # å»¶è¿ŸåŠ è½½

    @property
    def llm_client(self):
        if self._llm_client is None:
            self._llm_client = IntelligentLLMClient.create_auto()
        return self._llm_client
```

### 3. æ–¹æ¡ˆ B: åœ¨ --skip-llm æ¨¡å¼ä¸‹å°† Hybrid åŠ å…¥è·³è¿‡åˆ—è¡¨
ä¿®æ”¹ `run_all_experiments.py`:

```python
if self.skip_llm:
    strategies = ["timing.rule_based"]  # åªç”¨ rule-based
    # æˆ–è€…
    strategies = ["timing.rule_based", "timing.embedding"]  # è·³è¿‡ hybrid
```

### 4. éªŒè¯ä¿®å¤
```bash
python run_all_experiments.py --quick --skip-llm
# åº”è¯¥ä¸å†å‡ºç° vLLM åŠ è½½æ—¥å¿—
```

## éªŒæ”¶æ ‡å‡†
- [ ] `--skip-llm` æ¨¡å¼ä¸‹ä¸åŠ è½½ vLLM
- [ ] Hybrid ç­–ç•¥ä»ç„¶å¯ç”¨ï¼ˆåœ¨é skip-llm æ¨¡å¼ï¼‰

## å…³é”®æ–‡ä»¶
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py`
```

---

### Task X3: æ¢³ç†æ•°æ®æ–‡ä»¶ä½ç½®

**ä¼˜å…ˆçº§**: ğŸŸ¡ P1 | **é¢„ä¼°æ—¶é—´**: 1-2å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task X3: æ¢³ç†æ•°æ®æ–‡ä»¶ä½ç½®

## é—®é¢˜æè¿°
æ•°æ®æ–‡ä»¶åˆ†å¸ƒåœ¨å¤šä¸ªä½ç½®ï¼Œå®¹æ˜“æ··æ·†:

**å½“å‰çŠ¶æ€**:
- **sageData submodule** (`packages/sage-benchmark/src/sage/data/sources/`):
  - `agent_benchmark/splits/` - å·²æœ‰åŸºç¡€æ•°æ®ï¼ˆ600æ¡ tool_selection, 300æ¡ timing/planningï¼‰
  - `agent_tools/data/tool_catalog.jsonl` - 1200 ä¸ªå·¥å…·
  - `agent_sft/data/` - SFT è®­ç»ƒæ•°æ®

- **è¿è¡Œæ—¶ç”Ÿæˆ** (`.sage/benchmark/data/`):
  - `timing_judgment/` - è¿è¡Œæ—¶ç”Ÿæˆçš„å¢å¼ºæ•°æ®
  - `task_planning/` - è¿è¡Œæ—¶ç”Ÿæˆçš„å¢å¼ºæ•°æ®
  - `tool_selection/` - è¿è¡Œæ—¶ç”Ÿæˆçš„å¢å¼ºæ•°æ®

## ä»»åŠ¡æ¸…å•

### 1. æ˜ç¡®æ•°æ®åˆ†ç±»

| ç±»å‹ | ä½ç½® | è¯´æ˜ |
|------|------|------|
| é™æ€åŸºå‡†æ•°æ® | `sage-data` submodule | ç‰ˆæœ¬æ§åˆ¶ï¼Œä¸åº”ä¿®æ”¹ |
| è¿è¡Œæ—¶ç”Ÿæˆæ•°æ® | `.sage/benchmark/data/` | æ¯æ¬¡è¿è¡Œå¯é‡æ–°ç”Ÿæˆ |
| æ¨¡å‹æƒé‡/ç¼“å­˜ | `.sage/models/` | ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶ |

### 2. æ›´æ–° run_all_experiments.py æ•°æ®åŠ è½½é€»è¾‘

```python
def _get_data_path(self, dataset: str) -> Path:
    """è·å–æ•°æ®è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨ submodule ä¸­çš„é™æ€æ•°æ®"""
    # 1. å…ˆæ£€æŸ¥ submodule ä¸­çš„é™æ€æ•°æ®
    static_path = SAGE_DATA_ROOT / "sources" / dataset / "splits"
    if static_path.exists():
        return static_path

    # 2. å›é€€åˆ°è¿è¡Œæ—¶ç”Ÿæˆçš„æ•°æ®
    runtime_path = DEFAULT_DATA_DIR / dataset
    if runtime_path.exists():
        return runtime_path

    # 3. éƒ½æ²¡æœ‰åˆ™ç”Ÿæˆ
    self._prepare_data(dataset)
    return runtime_path
```

### 3. æ·»åŠ æ•°æ®æ¥æºæ ‡è®°

åœ¨ç»“æœä¸­æ ‡è®°æ•°æ®æ¥æº:
```python
result.metadata["data_source"] = "static" or "generated"
result.metadata["data_path"] = str(data_path)
```

### 4. æ›´æ–°æ–‡æ¡£

åœ¨ README ä¸­è¯´æ˜æ•°æ®æ–‡ä»¶ä½ç½®å’Œç”¨é€”ã€‚

## éªŒæ”¶æ ‡å‡†
- [ ] æ•°æ®åŠ è½½é€»è¾‘ä¼˜å…ˆä½¿ç”¨ submodule é™æ€æ•°æ®
- [ ] ç»“æœä¸­åŒ…å«æ•°æ®æ¥æºå…ƒä¿¡æ¯
- [ ] æ–‡æ¡£æ¸…æ™°è¯´æ˜æ•°æ®ä½ç½®

## å…³é”®æ–‡ä»¶
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py`
- `packages/sage-benchmark/src/sage/data/sources/` (submodule)
```

---

## ä»»åŠ¡ç»„ A: SOTA ç­–ç•¥å®ç° (å¯å®Œå…¨å¹¶è¡Œ)

### Task A1: å®ç° ToolLLM (DFSDT) å·¥å…·é€‰æ‹©ç­–ç•¥

**ä¼˜å…ˆçº§**: P0 | **é¢„ä¼°æ—¶é—´**: 4-6å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task A1: å®ç° ToolLLM (DFSDT) å·¥å…·é€‰æ‹©ç­–ç•¥

## èƒŒæ™¯
ToolLLM (Qin et al., 2023) æ˜¯å·¥å…·é€‰æ‹©é¢†åŸŸçš„ SOTA æ–¹æ³•ï¼Œä½¿ç”¨ DFSDT (Depth-First Search-based Decision Tree) ç®—æ³•è¿›è¡Œå·¥å…·é€‰æ‹©ã€‚æˆ‘ä»¬éœ€è¦å°†å…¶é›†æˆåˆ° SAGE benchmark æ¡†æ¶ä¸­ä½œä¸ºå¯¹æ¯” baselineã€‚

## å‚è€ƒèµ„æ–™
- è®ºæ–‡: "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"
- GitHub: https://github.com/OpenBMB/ToolBench

## ä»»åŠ¡æ¸…å•

### 1. ç®—æ³•å®ç°
åœ¨ `packages/sage-libs/src/sage/libs/agentic/agents/action/tool_selection/` ä¸‹åˆ›å»º:

```python
# toolllm_selector.py
class DFSDTSelector(BaseSelector):
    """
    ToolLLM çš„ DFSDT ç®—æ³•å®ç°

    æ ¸å¿ƒæ€æƒ³:
    1. å°†å·¥å…·é€‰æ‹©å»ºæ¨¡ä¸ºå†³ç­–æ ‘æœç´¢
    2. ä½¿ç”¨ LLM åœ¨æ¯ä¸ªèŠ‚ç‚¹è¯„ä¼°å·¥å…·ç›¸å…³æ€§
    3. æ·±åº¦ä¼˜å…ˆæœç´¢æ‰¾åˆ°æœ€ä½³å·¥å…·ç»„åˆ
    """

    def __init__(self, config: DFSDTConfig, llm_client: Any):
        self.max_depth = config.max_depth  # æœç´¢æ·±åº¦
        self.beam_width = config.beam_width  # æ¯å±‚ä¿ç•™çš„å€™é€‰æ•°
        self.llm_client = llm_client

    def select(self, query: ToolSelectionQuery, top_k: int = 5) -> list[ToolPrediction]:
        """DFSDT æœç´¢é€‰æ‹©å·¥å…·"""
        # 1. åˆå§‹åŒ–æœç´¢æ ‘
        # 2. DFS éå†ï¼Œç”¨ LLM è¯„åˆ†
        # 3. è¿”å› top_k å·¥å…·
        pass

    def _expand_node(self, node: SearchNode, candidates: list) -> list[SearchNode]:
        """æ‰©å±•æœç´¢èŠ‚ç‚¹"""
        pass

    def _score_with_llm(self, query: str, tool: ToolDefinition) -> float:
        """ä½¿ç”¨ LLM è¯„ä¼°å·¥å…·ç›¸å…³æ€§"""
        pass
```

### 2. é…ç½®ç±»
```python
# config.py ä¸­æ·»åŠ 
@dataclass
class DFSDTConfig(BaseSelectorConfig):
    name: str = "dfsdt"
    max_depth: int = 3
    beam_width: int = 5
    llm_model: str = "qwen2.5-7b-instruct"
    temperature: float = 0.1
```

### 3. æ³¨å†Œåˆ° AdapterRegistry
ä¿®æ”¹ `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`:

```python
# åœ¨ _register_builtins() ä¸­æ·»åŠ 
self._factories["selector.dfsdt"] = self._create_dfsdt_selector
self._factories["selector.toolllm"] = self._create_dfsdt_selector  # åˆ«å

def _create_dfsdt_selector(self, resources: Optional[Any] = None) -> SelectorAdapter:
    """Create ToolLLM DFSDT selector."""
    from sage.libs.agentic.agents.action.tool_selection import (
        DFSDTSelector, DFSDTConfig
    )
    from sage.common.components.sage_llm.client import IntelligentLLMClient

    config = DFSDTConfig(max_depth=3, beam_width=5)
    llm_client = IntelligentLLMClient.create_auto()
    selector = DFSDTSelector(config, llm_client)
    return SelectorAdapter(selector)
```

### 4. å•å…ƒæµ‹è¯•
åˆ›å»º `packages/sage-libs/tests/unit/agentic/tool_selection/test_dfsdt_selector.py`

### 5. éªŒè¯
è¿è¡Œå·¥å…·é€‰æ‹© benchmarkï¼Œå¯¹æ¯” DFSDT vs keyword vs embedding vs hybrid

## éªŒæ”¶æ ‡å‡†
- [ ] DFSDTSelector ç±»å®ç°å®Œæ•´
- [ ] æ³¨å†Œåˆ° AdapterRegistryï¼Œå¯é€šè¿‡ "selector.toolllm" è®¿é—®
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] åœ¨ benchmark ä¸­å¯è¿è¡Œå¹¶è¾“å‡ºæœ‰æ•ˆç»“æœ

## å…³é”®æ–‡ä»¶
- `packages/sage-libs/src/sage/libs/agentic/agents/action/tool_selection/`
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
- `packages/sage-libs/tests/unit/agentic/tool_selection/`
```

---

### Task A2: å®Œå–„ ReAct è§„åˆ’ç­–ç•¥

**ä¼˜å…ˆçº§**: P0 | **é¢„ä¼°æ—¶é—´**: 3-4å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task A2: å®Œå–„ ReAct è§„åˆ’ç­–ç•¥

## èƒŒæ™¯
ReAct (Yao et al., 2023) æ˜¯è§„åˆ’é¢†åŸŸçš„ç»å…¸æ–¹æ³•ï¼Œé€šè¿‡äº¤æ›¿è¿›è¡Œ Reasoning å’Œ Acting æ¥å®Œæˆå¤æ‚ä»»åŠ¡ã€‚å½“å‰ SAGE ä¸­æœ‰éƒ¨åˆ†å®ç°ï¼Œéœ€è¦å®Œå–„ reasoning trace åŠŸèƒ½ã€‚

## å‚è€ƒèµ„æ–™
- è®ºæ–‡: "ReAct: Synergizing Reasoning and Acting in Language Models"
- æ¦‚å¿µ: Thought â†’ Action â†’ Observation å¾ªç¯

## ä»»åŠ¡æ¸…å•

### 1. æ£€æŸ¥ç°æœ‰å®ç°
æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶:
- `packages/sage-libs/src/sage/libs/agentic/agents/action/planning/`
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`

### 2. å®ç°å®Œæ•´ ReAct Planner

```python
# packages/sage-libs/src/sage/libs/agentic/agents/action/planning/react_planner.py

@dataclass
class ReActStep:
    """ReAct å•æ­¥"""
    thought: str      # æ¨ç†è¿‡ç¨‹
    action: str       # é€‰æ‹©çš„åŠ¨ä½œ/å·¥å…·
    action_input: dict  # åŠ¨ä½œè¾“å…¥
    observation: str  # æ‰§è¡Œç»“æœ (å¯é€‰ï¼Œè§„åˆ’é˜¶æ®µä¸ºç©º)

@dataclass  
class ReActPlan:
    """ReAct è§„åˆ’ç»“æœ"""
    steps: list[ReActStep]
    final_answer: str
    reasoning_trace: str  # å®Œæ•´æ¨ç†é“¾

class ReActPlanner(BasePlanner):
    """
    ReAct è§„åˆ’å™¨

    å®ç° Thought-Action-Observation å¾ªç¯:
    1. Thought: åˆ†æå½“å‰çŠ¶æ€ï¼Œå†³å®šä¸‹ä¸€æ­¥
    2. Action: é€‰æ‹©å·¥å…·å¹¶å‡†å¤‡è¾“å…¥
    3. Observation: (æ‰§è¡Œæ—¶è·å–) å·¥å…·è¿”å›ç»“æœ
    """

    def __init__(self, config: ReActConfig, llm_client: Any):
        self.max_steps = config.max_steps
        self.llm_client = llm_client
        self.prompt_template = self._load_prompt_template()

    def plan(self, task: PlanningTask) -> ReActPlan:
        """ç”Ÿæˆ ReAct é£æ ¼çš„è§„åˆ’"""
        steps = []
        reasoning_trace = []

        for i in range(self.max_steps):
            # Generate thought
            thought = self._generate_thought(task, steps)
            reasoning_trace.append(f"Thought {i+1}: {thought}")

            # Decide action
            action, action_input = self._decide_action(task, thought, steps)

            if action == "finish":
                break

            step = ReActStep(
                thought=thought,
                action=action,
                action_input=action_input,
                observation=""  # è§„åˆ’é˜¶æ®µä¸æ‰§è¡Œ
            )
            steps.append(step)
            reasoning_trace.append(f"Action {i+1}: {action}({action_input})")

        return ReActPlan(
            steps=steps,
            final_answer=self._generate_final_answer(task, steps),
            reasoning_trace="\n".join(reasoning_trace)
        )

    def _generate_thought(self, task: PlanningTask, history: list) -> str:
        """ä½¿ç”¨ LLM ç”Ÿæˆæ¨ç†"""
        prompt = self.prompt_template.format(
            task=task.instruction,
            history=self._format_history(history)
        )
        return self.llm_client.chat([{"role": "user", "content": prompt}])
```

### 3. æ³¨å†Œåˆ° AdapterRegistry

```python
# adapter_registry.py
self._factories["planner.react"] = self._create_react_planner

def _create_react_planner(self, resources: Optional[Any] = None) -> PlannerAdapter:
    from sage.libs.agentic.agents.action.planning import ReActPlanner, ReActConfig
    from sage.common.components.sage_llm.client import IntelligentLLMClient

    config = ReActConfig(max_steps=10)
    llm_client = IntelligentLLMClient.create_auto()
    planner = ReActPlanner(config, llm_client)
    return PlannerAdapter(planner)
```

### 4. å•å…ƒæµ‹è¯•
åˆ›å»º `packages/sage-libs/tests/unit/agentic/planning/test_react_planner.py`

## éªŒæ”¶æ ‡å‡†
- [ ] ReActPlanner å®Œæ•´å®ç° Thought-Action å¾ªç¯
- [ ] reasoning_trace è®°å½•å®Œæ•´æ¨ç†é“¾
- [ ] æ³¨å†Œåˆ° AdapterRegistry
- [ ] åœ¨ planning benchmark ä¸­å¯è¿è¡Œ

## å…³é”®æ–‡ä»¶
- `packages/sage-libs/src/sage/libs/agentic/agents/action/planning/`
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
```

---

### Task A3: å®ç° Tree-of-Thoughts (ToT) è§„åˆ’ç­–ç•¥

**ä¼˜å…ˆçº§**: P1 | **é¢„ä¼°æ—¶é—´**: 5-6å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task A3: å®ç° Tree-of-Thoughts (ToT) è§„åˆ’ç­–ç•¥

## èƒŒæ™¯
Tree-of-Thoughts (Yao et al., 2023) é€šè¿‡æ ‘æœç´¢æ¢ç´¢å¤šä¸ªæ¨ç†è·¯å¾„ï¼Œæ¯”çº¿æ€§çš„ CoT æ›´å¼ºå¤§ã€‚

## å‚è€ƒèµ„æ–™
- è®ºæ–‡: "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
- æ ¸å¿ƒ: BFS/DFS æœç´¢ + LLM è¯„ä¼°èŠ‚ç‚¹

## ä»»åŠ¡æ¸…å•

### 1. å®ç° ToT Planner

```python
# packages/sage-libs/src/sage/libs/agentic/agents/action/planning/tot_planner.py

@dataclass
class ThoughtNode:
    """æ€ç»´æ ‘èŠ‚ç‚¹"""
    thought: str
    score: float
    children: list["ThoughtNode"]
    parent: Optional["ThoughtNode"]
    depth: int

class TreeOfThoughtsPlanner(BasePlanner):
    """
    Tree-of-Thoughts è§„åˆ’å™¨

    ç®—æ³•:
    1. ç”Ÿæˆå¤šä¸ªå€™é€‰ thought
    2. ç”¨ LLM è¯„ä¼°æ¯ä¸ª thought çš„ä»·å€¼
    3. BFS/DFS æœç´¢æœ€ä¼˜è·¯å¾„
    4. å›æº¯å¾—åˆ°æœ€ç»ˆè§„åˆ’
    """

    def __init__(self, config: ToTConfig, llm_client: Any):
        self.max_depth = config.max_depth
        self.branch_factor = config.branch_factor  # æ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆçš„å€™é€‰æ•°
        self.search_method = config.search_method  # "bfs" or "dfs"
        self.llm_client = llm_client

    def plan(self, task: PlanningTask) -> PlanningPrediction:
        """ç”Ÿæˆ ToT è§„åˆ’"""
        root = ThoughtNode(thought="", score=0.0, children=[], parent=None, depth=0)

        if self.search_method == "bfs":
            best_path = self._bfs_search(root, task)
        else:
            best_path = self._dfs_search(root, task)

        return self._path_to_plan(best_path, task)

    def _generate_thoughts(self, node: ThoughtNode, task: PlanningTask) -> list[str]:
        """ç”Ÿæˆå¤šä¸ªå€™é€‰ thought"""
        prompt = f"""Given task: {task.instruction}
Current path: {self._format_path(node)}

Generate {self.branch_factor} different next steps. Output as JSON list."""

        response = self.llm_client.chat([{"role": "user", "content": prompt}])
        return self._parse_thoughts(response)

    def _evaluate_thought(self, thought: str, task: PlanningTask) -> float:
        """è¯„ä¼° thought çš„ä»·å€¼ (0-1)"""
        prompt = f"""Task: {task.instruction}
Proposed step: {thought}

Rate how good this step is (0-10):"""

        response = self.llm_client.chat([{"role": "user", "content": prompt}])
        return self._parse_score(response) / 10.0

    def _bfs_search(self, root: ThoughtNode, task: PlanningTask) -> list[ThoughtNode]:
        """BFS æœç´¢"""
        queue = [root]
        best_path = []

        while queue and queue[0].depth < self.max_depth:
            node = queue.pop(0)
            thoughts = self._generate_thoughts(node, task)

            for thought in thoughts:
                score = self._evaluate_thought(thought, task)
                child = ThoughtNode(
                    thought=thought, score=score,
                    children=[], parent=node, depth=node.depth + 1
                )
                node.children.append(child)
                queue.append(child)

            # ä¿ç•™ top-k èŠ‚ç‚¹
            queue.sort(key=lambda n: n.score, reverse=True)
            queue = queue[:self.branch_factor * 2]

        # è¿”å›æœ€é«˜åˆ†è·¯å¾„
        return self._get_best_path(root)
```

### 2. é…ç½®å’Œæ³¨å†Œ
åŒ A1/A2 æ¨¡å¼

### 3. å•å…ƒæµ‹è¯•

## éªŒæ”¶æ ‡å‡†
- [ ] ToT æ ‘æœç´¢ç®—æ³•å®ç°
- [ ] æ”¯æŒ BFS å’Œ DFS ä¸¤ç§æœç´¢
- [ ] æ³¨å†Œåˆ° AdapterRegistry
- [ ] benchmark å¯è¿è¡Œ
```

---

### Task A4: å®ç° Gorilla å·¥å…·æ£€ç´¢ç­–ç•¥

**ä¼˜å…ˆçº§**: P2 | **é¢„ä¼°æ—¶é—´**: 3-4å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task A4: å®ç° Gorilla å·¥å…·æ£€ç´¢ç­–ç•¥

## èƒŒæ™¯
Gorilla (Patil et al., 2023) ä½¿ç”¨ API æ–‡æ¡£æ£€ç´¢å¢å¼ºæ¥é€‰æ‹©å·¥å…·ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†å¤§è§„æ¨¡ API åº“ã€‚

## å‚è€ƒèµ„æ–™
- è®ºæ–‡: "Gorilla: Large Language Model Connected with Massive APIs"
- æ ¸å¿ƒ: æ£€ç´¢å¢å¼º + API æ–‡æ¡£ç†è§£

## ä»»åŠ¡æ¸…å•

### 1. å®ç° Gorilla Selector

```python
# packages/sage-libs/src/sage/libs/agentic/agents/action/tool_selection/gorilla_selector.py

class GorillaSelector(BaseSelector):
    """
    Gorilla é£æ ¼çš„æ£€ç´¢å¢å¼ºå·¥å…·é€‰æ‹©

    æµç¨‹:
    1. ç”¨ embedding æ£€ç´¢ç›¸å…³ API æ–‡æ¡£
    2. å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ä½œä¸º context
    3. è®© LLM åŸºäºæ–‡æ¡£é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·
    """

    def __init__(self, config: GorillaConfig, embedding_client: Any, llm_client: Any):
        self.retriever = DocumentRetriever(embedding_client)
        self.llm_client = llm_client
        self.top_k_retrieve = config.top_k_retrieve

    def select(self, query: ToolSelectionQuery, top_k: int = 5) -> list[ToolPrediction]:
        # 1. æ£€ç´¢ç›¸å…³ API æ–‡æ¡£
        docs = self.retriever.retrieve(query.instruction, k=self.top_k_retrieve)

        # 2. æ„å»º prompt
        prompt = self._build_prompt(query, docs)

        # 3. LLM é€‰æ‹©
        response = self.llm_client.chat([{"role": "user", "content": prompt}])

        # 4. è§£æè¿”å›
        return self._parse_selection(response, query.candidate_tools)
```

## éªŒæ”¶æ ‡å‡†
- [ ] GorillaSelector å®ç°æ£€ç´¢å¢å¼º
- [ ] æ³¨å†Œåˆ° AdapterRegistry
- [ ] benchmark å¯è¿è¡Œ
```

---

### Task A5: é›†æˆ API-Bank è¯„æµ‹æ•°æ®

**ä¼˜å…ˆçº§**: P2 | **é¢„ä¼°æ—¶é—´**: 2-3å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task A5: é›†æˆ API-Bank è¯„æµ‹æ•°æ®

## èƒŒæ™¯
API-Bank (Li et al., 2023) æ˜¯ä¸€ä¸ª API è°ƒç”¨è¯„æµ‹åŸºå‡†ï¼ŒåŒ…å«å¤šç§çœŸå®åœºæ™¯çš„æµ‹è¯•ç”¨ä¾‹ã€‚

## å‚è€ƒèµ„æ–™
- è®ºæ–‡: "API-Bank: A Benchmark for Tool-Augmented LLMs"
- GitHub: https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank

## ä»»åŠ¡æ¸…å•

### 1. ä¸‹è½½å¹¶è½¬æ¢æ•°æ®
```python
# packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/evaluations/prepare_apibank_data.py

def download_apibank():
    """ä¸‹è½½ API-Bank æ•°æ®é›†"""
    # ä» GitHub ä¸‹è½½
    pass

def convert_to_sage_format(apibank_data: dict) -> list[dict]:
    """è½¬æ¢ä¸º SAGE benchmark æ ¼å¼"""
    samples = []
    for item in apibank_data:
        sample = {
            "sample_id": item["id"],
            "instruction": item["query"],
            "expected_tools": item["api_calls"],
            "context": item.get("context", {}),
            "source": "api-bank"
        }
        samples.append(sample)
    return samples
```

### 2. é›†æˆåˆ° DataManager

### 3. éªŒè¯æ•°æ®åŠ è½½

## éªŒæ”¶æ ‡å‡†
- [ ] API-Bank æ•°æ®å¯é€šè¿‡ DataManager åŠ è½½
- [ ] è½¬æ¢åæ ¼å¼ä¸ç°æœ‰ benchmark å…¼å®¹
- [ ] è‡³å°‘ 500 æ¡æµ‹è¯•æ ·æœ¬å¯ç”¨
```

---

## ä»»åŠ¡ç»„ B: SOTA å¾®è°ƒæ–¹æ³• (å¯å®Œå…¨å¹¶è¡Œ)

### Task B1: å®ç° FireAct è½¨è¿¹å¾®è°ƒ

**ä¼˜å…ˆçº§**: P1 | **é¢„ä¼°æ—¶é—´**: 4-5å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task B1: å®ç° FireAct è½¨è¿¹å¾®è°ƒ

## èƒŒæ™¯
FireAct (Chen et al., 2023) é€šè¿‡æ”¶é›† Agent æ‰§è¡Œè½¨è¿¹å¹¶è¿›è¡Œå¾®è°ƒï¼Œæå‡ Agent çš„ä»»åŠ¡å®Œæˆèƒ½åŠ›ã€‚

## å‚è€ƒèµ„æ–™
- è®ºæ–‡: "FireAct: Toward Language Agent Fine-tuning"
- æ ¸å¿ƒ: è½¨è¿¹æ”¶é›† â†’ è´¨é‡ç­›é€‰ â†’ SFT å¾®è°ƒ

## ä»»åŠ¡æ¸…å•

### 1. è½¨è¿¹æ”¶é›†å™¨

```python
# packages/sage-libs/src/sage/libs/finetune/agent/trajectory.py

@dataclass
class AgentTrajectory:
    """Agent æ‰§è¡Œè½¨è¿¹"""
    task_id: str
    instruction: str
    steps: list[TrajectoryStep]
    success: bool
    reward: float

@dataclass
class TrajectoryStep:
    """è½¨è¿¹å•æ­¥"""
    thought: str
    action: str
    action_input: dict
    observation: str
    reward: float

class TrajectoryCollector:
    """æ”¶é›† Agent æ‰§è¡Œè½¨è¿¹"""

    def __init__(self, agent: Any, environment: Any):
        self.agent = agent
        self.environment = environment

    def collect(self, tasks: list[str], max_steps: int = 10) -> list[AgentTrajectory]:
        """æ”¶é›†å¤šä¸ªä»»åŠ¡çš„è½¨è¿¹"""
        trajectories = []
        for task in tasks:
            traj = self._run_episode(task, max_steps)
            trajectories.append(traj)
        return trajectories

    def _run_episode(self, task: str, max_steps: int) -> AgentTrajectory:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼Œè®°å½•è½¨è¿¹"""
        pass
```

### 2. è½¨è¿¹è´¨é‡ç­›é€‰

```python
class TrajectoryFilter:
    """ç­›é€‰é«˜è´¨é‡è½¨è¿¹"""

    def filter(self, trajectories: list[AgentTrajectory],
               min_reward: float = 0.5) -> list[AgentTrajectory]:
        """ç­›é€‰æˆåŠŸä¸”é«˜å¥–åŠ±çš„è½¨è¿¹"""
        return [t for t in trajectories if t.success and t.reward >= min_reward]
```

### 3. è½¨è¿¹è½¬ SFT æ•°æ®

```python
class TrajectoryToSFTConverter:
    """å°†è½¨è¿¹è½¬æ¢ä¸º SFT è®­ç»ƒæ•°æ®"""

    def convert(self, trajectories: list[AgentTrajectory]) -> list[dict]:
        """è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼"""
        sft_data = []
        for traj in trajectories:
            dialog = self._trajectory_to_dialog(traj)
            sft_data.append(dialog)
        return sft_data
```

### 4. é›†æˆåˆ° MethodRegistry

```python
# method_comparison.py
"E_fireact": MethodConfig(
    name="E: FireAct",
    description="Agent trajectory fine-tuning",
    use_trajectory_collection=True,
    trajectory_min_reward=0.5,
    num_epochs=2,
)
```

## éªŒæ”¶æ ‡å‡†
- [ ] TrajectoryCollector å¯æ”¶é›†è½¨è¿¹
- [ ] TrajectoryFilter å¯ç­›é€‰é«˜è´¨é‡è½¨è¿¹
- [ ] è½¬æ¢åæ•°æ®å¯ç”¨äº AgentSFTTrainer
- [ ] åœ¨ MethodRegistry ä¸­æ³¨å†Œä¸º Method E
```

---

### Task B2: å®ç° AgentTuning é€šç”¨ Agent èƒ½åŠ›å¾®è°ƒ

**ä¼˜å…ˆçº§**: P1 | **é¢„ä¼°æ—¶é—´**: 3-4å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task B2: å®ç° AgentTuning é€šç”¨ Agent èƒ½åŠ›å¾®è°ƒ

## èƒŒæ™¯
AgentTuning (Zeng et al., 2023) é€šè¿‡å¤šä»»åŠ¡æ··åˆè®­ç»ƒï¼Œæå‡æ¨¡å‹çš„é€šç”¨ Agent èƒ½åŠ›ã€‚

## å‚è€ƒèµ„æ–™
- è®ºæ–‡: "AgentTuning: Enabling Generalized Agent Abilities for LLMs"
- æ ¸å¿ƒ: å¤šä»»åŠ¡æ··åˆ + èƒ½åŠ›æ³›åŒ–

## ä»»åŠ¡æ¸…å•

### 1. å¤šä»»åŠ¡æ•°æ®æ··åˆå™¨

```python
# packages/sage-libs/src/sage/libs/finetune/agent/multi_task.py

class MultiTaskMixer:
    """å¤šä»»åŠ¡æ•°æ®æ··åˆ"""

    def __init__(self, task_weights: dict[str, float]):
        """
        task_weights: {
            "tool_selection": 0.35,
            "planning": 0.30,
            "timing": 0.20,
            "general": 0.15
        }
        """
        self.task_weights = task_weights

    def mix(self, task_datasets: dict[str, list]) -> list:
        """æŒ‰æƒé‡æ··åˆå¤šä¸ªä»»åŠ¡çš„æ•°æ®"""
        mixed = []
        total = sum(len(d) for d in task_datasets.values())

        for task, dataset in task_datasets.items():
            weight = self.task_weights.get(task, 0.1)
            sample_size = int(total * weight)
            sampled = random.sample(dataset, min(sample_size, len(dataset)))
            mixed.extend(sampled)

        random.shuffle(mixed)
        return mixed
```

### 2. èƒ½åŠ›è¯„ä¼°å™¨

```python
class AgentCapabilityEvaluator:
    """è¯„ä¼° Agent å¤šç»´èƒ½åŠ›"""

    CAPABILITIES = ["tool_use", "planning", "reasoning", "instruction_following"]

    def evaluate(self, model, test_sets: dict) -> dict[str, float]:
        """è¯„ä¼°å„é¡¹èƒ½åŠ›"""
        scores = {}
        for cap in self.CAPABILITIES:
            if cap in test_sets:
                scores[cap] = self._eval_capability(model, test_sets[cap])
        return scores
```

### 3. é›†æˆåˆ° MethodRegistry

```python
"F_agenttuning": MethodConfig(
    name="F: AgentTuning",
    description="Multi-task agent capability tuning",
    use_multi_task=True,
    task_weights={
        "tool_selection": 0.35,
        "planning": 0.30,
        "timing": 0.20,
        "general": 0.15
    },
)
```

## éªŒæ”¶æ ‡å‡†
- [ ] MultiTaskMixer å®ç°å¤šä»»åŠ¡æ··åˆ
- [ ] æ”¯æŒè‡ªå®šä¹‰ä»»åŠ¡æƒé‡
- [ ] åœ¨ MethodRegistry ä¸­æ³¨å†Œä¸º Method F
```

---

### Task B3: é›†æˆ ToolAlpaca å·¥å…·ä½¿ç”¨æ•°æ®

**ä¼˜å…ˆçº§**: P2 | **é¢„ä¼°æ—¶é—´**: 2-3å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task B3: é›†æˆ ToolAlpaca å·¥å…·ä½¿ç”¨æ•°æ®

## èƒŒæ™¯
ToolAlpaca (Tang et al., 2023) æä¾›äº†å¤§é‡å·¥å…·ä½¿ç”¨çš„è®­ç»ƒæ•°æ®ã€‚

## å‚è€ƒèµ„æ–™
- è®ºæ–‡: "ToolAlpaca: Generalized Tool Learning for Language Models"
- æ•°æ®: åŒ…å« 3000+ å·¥å…·ä½¿ç”¨ç¤ºä¾‹

## ä»»åŠ¡æ¸…å•

### 1. ä¸‹è½½æ•°æ®
```python
# packages/sage-data/src/sage/data/sources/tool_alpaca.py

class ToolAlpacaDataLoader(BaseDataLoader):
    """ToolAlpaca æ•°æ®åŠ è½½å™¨"""

    SOURCE_URL = "https://github.com/tangqiaoyu/ToolAlpaca"

    def load(self) -> list[dict]:
        """åŠ è½½å¹¶è½¬æ¢æ•°æ®"""
        pass
```

### 2. è½¬æ¢ä¸º SAGE æ ¼å¼

### 3. æ³¨å†Œåˆ° DataManager

## éªŒæ”¶æ ‡å‡†
- [ ] ToolAlpaca æ•°æ®å¯åŠ è½½
- [ ] æ ¼å¼ä¸ agent_sft å…¼å®¹
- [ ] å¯ç”¨äº AgentSFTTrainer
```

---

### Task B4: å®ç° DoRA/LoRA+ é«˜æ•ˆå¾®è°ƒæ–¹æ³•

**ä¼˜å…ˆçº§**: P2 | **é¢„ä¼°æ—¶é—´**: 3-4å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task B4: å®ç° DoRA/LoRA+ é«˜æ•ˆå¾®è°ƒæ–¹æ³•

## èƒŒæ™¯
DoRA å’Œ LoRA+ æ˜¯ 2024 å¹´æå‡ºçš„æ”¹è¿›ç‰ˆ LoRAï¼Œè®­ç»ƒæ•ˆæœæ›´å¥½ã€‚

## å‚è€ƒèµ„æ–™
- DoRA: "DoRA: Weight-Decomposed Low-Rank Adaptation"
- LoRA+: "LoRA+: Efficient Low Rank Adaptation of Large Models"

## ä»»åŠ¡æ¸…å•

### 1. é›†æˆ DoRA (é€šè¿‡ PEFT)

```python
# packages/sage-libs/src/sage/libs/finetune/agent/config.py

@dataclass
class AgentSFTConfig:
    # ç°æœ‰å­—æ®µ...

    # æ–°å¢ DoRA æ”¯æŒ
    use_dora: bool = False

    def get_peft_config(self):
        if self.use_dora:
            return LoraConfig(
                use_dora=True,  # PEFT >= 0.9.0 æ”¯æŒ
                r=self.lora_r,
                lora_alpha=self.lora_alpha,
                # ...
            )
```

### 2. å®ç° LoRA+ å­¦ä¹ ç‡è°ƒåº¦

```python
class LoRAPlusScheduler:
    """LoRA+ çš„ä¸åŒå­¦ä¹ ç‡ç­–ç•¥"""

    def __init__(self, base_lr: float, lora_lr_ratio: float = 16.0):
        """
        LoRA+ å»ºè®® A/B çŸ©é˜µä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
        A çŸ©é˜µ: base_lr
        B çŸ©é˜µ: base_lr * lora_lr_ratio
        """
        self.base_lr = base_lr
        self.lora_lr_ratio = lora_lr_ratio

    def get_param_groups(self, model) -> list[dict]:
        """è¿”å›å‚æ•°ç»„é…ç½®"""
        pass
```

### 3. æ³¨å†Œåˆ° MethodRegistry

```python
"G_dora": MethodConfig(
    name="G: DoRA",
    description="Weight-decomposed LoRA",
    use_dora=True,
),
"H_lora_plus": MethodConfig(
    name="H: LoRA+",
    description="LoRA with differentiated learning rates",
    use_lora_plus=True,
    lora_lr_ratio=16.0,
)
```

## éªŒæ”¶æ ‡å‡†
- [ ] DoRA é€šè¿‡ PEFT é…ç½®å¯ç”¨
- [ ] LoRA+ å­¦ä¹ ç‡è°ƒåº¦å®ç°
- [ ] åœ¨ MethodRegistry ä¸­æ³¨å†Œ
```

---

## ä»»åŠ¡ç»„ C: åŸºç¡€è®¾æ–½ (å¯å¹¶è¡Œ)

### Task C1: ç»Ÿä¸€å®éªŒè„šæœ¬æ•´åˆ

**ä¼˜å…ˆçº§**: P0 | **é¢„ä¼°æ—¶é—´**: 2-3å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task C1: ç»Ÿä¸€å®éªŒè„šæœ¬æ•´åˆ

## èƒŒæ™¯
å½“å‰ `run_all_experiments.py` åªæ”¯æŒè¯„æµ‹ï¼Œéœ€è¦æ•´åˆ SFT è®­ç»ƒå¯¹æ¯”åŠŸèƒ½ã€‚

## ä»»åŠ¡æ¸…å•

### 1. æ·»åŠ  --train æ¨¡å¼

ä¿®æ”¹ `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py`:

```python
parser.add_argument("--train", action="store_true",
                    help="Run training comparison (Methods A-H)")
parser.add_argument("--train-methods", nargs="+",
                    default=["A_baseline", "D_combined"],
                    help="Methods to compare")
parser.add_argument("--train-model", default="Qwen/Qwen2.5-1.5B-Instruct",
                    help="Base model for training")
```

### 2. é›†æˆè®­ç»ƒæµç¨‹

```python
def run_training_comparison(self, methods: list[str], base_model: str):
    """è¿è¡Œè®­ç»ƒæ–¹æ³•å¯¹æ¯”"""
    from sage.benchmark.benchmark_agent.experiments.method_comparison import (
        MethodComparisonExperiment, MethodRegistry
    )

    exp = MethodComparisonExperiment(
        output_dir=self.output_dir / "training",
        base_model=base_model,
        methods={k: MethodRegistry.get_all_methods()[k] for k in methods}
    )

    results = exp.run_all_methods()
    self.results.training = results

    # ç”Ÿæˆè®­ç»ƒå¯¹æ¯”å›¾è¡¨
    exp.generate_comparison_chart()
```

### 3. æ›´æ–° main() æµç¨‹

```python
if args.train:
    runner.run_training_comparison(
        methods=args.train_methods,
        base_model=args.train_model
    )
```

### 4. æ›´æ–°æ–‡æ¡£

## éªŒæ”¶æ ‡å‡†
- [ ] `--train` æ¨¡å¼å¯ç”¨
- [ ] è®­ç»ƒç»“æœä¿å­˜åˆ° `.sage/benchmark/results/training/`
- [ ] ç”Ÿæˆè®­ç»ƒå¯¹æ¯”å›¾è¡¨
```

---

### Task C2: å•å…ƒæµ‹è¯•è¦†ç›–è¡¥å……

**ä¼˜å…ˆçº§**: P1 | **é¢„ä¼°æ—¶é—´**: 3-4å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task C2: å•å…ƒæµ‹è¯•è¦†ç›–è¡¥å……

## èƒŒæ™¯
`sage-libs/finetune/agent/` æ¨¡å—ç¼ºå°‘å•å…ƒæµ‹è¯•ã€‚

## ä»»åŠ¡æ¸…å•

### 1. æµ‹è¯• CoresetSelector

```python
# packages/sage-libs/tests/unit/finetune/agent/test_continual.py

class TestCoresetSelector:
    def test_loss_topk_selection(self):
        """æµ‹è¯• loss_topk ç­–ç•¥"""
        selector = CoresetSelector(strategy="loss_topk", target_size=100)
        samples = [{"id": i, "loss": random.random()} for i in range(1000)]
        selected = selector.select(samples)
        assert len(selected) == 100
        # éªŒè¯é€‰æ‹©çš„æ˜¯ loss æœ€é«˜çš„

    def test_diversity_selection(self):
        """æµ‹è¯• diversity ç­–ç•¥"""
        pass

    def test_hybrid_selection(self):
        """æµ‹è¯• hybrid ç­–ç•¥"""
        pass
```

### 2. æµ‹è¯• OnlineContinualLearner

```python
class TestOnlineContinualLearner:
    def test_buffer_management(self):
        """æµ‹è¯• replay buffer ç®¡ç†"""
        pass

    def test_replay_sampling(self):
        """æµ‹è¯• replay é‡‡æ ·"""
        pass
```

### 3. æµ‹è¯• AgentSFTTrainer (mock)

### 4. è¿è¡Œæµ‹è¯•

```bash
pytest packages/sage-libs/tests/unit/finetune/agent/ -v --cov
```

## éªŒæ”¶æ ‡å‡†
- [ ] CoresetSelector æµ‹è¯•è¦†ç›–ä¸‰ç§ç­–ç•¥
- [ ] OnlineContinualLearner æµ‹è¯•è¦†ç›– buffer ç®¡ç†
- [ ] æµ‹è¯•é€šè¿‡ç‡ 100%
- [ ] è¦†ç›–ç‡ >= 80%
```

---

### Task C3: API æ–‡æ¡£å®Œå–„

**ä¼˜å…ˆçº§**: P2 | **é¢„ä¼°æ—¶é—´**: 2-3å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task C3: API æ–‡æ¡£å®Œå–„

## èƒŒæ™¯
Agent è®­ç»ƒç›¸å…³æ¨¡å—ç¼ºå°‘ API æ–‡æ¡£ã€‚

## ä»»åŠ¡æ¸…å•

### 1. æ›´æ–° README

æ›´æ–° `packages/sage-libs/README.md`ï¼Œæ·»åŠ  agent finetune éƒ¨åˆ†:

```markdown
## Agent Fine-tuning

### Quick Start

```python
from sage.libs.finetune.agent import AgentSFTConfig, AgentSFTTrainer

config = AgentSFTConfig(
    base_model="Qwen/Qwen2.5-1.5B-Instruct",
    train_data="agent_sft:train",
    num_epochs=1,
)

trainer = AgentSFTTrainer(config)
trainer.train()
```

### Available Methods

| Method | Description | Config |
|--------|-------------|--------|
| A: Baseline | Standard SFT | `use_coreset=False, use_continual=False` |
| B: Coreset | Sample selection | `use_coreset=True, coreset_strategy="hybrid"` |
| C: Continual | Experience replay | `use_continual=True` |
| D: Combined | Coreset + Continual | `use_coreset=True, use_continual=True` |
```

### 2. æ·»åŠ  docstring

ç¡®ä¿æ‰€æœ‰å…¬å…±ç±»å’Œæ–¹æ³•æœ‰å®Œæ•´çš„ docstringã€‚

### 3. ç”Ÿæˆ API å‚è€ƒ

## éªŒæ”¶æ ‡å‡†
- [ ] README åŒ…å«ä½¿ç”¨ç¤ºä¾‹
- [ ] æ‰€æœ‰å…¬å…± API æœ‰ docstring
- [ ] æ–¹æ³•å¯¹æ¯”è¡¨æ ¼å®Œæ•´
```

---

### Task C4: ä¼˜åŒ– Rule-based Timing Decider åŸºå‡†çº¿

**ä¼˜å…ˆçº§**: P1 | **é¢„ä¼°æ—¶é—´**: 2-3å°æ—¶ | **ä¾èµ–**: Task X1 å®Œæˆ

```markdown
# Task C4: ä¼˜åŒ– Rule-based Timing Decider åŸºå‡†çº¿

## èƒŒæ™¯
å½“å‰åŸºå‡†çº¿æ€§èƒ½æœªè¾¾æ ‡:

| Challenge | Best Strategy | Score | Target | Gap |
|-----------|---------------|-------|--------|-----|
| Timing Detection | Rule-based | 78.0% | 95% | -17% |
| Task Planning | Hierarchical | 26.7% | 90% | -63.3% |
| Tool Selection | - | éœ€ä¿®å¤åé‡æµ‹ | 95% | - |

Timing Detection çš„ Rule-based ç­–ç•¥åªæœ‰ 78%ï¼Œéœ€è¦ä¼˜åŒ–å…³é”®è¯åŒ¹é…ç­–ç•¥ã€‚

## ä»»åŠ¡æ¸…å•

### 1. åˆ†æå½“å‰è§„åˆ™
æŸ¥çœ‹ `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`:

```python
def _create_rule_based_decider(self, resources):
    # æ£€æŸ¥å½“å‰ä½¿ç”¨çš„è§„åˆ™
    pass
```

### 2. åˆ†æé”™è¯¯æ ·æœ¬
```python
# æ”¶é›† rule-based åˆ¤æ–­é”™è¯¯çš„æ ·æœ¬
errors = []
for sample in test_data:
    pred = decider.decide(sample.message)
    if pred.should_call_tool != sample.should_call_tool:
        errors.append({
            "message": sample.message,
            "expected": sample.should_call_tool,
            "predicted": pred.should_call_tool,
            "reasoning": pred.reasoning
        })

# åˆ†æé”™è¯¯æ¨¡å¼
print(f"False Positives: {len([e for e in errors if e['predicted']])}")
print(f"False Negatives: {len([e for e in errors if not e['predicted']])}")
```

### 3. æ”¹è¿›è§„åˆ™

åŸºäºé”™è¯¯åˆ†ææ”¹è¿›è§„åˆ™:

```python
class ImprovedRuleBasedDecider:
    """æ”¹è¿›çš„è§„åˆ™åˆ¤æ–­å™¨"""

    # éœ€è¦å·¥å…·çš„å…³é”®è¯
    TOOL_KEYWORDS = [
        # åŠ¨ä½œç±»
        "æŸ¥è¯¢", "æœç´¢", "è®¡ç®—", "è½¬æ¢", "è·å–", "æŸ¥æ‰¾", "åˆ†æ",
        "search", "query", "calculate", "convert", "get", "find",
        # æ„å›¾ç±»
        "å¸®æˆ‘", "è¯·å¸®", "èƒ½å¦", "å¯ä»¥",
        "please", "can you", "could you", "help me",
        # æ•°æ®ç±»
        "å¤©æ°”", "è‚¡ç¥¨", "æ±‡ç‡", "æ—¥æœŸ", "æ—¶é—´",
        "weather", "stock", "exchange rate", "date", "time",
    ]

    # ä¸éœ€è¦å·¥å…·çš„å…³é”®è¯
    DIRECT_KEYWORDS = [
        # é—²èŠç±»
        "ä½ å¥½", "è°¢è°¢", "å†è§", "æ˜¯ä»€ä¹ˆ", "ä»€ä¹ˆæ˜¯",
        "hello", "thanks", "bye", "what is", "who are",
        # çŸ¥è¯†ç±»
        "è§£é‡Š", "æè¿°", "ä»‹ç»", "ä¸ºä»€ä¹ˆ",
        "explain", "describe", "introduce", "why",
    ]

    def decide(self, message: str) -> TimingDecision:
        message_lower = message.lower()

        # è®¡ç®—å·¥å…·å…³é”®è¯åŒ¹é…åˆ†æ•°
        tool_score = sum(1 for kw in self.TOOL_KEYWORDS if kw in message_lower)
        direct_score = sum(1 for kw in self.DIRECT_KEYWORDS if kw in message_lower)

        # é•¿åº¦å› ç´ ï¼šè¾ƒé•¿çš„æ¶ˆæ¯æ›´å¯èƒ½æ˜¯å¤æ‚ä»»åŠ¡
        length_factor = min(len(message) / 100, 1.0)

        # ç»¼åˆåˆ¤æ–­
        should_call = (tool_score > direct_score) or (tool_score > 0 and length_factor > 0.5)
        confidence = min((tool_score + length_factor) / 3, 1.0)

        return TimingDecision(
            should_call_tool=should_call,
            confidence=confidence,
            reasoning=f"Tool keywords: {tool_score}, Direct keywords: {direct_score}"
        )
```

### 4. éªŒè¯æ”¹è¿›æ•ˆæœ
```bash
python run_all_experiments.py --quick --skip-llm
# æ£€æŸ¥ Timing Detection å‡†ç¡®ç‡æ˜¯å¦æå‡
```

## éªŒæ”¶æ ‡å‡†
- [ ] åˆ†æå‡ºä¸»è¦é”™è¯¯æ¨¡å¼
- [ ] æ”¹è¿›è§„åˆ™åå‡†ç¡®ç‡ >= 85%
- [ ] æ²¡æœ‰å¼•å…¥æ–°çš„ä¸¥é‡é—®é¢˜

## å…³é”®æ–‡ä»¶
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
- `packages/sage-libs/src/sage/libs/agentic/agents/action/timing/`
```

---

### Task C5: å®ç° LLM æœåŠ¡ç¼“å­˜/é¢„åŠ è½½æœºåˆ¶

**ä¼˜å…ˆçº§**: P2 | **é¢„ä¼°æ—¶é—´**: 2-3å°æ—¶ | **ä¾èµ–**: æ— 

```markdown
# Task C5: å®ç° LLM æœåŠ¡ç¼“å­˜æœºåˆ¶

## èƒŒæ™¯
æ¯æ¬¡è¿è¡Œ Hybrid/LLM ç­–ç•¥éƒ½è¦é‡æ–°åŠ è½½ vLLM æ¨¡å‹ï¼ˆçº¦30ç§’ï¼‰ï¼Œå½±å“å¼€å‘æ•ˆç‡ã€‚

## æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|
| A: æ¨¡å‹ç¼“å­˜å•ä¾‹ | ç®€å•ï¼ŒåŒè¿›ç¨‹å¤ç”¨ | è¿›ç¨‹é€€å‡ºåå¤±æ•ˆ |
| B: å¤–éƒ¨ API æœåŠ¡ | å¤šè¿›ç¨‹å…±äº«ï¼ŒæŒä¹…åŒ– | éœ€è¦é¢å¤–å¯åŠ¨æœåŠ¡ |
| C: æ¨¡å‹é¢„åŠ è½½è„šæœ¬ | æå‰å‡†å¤‡å¥½ | éœ€è¦æ‰‹åŠ¨è¿è¡Œ |

## ä»»åŠ¡æ¸…å•

### 1. æ–¹æ¡ˆ A: å®ç° LLM Client å•ä¾‹æ¨¡å¼

```python
# packages/sage-common/src/sage/common/components/sage_llm/client.py

class IntelligentLLMClient:
    _instances: dict[str, "IntelligentLLMClient"] = {}

    @classmethod
    def get_instance(cls, model_name: str = None, **kwargs) -> "IntelligentLLMClient":
        """è·å–æˆ–åˆ›å»º LLM å®¢æˆ·ç«¯å•ä¾‹"""
        key = model_name or "default"
        if key not in cls._instances:
            cls._instances[key] = cls.create_auto(model_name=model_name, **kwargs)
        return cls._instances[key]

    @classmethod
    def clear_instances(cls):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜çš„å®ä¾‹"""
        cls._instances.clear()
```

### 2. æ›´æ–° adapter_registry.py ä½¿ç”¨å•ä¾‹

```python
def _create_llm_timing_decider(self, resources):
    # ä½¿ç”¨å•ä¾‹è€Œä¸æ˜¯æ¯æ¬¡åˆ›å»ºæ–°å®ä¾‹
    llm_client = IntelligentLLMClient.get_instance()
    return TimingAdapter(LLMTimingDecider(llm_client))
```

### 3. æ–¹æ¡ˆ B: æ¨èä½¿ç”¨å¤–éƒ¨æœåŠ¡

åœ¨æ–‡æ¡£ä¸­æ¨èç”¨æˆ·é¢„å…ˆå¯åŠ¨ vLLM æœåŠ¡:

```bash
# æ¨è: å…ˆå¯åŠ¨ vLLM æœåŠ¡
vllm serve Qwen/Qwen2.5-7B-Instruct --port 8001

# ç„¶åè¿è¡Œå®éªŒ (ä¼šè‡ªåŠ¨æ£€æµ‹åˆ°æœ¬åœ°æœåŠ¡)
python run_all_experiments.py --full
```

### 4. æ·»åŠ å¯åŠ¨æ£€æµ‹æç¤º

```python
# run_all_experiments.py
def check_llm_service():
    """æ£€æŸ¥å¹¶æç¤º LLM æœåŠ¡çŠ¶æ€"""
    from sage.common.components.sage_llm.client import IntelligentLLMClient

    # æ£€æµ‹æœ¬åœ°æœåŠ¡
    for port in [8001, 8000]:
        result = IntelligentLLMClient._probe_vllm_service(f"http://localhost:{port}/v1")
        if result:
            print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ° vLLM æœåŠ¡: localhost:{port} (model: {result})")
            return True

    print("âš ï¸  æœªæ£€æµ‹åˆ°æœ¬åœ° vLLM æœåŠ¡")
    print("   å»ºè®®å…ˆå¯åŠ¨: vllm serve Qwen/Qwen2.5-7B-Instruct --port 8001")
    print("   æˆ–ä½¿ç”¨ --skip-llm è·³è¿‡ LLM ç­–ç•¥")
    return False
```

## éªŒæ”¶æ ‡å‡†
- [ ] LLM Client å•ä¾‹æ¨¡å¼å®ç°
- [ ] ç¬¬äºŒæ¬¡è°ƒç”¨ LLM ç­–ç•¥æ— éœ€é‡æ–°åŠ è½½
- [ ] æ–‡æ¡£è¯´æ˜æ¨èçš„æœåŠ¡å¯åŠ¨æ–¹å¼

## å…³é”®æ–‡ä»¶
- `packages/sage-common/src/sage/common/components/sage_llm/client.py`
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py`
- `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py`
```

---

## æœ€ç»ˆä»»åŠ¡ D: å®Œæ•´å®éªŒè¿è¡Œ (ä¾èµ– X+A+B+C)

### Task D1: å®Œæ•´å®éªŒè¿è¡Œä¸è®ºæ–‡å›¾è¡¨ç”Ÿæˆ

**ä¼˜å…ˆçº§**: P0 | **é¢„ä¼°æ—¶é—´**: 4-6å°æ—¶ | **ä¾èµ–**: A1-A5, B1-B4, C1

```markdown
# Task D1: å®Œæ•´å®éªŒè¿è¡Œä¸è®ºæ–‡å›¾è¡¨ç”Ÿæˆ

## èƒŒæ™¯
æ‰€æœ‰ SOTA æ–¹æ³•å®ç°å®Œæˆåï¼Œè¿è¡Œå®Œæ•´å®éªŒå¹¶ç”Ÿæˆè®ºæ–‡æ‰€éœ€çš„å›¾è¡¨å’Œè¡¨æ ¼ã€‚

## å‰ç½®æ¡ä»¶
- Task A1-A5 å®Œæˆ (SOTA ç­–ç•¥)
- Task B1-B4 å®Œæˆ (SOTA å¾®è°ƒ)
- Task C1 å®Œæˆ (ç»Ÿä¸€è„šæœ¬)

## ä»»åŠ¡æ¸…å•

### 1. è¿è¡Œå®Œæ•´è¯„æµ‹å®éªŒ

```bash
cd /home/shuhao/SAGE

# å®Œæ•´è¯„æµ‹ (æ‰€æœ‰ç­–ç•¥)
python packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py \
    --eval-only \
    --max-samples 500

# ç»“æœä½ç½®: ~/.sage/benchmark/results/
```

### 2. è¿è¡Œå®Œæ•´è®­ç»ƒå¯¹æ¯”

```bash
# å®Œæ•´è®­ç»ƒå¯¹æ¯” (Methods A-H)
python packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py \
    --train \
    --train-methods A_baseline B3_coreset_hybrid C_continual D_combined E_fireact F_agenttuning G_dora \
    --train-model Qwen/Qwen2.5-7B-Instruct

# éœ€è¦ A100 æˆ–æ›´é«˜é…ç½® GPU
```

### 3. ç”Ÿæˆè®ºæ–‡å›¾è¡¨

```bash
# ç”Ÿæˆæ‰€æœ‰å›¾è¡¨å’Œè¡¨æ ¼
python packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py \
    --paper-only \
    --results-dir ~/.sage/benchmark/results/
```

### 4. éªŒè¯ç»“æœ

ç¡®è®¤ç”Ÿæˆçš„æ–‡ä»¶:
- `~/.sage/benchmark/results/figures/fig1_strategy_comparison.pdf`
- `~/.sage/benchmark/results/figures/fig2_training_comparison.pdf`
- `~/.sage/benchmark/results/figures/fig3_ablation.pdf`
- `~/.sage/benchmark/results/tables/table1_main_results.tex`
- `~/.sage/benchmark/results/tables/table2_sota_comparison.tex`

### 5. æ›´æ–° TODO.md

å°†æ‰€æœ‰ âŒ æ›´æ–°ä¸º âœ…

## éªŒæ”¶æ ‡å‡†
- [ ] æ‰€æœ‰ç­–ç•¥åœ¨ä¸‰ä¸ª Challenge ä¸Šæœ‰çœŸå®è¯„æµ‹ç»“æœ
- [ ] æ‰€æœ‰è®­ç»ƒæ–¹æ³•æœ‰å¯¹æ¯”æ•°æ®
- [ ] è®ºæ–‡å›¾è¡¨ PDF ç”Ÿæˆ
- [ ] LaTeX è¡¨æ ¼å¯ç›´æ¥ç”¨äºè®ºæ–‡
- [ ] è‡³å°‘ä¸€ä¸ªæ–¹æ³•åœ¨æ¯ä¸ª Challenge è¾¾åˆ°ç›®æ ‡ (95%/90%/95%)
```

---

## ğŸ“Š ä»»åŠ¡æ€»è§ˆä¸åˆ†é…å»ºè®®

### ä»»åŠ¡æ€»è§ˆ

| ä»»åŠ¡ç»„ | ä»»åŠ¡ ID | ä»»åŠ¡åç§° | ä¼˜å…ˆçº§ | é¢„ä¼°æ—¶é—´ | å¯å¹¶è¡Œ |
|--------|---------|----------|--------|----------|--------|
| **X: Bug ä¿®å¤** | X1 | Tool Selection è¯„ä¼° Bug | ğŸ”´ P0 | 2-3h | âœ… |
| | X2 | Hybrid Timing LLM é—®é¢˜ | ğŸŸ¡ P1 | 1h | âœ… |
| | X3 | æ•°æ®æ–‡ä»¶ä½ç½®æ¢³ç† | ğŸŸ¡ P1 | 1-2h | âœ… |
| **A: SOTA ç­–ç•¥** | A1 | ToolLLM (DFSDT) | ğŸ”´ P0 | 4-6h | âœ… |
| | A2 | ReAct å®Œå–„ | ğŸ”´ P0 | 3-4h | âœ… |
| | A3 | Tree-of-Thoughts | ğŸŸ¡ P1 | 5-6h | âœ… |
| | A4 | Gorilla | ğŸŸ¢ P2 | 3-4h | âœ… |
| | A5 | API-Bank æ•°æ® | ğŸŸ¢ P2 | 2-3h | âœ… |
| **B: SOTA å¾®è°ƒ** | B1 | FireAct | ğŸŸ¡ P1 | 4-5h | âœ… |
| | B2 | AgentTuning | ğŸŸ¡ P1 | 3-4h | âœ… |
| | B3 | ToolAlpaca æ•°æ® | ğŸŸ¢ P2 | 2-3h | âœ… |
| | B4 | DoRA/LoRA+ | ğŸŸ¢ P2 | 3-4h | âœ… |
| **C: åŸºç¡€è®¾æ–½** | C1 | ç»Ÿä¸€å®éªŒè„šæœ¬æ•´åˆ | ğŸ”´ P0 | 2-3h | âœ… |
| | C2 | å•å…ƒæµ‹è¯•è¦†ç›– | ğŸŸ¡ P1 | 3-4h | âœ… |
| | C3 | API æ–‡æ¡£å®Œå–„ | ğŸŸ¢ P2 | 2-3h | âœ… |
| | C4 | Rule-based åŸºå‡†çº¿ä¼˜åŒ– | ğŸŸ¡ P1 | 2-3h | âœ… |
| | C5 | LLM ç¼“å­˜æœºåˆ¶ | ğŸŸ¢ P2 | 2-3h | âœ… |
| **D: æœ€ç»ˆæ•´åˆ** | D1 | å®Œæ•´å®éªŒ + è®ºæ–‡å›¾è¡¨ | ğŸ”´ P0 | 4-6h | âŒ |

### ä»»åŠ¡ä¾èµ–å…³ç³»

```
X1 (Tool Selection Bug) â”€â”€â”€â”€â”€â”
X2 (Hybrid Timing LLM) â”€â”€â”€â”€â”€â”€â”¼â”€â”€â†’ å¯ç«‹å³å¼€å§‹ A/B/C ä»»åŠ¡
X3 (æ•°æ®ä½ç½®æ¢³ç†) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

A1-A5 (SOTA ç­–ç•¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
B1-B4 (SOTA å¾®è°ƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â†’ D1 (å®Œæ•´å®éªŒ)
C1-C5 (åŸºç¡€è®¾æ–½) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ¨èåˆ†é…æ–¹æ¡ˆ (6 ä¸ª Agent)

```
ğŸ”´ Agent 0 (ç´§æ€¥): Task X1 + X2 + X3 (Bug ä¿®å¤, æœ€å…ˆæ‰§è¡Œ)

ğŸ”µ Agent 1: Task A1 (ToolLLM) + Task A4 (Gorilla)         [7-10h]
ğŸ”µ Agent 2: Task A2 (ReAct) + Task A3 (ToT)               [8-10h]
ğŸ”µ Agent 3: Task B1 (FireAct) + Task B2 (AgentTuning)     [7-9h]
ğŸ”µ Agent 4: Task B3 + Task B4 + Task A5                   [7-10h]
ğŸ”µ Agent 5: Task C1 + C2 + C3 + C4 + C5                   [11-16h]

ğŸŸ¢ æœ€å: ä»»æ„ Agent æ‰§è¡Œ Task D1 (å®Œæ•´å®éªŒ)
```

### ç²¾ç®€åˆ†é…æ–¹æ¡ˆ (3 ä¸ª Agent)

```
ğŸ”´ Agent 1: X1 + X2 + X3 + C1 + C4 (Bug ä¿®å¤ + åŸºç¡€è¯„æµ‹)   [8-12h]
ğŸ”µ Agent 2: A1 + A2 + A3 (SOTA ç­–ç•¥æ ¸å¿ƒ)                   [12-16h]
ğŸ”µ Agent 3: B1 + B2 + B4 (SOTA å¾®è°ƒæ ¸å¿ƒ)                   [10-13h]

ğŸŸ¢ æœ€å: Agent 1 æ‰§è¡Œ Task D1
```

---

## é‡è¦æ–‡ä»¶è·¯å¾„é€ŸæŸ¥

```
# å·¥å…·é€‰æ‹©ç­–ç•¥
packages/sage-libs/src/sage/libs/agentic/agents/action/tool_selection/

# è§„åˆ’ç­–ç•¥
packages/sage-libs/src/sage/libs/agentic/agents/action/planning/

# Timing ç­–ç•¥
packages/sage-libs/src/sage/libs/agentic/agents/action/timing/

# å¾®è°ƒè®­ç»ƒ
packages/sage-libs/src/sage/libs/finetune/agent/

# Benchmark é€‚é…å™¨æ³¨å†Œ
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/adapter_registry.py

# æ–¹æ³•å¯¹æ¯”æ¡†æ¶
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/experiments/method_comparison.py

# ç»Ÿä¸€å®éªŒè„šæœ¬
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts/run_all_experiments.py

# æ•°æ®ç›®å½• (é™æ€)
packages/sage-benchmark/src/sage/data/sources/agent_benchmark/
packages/sage-benchmark/src/sage/data/sources/agent_tools/
packages/sage-benchmark/src/sage/data/sources/agent_sft/

# æ•°æ®ç›®å½• (è¿è¡Œæ—¶)
~/.sage/benchmark/data/

# ç»“æœç›®å½•
~/.sage/benchmark/results/
```

---

## âœ… å·²å®Œæˆçš„å·¥ä½œ (å‚è€ƒ)

1. **ä¸‰ä¸ªæŒ‘æˆ˜çš„è¯„ä¼°æ¡†æ¶**
   - Timing Detection: rule_based, llm_based, hybrid ç­–ç•¥
   - Task Planning: simple, hierarchical, llm_based ç­–ç•¥
   - Tool Selection: keyword, embedding, hybrid ç­–ç•¥

2. **æ•°æ®å‡†å¤‡è„šæœ¬**
   - `prepare_timing_data.py` - ç”Ÿæˆ 1000 æ¡ timing judgment æ ·æœ¬
   - `prepare_planning_data.py` - ç”Ÿæˆ 300 æ¡ planning æ ·æœ¬
   - `prepare_tool_selection_data.py` - ç”Ÿæˆ tool selection æ ·æœ¬

3. **è®ºæ–‡ææ–™ç”Ÿæˆ**
   - 5 ä¸ªå›¾è¡¨ (PDF + PNG)
   - 4 ä¸ª LaTeX è¡¨æ ¼
   - è®ºæ–‡å¼•ç”¨çš„æ–‡ä»¶ååˆ«å

4. **ä¸€é”®è¿è¡Œè„šæœ¬**
   - `--quick` å¿«é€Ÿæ¨¡å¼
   - `--skip-llm` è·³è¿‡ LLM ç­–ç•¥
   - `--paper-only` ä»…ç”Ÿæˆè®ºæ–‡ææ–™

5. **åŸºç¡€å¾®è°ƒæ¡†æ¶**
   - AgentSFTTrainer + AgentSFTConfig
   - CoresetSelector (loss_topk, diversity, hybrid)
   - OnlineContinualLearner

---

## ğŸ“ ä½¿ç”¨è¯´æ˜

1. **åˆ†å‘ä»»åŠ¡**: å¤åˆ¶å¯¹åº” Task çš„ markdown ä»£ç å—ç»™ä¸åŒçš„ Agent
2. **å¹¶è¡Œæ‰§è¡Œ**: åŒä¸€ä»»åŠ¡ç»„å†…çš„ä»»åŠ¡å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
3. **éªŒæ”¶æ£€æŸ¥**: æ¯ä¸ªä»»åŠ¡å®Œæˆåæ£€æŸ¥ã€ŒéªŒæ”¶æ ‡å‡†ã€åˆ—è¡¨
4. **ä¾èµ–é¡ºåº**: Task D1 éœ€è¦ç­‰å…¶ä»–ä»»åŠ¡å®Œæˆåæ‰§è¡Œ
