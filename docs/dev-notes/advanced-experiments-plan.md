# SAGE-Bench Advanced Experiments Implementation Plan

## ğŸ“‹ Overview

æœ¬æ–‡æ¡£è¯¦ç»†åˆ—å‡ºä¸º Paper 1 æ‰©å±•å®éªŒçš„ä»£ç æ”¹è¿›è®¡åˆ’ã€‚å…± 9 ç±»å®éªŒï¼Œåˆ†ä¸ºä¸‰å¤§ç±»ï¼š

1. **ç°è±¡åˆ†æå‹ (3ä¸ª)**: æš´éœ²ç°æœ‰æ–¹æ³•çš„ç›²åŒº
2. **å˜é‡æ§åˆ¶å‹ (3ä¸ª)**: ç³»ç»Ÿæ€§åˆ†æå…³é”®å˜é‡å½±å“
3. **è¶‹åŠ¿å¯¹é½å‹ (3ä¸ª)**: å¯¹æ ‡é¡¶ä¼š/å¤§å‚ç ”ç©¶èŒƒå¼

---

## ğŸ” ä¸€ç±»ï¼šç°è±¡åˆ†æå‹ (Error Analysis)

### 1. Error Type Breakdown by Challenge

**ç›®æ ‡**: æŒ‰ Challenge åˆ†ç±»ç»Ÿè®¡å¸¸è§é”™è¯¯ç±»å‹

**æ–°å¢æ–‡ä»¶**:
```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ evaluation/analyzers/
â”‚   â”œâ”€â”€ error_breakdown_analyzer.py      # æ–°å¢ï¼šé”™è¯¯ç±»å‹åˆ†è§£åˆ†æå™¨
â”‚   â””â”€â”€ __init__.py                      # æ›´æ–°ï¼šå¯¼å‡ºæ–°åˆ†æå™¨
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ error_analysis_exp.py            # æ–°å¢ï¼šé”™è¯¯åˆ†æå®éªŒ
```

**ä»£ç æ”¹åŠ¨æ¸…å•**:

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | æè¿° |
|------|----------|------|
| `error_breakdown_analyzer.py` | æ–°å¢ | å®ç° `ErrorBreakdownAnalyzer` ç±» |
| `timing_analyzer.py` | æ‰©å±• | æ·»åŠ  false_positive/false_negative è¯¦ç»†åˆ†è§£ |
| `planning_analyzer.py` | æ‰©å±• | æ·»åŠ  step_missing/wrong_order/invalid_step åˆ†è§£ |
| `tool_selection_analyzer.py` | æ‰©å±• | æ·»åŠ  top1_error/topk_rank_volatility åˆ†è§£ |
| `run_all_experiments.py` | æ‰©å±• | æ·»åŠ  `--error-analysis` flag |

**å…³é”®å®ç°**:

```python
# error_breakdown_analyzer.py
class ErrorBreakdownAnalyzer:
    """
    Unified error type breakdown analyzer across all challenges.
    """

    def analyze_timing_errors(self, predictions, references, confidences=None):
        """
        Timing é”™è¯¯åˆ†è§£:
        - false_positive_rate: ä¸è¯¥è°ƒç”¨å´è°ƒç”¨
        - false_negative_rate: è¯¥è°ƒç”¨å´æ²¡è°ƒç”¨
        - confidence_calibration: é«˜ç½®ä¿¡åº¦ä½†é”™è¯¯çš„æ¯”ä¾‹
        """

    def analyze_planning_errors(self, predictions, references):
        """
        Planning é”™è¯¯åˆ†è§£:
        - step_missing_rate: ç¼ºå¤±å…³é”®æ­¥éª¤
        - wrong_order_rate: æ­¥éª¤é¡ºåºé”™è¯¯
        - invalid_step_rate: æ­¥éª¤ä¸åˆç†/å¹»è§‰
        - extra_step_rate: å¤šä½™æ­¥éª¤
        """

    def analyze_selection_errors(self, predictions, references, k=5):
        """
        Selection é”™è¯¯åˆ†è§£:
        - top1_error_rate: ç¬¬ä¸€ä¸ªé€‰æ‹©å°±é”™
        - topk_rank_volatility: top-k å†…æ’åæŠ–åŠ¨
        - category_confusion_matrix: è·¨ç±»åˆ«æ··æ·†
        - similar_tool_confusion: ç›¸ä¼¼å·¥å…·æ··æ·†ç‡
        """
```

**è¾“å‡º**:
```
figures/fig_error_breakdown_timing.pdf     # FP/FN å¯¹æ¯”æŸ±çŠ¶å›¾
figures/fig_error_breakdown_planning.pdf   # 4ç§é”™è¯¯ç±»å‹å †å å›¾
figures/fig_error_breakdown_selection.pdf  # é”™è¯¯æ¨¡å¼åˆ†å¸ƒé¥¼å›¾
tables/table_error_breakdown.tex           # è¯¦ç»†é”™è¯¯ç»Ÿè®¡è¡¨
```

---

### 2. Failure Cascading Analysis

**ç›®æ ‡**: åˆ†ææ—©æœŸé”™è¯¯å¯¼è‡´çš„çº§è”å¤±è´¥

**æ–°å¢æ–‡ä»¶**:
```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ evaluation/analyzers/
â”‚   â””â”€â”€ cascading_failure_analyzer.py    # æ–°å¢
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ cascading_analysis_exp.py        # æ–°å¢
```

**ä»£ç æ”¹åŠ¨æ¸…å•**:

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | æè¿° |
|------|----------|------|
| `cascading_failure_analyzer.py` | æ–°å¢ | å®ç°çº§è”å¤±è´¥åˆ†æ |
| `planning_exp.py` | æ‰©å±• | è®°å½•æ¯æ­¥æ‰§è¡Œ trace |
| `base_experiment.py` | æ‰©å±• | æ·»åŠ  `ActionTrace` æ•°æ®æ¨¡å‹ |
| `run_all_experiments.py` | æ‰©å±• | æ·»åŠ  `--cascade-analysis` flag |

**å…³é”®å®ç°**:

```python
# cascading_failure_analyzer.py
class CascadingFailureAnalyzer:
    """
    Analyze failure cascading patterns in multi-step agent tasks.
    """

    def compute_first_error_distribution(self, traces):
        """
        è®¡ç®— "first error step index" åˆ†å¸ƒ
        - è¿”å›ç›´æ–¹å›¾: [step_1_error_count, step_2_error_count, ...]
        """

    def compare_correct_vs_failed_trajectories(self, traces):
        """
        å¯¹æ¯”æ­£ç¡® vs å‡ºé”™è½¨è¿¹çš„å‰ N æ­¥
        - è¿”å›: divergence_point_distribution
        """

    def compute_recovery_rate(self, traces):
        """
        è®¡ç®— agent ä»é”™è¯¯ä¸­æ¢å¤çš„èƒ½åŠ›
        - recovery_after_error_rate: å‡ºé”™åèƒ½å¦è‡ªæˆ‘çº æ­£
        """

# base_experiment.py æ–°å¢
@dataclass
class ActionTrace:
    """Single action in execution trace."""
    step_index: int
    action_type: str  # "tool_selection" | "planning" | "timing"
    prediction: Any
    ground_truth: Any
    is_correct: bool
    confidence: float = 0.0
    error_type: Optional[str] = None
```

**è¾“å‡º**:
```
figures/fig_cascade_first_error_dist.pdf     # é¦–æ¬¡é”™è¯¯æ­¥éª¤åˆ†å¸ƒ
figures/fig_cascade_trajectory_compare.pdf   # æ­£ç¡®vsé”™è¯¯è½¨è¿¹å¯¹æ¯”
tables/table_cascade_analysis.tex            # çº§è”å¤±è´¥ç»Ÿè®¡
```

---

### 3. Cross-Task Generalization Evaluation

**ç›®æ ‡**: æµ‹è¯•è¯­ä¹‰å˜åŒ–çš„é²æ£’æ€§

**æ–°å¢æ–‡ä»¶**:
```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ semantic_variations.jsonl        # æ–°å¢ï¼šè¯­ä¹‰å˜ä½“æ•°æ®
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ generalization_exp.py            # æ–°å¢
â”œâ”€â”€ evaluation/analyzers/
â”‚   â””â”€â”€ generalization_analyzer.py       # æ–°å¢
```

**ä»£ç æ”¹åŠ¨æ¸…å•**:

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | æè¿° |
|------|----------|------|
| `generalization_exp.py` | æ–°å¢ | å®ç°è·¨ä»»åŠ¡æ³›åŒ–å®éªŒ |
| `generalization_analyzer.py` | æ–°å¢ | åˆ†æè¯­ä¹‰å˜åŒ–é²æ£’æ€§ |
| `semantic_variations.jsonl` | æ–°å¢ | åŒä»»åŠ¡ä¸åŒæè¿°çš„æµ‹è¯•æ•°æ® |
| `run_all_experiments.py` | æ‰©å±• | æ·»åŠ  `--generalization` flag |

**æ•°æ®æ ¼å¼**:
```jsonl
{
  "task_id": "find_contact_001",
  "original": "æŸ¥æ‰¾å¼ ä¼Ÿçš„è”ç³»æ–¹å¼",
  "variations": [
    {"type": "paraphrase", "text": "ç»™å¼ ä¼Ÿæ‰“ç”µè¯å‰è·å–ä»–çš„å·ç "},
    {"type": "formal", "text": "è¯·æ£€ç´¢å¼ ä¼Ÿå…ˆç”Ÿçš„è”ç»œä¿¡æ¯"},
    {"type": "casual", "text": "å¼ ä¼Ÿç”µè¯å¤šå°‘"},
    {"type": "adversarial", "text": "æˆ‘ä¸æƒ³æ‰¾å¼ ä¼Ÿï¼Œä½†å‡å¦‚è¦æ‰¾çš„è¯æ€ä¹ˆè”ç³»"}
  ],
  "expected_tool": "contact_search",
  "expected_timing": true
}
```

**å…³é”®å®ç°**:

```python
# generalization_exp.py
class GeneralizationExperiment(BaseExperiment):
    """
    Test semantic variation robustness.
    """

    def run_variation_test(self, strategy, variations_data):
        """
        å¯¹æ¯ä¸ªä»»åŠ¡æµ‹è¯•å¤šä¸ªè¯­ä¹‰å˜ä½“:
        1. æ‰€æœ‰å˜ä½“æ˜¯å¦é€‰æ‹©ç›¸åŒå·¥å…·ï¼Ÿ
        2. Timing åˆ¤æ–­æ˜¯å¦ä¸€è‡´ï¼Ÿ
        3. å“ªç§å˜ä½“ç±»å‹æœ€å®¹æ˜“å‡ºé”™ï¼Ÿ
        """

# generalization_analyzer.py
class GeneralizationAnalyzer:
    def compute_consistency_score(self, results):
        """åŒä»»åŠ¡ä¸åŒæè¿°çš„ä¸€è‡´æ€§å¾—åˆ†"""

    def compute_variation_sensitivity(self, results):
        """å„å˜ä½“ç±»å‹çš„æ•æ„Ÿåº¦åˆ†æ"""

    def detect_template_overfitting(self, results):
        """æ£€æµ‹æ˜¯å¦è¿‡æ‹Ÿåˆç‰¹å®šæ¨¡æ¿"""
```

**è¾“å‡º**:
```
figures/fig_generalization_consistency.pdf   # ä¸€è‡´æ€§å¾—åˆ†å¯¹æ¯”
figures/fig_generalization_sensitivity.pdf   # å„å˜ä½“ç±»å‹æ•æ„Ÿåº¦
tables/table_generalization_results.tex      # è¯¦ç»†æ³›åŒ–ç»“æœ
```

---

## âš™ï¸ äºŒç±»ï¼šå˜é‡æ§åˆ¶å‹ (Ablation Studies)

### 4. Tool Set Size Scaling Curve

**ç›®æ ‡**: æµ‹è¯•å·¥å…·æ•°é‡å¯¹å‡†ç¡®ç‡çš„å½±å“

**æ–°å¢æ–‡ä»¶**:
```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ scaling_exp.py                   # æ–°å¢
â”œâ”€â”€ evaluation/analyzers/
â”‚   â””â”€â”€ scaling_analyzer.py              # æ–°å¢
â”œâ”€â”€ data/
â”‚   â””â”€â”€ noise_tools.jsonl                # æ–°å¢ï¼šå¹²æ‰°å·¥å…·åº“
```

**ä»£ç æ”¹åŠ¨æ¸…å•**:

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | æè¿° |
|------|----------|------|
| `scaling_exp.py` | æ–°å¢ | å®ç°å·¥å…·æ•°é‡ scaling å®éªŒ |
| `scaling_analyzer.py` | æ–°å¢ | åˆ†æ scaling æ›²çº¿ |
| `noise_tools.jsonl` | æ–°å¢ | ç›¸ä¼¼åç§°/ç±»åˆ«çš„å¹²æ‰°å·¥å…· |
| `adapter_registry.py` | æ‰©å±• | æ”¯æŒåŠ¨æ€å·¥å…·é›†å¤§å° |
| `run_all_experiments.py` | æ‰©å±• | æ·»åŠ  `--scaling-study` flag |

**å…³é”®å®ç°**:

```python
# scaling_exp.py
class ToolSetScalingExperiment(BaseExperiment):
    """
    Test tool selection accuracy vs candidate set size.
    """

    SCALE_POINTS = [10, 25, 50, 100, 200, 500, 1000]

    def run_scaling_test(self, strategy, base_tools, noise_tools):
        """
        é€æ­¥å¢åŠ å€™é€‰å·¥å…·æ•°é‡:
        1. åŸºç¡€: 10 ä¸ªç›¸å…³å·¥å…·
        2. é€æ­¥æ·»åŠ  noise tools
        3. è®°å½•å„è§„æ¨¡ä¸‹çš„ accuracy/MRR/latency
        """

    def add_noise_tools(self, base_tools, noise_tools, target_size):
        """
        æ·»åŠ å¹²æ‰°å·¥å…·:
        - similar_name: åç§°ç›¸ä¼¼ä½†åŠŸèƒ½ä¸åŒ
        - similar_category: åŒç±»åˆ«ä½†ä¸ç›¸å…³
        - random: å®Œå…¨æ— å…³
        """

# scaling_analyzer.py
class ScalingAnalyzer:
    def fit_scaling_curve(self, results):
        """æ‹Ÿåˆ scaling æ›²çº¿ (å¯¹æ•°/çº¿æ€§/æŒ‡æ•°)"""

    def compute_noise_resistance(self, results):
        """è®¡ç®—å„ç­–ç•¥çš„æŠ—å¹²æ‰°èƒ½åŠ›"""

    def find_critical_scale(self, results, threshold=0.9):
        """æ‰¾åˆ°å‡†ç¡®ç‡ä¸‹é™åˆ°é˜ˆå€¼çš„ä¸´ç•Œè§„æ¨¡"""
```

**è¾“å‡º**:
```
figures/fig_scaling_curve.pdf                # X: å·¥å…·æ•°, Y: accuracy
figures/fig_scaling_noise_resistance.pdf     # å„ç­–ç•¥æŠ—å¹²æ‰°å¯¹æ¯”
tables/table_scaling_results.tex             # è¯¦ç»† scaling æ•°æ®
```

---

### 5. Prompt Length Ablation

**ç›®æ ‡**: æµ‹è¯• prompt é•¿åº¦/å†…å®¹å¯¹ LLM-based æ–¹æ³•çš„å½±å“

**æ–°å¢æ–‡ä»¶**:
```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ prompt_ablation_exp.py           # æ–°å¢
â”œâ”€â”€ config/
â”‚   â””â”€â”€ prompt_templates/
â”‚       â”œâ”€â”€ minimal.yaml                 # æœ€å° prompt
â”‚       â”œâ”€â”€ standard.yaml                # æ ‡å‡† prompt
â”‚       â”œâ”€â”€ with_examples.yaml           # å¸¦ç¤ºä¾‹
â”‚       â””â”€â”€ with_cot.yaml                # å¸¦ CoT
```

**ä»£ç æ”¹åŠ¨æ¸…å•**:

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | æè¿° |
|------|----------|------|
| `prompt_ablation_exp.py` | æ–°å¢ | å®ç° prompt æ¶ˆèå®éªŒ |
| `prompt_templates/*.yaml` | æ–°å¢ | ä¸åŒå¤æ‚åº¦çš„ prompt æ¨¡æ¿ |
| `adapter_registry.py` | æ‰©å±• | æ”¯æŒè‡ªå®šä¹‰ prompt æ³¨å…¥ |
| `run_all_experiments.py` | æ‰©å±• | æ·»åŠ  `--prompt-ablation` flag |

**Prompt å˜ä½“**:
```yaml
# minimal.yaml - æœ€å°ä¿¡æ¯
system: "Select the most relevant tool."
user_template: "Query: {query}\nTools: {tools}\nAnswer:"

# standard.yaml - æ ‡å‡† prompt
system: "You are a tool selection assistant. Given a user query, select the most relevant tools."
user_template: |
  Query: {query}
  Available Tools:
  {tools}
  Select the top-{k} most relevant tools.

# with_examples.yaml - å¸¦ few-shot ç¤ºä¾‹
system: "..."
examples:
  - query: "What's the weather today?"
    tools: ["weather_api", "calendar", "news"]
    answer: ["weather_api"]
user_template: "..."

# with_cot.yaml - å¸¦ Chain-of-Thought
system: "Think step by step before selecting tools."
user_template: |
  Query: {query}
  Tools: {tools}

  Let's think step by step:
  1. What is the user trying to do?
  2. What capabilities are needed?
  3. Which tools match these capabilities?

  Final Selection:
```

**å…³é”®å®ç°**:

```python
# prompt_ablation_exp.py
class PromptAblationExperiment(BaseExperiment):
    """
    Ablation study on prompt design for LLM-based methods.
    """

    PROMPT_VARIANTS = ["minimal", "standard", "with_examples", "with_cot"]

    def run_ablation(self, strategy, challenge):
        """
        å¯¹æ¯ä¸ª prompt å˜ä½“è¿è¡Œå®éªŒ:
        - è®°å½• accuracy, latency, token_count
        - åˆ†æ prompt é•¿åº¦ vs æ€§èƒ½çš„å…³ç³»
        """

    def measure_context_window_effect(self, strategy, max_tokens_list):
        """
        æµ‹è¯•ä¸Šä¸‹æ–‡çª—å£é™åˆ¶çš„å½±å“:
        - é€æ­¥å¢åŠ å€™é€‰å·¥å…·æ•°ç›´åˆ°è¶…å‡ºçª—å£
        - è®°å½• truncation å¯¹æ€§èƒ½çš„å½±å“
        """
```

**è¾“å‡º**:
```
figures/fig_prompt_ablation.pdf              # Prompt å˜ä½“æ€§èƒ½å¯¹æ¯”
figures/fig_prompt_length_vs_accuracy.pdf    # é•¿åº¦ vs å‡†ç¡®ç‡
tables/table_prompt_ablation.tex             # è¯¦ç»†æ¶ˆèç»“æœ
```

---

### 6. Tool Reliability Injection

**ç›®æ ‡**: æµ‹è¯•å·¥å…·å¤±è´¥/å»¶è¿Ÿå¯¹ agent çš„å½±å“

**æ–°å¢æ–‡ä»¶**:
```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ reliability_exp.py               # æ–°å¢
â”œâ”€â”€ mocks/
â”‚   â””â”€â”€ unreliable_tool_wrapper.py       # æ–°å¢ï¼šä¸å¯é å·¥å…·æ¨¡æ‹Ÿ
```

**ä»£ç æ”¹åŠ¨æ¸…å•**:

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | æè¿° |
|------|----------|------|
| `reliability_exp.py` | æ–°å¢ | å®ç°å¯é æ€§æµ‹è¯•å®éªŒ |
| `unreliable_tool_wrapper.py` | æ–°å¢ | æ¨¡æ‹Ÿå·¥å…·å¤±è´¥/å»¶è¿Ÿ |
| `run_all_experiments.py` | æ‰©å±• | æ·»åŠ  `--reliability-test` flag |

**å…³é”®å®ç°**:

```python
# unreliable_tool_wrapper.py
class UnreliableToolWrapper:
    """
    Wrapper to inject failures/delays into tool calls.
    """

    def __init__(self, tool, failure_rate=0.05, latency_spike_rate=0.1):
        self.tool = tool
        self.failure_rate = failure_rate
        self.latency_spike_rate = latency_spike_rate

    def call(self, *args, **kwargs):
        # Random failure
        if random.random() < self.failure_rate:
            raise ToolExecutionError("Simulated failure")

        # Latency spike (2-5x normal)
        if random.random() < self.latency_spike_rate:
            time.sleep(random.uniform(2, 5) * self.base_latency)

        return self.tool.call(*args, **kwargs)

# reliability_exp.py
class ReliabilityExperiment(BaseExperiment):
    """
    Test agent robustness under tool reliability issues.
    """

    FAILURE_RATES = [0.0, 0.05, 0.10, 0.20]
    LATENCY_SPIKE_RATES = [0.0, 0.10, 0.20, 0.30]

    def run_reliability_test(self, strategy):
        """
        æµ‹è¯•ä¸åŒå¤±è´¥ç‡ä¸‹çš„ agent è¡Œä¸º:
        - detect_rate: æ˜¯å¦èƒ½æ£€æµ‹åˆ°å¤±è´¥
        - retry_rate: æ˜¯å¦å°è¯•é‡è¯•
        - recovery_rate: æ˜¯å¦èƒ½æ¢å¤
        - silent_fail_rate: é™é»˜å¤±è´¥ç‡
        """

    def analyze_failure_handling(self, traces):
        """
        åˆ†æå¤±è´¥å¤„ç†ç­–ç•¥:
        - æœ‰æ—  retry æœºåˆ¶
        - æœ‰æ—  fallback ç­–ç•¥
        - é”™è¯¯ä¼ æ’­èŒƒå›´
        """
```

**è¾“å‡º**:
```
figures/fig_reliability_failure_rate.pdf     # å¤±è´¥ç‡ vs æ€§èƒ½
figures/fig_reliability_recovery.pdf         # æ¢å¤èƒ½åŠ›å¯¹æ¯”
tables/table_reliability_results.tex         # è¯¦ç»†å¯é æ€§æµ‹è¯•ç»“æœ
```

---

## ğŸš€ ä¸‰ç±»ï¼šè¶‹åŠ¿å¯¹é½å‹ (Scaling & SOTA Comparison)

### 7. LLM Size Scaling Study

**ç›®æ ‡**: æµ‹è¯•ä¸åŒ LLM å¤§å°å¯¹æ€§èƒ½çš„å½±å“

**æ–°å¢æ–‡ä»¶**:
```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ llm_scaling_exp.py               # æ–°å¢
â”œâ”€â”€ config/
â”‚   â””â”€â”€ model_configs/
â”‚       â”œâ”€â”€ qwen_0.5b.yaml
â”‚       â”œâ”€â”€ qwen_1.5b.yaml
â”‚       â”œâ”€â”€ qwen_7b.yaml
â”‚       â”œâ”€â”€ qwen_14b.yaml
â”‚       â””â”€â”€ gpt4.yaml
```

**ä»£ç æ”¹åŠ¨æ¸…å•**:

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | æè¿° |
|------|----------|------|
| `llm_scaling_exp.py` | æ–°å¢ | å®ç° LLM scaling å®éªŒ |
| `model_configs/*.yaml` | æ–°å¢ | å„æ¨¡å‹é…ç½® |
| `adapter_registry.py` | æ‰©å±• | æ”¯æŒåŠ¨æ€æ¨¡å‹åˆ‡æ¢ |
| `run_all_experiments.py` | æ‰©å±• | æ·»åŠ  `--llm-scaling` flag |

**å…³é”®å®ç°**:

```python
# llm_scaling_exp.py
class LLMScalingExperiment(BaseExperiment):
    """
    Study performance scaling with LLM size.
    """

    MODELS = [
        ("Qwen/Qwen2.5-0.5B-Instruct", "0.5B"),
        ("Qwen/Qwen2.5-1.5B-Instruct", "1.5B"),
        ("Qwen/Qwen2.5-7B-Instruct", "7B"),
        ("Qwen/Qwen2.5-14B-Instruct", "14B"),
        ("gpt-4", "GPT-4"),  # via API
    ]

    def run_scaling_study(self, challenges=["planning"]):
        """
        å¯¹æ¯ä¸ªæ¨¡å‹è¿è¡ŒæŒ‡å®š challenge:
        - è®°å½• accuracy, latency, cost
        - åˆ†ææ˜¯å¦å­˜åœ¨ "emergent ability" è·³è·ƒ
        """

    def detect_emergent_abilities(self, results):
        """
        æ£€æµ‹éçº¿æ€§æå‡:
        - è®¡ç®—å„æ¨¡å‹é—´çš„æ€§èƒ½å¢é‡
        - è¯†åˆ«çªç„¶è§£é”çš„èƒ½åŠ›
        """

    def compute_cost_efficiency(self, results):
        """
        è®¡ç®—æ€§ä»·æ¯”:
        - accuracy_per_dollar
        - accuracy_per_token
        """
```

**è¾“å‡º**:
```
figures/fig_llm_scaling_curve.pdf            # æ¨¡å‹å¤§å° vs æ€§èƒ½
figures/fig_llm_scaling_cost.pdf             # æ€§èƒ½ vs æˆæœ¬
tables/table_llm_scaling.tex                 # è¯¦ç»† scaling æ•°æ®
```

---

### 8. Instruction Quality Sensitivity

**ç›®æ ‡**: æµ‹è¯•æŒ‡ä»¤è´¨é‡å¯¹æ€§èƒ½çš„å½±å“

**æ–°å¢æ–‡ä»¶**:
```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ instruction_sensitivity_exp.py   # æ–°å¢
â”œâ”€â”€ data/
â”‚   â””â”€â”€ instruction_variations.jsonl     # æ–°å¢
```

**ä»£ç æ”¹åŠ¨æ¸…å•**:

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | æè¿° |
|------|----------|------|
| `instruction_sensitivity_exp.py` | æ–°å¢ | å®ç°æŒ‡ä»¤æ•æ„Ÿåº¦å®éªŒ |
| `instruction_variations.jsonl` | æ–°å¢ | ä¸‰ç§è´¨é‡çš„æŒ‡ä»¤æ•°æ® |
| `run_all_experiments.py` | æ‰©å±• | æ·»åŠ  `--instruction-sensitivity` flag |

**æ•°æ®æ ¼å¼**:
```jsonl
{
  "task_id": "weather_query_001",
  "ground_truth": {"tool": "weather_api", "plan": ["get_location", "query_weather"]},
  "instructions": {
    "human_written": "What's the weather like in Beijing today?",
    "synthetic_template": "Query weather information for location: Beijing, date: today",
    "adversarial": "I definitely don't want to know the weather, but if I hypothetically did want to check conditions in Beijing..."
  }
}
```

**å…³é”®å®ç°**:

```python
# instruction_sensitivity_exp.py
class InstructionSensitivityExperiment(BaseExperiment):
    """
    Test robustness to instruction quality variations.
    """

    INSTRUCTION_TYPES = ["human_written", "synthetic_template", "adversarial"]

    def run_sensitivity_test(self, strategy, challenge):
        """
        å¯¹æ¯ç§æŒ‡ä»¤ç±»å‹è¿è¡Œæµ‹è¯•:
        - æ¯”è¾ƒ accuracy å·®å¼‚
        - åˆ†æå“ªç§ç­–ç•¥å¯¹æŒ‡ä»¤è´¨é‡æœ€æ•æ„Ÿ
        """

    def compute_robustness_score(self, results):
        """
        è®¡ç®—é²æ£’æ€§å¾—åˆ†:
        - min_accuracy / max_accuracy æ¯”å€¼
        - å„ç±»å‹é—´çš„æ–¹å·®
        """
```

**è¾“å‡º**:
```
figures/fig_instruction_sensitivity.pdf      # å„æŒ‡ä»¤ç±»å‹æ€§èƒ½å¯¹æ¯”
figures/fig_instruction_robustness.pdf       # é²æ£’æ€§å¾—åˆ†
tables/table_instruction_sensitivity.tex     # è¯¦ç»†æ•æ„Ÿåº¦åˆ†æ
```

---

### 9. In-Context Learning vs Fine-tuning

**ç›®æ ‡**: å¯¹æ¯” ICL å’Œå¾®è°ƒçš„æ•ˆæœ

**æ–°å¢æ–‡ä»¶**:
```
packages/sage-benchmark/src/sage/benchmark/benchmark_agent/
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ icl_vs_finetune_exp.py           # æ–°å¢
â”œâ”€â”€ training/
â”‚   â””â”€â”€ finetune_runner.py               # æ–°å¢ï¼šç®€å•å¾®è°ƒè„šæœ¬
```

**ä»£ç æ”¹åŠ¨æ¸…å•**:

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | æè¿° |
|------|----------|------|
| `icl_vs_finetune_exp.py` | æ–°å¢ | å®ç° ICL vs å¾®è°ƒå¯¹æ¯”å®éªŒ |
| `finetune_runner.py` | æ–°å¢ | ç®€å• LoRA å¾®è°ƒè„šæœ¬ |
| `run_all_experiments.py` | æ‰©å±• | æ·»åŠ  `--icl-finetune` flag |

**å…³é”®å®ç°**:

```python
# icl_vs_finetune_exp.py
class ICLvsFinetuneExperiment(BaseExperiment):
    """
    Compare In-Context Learning vs Fine-tuning approaches.
    """

    def run_icl_variants(self, model, challenge, n_shots=[0, 1, 3, 5, 10]):
        """
        æµ‹è¯•ä¸åŒ shot æ•°é‡çš„ ICL:
        - 0-shot (zero-shot)
        - 1-shot
        - few-shot (3, 5, 10)
        """

    def run_finetune_variants(self, model, challenge, train_sizes=[100, 500, 1000]):
        """
        æµ‹è¯•ä¸åŒè®­ç»ƒæ•°æ®é‡çš„å¾®è°ƒ:
        - 100 samples
        - 500 samples
        - 1000 samples
        """

    def compute_data_efficiency(self, icl_results, ft_results):
        """
        è®¡ç®—æ•°æ®æ•ˆç‡:
        - ICL: shots_needed_for_X_accuracy
        - FT: samples_needed_for_X_accuracy
        """

# finetune_runner.py
class SimpleFinetuneRunner:
    """
    Simple LoRA fine-tuning for benchmark comparison.
    """

    def finetune(self, model_id, train_data, task="planning"):
        """
        ä½¿ç”¨ LoRA è¿›è¡Œè½»é‡å¾®è°ƒ:
        - lora_r: 8
        - lora_alpha: 16
        - epochs: 3
        """
```

**è¾“å‡º**:
```
figures/fig_icl_vs_finetune.pdf              # ICL vs FT å¯¹æ¯”
figures/fig_data_efficiency.pdf              # æ•°æ®æ•ˆç‡æ›²çº¿
tables/table_icl_vs_finetune.tex             # è¯¦ç»†å¯¹æ¯”ç»“æœ
```

---

## ğŸ“Š ç»Ÿä¸€è¾“å‡ºç»“æ„

```
.sage/benchmark/results/
â”œâ”€â”€ all_results.json                         # ç°æœ‰
â”œâ”€â”€ advanced_experiments/                    # æ–°å¢ç›®å½•
â”‚   â”œâ”€â”€ error_analysis/
â”‚   â”‚   â”œâ”€â”€ error_breakdown.json
â”‚   â”‚   â”œâ”€â”€ cascading_analysis.json
â”‚   â”‚   â””â”€â”€ generalization.json
â”‚   â”œâ”€â”€ ablation_studies/
â”‚   â”‚   â”œâ”€â”€ tool_scaling.json
â”‚   â”‚   â”œâ”€â”€ prompt_ablation.json
â”‚   â”‚   â””â”€â”€ reliability.json
â”‚   â””â”€â”€ scaling_studies/
â”‚       â”œâ”€â”€ llm_scaling.json
â”‚       â”œâ”€â”€ instruction_sensitivity.json
â”‚       â””â”€â”€ icl_vs_finetune.json
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ fig1-5 (ç°æœ‰)
â”‚   â”œâ”€â”€ fig6_error_breakdown_*.pdf           # æ–°å¢
â”‚   â”œâ”€â”€ fig7_cascade_*.pdf
â”‚   â”œâ”€â”€ fig8_generalization_*.pdf
â”‚   â”œâ”€â”€ fig9_scaling_*.pdf
â”‚   â”œâ”€â”€ fig10_prompt_*.pdf
â”‚   â”œâ”€â”€ fig11_reliability_*.pdf
â”‚   â”œâ”€â”€ fig12_llm_scaling_*.pdf
â”‚   â”œâ”€â”€ fig13_instruction_*.pdf
â”‚   â””â”€â”€ fig14_icl_finetune_*.pdf
â””â”€â”€ tables/
    â”œâ”€â”€ table1-2 (ç°æœ‰)
    â”œâ”€â”€ table_error_breakdown.tex            # æ–°å¢
    â”œâ”€â”€ table_cascade_analysis.tex
    â”œâ”€â”€ table_generalization.tex
    â”œâ”€â”€ table_scaling.tex
    â”œâ”€â”€ table_prompt_ablation.tex
    â”œâ”€â”€ table_reliability.tex
    â”œâ”€â”€ table_llm_scaling.tex
    â”œâ”€â”€ table_instruction_sensitivity.tex
    â””â”€â”€ table_icl_vs_finetune.tex
```

---

## ğŸ› ï¸ CLI æ‰©å±•

```bash
# ç°æœ‰å‘½ä»¤
sage-bench run                              # åŸºç¡€å®éªŒ
sage-bench run --quick                      # å¿«é€Ÿæ¨¡å¼

# æ–°å¢å‘½ä»¤
sage-bench run --advanced                   # è¿è¡Œæ‰€æœ‰é«˜çº§å®éªŒ
sage-bench run --error-analysis             # ä»…é”™è¯¯åˆ†æ
sage-bench run --cascade-analysis           # ä»…çº§è”åˆ†æ
sage-bench run --generalization             # ä»…æ³›åŒ–æµ‹è¯•
sage-bench run --scaling-study              # å·¥å…·æ•°é‡ scaling
sage-bench run --prompt-ablation            # Prompt æ¶ˆè
sage-bench run --reliability-test           # å¯é æ€§æµ‹è¯•
sage-bench run --llm-scaling                # LLM å¤§å° scaling
sage-bench run --instruction-sensitivity    # æŒ‡ä»¤æ•æ„Ÿåº¦
sage-bench run --icl-finetune               # ICL vs å¾®è°ƒ

# ç»„åˆ
sage-bench run --error-analysis --cascade-analysis  # ç°è±¡åˆ†æç±»
sage-bench run --scaling-study --prompt-ablation    # å˜é‡æ§åˆ¶ç±»
```

---

## ğŸ“… å®ç°ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | å®éªŒ | é¢„ä¼°å·¥ä½œé‡ | ä¾èµ– |
|--------|------|------------|------|
| P0 | 1. Error Type Breakdown | 2å¤© | ç°æœ‰ analyzer |
| P0 | 4. Tool Set Scaling | 2å¤© | ç°æœ‰ selector |
| P1 | 7. LLM Size Scaling | 3å¤© | vLLM æœåŠ¡ |
| P1 | 3. Cross-Task Generalization | 2å¤© | æ•°æ®å‡†å¤‡ |
| P2 | 2. Failure Cascading | 2å¤© | trace æœºåˆ¶ |
| P2 | 5. Prompt Length Ablation | 2å¤© | prompt æ¨¡æ¿ |
| P2 | 8. Instruction Sensitivity | 2å¤© | æ•°æ®å‡†å¤‡ |
| P3 | 6. Tool Reliability | 3å¤© | mock å·¥å…· |
| P3 | 9. ICL vs Fine-tuning | 4å¤© | å¾®è°ƒè„šæœ¬ |

**æ€»é¢„ä¼°**: 22 å¤© (å•äºº)

---

## âœ… å®ç°æ£€æŸ¥æ¸…å•

### Phase 1: åŸºç¡€è®¾æ–½ (Week 1)
- [ ] æ‰©å±• `base_experiment.py` æ·»åŠ  `ActionTrace` æ•°æ®æ¨¡å‹
- [ ] åˆ›å»º `advanced_experiments/` è¾“å‡ºç›®å½•ç»“æ„
- [ ] æ‰©å±• `run_all_experiments.py` æ·»åŠ æ–° flags
- [ ] åˆ›å»ºç»Ÿä¸€çš„ figure/table ç”Ÿæˆæ¨¡æ¿

### Phase 2: ç°è±¡åˆ†æ (Week 2)
- [ ] å®ç° `error_breakdown_analyzer.py`
- [ ] å®ç° `cascading_failure_analyzer.py`
- [ ] å®ç° `generalization_analyzer.py`
- [ ] åˆ›å»ºè¯­ä¹‰å˜ä½“æµ‹è¯•æ•°æ®

### Phase 3: å˜é‡æ§åˆ¶ (Week 3)
- [ ] å®ç° `scaling_exp.py` + noise tools æ•°æ®
- [ ] å®ç° `prompt_ablation_exp.py` + prompt æ¨¡æ¿
- [ ] å®ç° `reliability_exp.py` + å·¥å…·æ¨¡æ‹Ÿå™¨

### Phase 4: è¶‹åŠ¿å¯¹é½ (Week 4)
- [ ] å®ç° `llm_scaling_exp.py` + æ¨¡å‹é…ç½®
- [ ] å®ç° `instruction_sensitivity_exp.py` + æ•°æ®
- [ ] å®ç° `icl_vs_finetune_exp.py` + å¾®è°ƒè„šæœ¬

### Phase 5: é›†æˆæµ‹è¯• (Week 5)
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•æ‰€æœ‰æ–°å®éªŒ
- [ ] ç”Ÿæˆç¤ºä¾‹ figures å’Œ tables
- [ ] æ›´æ–°æ–‡æ¡£å’Œ README
- [ ] CI/CD é›†æˆ

---

## ğŸ“ Notes

1. **æ§åˆ¶å˜é‡**: æ‰€æœ‰æ–°å®éªŒç»§ç»­ä½¿ç”¨ `BENCHMARK_EMBEDDING_MODEL` å’Œ `BENCHMARK_LLM_TEMPERATURE`
2. **éšæœºç§å­**: ç»§ç»­ä½¿ç”¨ SEED=42 ç¡®ä¿å¯å¤ç°
3. **æ•°æ®æ ¼å¼**: æ–°æ•°æ®æ–‡ä»¶ç»Ÿä¸€ä½¿ç”¨ JSONL æ ¼å¼
4. **è¾“å‡ºæ ¼å¼**: æ‰€æœ‰ figures åŒæ—¶è¾“å‡º PDF + PNGï¼Œtables è¾“å‡º LaTeX
