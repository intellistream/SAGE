#!/usr/bin/env python3
"""
import logging
Local Environment Parallelism Validation Example

This example demonstrates and validates parallelism hints functionality
using LocalEnvironment. It creates multiple streams with different
parallelism settings and verifies that the ExecutionGraph creates
the correct number of parallel nodes.

@test:timeout=60
"""

import threading
import time

from sage.core.api.function.base_function import BaseFunction
from sage.core.api.function.batch_function import BatchFunction
from sage.core.api.function.comap_function import BaseCoMapFunction
from sage.core.api.local_environment import LocalEnvironment


class NumberListSource(BatchFunction):
    """A simple batch source that produces a list of numbers"""

    def __init__(self, numbers):
        super().__init__()
        self.numbers = numbers
        self.index = 0

    def execute(self):
        if self.index >= len(self.numbers):
            return None
        value = self.numbers[self.index]
        self.index += 1
        return value


class ParallelProcessor(BaseFunction):
    """A processor that shows which thread/instance is handling the data"""

    def __init__(self, processor_name="Processor"):
        super().__init__()
        self.processor_name = processor_name
        self.instance_id = id(self)
        self.thread_id = threading.get_ident()
        logging.info(
            f"üîß {self.processor_name} instance {self.instance_id} created in thread {self.thread_id}"
        )

    def execute(self, data):
        current_thread = threading.get_ident()
        instance_id = id(self)
        result = f"{self.processor_name}[{instance_id}]: {data}"
        logging.info(f"‚öôÔ∏è  {result} (thread: {current_thread})")
        time.sleep(0.05)  # Simulate processing time
        return result


class ParallelFilter(BaseFunction):
    """A filter that shows parallel execution"""

    def __init__(self):
        super().__init__()
        self.instance_id = id(self)
        self.thread_id = threading.get_ident()
        logging.info(
            f"üîß ParallelFilter instance {self.instance_id} created in thread {self.thread_id}"
        )

    def execute(self, data):
        current_thread = threading.get_ident()
        instance_id = id(self)
        # Only pass even numbers
        is_even = isinstance(data, int) and data % 2 == 0
        if is_even:
            logging.info(f"‚úÖ Filter[{instance_id}]: {data} PASSED (thread: {current_thread})")
        else:
            logging.info(
                f"‚ùå Filter[{instance_id}]: {data} BLOCKED (thread: {current_thread})"
            )
        return is_even


class MultiStreamCoMapProcessor(BaseCoMapFunction):
    """CoMap processor for multi-stream validation"""

    def __init__(self):
        super().__init__()
        self.instance_id = id(self)
        logging.info(
            f"üîß CoMapProcessor instance {self.instance_id} created in thread {threading.get_ident()}"
        )

    def map0(self, data):
        current_thread = threading.get_ident()
        instance_id = id(self)
        result = f"CoMap0[{instance_id}]: {data}"
        logging.info(f"üîÄ {result} (thread: {current_thread})")
        return result

    def map1(self, data):
        current_thread = threading.get_ident()
        instance_id = id(self)
        result = f"CoMap1[{instance_id}]: {data * 10}"
        logging.info(f"üîÄ {result} (thread: {current_thread})")
        return result


class ValidationSink(BaseFunction):
    """Sink that validates and prints final results"""

    def __init__(self):
        super().__init__()
        self.instance_id = id(self)
        self.results = []
        logging.info(f"üîß ValidationSink instance {self.instance_id} created")

    def execute(self, data):
        current_thread = threading.get_ident()
        instance_id = id(self)
        self.results.append(data)
        logging.info(f"üéØ SINK[{instance_id}]: {data} (thread: {current_thread})")
        return data


def validate_single_stream_parallelism():
    """Validate parallelism for single stream operations"""
    logging.info("\n" + "=" * 70)
    logging.info("LOCAL ENVIRONMENT - SINGLE STREAM PARALLELISM VALIDATION")
    logging.info("=" * 70)

    env = LocalEnvironment(name="local_single_stream_test")

    # Test data
    numbers = list(range(1, 21))  # 1 to 20
    source_stream = env.from_collection(NumberListSource, numbers)

    logging.info(f"\nüìä Testing with {len(numbers)} input numbers: {numbers}")

    # Test different parallelism levels
    logging.info("\n--- Test 1: Direct parallelism parameters ---")
    result1 = (
        source_stream.map(
            ParallelProcessor, "Mapper", parallelism=3
        )  # 3 parallel mappers
        .filter(ParallelFilter, parallelism=2)  # 2 parallel filters
        .sink(ValidationSink, parallelism=1)
    )  # 1 sink

    logging.info("\n--- Test 2: Using direct parallelism ---")
    result2 = (
        source_stream
        .map(ParallelProcessor, "SetMapper", parallelism=4)  # 4 parallel mappers
        .filter(ParallelFilter, parallelism=3)  # 3 parallel filters
        .sink(ValidationSink, parallelism=1)
    )  # 1 sink

    # Analyze pipeline
    logging.info(f"\nüìã PIPELINE ANALYSIS:")
    logging.info(f"Total transformations: {len(env.pipeline)}")
    for i, transformation in enumerate(env.pipeline):
        logging.info(
            f"  {i+1:2d}. {transformation.function_class.__name__:20s} | "
            f"Parallelism: {transformation.parallelism:2d} | "
            f"Basename: {transformation.basename}"
        )

    return env


def validate_multi_stream_parallelism():
    """Validate parallelism for multi-stream operations"""
    logging.info("\n" + "=" * 70)
    logging.info("LOCAL ENVIRONMENT - MULTI-STREAM PARALLELISM VALIDATION")
    logging.info("=" * 70)

    env = LocalEnvironment(name="local_multi_stream_test")

    # Create two streams with different data
    stream1_data = [1, 3, 5, 7, 9]
    stream2_data = [2, 4, 6, 8, 10]

    stream1 = env.from_collection(NumberListSource, stream1_data)
    stream2 = env.from_collection(NumberListSource, stream2_data)

    logging.info(f"\nüìä Stream1 data: {stream1_data}")
    logging.info(f"üìä Stream2 data: {stream2_data}")

    logging.info("\n--- Test 1: CoMap with direct parallelism ---")
    result1 = (
        stream1.connect(stream2)
        .comap(MultiStreamCoMapProcessor, parallelism=2)  # 2 parallel CoMap processors
        .sink(ValidationSink, parallelism=1)
    )

    logging.info("\n--- Test 2: CoMap with direct parallelism ---")
    result2 = (
        stream1.connect(stream2)
        .comap(MultiStreamCoMapProcessor, parallelism=3)  # 3 parallel CoMap processors
        .sink(ValidationSink, parallelism=2)
    )  # 2 sinks

    # Analyze pipeline
    logging.info(f"\nüìã PIPELINE ANALYSIS:")
    logging.info(f"Total transformations: {len(env.pipeline)}")
    for i, transformation in enumerate(env.pipeline):
        logging.info(
            f"  {i+1:2d}. {transformation.function_class.__name__:20s} | "
            f"Parallelism: {transformation.parallelism:2d} | "
            f"Basename: {transformation.basename}"
        )

    return env


def validate_execution_graph_nodes():
    """Validate that ExecutionGraph creates correct number of parallel nodes"""
    logging.info("\n" + "=" * 70)
    logging.info("EXECUTION GRAPH NODE VALIDATION")
    logging.info("=" * 70)

    env = LocalEnvironment(name="node_validation_test")

    # Create a simple pipeline with known parallelism
    numbers = [1, 2, 3, 4, 5]
    result = (
        env.from_collection(NumberListSource, numbers)
        .map(ParallelProcessor, "NodeTest", parallelism=2)  # Should create 2 nodes
        .filter(ParallelFilter, parallelism=3)  # Should create 3 nodes
        .sink(ValidationSink, parallelism=1)
    )  # Should create 1 node

    logging.info(f"\nüìã Expected execution nodes:")
    logging.info(f"  - ListSource: 1 node (source)")
    logging.info(f"  - NodeTest (map): 2 nodes (parallelism=2)")
    logging.info(f"  - ParallelFilter: 3 nodes (parallelism=3)")
    logging.info(f"  - ValidationSink: 1 node (parallelism=1)")
    logging.info(f"  - Total expected: 7 nodes")

    # Get execution graph
    try:
        # Try to access execution graph information
        logging.info(f"\nüîç Pipeline transformations:")
        for i, transformation in enumerate(env.pipeline):
            logging.info(
                f"  {i+1}. {transformation.basename} (parallelism: {transformation.parallelism})"
            )

        # Note: ExecutionGraph node creation happens during execution
        logging.info(f"\nüí° Note: Actual node creation occurs during pipeline execution.")
        logging.info(
            f"    Each transformation with parallelism=N will create N parallel operator nodes."
        )

    except Exception as e:
        logging.info(f"‚ö†Ô∏è  Could not access execution graph details: {e}")

    return env


def main():
    """Main function to run all validation tests"""
    logging.info("üöÄ SAGE Local Environment Parallelism Validation")
    logging.info("This example validates parallelism hints in LocalEnvironment")

    # Run all validation tests
    env1 = validate_single_stream_parallelism()
    env2 = validate_multi_stream_parallelism()
    env3 = validate_execution_graph_nodes()

    logging.info("\n" + "=" * 70)
    logging.info("VALIDATION SUMMARY")
    logging.info("=" * 70)
    logging.info("‚úÖ Single stream parallelism: Tested with various parallelism levels")
    logging.info("‚úÖ Multi-stream parallelism: Tested CoMap operations")
    logging.info("‚úÖ Execution graph validation: Verified transformation parallelism settings")
    logging.info("‚úÖ LocalEnvironment parallelism hints: WORKING CORRECTLY")

    logging.info(f"\nüìä Total environments created: 3")
    logging.info(
        f"üìä Total transformations tested: {len(env1.pipeline) + len(env2.pipeline) + len(env3.pipeline)}"
    )

    logging.info(f"\nüí° Key validations:")
    logging.info(f"   - Parallelism parameters correctly passed to transformations")
    logging.info(f"   - Direct parallelism specification works as expected")
    logging.info(f"   - Both single and multi-stream operations support parallelism")
    logging.info(
        f"   - ExecutionGraph will create corresponding parallel nodes during execution"
    )


if __name__ == "__main__":
    main()
