# @test:require-api
# @test:timeout=180
"""æµ‹è¯• Pipeline-as-Service with RAG Memory - Sequential Question Processing

æ­¤ç¤ºä¾‹éœ€è¦ OpenAI API å¯†é’¥ï¼Œä¸”éœ€è¦è¾ƒé•¿è¿è¡Œæ—¶é—´ï¼ˆçº¦ 60-90 ç§’ï¼‰ã€‚
åœ¨ CI ç¯å¢ƒä¸­ä¼šè¢«è‡ªåŠ¨è·³è¿‡ã€‚
"""

import queue
import sys
import time
from pathlib import Path

import yaml
from rag_memory_service import RAGMemoryService

from sage.common.core.functions.map_function import MapFunction
from sage.common.core.functions.sink_function import SinkFunction
from sage.common.core.functions.source_function import SourceFunction
from sage.common.utils.logging.custom_logger import CustomLogger
from sage.kernel.api.local_environment import LocalEnvironment
from sage.kernel.api.service.base_service import BaseService
from sage.middleware.operators.rag import OpenAIGenerator, QAPromptor


class PipelineBridge:
    def __init__(self):
        self._queue = queue.Queue()
        self._closed = False

    def submit(self, payload):
        if self._closed:
            raise RuntimeError("PipelineBridge is closed")
        response_q = queue.Queue()
        self._queue.put({"payload": payload, "response_queue": response_q})
        return response_q

    def next(self, timeout=0.1):
        if self._closed:
            return None
        try:
            return self._queue.get(timeout=timeout)
        except queue.Empty:
            return None

    def close(self):
        self._closed = True


# ==================== æ£€ç´¢æœåŠ¡ ====================


class RetrievalSource(SourceFunction):
    def __init__(self, bridge):
        super().__init__()
        self.bridge = bridge

    def execute(self, data=None):
        # å¦‚æœ bridge å·²å…³é—­ï¼Œåœæ­¢ç”Ÿæˆæ•°æ®
        if self.bridge._closed:
            return None
        request = self.bridge.next(timeout=0.1)
        return request if request else None


class RetrievalMap(MapFunction):
    def execute(self, data):
        if not data:
            return None
        question = data["payload"]["question"]
        # è°ƒç”¨ RAG Memory æœåŠ¡è¿›è¡Œæ£€ç´¢
        results = self.call_service("rag_memory", question, method="retrieve")

        if results:
            print(f"ğŸ” æ£€ç´¢åˆ° {len(results)} æ¡å†å²è®°å¿†:")
            for i, r in enumerate(results, 1):
                hist_q = r["history_query"]
                if len(hist_q) > 50:
                    hist_q = hist_q[:50] + "..."
                print(f"   {i}. {hist_q}")
        else:
            print("ğŸ” æœªæ£€ç´¢åˆ°å†å²è®°å¿†ï¼ˆç´¢å¼•ä¸ºç©ºï¼‰")

        context = results
        return {
            "payload": {"question": question, "context": context},
            "response_queue": data["response_queue"],
        }


class RetrievalSink(SinkFunction):
    def execute(self, data):
        if not data:
            return
        data["response_queue"].put(data["payload"])


class RetrievalService(BaseService):
    def __init__(self, bridge):
        super().__init__()
        self.bridge = bridge

    def retrieve(self, data):
        response_q = self.bridge.submit(data)
        return response_q.get(timeout=10.0)


# ==================== å†™å…¥æœåŠ¡ ====================


class WritingSource(SourceFunction):
    def __init__(self, bridge):
        super().__init__()
        self.bridge = bridge

    def execute(self, data=None):
        request = self.bridge.next(timeout=0.1)
        return request if request else None


class WritingMap(MapFunction):
    def __init__(self, config):
        super().__init__()
        self.config = config

    def execute(self, data):
        if not data:
            return None
        payload = data["payload"]
        question = payload["question"]
        payload.get("context", [])

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ promptor é…ç½®
        promptor_config = self.config.get("promptor", {})
        if not promptor_config.get("template"):
            promptor_config["template"] = """ä½ æ˜¯ä¸€ä½å…·å¤‡é•¿æœŸè®°å¿†çš„ä¸ªäººå¥åº·åŠ©æ‰‹ã€‚

{%- if external_corpus %}
ä»¥ä¸‹æ˜¯ç›¸å…³çš„å†å²é—®ç­”ï¼š
{{ external_corpus }}
{%- endif %}"""

        promptor = QAPromptor(promptor_config)
        prompted = promptor.execute(payload)

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ generator é…ç½®
        generator_config = self.config.get("generator", {}).get("vllm", {})
        generator = OpenAIGenerator(generator_config)
        generator.ctx = self.ctx
        answer = generator.execute(prompted)

        # å†™å…¥è®°å¿†
        self.call_service(
            "rag_memory",
            question,
            {"answer": answer, "topic": "å¥åº·-ä¸ªæ€§åŒ–"},
            method="insert",
        )
        print("ğŸ’¾ å·²å°†é—®ç­”å­˜å…¥è®°å¿†ç´¢å¼•")

        return {
            "payload": {"question": question, "answer": answer},
            "response_queue": data["response_queue"],
        }


class WritingSink(SinkFunction):
    def execute(self, data):
        if not data:
            return
        data["response_queue"].put(data["payload"])


class WritingService(BaseService):
    def __init__(self, bridge, config):
        super().__init__()
        self.bridge = bridge
        self.config = config

    def write(self, data):
        response_q = self.bridge.submit(data)
        return response_q.get(timeout=10.0)


# ==================== QA Pipeline Serviceï¼ˆæ£€ç´¢+ç”Ÿæˆ+å†™å…¥ï¼‰====================


class QAPipelineSource(SourceFunction):
    """ä» bridge æ¥æ”¶ QA è¯·æ±‚"""

    def __init__(self, bridge):
        super().__init__()
        self.bridge = bridge

    def execute(self, data=None):
        # å¦‚æœ bridge å·²å…³é—­ï¼Œåœæ­¢ç”Ÿæˆæ•°æ®
        if self.bridge._closed:
            return None
        request = self.bridge.next(timeout=0.1)
        return request if request else None


class QAPipelineMap(MapFunction):
    """æ‰§è¡Œæ£€ç´¢ã€ç”Ÿæˆç­”æ¡ˆã€å†™å…¥è®°å¿†çš„å®Œæ•´æµç¨‹"""

    def __init__(self, config):
        super().__init__()
        self.config = config

    def execute(self, data):
        if not data:
            return None

        payload = data["payload"]
        question = payload["question"]

        # æ­¥éª¤ 1: æ£€ç´¢å†å²è®°å¿†
        retrieval_result = self.call_service(
            "retrieval_service", {"question": question}, method="retrieve"
        )
        context = retrieval_result["context"]

        # æ­¥éª¤ 2: å‡†å¤‡ prompt
        promptor_config = self.config.get("promptor", {})
        if not promptor_config.get("template"):
            promptor_config["template"] = """ä½ æ˜¯ä¸€ä½å…·å¤‡é•¿æœŸè®°å¿†çš„ä¸ªäººå¥åº·åŠ©æ‰‹ã€‚

{%- if external_corpus %}
ä»¥ä¸‹æ˜¯ç›¸å…³çš„å†å²é—®ç­”ï¼š
{{ external_corpus }}
{%- endif %}"""

        promptor = QAPromptor(promptor_config)
        prompted = promptor.execute({"question": question, "external_corpus": context})

        # æ­¥éª¤ 3: ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ generator é…ç½®ï¼‰
        generator_config = self.config.get("generator", {}).get("vllm", {})
        generator = OpenAIGenerator(generator_config)
        generator.ctx = self.ctx
        answer = generator.execute(prompted)

        # æ­¥éª¤ 4: å†™å…¥è®°å¿†
        self.call_service(
            "rag_memory",
            question,
            {"answer": answer, "topic": "å¥åº·-ä¸ªæ€§åŒ–"},
            method="insert",
        )

        return {
            "payload": {"question": question, "answer": answer, "context": context},
            "response_queue": data["response_queue"],
        }


class QAPipelineSink(SinkFunction):
    """å°†ç­”æ¡ˆè¿”å›ç»™è°ƒç”¨è€…"""

    def execute(self, data):
        if not data:
            return
        data["response_queue"].put(data["payload"])


class QAPipelineService(BaseService):
    """QA Pipeline Serviceï¼šæ¥æ”¶é—®é¢˜ï¼Œè¿”å›ç­”æ¡ˆ"""

    def __init__(self, bridge, config):
        super().__init__()
        self.bridge = bridge
        self.config = config

    def process(self, question_data):
        """å¤„ç†å•ä¸ªé—®é¢˜"""
        response_q = self.bridge.submit(question_data)
        return response_q.get(timeout=120.0)


# ==================== Controller Pipelineï¼ˆé¡ºåºå‘é€é—®é¢˜ï¼‰====================


class QuestionController(SourceFunction):
    """é¡ºåºå‘é€é—®é¢˜ï¼Œæ¯æ¬¡åªå‘é€ä¸€ä¸ª"""

    def __init__(self, config):
        super().__init__()
        self.questions = config.get("questions")
        self.max = config.get("max_index", len(self.questions))
        self.index = 0

    def execute(self, data=None):
        if self.index >= self.max:
            return None

        q = self.questions[self.index]
        self.index += 1

        # ä¸åœ¨è¿™é‡Œæ‰“å°ï¼Œè®© ProcessQuestion æ‰“å°ï¼Œè¿™æ ·æ‰èƒ½ä¿è¯ä¸€é—®ä¸€ç­”
        return {"question": q, "index": self.index, "total": self.max}


class ProcessQuestion(MapFunction):
    """è°ƒç”¨ QA Pipeline Service å¤„ç†é—®é¢˜"""

    def execute(self, data):
        if not data:
            return None

        question = data["question"]
        index = data["index"]
        total = data.get("total", index)

        # åœ¨è°ƒç”¨æœåŠ¡ä¹‹å‰æ‰“å°é—®é¢˜
        print(f"\n{'=' * 60}")
        print(f"ğŸ“ é—®é¢˜ {index}/{total}: {question}")
        print(f"{'=' * 60}")

        # è°ƒç”¨ QA Pipeline Serviceï¼ˆé˜»å¡ç­‰å¾…ç­”æ¡ˆï¼‰
        result = self.call_service("qa_pipeline", {"question": question}, timeout=180.0)

        result["index"] = index
        return result


class DisplayAnswer(SinkFunction):
    """æ˜¾ç¤ºç­”æ¡ˆ"""

    def __init__(self, bridges=None, total_questions=5):
        super().__init__()
        self.bridges = bridges or []
        self.total_questions = total_questions
        self.processed_count = 0

    @staticmethod
    def _render_markdown(text):
        """ç®€å•çš„ Markdown æ¸²æŸ“ï¼Œç”¨äºç»ˆç«¯æ˜¾ç¤º"""
        import re

        lines = text.split("\n")
        formatted_lines = []

        for line in lines:
            # å¤„ç† ### ä¸‰çº§æ ‡é¢˜
            if line.startswith("###"):
                # å»æ‰ ### å¹¶åŠ ç²—æ•´è¡Œ
                title = line.replace("###", "").strip()
                line = f"\033[1m{title}\033[0m"

            # å¤„ç† **åŠ ç²—** (ä½†è·³è¿‡å·²ç»è¢«æ ‡é¢˜å¤„ç†è¿‡çš„è¡Œ)
            elif "**" in line:
                line = re.sub(r"\*\*(.+?)\*\*", r"\033[1m\1\033[0m", line)

            # å¤„ç†æ•°å­—åˆ—è¡¨é¡¹
            if re.match(r"^\d+\.\s+", line):
                # ç»™æ•°å­—åŠ ç²—
                line = re.sub(r"^(\d+)\.\s+", r"\033[1m\1.\033[0m ", line)

            # å¤„ç†ç¼©è¿›çš„ç ´æŠ˜å·åˆ—è¡¨é¡¹
            elif re.match(r"^\s+-\s+", line):
                # ç»™ç ´æŠ˜å·åŠ ç²—
                line = re.sub(r"^(\s+)-\s+", r"\1\033[1m-\033[0m ", line)

            formatted_lines.append(line)

        return "\n".join(formatted_lines)

    def execute(self, data):
        if not data:
            return

        data.get("question", "")
        answer_data = data.get("answer", {})
        context = data.get("context", [])
        data.get("index", 0)

        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„å†å²
        if context:
            print(f"\nğŸ” æ£€ç´¢åˆ° {len(context)} æ¡å†å²è®°å¿†:")
            for i, item in enumerate(context[:5], 1):  # æœ€å¤šæ˜¾ç¤º5æ¡
                hist_q = item.get("history_query", "")
                if len(hist_q) > 60:
                    hist_q = hist_q[:60] + "..."
                print(f"   {i}. {hist_q}")
        else:
            print("\nğŸ” æœªæ£€ç´¢åˆ°å†å²è®°å¿†ï¼ˆç´¢å¼•ä¸ºç©ºï¼‰")

        print("ğŸ’¾ å·²å°†é—®ç­”å­˜å…¥è®°å¿†ç´¢å¼•")

        # æ˜¾ç¤ºç­”æ¡ˆ
        if isinstance(answer_data, dict):
            answer_text = answer_data.get("generated", str(answer_data))
            generate_time = answer_data.get("generate_time", 0)
        else:
            answer_text = str(answer_data)
            generate_time = 0

        # æ¸²æŸ“ Markdown
        rendered_answer = self._render_markdown(answer_text)

        print(f"\n{'=' * 60}")
        print("ğŸ’¡ AI å›ç­”:")
        print(f"{'=' * 60}")
        print(rendered_answer)
        print(f"{'=' * 60}")
        if generate_time > 0:
            print(f"â±ï¸  ç”Ÿæˆè€—æ—¶: {generate_time:.2f}ç§’")
        print()

        # æ›´æ–°å¤„ç†è®¡æ•°
        self.processed_count += 1

        # å¦‚æœæ‰€æœ‰é—®é¢˜éƒ½å¤„ç†å®Œäº†ï¼Œå…³é—­æ‰€æœ‰ bridges
        if self.processed_count >= self.total_questions:
            print(f"\nâœ… æ‰€æœ‰ {self.total_questions} ä¸ªé—®é¢˜å·²å¤„ç†å®Œæˆï¼Œå…³é—­ bridges...")
            for bridge in self.bridges:
                bridge.close()
            print("âœ… Bridges å·²å…³é—­ï¼ŒPipeline å³å°†åœæ­¢...")


def main():
    sys.stdout.flush()
    sys.stderr.flush()
    print("=== main() ç¬¬ 1 è¡Œ ===", file=sys.stderr, flush=True)

    script_dir = Path(__file__).parent
    print("=== main() ç¬¬ 2 è¡Œ ===", file=sys.stderr, flush=True)

    config_file = script_dir / "config" / "config_rag_memory_pipeline.yaml"
    print(f"=== é…ç½®æ–‡ä»¶: {config_file} ===", file=sys.stderr, flush=True)

    if not config_file.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}", file=sys.stderr)
        sys.exit(1)

    print("=== å¼€å§‹åŠ è½½é…ç½® ===", file=sys.stderr, flush=True)
    with open(config_file) as f:
        config = yaml.safe_load(f)
    print("=== é…ç½®åŠ è½½å®Œæˆ ===", file=sys.stderr, flush=True)

    print("åˆ›å»ºç¯å¢ƒ...")
    env = LocalEnvironment("rag_memory_pipeline")

    try:
        # æ³¨å†Œ RAG Memory æœåŠ¡
        print("æ³¨å†Œ RAG Memory æœåŠ¡...")
        env.register_service("rag_memory", RAGMemoryService, config["rag_config"])

        print("åˆ›å»º Pipeline Bridges...")
        retrieval_bridge = PipelineBridge()
        qa_pipeline_bridge = PipelineBridge()

        # æ³¨å†ŒæœåŠ¡
        print("æ³¨å†ŒæœåŠ¡...")
        env.register_service("retrieval_service", RetrievalService, retrieval_bridge)
        env.register_service(
            "qa_pipeline", QAPipelineService, qa_pipeline_bridge, config
        )

        # æ£€ç´¢ Pipelineï¼ˆä¸º QA Pipeline æä¾›æ£€ç´¢åŠŸèƒ½ï¼‰
        print("åˆ›å»ºæ£€ç´¢ Pipeline...")
        env.from_source(RetrievalSource, retrieval_bridge).map(RetrievalMap).sink(
            RetrievalSink
        )

        # QA Pipelineï¼ˆæ£€ç´¢ + ç”Ÿæˆ + å†™å…¥ï¼‰
        print("åˆ›å»º QA Pipeline...")
        env.from_source(QAPipelineSource, qa_pipeline_bridge).map(
            QAPipelineMap, config
        ).sink(QAPipelineSink)

        # Controller Pipelineï¼ˆé¡ºåºå‘é€é—®é¢˜ï¼‰
        print("åˆ›å»º Controller Pipeline...")
        total_questions = config["source"].get("max_index", 5)
        bridges = [retrieval_bridge, qa_pipeline_bridge]
        env.from_source(QuestionController, config["source"]).map(ProcessQuestion).sink(
            DisplayAnswer, bridges, total_questions
        )

        print("ğŸš€ å¯åŠ¨ RAG Memory Pipeline...")
        env.submit(
            autostop=False
        )  # ä½¿ç”¨ autostop=Falseï¼Œå› ä¸º Service Pipelines ä¼šæŒç»­è½®è¯¢

        # ç­‰å¾…è¶³å¤Ÿçš„æ—¶é—´è®©æ‰€æœ‰é—®é¢˜å¤„ç†å®Œæˆ
        # æ¯ä¸ªé—®é¢˜å¤§çº¦éœ€è¦ 8-10 ç§’ï¼ˆæ£€ç´¢ + ç”Ÿæˆ + å†™å…¥ï¼‰
        expected_time = total_questions * 10 + 5  # ç»™æ¯ä¸ªé—®é¢˜ 10 ç§’ + 5 ç§’ç¼“å†²
        print(f"â³ ç­‰å¾… {expected_time} ç§’è®©æ‰€æœ‰é—®é¢˜å¤„ç†å®Œæˆ...")
        time.sleep(expected_time)

        print("âœ… Pipeline æ‰§è¡Œå®Œæˆ!")

    finally:
        # ============================================================
        # ã€é‡è¦ã€‘æ¸…ç†èµ„æº - æœ€ä½³å®è·µ
        # ============================================================
        # åœ¨åº”ç”¨ç»“æŸæ—¶ï¼ŒåŠ¡å¿…è°ƒç”¨ env.stop() + env.close() æ¥ä¼˜é›…åœ°å…³é—­æ‰€æœ‰èµ„æºï¼š
        # 1. env.stop() - åœæ­¢æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ Pipeline
        # 2. env.close() - å…³é—­æ‰€æœ‰æ³¨å†Œçš„ Service å¹¶é‡Šæ”¾èµ„æº
        # 3. é˜²æ­¢èµ„æºæ³„æ¼å’Œåƒµå°¸è¿›ç¨‹
        #
        # ä½¿ç”¨ try-finally ç¡®ä¿å³ä½¿å‘ç”Ÿå¼‚å¸¸ä¹Ÿèƒ½æ¸…ç†èµ„æºã€‚
        # è¿™æ˜¯å¼€å‘ SAGE åº”ç”¨çš„æ¨èåšæ³•ï¼Œå¯ä»¥å‡å°‘ bug å’Œæ„å¤–è¡Œä¸ºã€‚
        # ============================================================
        print("ğŸ›‘ åœæ­¢ Pipeline...")
        env.stop()

        print("ğŸ§¹ æ¸…ç†ç¯å¢ƒèµ„æº...")
        env.close()
        print("âœ… ç¯å¢ƒå·²æ¸…ç†ï¼Œç¨‹åºæ­£å¸¸é€€å‡º")


if __name__ == "__main__":
    print("=== ç¨‹åºå¼€å§‹æ‰§è¡Œ ===", flush=True)
    CustomLogger.disable_global_console_debug()
    print("=== CustomLogger å·²ç¦ç”¨ ===", flush=True)
    main()
    print("=== ç¨‹åºæ‰§è¡Œå®Œæ¯• ===", flush=True)
