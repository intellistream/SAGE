#!/usr/bin/env python3
"""
Real SAGE MemoryService pipeline demo (no mocks)

This example builds a SAGE pipeline that uses the real KV, VDB (FAISS), and Graph services,
orchestrated by MemoryService. It demonstrates storing conversations as memories and then
querying them via vector similarity search, all through self.call_service within pipeline
functions.

Requirements:
  - Python 3.10+
  - faiss-cpu installed (pip install faiss-cpu)

Quick run:
  - python examples/memory/test_real_memory_service.py

Notes:
  - Services are registered via env.register_service_factory(name, factory)
  - Pipeline uses env.from_batch(...).map(...).sink(...)
  - Service calls happen via self.call_service["memory_service"].<method>
"""

import os
import time
from typing import Any, Dict, List

from sage.common.utils.logging.custom_logger import CustomLogger
from sage.core.api.function.base_function import BaseFunction
from sage.core.api.local_environment import LocalEnvironment
from sage.middleware.services.graph.graph_service import \
    create_graph_service_factory
from sage.middleware.services.kv.kv_service import create_kv_service_factory
from sage.middleware.services.memory.memory_service import \
    create_memory_service_factory
from sage.middleware.services.vdb.vdb_service import create_vdb_service_factory


def mock_embedding(text: str) -> List[float]:
    """Deterministic mock embedding based on content hash (demo only)."""
    import hashlib

    import numpy as np

    h = hashlib.md5(text.encode()).hexdigest()
    seed = int(h, 16) % (2**32)
    rng = np.random.default_rng(seed)
    return rng.random(384, dtype=np.float32).tolist()


def create_sample_conversations() -> List[Dict[str, Any]]:
    return [
        {
            "user_message": "ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹ï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ",
            "ai_response": "ä½ å¥½ï¼å­¦ä¹ Pythonç¼–ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚è¦å¼€å§‹å­¦ä¹ ï¼Œæˆ‘å»ºè®®ï¼š1. å®‰è£…Pythonç¯å¢ƒ 2. å­¦ä¹ åŸºç¡€è¯­æ³• 3. ç»ƒä¹ ç¼–å†™ç®€å•ç¨‹åº",
            "topic": "programming",
        },
        {
            "user_message": "Pythonä¸­å˜é‡å’Œæ•°æ®ç±»å‹æœ‰å“ªäº›ï¼Ÿ",
            "ai_response": "Pythonæœ‰ä»¥ä¸‹åŸºæœ¬æ•°æ®ç±»å‹ï¼š1. æ•´æ•°(int) 2. æµ®ç‚¹æ•°(float) 3. å­—ç¬¦ä¸²(str) 4. å¸ƒå°”å€¼(bool) 5. åˆ—è¡¨(list) 6. å…ƒç»„(tuple) 7. å­—å…¸(dict)",
            "topic": "programming",
        },
        {
            "user_message": "æˆ‘å¯¹æœºå™¨å­¦ä¹ å¾ˆæ„Ÿå…´è¶£ï¼Œæœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ",
            "ai_response": "æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰æ™¯çš„é¢†åŸŸï¼å»ºè®®ä»ä»¥ä¸‹æ–¹é¢å¼€å§‹ï¼š1. å­¦ä¹ PythonåŸºç¡€ 2. æŒæ¡æ•°å­¦åŸºç¡€ï¼ˆçº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºï¼‰3. å­¦ä¹ scikit-learnç­‰åº“ 4. å®è·µé¡¹ç›®",
            "topic": "ai",
        },
        {
            "user_message": "å‘¨æœ«æˆ‘æƒ³æ”¾æ¾ä¸€ä¸‹ï¼Œæœ‰ä»€ä¹ˆæ¨èï¼Ÿ",
            "ai_response": "å‘¨æœ«æ”¾æ¾å¾ˆé‡è¦ï¼å¯ä»¥è€ƒè™‘ï¼š1. æˆ·å¤–æ´»åŠ¨å¦‚æ•£æ­¥æˆ–éª‘è¡Œ 2. é˜…è¯»ä¹¦ç± 3. å­¦ä¹ æ–°æŠ€èƒ½ 4. å’Œæœ‹å‹èšé¤ è®°ä½ä¿æŒå·¥ä½œç”Ÿæ´»å¹³è¡¡ï¼",
            "topic": "lifestyle",
        },
        {
            "user_message": "æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰ç–²æƒ«ï¼Œæœ‰ä»€ä¹ˆæ”¹å–„æ–¹æ³•ï¼Ÿ",
            "ai_response": "æ„Ÿè§‰ç–²æƒ«å¯èƒ½æ˜¯å¤šç§åŸå› é€ æˆçš„ã€‚å»ºè®®ï¼š1. ä¿è¯å……è¶³ç¡çœ ï¼ˆ7-9å°æ—¶ï¼‰2. è§„å¾‹é¥®é£Ÿï¼Œæ‘„å…¥å‡è¡¡è¥å…» 3. é€‚é‡è¿åŠ¨ 4. ç®¡ç†å‹åŠ› 5. å¦‚æŒç»­ä¸¥é‡ï¼Œå»ºè®®å’¨è¯¢åŒ»ç”Ÿ",
            "topic": "health",
        },
    ]


class StoreMemories(BaseFunction):
    """Store user/AI messages (and optional semantic memory) via MemoryService"""

    def __init__(self, session_id: str):
        super().__init__()
        self.session_id = session_id

    def execute(self, conversation: Dict[str, Any]):
        user_message = conversation["user_message"]
        ai_response = conversation["ai_response"]
        topic = conversation.get("topic", "general")

        memory = self.call_service["memory_service"]

        stored_ids: List[str] = []
        now = time.time()

        # user message
        uid = memory.store_memory(
            content=user_message,
            vector=mock_embedding(user_message),
            session_id=self.session_id,
            memory_type="conversation",
            metadata={"speaker": "user", "topic": topic, "timestamp": now},
        )
        stored_ids.append(uid)

        # ai response
        aid = memory.store_memory(
            content=ai_response,
            vector=mock_embedding(ai_response),
            session_id=self.session_id,
            memory_type="conversation",
            metadata={"speaker": "ai", "topic": topic, "timestamp": now},
        )
        stored_ids.append(aid)

        # optional semantic memory
        if "python" in user_message.lower() or "ç¼–ç¨‹" in user_message:
            semantic_content = f"ç”¨æˆ·å¯¹ç¼–ç¨‹æ„Ÿå…´è¶£ï¼š{user_message[:50]}..."
            sid = memory.store_memory(
                content=semantic_content,
                vector=mock_embedding(semantic_content),
                session_id=self.session_id,
                memory_type="knowledge",
                metadata={
                    "topic": "programming",
                    "importance": "medium",
                    "timestamp": now,
                },
                create_knowledge_graph=True,
            )
            stored_ids.append(sid)

        return {"session_id": self.session_id, "stored_ids": stored_ids, "topic": topic}


class PrintStoreResult(BaseFunction):
    def execute(self, data: Dict[str, Any]):
        print(
            f"ğŸ—„ï¸ Stored {len(data.get('stored_ids', []))} memories for session {data['session_id']} (topic={data.get('topic')})"
        )
        return data


class SearchMemories(BaseFunction):
    def __init__(self, session_id: str, top_k: int = 3):
        super().__init__()
        self.session_id = session_id
        self.top_k = top_k

    def execute(self, query: str):
        vector = mock_embedding(query)
        memory = self.call_service["memory_service"]
        results = memory.search_memories(
            query_vector=vector, session_id=self.session_id, limit=self.top_k
        )
        summary = [
            {
                "id": r.get("id"),
                "type": r.get("memory_type"),
                "score": r.get("similarity_score"),
                "preview": (r.get("content") or "")[:60],
            }
            for r in results
        ]
        return {"query": query, "results": summary}


class PrintSearchResult(BaseFunction):
    def execute(self, data: Dict[str, Any]):
        print(f"\nğŸ” æŸ¥è¯¢: {data['query']}")
        for i, r in enumerate(data.get("results", []), 1):
            print(f"  {i}. [{r['type']}] {r['preview']}... (ç›¸ä¼¼åº¦: {r['score']:.4f})")
        return data


def run_memory_service_pipeline():
    print("ğŸš€ SAGE MemoryService Pipeline Demo")
    print("=" * 60)

    # Skip in test mode (CI)
    if os.getenv("SAGE_EXAMPLES_MODE") == "test":
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šè·³è¿‡ MemoryService ç¤ºä¾‹ï¼ˆéœ€è¦FAISSä¾èµ–ï¼‰")
        return

    # Verify FAISS availability early to avoid runtime failure inside service
    try:
        import faiss  # noqa: F401
    except Exception:
        print("âŒ FAISS not installed. Please run: pip install faiss-cpu")
        return

    print("ğŸ“‹ åˆå§‹åŒ–SAGEç¯å¢ƒå’ŒæœåŠ¡æ³¨å†Œ...")
    env = LocalEnvironment("memory_service_pipeline")

    # Register services (KV, VDB, Graph, Memory)
    kv_factory = create_kv_service_factory(
        service_name="kv_service", backend_type="memory", max_size=10000
    )
    env.register_service_factory("kv_service", kv_factory)

    vdb_factory = create_vdb_service_factory(
        service_name="vdb_service", embedding_dimension=384, index_type="IndexFlatL2"
    )
    env.register_service_factory("vdb_service", vdb_factory)

    graph_factory = create_graph_service_factory(
        service_name="graph_service",
        backend_type="memory",
        max_nodes=10000,
        max_relationships=50000,
    )
    env.register_service_factory("graph_service", graph_factory)

    memory_factory = create_memory_service_factory(
        service_name="memory_service",
        kv_service_name="kv_service",
        vdb_service_name="vdb_service",
        graph_service_name="graph_service",
        default_vector_dimension=384,
        enable_knowledge_graph=True,
    )
    env.register_service_factory("memory_service", memory_factory)

    # Data and session
    conversations = create_sample_conversations()
    session_id = "test_session_001"

    # Stage 1: store memories
    (
        env.from_batch(conversations)
        .map(StoreMemories, session_id=session_id, parallelism=1)
        .sink(PrintStoreResult, parallelism=1)
    )

    # Stage 2: queries
    queries = ["Pythonç¼–ç¨‹å­¦ä¹ ", "å¥åº·å’Œç–²æƒ«", "æœºå™¨å­¦ä¹ å»ºè®®"]
    (
        env.from_batch(queries)
        .map(SearchMemories, session_id=session_id, top_k=3, parallelism=1)
        .sink(PrintSearchResult, parallelism=1)
    )

    # Submit and wait for completion
    print("\nâ–¶ï¸  æäº¤ç®¡é“...")
    env.submit(autostop=True)
    print("âœ… ç®¡é“æ‰§è¡Œå®Œæˆ")


if __name__ == "__main__":
    CustomLogger.disable_global_console_debug()
    try:
        run_memory_service_pipeline()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å·²ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()
