# Multi-Strategy Embedding Pipeline
# Generated by: sage pipeline create-embedding -t multi-strategy --query-method hash --doc-method openai --batch-method vllm

pipeline:
  name: multi-strategy-embedding
  description: Intelligent routing to different embedding strategies based on use case
  version: 1.0.0
  type: local

environment:
  config:
    log_level: INFO

services:
  # Fast embedding for real-time queries (< 1ms)
  - name: embedding-service-fast
    class: sage.components.sage_embedding.service.EmbeddingService
    params:
      method: hash
      cache_embeddings: true
      normalize: true

  # High-quality embedding for documents
  - name: embedding-service-quality
    class: sage.components.sage_embedding.service.EmbeddingService
    params:
      method: openai
      model_name: text-embedding-3-large
      cache_embeddings: true
      normalize: true
      api_key: ${OPENAI_API_KEY}

  # High-throughput embedding for batch processing
  - name: embedding-service-batch
    class: sage.components.sage_embedding.service.EmbeddingService
    params:
      method: vllm
      model_name: BAAI/bge-large-zh-v1.5
      cache_embeddings: false
      normalize: true
      batch_size: 512

  # VLLMService backend for batch embedding
  - name: vllm-backend
    class: sage.common.components.sage_vllm.service.VLLMService
    params:
      model_name: BAAI/bge-large-zh-v1.5
      task: embed
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.9

  # Vector database
  - name: vector-db
    class: sage.middleware.components.sage_db.service.SageDBService
    params:
      collection_name: multi_strategy_vectors
      vector_dim: 1024
      metric_type: cosine

source:
  kind: batch
  class: sage.libs.rag.loader.DirectoryLoader
  params:
    directory: ./data/documents
    file_pattern: "*.{txt,md}"
    recursive: true

stages:
  # Stage 1: Load and classify input
  - id: load-and-classify
    kind: map
    class: sage.libs.rag.classifier.InputClassifier
    params:
      classify_by: type  # type: query, document, batch
    summary: Classify input by type for routing

  # Stage 2: Route to appropriate embedding strategy
  - id: router
    kind: map
    class: sage.libs.rag.router.EmbeddingRouter
    params:
      routes:
        # Route 1: Real-time queries → Fast embedding (hash)
        - condition: "input.type == 'query'"
          service: embedding-service-fast
          description: "Fast embedding for real-time queries"
          
        # Route 2: Individual documents → Quality embedding (OpenAI)
        - condition: "input.type == 'document'"
          service: embedding-service-quality
          description: "High-quality embedding for documents"
          
        # Route 3: Batch processing → Throughput embedding (vLLM)
        - condition: "input.type == 'batch'"
          service: embedding-service-batch
          description: "High-throughput embedding for batches"
          
        # Default route
        - condition: "default"
          service: embedding-service-fast
          description: "Default to fast embedding"
    summary: Intelligent routing based on input type

  # Stage 3: Index embeddings
  - id: index-vectors
    kind: map
    class: vector-db
    params:
      operation: insert
      batch_insert: true
    summary: Index all embeddings into unified vector store

  # Stage 4: Track routing statistics
  - id: track-stats
    kind: map
    class: sage.libs.monitoring.RoutingStatsTracker
    params:
      metrics:
        - route_counts
        - avg_latency_per_route
        - throughput_per_route
    summary: Monitor routing decisions and performance

sink:
  class: sage.libs.sinks.ConsoleSink
  params:
    format: json
    pretty: true
    include_metadata: true

monitors:
  - type: metrics
    params:
      track_latency: true
      track_throughput: true
      track_route_distribution: true
      
  - type: dashboard
    params:
      port: 8080
      metrics:
        - name: route_distribution
          type: pie_chart
        - name: latency_by_route
          type: bar_chart
        - name: throughput_timeline
          type: line_chart

notes:
  - "This pipeline intelligently routes inputs to different embedding strategies"
  - "Query (real-time): hash embedding (~0.5ms, immediate response)"
  - "Document (offline): OpenAI text-embedding-3-large (high quality)"
  - "Batch (large-scale): vLLM with 512 batch size (~10k docs/min)"
  - "All embeddings are stored in the same vector database for unified search"
  - "Routing logic can be customized via input.type metadata field"
  - "Performance characteristics:"
  - "  - Fast route: < 1ms latency, 100k+ req/s"
  - "  - Quality route: ~50ms latency, 100 req/s, best accuracy"
  - "  - Batch route: ~10ms/doc, 10k docs/min, GPU optimized"
  - "Use cases:"
  - "  - Fast: User queries in production"
  - "  - Quality: Important documents, legal contracts"
  - "  - Batch: Nightly indexing jobs, data migrations"

# Example usage:
# 
# 1. Real-time query:
#    input = {"text": "What is RAG?", "type": "query"}
#    → Routes to hash embedding → < 1ms response
#
# 2. Single document:
#    input = {"text": "...", "type": "document"}
#    → Routes to OpenAI embedding → High quality vector
#
# 3. Batch of 10,000 documents:
#    input = {"documents": [...], "type": "batch"}
#    → Routes to vLLM embedding → 512 batch size, ~1 minute total
