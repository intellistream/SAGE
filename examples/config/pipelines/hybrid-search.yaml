# Hybrid Search Pipeline (Dense + Sparse)
# Generated by: sage pipeline create-embedding -t hybrid-search --dense-method openai --sparse-method bm25s

pipeline:
  name: hybrid-search-pipeline
  description: Hybrid retrieval combining dense and sparse embeddings
  version: 1.0.0
  type: local

environment:
  config:
    log_level: INFO

services:
  # Dense embedding service (OpenAI)
  - name: embedding-service-dense
    class: sage.components.sage_embedding.service.EmbeddingService
    params:
      method: openai
      model_name: text-embedding-3-small
      cache_embeddings: true
      normalize: true
      api_key: ${OPENAI_API_KEY}

  # Sparse embedding service (BM25s)
  - name: embedding-service-sparse
    class: sage.components.sage_embedding.service.EmbeddingService
    params:
      method: bm25s
      cache_embeddings: true
      normalize: false  # BM25s doesn't need normalization

  # Dense vector database
  - name: dense-vector-db
    class: sage.middleware.components.sage_db.service.SageDBService
    params:
      collection_name: dense_vectors
      vector_dim: 1536  # OpenAI text-embedding-3-small dimension
      metric_type: cosine

  # Sparse vector database
  - name: sparse-vector-db
    class: sage.middleware.components.sage_db.service.SageDBService
    params:
      collection_name: sparse_vectors
      vector_dim: 10000  # BM25s vocabulary size
      metric_type: inner_product

source:
  kind: batch
  class: sage.libs.rag.loader.DirectoryLoader
  params:
    directory: ./data/documents
    file_pattern: "*.{txt,md}"
    recursive: true

stages:
  # Stage 1: Load documents
  - id: load-documents
    kind: batch
    class: sage.libs.rag.loader.DocumentLoader
    summary: Load source documents

  # Stage 2: Chunk documents
  - id: chunk-documents
    kind: map
    class: sage.libs.rag.chunker.RecursiveCharacterTextSplitter
    params:
      chunk_size: 512
      chunk_overlap: 50
    summary: Split documents into chunks

  # === Dense embedding path ===
  
  # Stage 3a: Dense embedding (OpenAI)
  - id: embed-dense
    kind: service
    class: embedding-service-dense
    params:
      batch_size: 32
      show_progress: true
    summary: Generate dense semantic embeddings

  # Stage 4a: Index dense vectors
  - id: index-dense
    kind: map
    class: dense-vector-db
    params:
      operation: insert
      batch_insert: true
    summary: Index dense vectors for semantic search

  # === Sparse embedding path ===
  
  # Stage 3b: Sparse embedding (BM25s)
  - id: embed-sparse
    kind: service
    class: embedding-service-sparse
    params:
      batch_size: 64
      show_progress: true
    summary: Generate sparse keyword-based embeddings

  # Stage 4b: Index sparse vectors
  - id: index-sparse
    kind: map
    class: sparse-vector-db
    params:
      operation: insert
      batch_insert: true
    summary: Index sparse vectors for keyword search

  # === Hybrid retrieval (query time) ===
  
  # Stage 5: Hybrid retrieval with fusion
  - id: hybrid-retrieval
    kind: map
    class: sage.libs.rag.retriever.HybridRetriever
    params:
      dense_retriever: dense-vector-db
      sparse_retriever: sparse-vector-db
      fusion_strategy: reciprocal_rank
      dense_weight: 0.7
      sparse_weight: 0.3
      top_k_dense: 10
      top_k_sparse: 10
      top_k_final: 5
    summary: Fuse dense and sparse results with RRF

  # Stage 6: Rerank results
  - id: rerank
    kind: map
    class: sage.libs.rag.reranker.CrossEncoderReranker
    params:
      model_name: cross-encoder/ms-marco-MiniLM-L-6-v2
      top_k: 3
    summary: Rerank fused results for higher precision

sink:
  class: sage.libs.sinks.ConsoleSink
  params:
    format: json
    pretty: true

monitors:
  - type: metrics
    params:
      track_latency: true
      track_throughput: true

notes:
  - "Hybrid search combines dense semantic and sparse keyword matching"
  - "Dense (OpenAI): Captures semantic similarity (weight=0.7)"
  - "Sparse (BM25s): Captures exact keyword matches (weight=0.3)"
  - "Reciprocal Rank Fusion (RRF) merges results from both methods"
  - "Cross-encoder reranker further improves precision"
  - "Recommended for: legal documents, technical docs, code search"
  - "Adjust weights based on your use case (semantic vs keyword importance)"
