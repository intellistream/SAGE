# Knowledge Base Builder with vLLM
# Generated by: sage pipeline create-embedding -t knowledge-base --vllm --batch-size 512

pipeline:
  name: knowledge-base-builder-vllm
  description: High-throughput knowledge base indexing with vLLM embedding
  version: 1.0.0
  type: local

environment:
  config:
    log_level: INFO
    max_workers: 8

services:
  # EmbeddingService with vLLM backend for high performance
  - name: embedding-service
    class: sage.components.sage_embedding.service.EmbeddingService
    params:
      method: vllm
      model_name: BAAI/bge-large-zh-v1.5
      cache_embeddings: false  # Disable cache for batch processing
      normalize: true
      batch_size: 512  # vLLM supports very large batches

  # VLLMService for embedding backend
  - name: vllm-backend
    class: sage.common.components.sage_vllm.service.VLLMService
    params:
      model_name: BAAI/bge-large-zh-v1.5
      task: embed
      tensor_parallel_size: 2
      gpu_memory_utilization: 0.95
      max_model_len: 2048

  # Vector database
  - name: vector-db
    class: sage.middleware.components.sage_db.service.SageDBService
    params:
      collection_name: knowledge_base
      vector_dim: 1024
      metric_type: cosine
      index_type: IVF_FLAT
      nlist: 1024

source:
  kind: batch
  class: sage.libs.rag.loader.DirectoryLoader
  params:
    directory: ./data/knowledge_base
    file_pattern: "*.{txt,md,json,pdf,docx}"
    recursive: true
    max_files: 100000

stages:
  # Stage 1: Load all documents
  - id: load-documents
    kind: batch
    class: sage.libs.rag.loader.ParallelDocumentLoader
    params:
      num_workers: 8
      chunk_size: 1000
    summary: Parallel load documents with 8 workers

  # Stage 2: Preprocess and clean
  - id: preprocess
    kind: map
    class: sage.libs.rag.preprocessor.TextCleaner
    params:
      remove_html: true
      remove_urls: true
      normalize_whitespace: true
    summary: Clean and normalize text

  # Stage 3: Chunk documents
  - id: chunk-documents
    kind: map
    class: sage.libs.rag.chunker.RecursiveCharacterTextSplitter
    params:
      chunk_size: 512
      chunk_overlap: 50
      length_function: len
    summary: Split into fixed-size chunks

  # Stage 4: Batch embed with vLLM (high throughput)
  - id: batch-embed
    kind: service
    class: embedding-service
    params:
      batch_size: 512  # Maximum batch size for vLLM
      show_progress: true
      timeout: 300
    summary: High-throughput embedding with vLLM (512 batch size)

  # Stage 5: Batch index
  - id: batch-index
    kind: map
    class: vector-db
    params:
      operation: batch_insert
      batch_size: 1000
      create_index: true
    summary: Batch insert embeddings into vector DB

  # Stage 6: Build index
  - id: build-index
    kind: map
    class: vector-db
    params:
      operation: build_index
      index_params:
        metric_type: cosine
        index_type: IVF_FLAT
        params: {"nlist": 1024}
    summary: Build IVF index for fast search

  # Stage 7: Persist
  - id: persist-index
    kind: map
    class: sage.libs.rag.storage.IndexPersister
    params:
      index_path: ./output/kb_index
      compression: true
    summary: Save index to disk with compression

sink:
  class: sage.libs.sinks.StatsSink
  params:
    metrics:
      - total_documents
      - total_chunks
      - total_vectors
      - indexing_time
      - throughput_docs_per_sec

monitors:
  - type: metrics
    params:
      track_latency: true
      track_throughput: true
      track_memory: true
      
  - type: progress
    params:
      update_interval: 100
      show_eta: true

notes:
  - "This pipeline is optimized for large-scale batch indexing (100k+ documents)"
  - "vLLM embedding backend supports batch_size=512 for maximum throughput"
  - "Cache is disabled to save memory during batch processing"
  - "Expected throughput: ~10,000 documents/minute on 2x GPU"
  - "IVF_FLAT index provides fast approximate search with good recall"
  - "After indexing, use sage.libs.rag.retriever.VectorRetriever for queries"
