# Milvus 稠密向量检索配置（适配 MilvusDenseRetriever）

source:
  data_path: "./examples/data/queries.jsonl"
  platform: "local"

retriever:
  preload_knowledge_file: "./examples/data/qa_knowledge_base.txt"

  # 通用参数
  dimension: 384            # 向量维度，应与 embedding 模型一致
  top_k: 2                 # 返回文档数量

  # 嵌入模型配置（用于对文档和查询编码）
  embedding:
    method: "hf"
    model: "sentence-transformers/all-MiniLM-L6-v2"

  # Milvus 后端（稠密检索）
  milvus_dense:
    # 本地 Milvus Lite（推荐用于快速试用）
    persistence_path: "./examples/rag/milvus_qa_dense.db"

    # 远程 Milvus（如需远程，请注释上面的 persistence_path，改为如下配置）
    # host: "127.0.0.1"
    # port: 19530
    # force_http: true

    collection_name: "qa_dense_collection"
    dim: 384
    metric_type: "COSINE"   # 只允许: IP / COSINE / L2
    search_type: "dense"     # 稠密检索

    # 可选项
    dense_insert_batch_size: 128
    # 知识文件（可选）：提供后将自动按段落读取并入库
    # knowledge_file: "./examples/data/qa_knowledge_base.txt"

promptor:
  template: |
    基于以下检索到的相关文档，回答用户问题：

    相关文档：
    {retrieved_documents}

    用户问题：{query}

    请提供准确、有用的回答：

generator:
  vllm:
    api_key: "token-abc123"
    method: "openai"
    model_name: "meta-llama/Llama-2-7b-chat-hf"
    base_url: "http://sage3:8000/v1"
    seed: 42

sink:
  enable_log: true
