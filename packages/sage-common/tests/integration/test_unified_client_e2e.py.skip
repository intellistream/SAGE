# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the SAGE project

"""Control-plane-only integration tests for UnifiedInferenceClient.

NOTE: This test file is for the legacy complex UnifiedInferenceClient API.
After Flag-Day refactor, the client has been simplified. These tests are
kept for reference but skipped until they are updated for the new API.
"""

from __future__ import annotations

import pytest

# Skip this entire module - tests legacy API that no longer exists
pytest.skip(
    "Legacy tests for old UnifiedInferenceClient API. "
    "New API is minimal and doesn't include InferenceResult, "
    "UnifiedClientConfig classes.",
    allow_module_level=True,
)


@pytest.fixture(autouse=True)
def clean_environment():
    """Clean environment variables and cached instances before each test."""
    env_vars = [
        "SAGE_UNIFIED_BASE_URL",
        "SAGE_UNIFIED_MODEL",
        "SAGE_UNIFIED_API_KEY",
        "SAGE_CHAT_BASE_URL",
        "SAGE_CHAT_MODEL",
        "SAGE_CHAT_API_KEY",
        "SAGE_EMBEDDING_BASE_URL",
        "SAGE_EMBEDDING_MODEL",
        "SAGE_EMBEDDING_API_KEY",
    ]
    original = {k: os.environ.get(k) for k in env_vars}

    for k in env_vars:
        os.environ.pop(k, None)

    UnifiedInferenceClient.clear_instances()

    yield

    for k, v in original.items():
        if v is not None:
            os.environ[k] = v
        else:
            os.environ.pop(k, None)

    UnifiedInferenceClient.clear_instances()


@pytest.fixture
def mock_openai_client():
    """Create a mock OpenAI-compatible client."""
    mock = MagicMock()

    mock_chat_response = MagicMock()
    mock_chat_response.choices = [MagicMock()]
    mock_chat_response.choices[0].message = MagicMock()
    mock_chat_response.choices[0].message.content = "Mock chat response"
    mock_chat_response.model = "test-model"
    mock_chat_response.usage = MagicMock()
    mock_chat_response.usage.prompt_tokens = 10
    mock_chat_response.usage.completion_tokens = 20
    mock_chat_response.usage.total_tokens = 30
    mock.chat.completions.create.return_value = mock_chat_response

    mock_completion_response = MagicMock()
    mock_completion_response.choices = [MagicMock()]
    mock_completion_response.choices[0].text = "Mock completion response"
    mock_completion_response.model = "test-model"
    mock_completion_response.usage = MagicMock()
    mock_completion_response.usage.prompt_tokens = 5
    mock_completion_response.usage.completion_tokens = 15
    mock_completion_response.usage.total_tokens = 20
    mock.completions.create.return_value = mock_completion_response

    mock_embedding_response = MagicMock()
    mock_embedding_response.data = [MagicMock(), MagicMock()]
    mock_embedding_response.data[0].embedding = [0.1, 0.2, 0.3]
    mock_embedding_response.data[1].embedding = [0.4, 0.5, 0.6]
    mock_embedding_response.model = "test-embedding-model"
    mock_embedding_response.usage = MagicMock()
    mock_embedding_response.usage.prompt_tokens = 10
    mock_embedding_response.usage.total_tokens = 10
    mock.embeddings.create.return_value = mock_embedding_response

    return mock


# =============================================================================
# Control Plane Resolution Tests
# =============================================================================


class TestControlPlaneResolution:
    """Tests for control-plane-only endpoint resolution."""

    def test_resolve_from_unified_env_var(self, mock_openai_client):
        os.environ["SAGE_UNIFIED_BASE_URL"] = "http://127.0.0.1:8000"
        os.environ["SAGE_UNIFIED_MODEL"] = "unified-model"
        os.environ["SAGE_UNIFIED_API_KEY"] = "unified-key"  # pragma: allowlist secret

        with patch("sage.llm.unified_client.OpenAI") as mock_openai:
            mock_openai.return_value = mock_openai_client
            client = UnifiedInferenceClient.create()

        assert client.config.llm_base_url == "http://127.0.0.1:8000/v1"
        assert client.config.embedding_base_url == "http://127.0.0.1:8000/v1"
        assert client.config.llm_model == "unified-model"
        assert client.config.embedding_model == "unified-model"
        assert client.config.llm_api_key == "unified-key"  # pragma: allowlist secret
        assert client.config.embedding_api_key == "unified-key"  # pragma: allowlist secret

    def test_default_gateway_when_no_env(self, mock_openai_client):
        with patch("sage.llm.unified_client.OpenAI") as mock_openai:
            mock_openai.return_value = mock_openai_client
            client = UnifiedInferenceClient.create()

        expected = f"http://localhost:{SagePorts.GATEWAY_DEFAULT}/v1"
        assert client.config.llm_base_url == expected
        assert client.config.embedding_base_url == expected

    def test_control_plane_url_overrides_env(self, mock_openai_client):
        os.environ["SAGE_UNIFIED_BASE_URL"] = "http://127.0.0.1:8000"

        with patch("sage.llm.unified_client.OpenAI") as mock_openai:
            mock_openai.return_value = mock_openai_client
            client = UnifiedInferenceClient.create(control_plane_url="http://remote:9999")

        assert client.config.llm_base_url == "http://remote:9999/v1"
        assert client.config.embedding_base_url == "http://remote:9999/v1"


# =============================================================================
# Basic Mode Tests
# =============================================================================


class TestBasicMode:
    """Tests for basic mode (direct API calls via create())."""

    def test_explicit_configuration(self, mock_openai_client):
        with patch("sage.llm.unified_client.OpenAI") as mock_openai:
            mock_openai.return_value = mock_openai_client
            client = UnifiedInferenceClient.create(
                control_plane_url="http://127.0.0.1:8000",
                default_llm_model="test-llm",
                default_embedding_model="test-embed",
            )

        assert client.config.llm_base_url == "http://127.0.0.1:8000/v1"
        assert client.config.embedding_base_url == "http://127.0.0.1:8000/v1"
        assert client.config.llm_model == "test-llm"
        assert client.config.embedding_model == "test-embed"

    def test_chat_method(self, mock_openai_client):
        with patch("sage.llm.unified_client.OpenAI") as mock_openai:
            mock_openai.return_value = mock_openai_client
            client = UnifiedInferenceClient.create(control_plane_url="http://127.0.0.1:8000")

            client._llm_available = True
            client._llm_client = mock_openai_client

            response = client.chat([{"role": "user", "content": "Hello"}])

        assert response == "Mock chat response"
        mock_openai_client.chat.completions.create.assert_called_once()

    def test_generate_method(self, mock_openai_client):
        with patch("sage.llm.unified_client.OpenAI") as mock_openai:
            mock_openai.return_value = mock_openai_client
            client = UnifiedInferenceClient.create(control_plane_url="http://127.0.0.1:8000")

            client._llm_available = True
            client._llm_client = mock_openai_client

            _ = client.generate("Once upon a time")

        assert (
            mock_openai_client.chat.completions.create.called
            or mock_openai_client.completions.create.called
        )

    def test_embed_method(self, mock_openai_client):
        with patch("sage.llm.unified_client.OpenAI") as mock_openai:
            mock_openai.return_value = mock_openai_client
            client = UnifiedInferenceClient.create(control_plane_url="http://127.0.0.1:8000")

            client._embedding_available = True
            client._embedding_client = mock_openai_client

            result = client.embed(["text1", "text2"])

        assert isinstance(result, list)
        mock_openai_client.embeddings.create.assert_called_once()

    def test_llm_not_available_error(self):
        client = UnifiedInferenceClient.create(control_plane_url="http://127.0.0.1:8000")
        client._llm_available = False
        client._llm_client = None

        with pytest.raises(RuntimeError, match="No LLM backend available"):
            client.chat([{"role": "user", "content": "Hello"}])

    def test_embedding_not_available_error(self):
        client = UnifiedInferenceClient.create(control_plane_url="http://127.0.0.1:8000")
        client._embedding_available = False
        client._embedding_client = None

        with pytest.raises(RuntimeError, match="No Embedding backend available"):
            client.embed(["text"])


# =============================================================================
# Control Plane Mode Tests
# =============================================================================


class TestControlPlaneMode:
    """Tests for Control Plane mode with hybrid scheduling."""

    def test_control_plane_mode_property(self):
        client = UnifiedInferenceClient.create()
        assert client.is_control_plane_mode is True


# =============================================================================
# Singleton and Caching Tests
# =============================================================================


class TestSingletonPattern:
    """Tests for singleton instance caching."""

    def test_get_instance_returns_same_object(self):
        with patch("sage.llm.unified_client.OpenAI"):
            instance1 = UnifiedInferenceClient.get_instance("key1")
            instance2 = UnifiedInferenceClient.get_instance("key1")

        assert instance1 is instance2

    def test_different_keys_different_instances(self):
        with patch("sage.llm.unified_client.OpenAI"):
            instance1 = UnifiedInferenceClient.get_instance("key1")
            instance2 = UnifiedInferenceClient.get_instance("key2")

        assert instance1 is not instance2

    def test_clear_instances(self):
        with patch("sage.llm.unified_client.OpenAI"):
            instance1 = UnifiedInferenceClient.get_instance("key1")
            UnifiedInferenceClient.clear_instances()
            instance2 = UnifiedInferenceClient.get_instance("key1")

        assert instance1 is not instance2


# =============================================================================
# Error Handling Tests
# =============================================================================


class TestErrorHandling:
    """Tests for error handling scenarios."""

    def test_chat_handles_api_error(self, mock_openai_client):
        mock_openai_client.chat.completions.create.side_effect = Exception("API Error")

        with patch("sage.llm.unified_client.OpenAI") as mock_openai:
            mock_openai.return_value = mock_openai_client

            client = UnifiedInferenceClient.create(control_plane_url="http://127.0.0.1:8000")
            client._llm_available = True
            client._llm_client = mock_openai_client

            with pytest.raises(Exception, match="API Error"):
                client.chat([{"role": "user", "content": "Hello"}])

    def test_embed_handles_api_error(self, mock_openai_client):
        mock_openai_client.embeddings.create.side_effect = Exception("Embedding Error")

        with patch("sage.llm.unified_client.OpenAI") as mock_openai:
            mock_openai.return_value = mock_openai_client

            client = UnifiedInferenceClient.create(control_plane_url="http://127.0.0.1:8000")
            client._embedding_available = True
            client._embedding_client = mock_openai_client

            with pytest.raises(Exception, match="Embedding Error"):
                client.embed(["text"])

    def test_timeout_configuration(self):
        with patch("sage.llm.unified_client.OpenAI"):
            client = UnifiedInferenceClient.create(timeout=30.0)
        assert client.config.timeout == 30.0

    def test_max_retries_configuration(self):
        config = UnifiedClientConfig(max_retries=5)
        assert config.max_retries == 5


# =============================================================================
# Inference Result Tests
# =============================================================================


class TestInferenceResult:
    """Tests for the InferenceResult dataclass."""

    def test_inference_result_defaults(self):
        result = InferenceResult(
            request_id="req-1",
            request_type="chat",
            content="hello",
            model="test-model",
        )

        assert result.usage == {}
        assert result.latency_ms == 0.0
        assert result.metadata == {}
