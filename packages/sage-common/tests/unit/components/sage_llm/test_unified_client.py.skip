# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the SAGE project

"""Unit tests for UnifiedInferenceClient.

Tests the unified client that combines LLM and Embedding capabilities
via Control Plane mode (unified entry point).

NOTE: This test file is for the legacy complex UnifiedInferenceClient API.
After Flag-Day refactor, the client has been simplified. These tests are
kept for reference but skipped until they are updated for the new API.
"""

from __future__ import annotations

import pytest

# Skip this entire module - tests legacy API that no longer exists
pytest.skip(
    "Legacy tests for old UnifiedInferenceClient API. "
    "New API is minimal and doesn't include InferenceResult, "
    "UnifiedClientConfig, UnifiedClientMode classes.",
    allow_module_level=True,
)

if TYPE_CHECKING:
    pass


class TestUnifiedClientConfig:
    """Tests for UnifiedClientConfig dataclass."""

    def test_default_values(self):
        """Test default configuration values."""
        config = UnifiedClientConfig()

        assert config.llm_base_url is None
        assert config.llm_model is None
        assert config.llm_api_key == ""
        assert config.embedding_base_url is None
        assert config.embedding_model is None
        assert config.embedding_api_key == ""
        assert config.timeout == 60.0
        assert config.max_retries == 3
        assert config.temperature == 0.7
        assert config.max_tokens == 512
        assert config.top_p == 1.0

    def test_custom_values(self):
        """Test configuration with custom values."""
        config = UnifiedClientConfig(
            llm_base_url="http://localhost:8001/v1",
            llm_model="qwen-7b",
            llm_api_key="test-key",  # pragma: allowlist secret
            embedding_base_url="http://localhost:8090/v1",
            embedding_model="bge-m3",
            timeout=30.0,
            max_retries=5,
            temperature=0.5,
        )

        assert config.llm_base_url == "http://localhost:8001/v1"
        assert config.llm_model == "qwen-7b"
        assert config.llm_api_key == "test-key"  # pragma: allowlist secret
        assert config.embedding_base_url == "http://localhost:8090/v1"
        assert config.embedding_model == "bge-m3"
        assert config.timeout == 30.0
        assert config.max_retries == 5
        assert config.temperature == 0.5


class TestUnifiedClientMode:
    """Tests for UnifiedClientMode constants."""

    def test_control_plane_mode(self):
        """Test CONTROL_PLANE mode value."""
        assert UnifiedClientMode.CONTROL_PLANE == "control_plane"


class TestInferenceResult:
    """Tests for InferenceResult dataclass."""

    def test_chat_result(self):
        """Test creating a chat inference result."""
        result = InferenceResult(
            request_id="req-123",
            request_type="chat",
            content="Hello, world.",
            model="qwen-7b",
            usage={"prompt_tokens": 10, "completion_tokens": 5, "total_tokens": 15},
            latency_ms=100.5,
        )

        assert result.request_id == "req-123"
        assert result.request_type == "chat"
        assert result.content == "Hello, world."
        assert result.model == "qwen-7b"
        assert result.usage["total_tokens"] == 15
        assert result.latency_ms == 100.5

    def test_embed_result(self):
        """Test creating an embedding inference result."""
        embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]
        result = InferenceResult(
            request_id="emb-456",
            request_type="embed",
            content=embeddings,
            model="bge-m3",
            usage={"prompt_tokens": 20},
            latency_ms=50.0,
        )

        assert result.request_id == "emb-456"
        assert result.request_type == "embed"
        assert result.content == embeddings
        assert len(result.content) == 2


class TestUnifiedInferenceClientCreate:
    """Tests for UnifiedInferenceClient.create() factory method."""

    def test_direct_init_blocked(self):
        """Test that direct instantiation is blocked."""
        with pytest.raises(RuntimeError, match="Use UnifiedInferenceClient.create"):
            UnifiedInferenceClient(
                llm_base_url="http://localhost:8001/v1",
                llm_model="qwen-7b",
            )

    def test_create_defaults_to_gateway(self):
        """create() defaults to Gateway/Control Plane port when no env provided."""
        client = UnifiedInferenceClient.create()

        expected = f"http://localhost:{SagePorts.GATEWAY_DEFAULT}/v1"
        assert client.config.llm_base_url == expected
        assert client.config.embedding_base_url == expected
        assert client.config.llm_model is None
        assert client.config.embedding_model is None

    @patch(
        "sage.llm.unified_client.os.environ",
        {
            "SAGE_UNIFIED_BASE_URL": "http://unified:8000/v1",
            "SAGE_UNIFIED_MODEL": "unified-model",
        },
    )
    def test_create_with_unified_env(self):
        """Env variables should set the control-plane URL and model."""
        client = UnifiedInferenceClient.create()

        assert client.config.llm_base_url == "http://unified:8000/v1"
        assert client.config.embedding_base_url == "http://unified:8000/v1"
        assert client.config.llm_model == "unified-model"
        assert client.config.embedding_model == "unified-model"

    def test_create_with_control_plane_url(self):
        """Explicit control_plane_url should override defaults."""
        client = UnifiedInferenceClient.create(
            control_plane_url="http://my-control-plane:8000/v1",
            default_llm_model="my-model",
            default_embedding_model="my-embed",
        )

        assert client.config.llm_base_url == "http://my-control-plane:8000/v1"
        assert client.config.embedding_base_url == "http://my-control-plane:8000/v1"
        assert client.config.llm_model == "my-model"
        assert client.config.embedding_model == "my-embed"

    def test_create_rejects_embedded(self):
        """Embedded mode should raise because embedded mode is disabled."""
        with pytest.raises(ValueError, match="Embedded Control Plane mode is disabled"):
            UnifiedInferenceClient.create(
                embedded=True  # allow-control-plane-bypass
            )  # allow-control-plane-bypass (assert reject)

    def test_create_respects_timeout(self):
        """Timeout should propagate into configuration."""
        client = UnifiedInferenceClient.create(timeout=120.0)
        assert client.config.timeout == 120.0

    def test_create_rejects_legacy_autodetect_args(self):
        """Legacy autodetect parameters should be rejected."""
        with pytest.raises(ValueError, match="Control-plane-only mode"):
            UnifiedInferenceClient.create(
                prefer_local=False  # allow-control-plane-bypass
            )  # allow-control-plane-bypass (assert reject)


class TestUnifiedClientProperties:
    """Tests for UnifiedInferenceClient properties."""

    def test_is_llm_available(self):
        """Test is_llm_available property."""
        client = UnifiedInferenceClient.create()
        assert client.is_llm_available == client._llm_available

    def test_is_embedding_available(self):
        """Test is_embedding_available property."""
        client = UnifiedInferenceClient.create()
        assert client.is_embedding_available == client._embedding_available

    def test_is_control_plane_mode(self):
        """Control Plane mode should always be enabled."""
        client = UnifiedInferenceClient.create()
        assert client.is_control_plane_mode is True

    def test_get_status(self):
        """Test get_status method returns control-plane info."""
        client = UnifiedInferenceClient.create()
        status = client.get_status()

        assert status["mode"] == UnifiedClientMode.CONTROL_PLANE
        assert "llm_base_url" in status
        assert "embedding_base_url" in status


class TestUnifiedClientHealthCheck:
    """Tests for endpoint health check."""

    @patch("sage.llm.unified_client.httpx.Client")
    def test_check_endpoint_health_models_endpoint(self, mock_client_class):
        """Test health check using /models endpoint."""
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_client = MagicMock()
        mock_client.__enter__ = MagicMock(return_value=mock_client)
        mock_client.__exit__ = MagicMock(return_value=False)
        mock_client.get.return_value = mock_response
        mock_client_class.return_value = mock_client

        result = UnifiedInferenceClient._check_endpoint_health("http://localhost:8001/v1")

        assert result is True

    @patch("sage.llm.unified_client.httpx.Client")
    def test_check_endpoint_health_failure(self, mock_client_class):
        """Test health check when endpoint is down."""
        mock_client = MagicMock()
        mock_client.__enter__ = MagicMock(return_value=mock_client)
        mock_client.__exit__ = MagicMock(return_value=False)
        mock_client.get.side_effect = Exception("Connection refused")
        mock_client_class.return_value = mock_client

        result = UnifiedInferenceClient._check_endpoint_health("http://localhost:8001/v1")

        assert result is False


class TestUnifiedClientChat:
    """Tests for chat method."""

    def test_chat_no_llm_available(self):
        """Test chat raises error when LLM not available."""
        client = UnifiedInferenceClient.create()
        client._llm_available = False
        client._backend_refresh_enabled = False
        client._llm_client = None

        with pytest.raises(RuntimeError, match="No LLM backend available"):
            client.chat([{"role": "user", "content": "Hello"}])

    @patch.object(UnifiedInferenceClient, "_get_default_llm_model", return_value="test-model")
    def test_chat_mode(self, mock_get_model):
        """Test chat method."""
        client = UnifiedInferenceClient.create()
        client._llm_available = True

        # Mock the OpenAI client
        mock_response = MagicMock()
        mock_response.id = "resp-123"
        mock_response.model = "qwen-7b"
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = "Hello."
        mock_response.usage = MagicMock()
        mock_response.usage.prompt_tokens = 10
        mock_response.usage.completion_tokens = 5
        mock_response.usage.total_tokens = 15

        client._llm_client = MagicMock()
        client._llm_client.chat.completions.create.return_value = mock_response

        result = client.chat([{"role": "user", "content": "Hi"}])

        assert result == "Hello."

    @patch.object(UnifiedInferenceClient, "_get_default_llm_model", return_value="test-model")
    def test_chat_with_return_result(self, mock_get_model):
        """Test chat with return_result=True."""
        client = UnifiedInferenceClient.create()
        client._llm_available = True

        mock_response = MagicMock()
        mock_response.id = "resp-123"
        mock_response.model = "qwen-7b"
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = "Hello."
        mock_response.usage = MagicMock()
        mock_response.usage.prompt_tokens = 10
        mock_response.usage.completion_tokens = 5
        mock_response.usage.total_tokens = 15

        client._llm_client = MagicMock()
        client._llm_client.chat.completions.create.return_value = mock_response

        result = client.chat(
            [{"role": "user", "content": "Hi"}],
            return_result=True,
        )

        assert isinstance(result, InferenceResult)
        assert result.request_type == "chat"
        assert result.content == "Hello."


class TestUnifiedClientGenerate:
    """Tests for generate method."""

    def test_generate_no_llm_available(self):
        """Test generate raises error when LLM not available."""
        client = UnifiedInferenceClient.create()
        client._llm_available = False
        client._backend_refresh_enabled = False
        client._llm_client = None

        with pytest.raises(RuntimeError, match="LLM endpoint not available"):
            client.generate("Hello")

    @patch.object(UnifiedInferenceClient, "_get_default_llm_model", return_value="test-model")
    def test_generate_mode(self, mock_get_model):
        """Test generate method."""
        client = UnifiedInferenceClient.create()
        client._llm_available = True

        # Mock the OpenAI client (completions API)
        mock_response = MagicMock()
        mock_response.id = "gen-123"
        mock_response.model = "qwen-7b"
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].text = "Once upon a time..."
        mock_response.usage = MagicMock()
        mock_response.usage.prompt_tokens = 5
        mock_response.usage.completion_tokens = 10
        mock_response.usage.total_tokens = 15

        client._llm_client = MagicMock()
        client._llm_client.completions.create.return_value = mock_response

        result = client.generate("Once upon")

        assert result == "Once upon a time..."


class TestUnifiedClientEmbed:
    """Tests for embed method."""

    def test_embed_no_embedding_available(self):
        """Test embed raises error when embedding not available."""
        client = UnifiedInferenceClient.create()
        client._embedding_available = False
        client._backend_refresh_enabled = False
        client._embedding_client = None

        with pytest.raises(RuntimeError, match="No Embedding backend available"):
            client.embed(["Hello"])

    @patch.object(UnifiedInferenceClient, "_get_default_embedding_model", return_value="bge-m3")
    def test_embed_single_text(self, mock_get_model):
        """Test embed with single text input."""
        client = UnifiedInferenceClient.create()
        client._embedding_available = True

        mock_embedding = MagicMock()
        mock_embedding.embedding = [0.1, 0.2, 0.3]

        mock_response = MagicMock()
        mock_response.data = [mock_embedding]
        mock_response.model = "bge-m3"
        mock_response.usage = MagicMock()
        mock_response.usage.prompt_tokens = 5
        mock_response.usage.total_tokens = 5

        client._embedding_client = MagicMock()
        client._embedding_client.embeddings.create.return_value = mock_response

        result = client.embed("Hello")

        assert isinstance(result, list)
        assert len(result) == 1
        assert result[0] == [0.1, 0.2, 0.3]

    @patch.object(UnifiedInferenceClient, "_get_default_embedding_model", return_value="bge-m3")
    def test_embed_multiple_texts(self, mock_get_model):
        """Test embed with multiple texts."""
        client = UnifiedInferenceClient.create()
        client._embedding_available = True

        mock_embedding1 = MagicMock()
        mock_embedding1.embedding = [0.1, 0.2, 0.3]
        mock_embedding2 = MagicMock()
        mock_embedding2.embedding = [0.4, 0.5, 0.6]

        mock_response = MagicMock()
        mock_response.data = [mock_embedding1, mock_embedding2]
        mock_response.model = "bge-m3"
        mock_response.usage = MagicMock()
        mock_response.usage.prompt_tokens = 10
        mock_response.usage.total_tokens = 10

        client._embedding_client = MagicMock()
        client._embedding_client.embeddings.create.return_value = mock_response

        result = client.embed(["Hello", "World"])

        # Result should be list of embeddings when return_result=False (default)
        assert isinstance(result, list)
        assert len(result) == 2
        assert result[0] == [0.1, 0.2, 0.3]
        assert result[1] == [0.4, 0.5, 0.6]

    @patch.object(UnifiedInferenceClient, "_get_default_embedding_model", return_value="bge-m3")
    def test_embed_with_return_result(self, mock_get_model):
        """Test embed with return_result=True."""
        client = UnifiedInferenceClient.create()
        client._embedding_available = True

        mock_embedding = MagicMock()
        mock_embedding.embedding = [0.1, 0.2, 0.3]

        mock_response = MagicMock()
        mock_response.data = [mock_embedding]
        mock_response.model = "bge-m3"
        mock_response.usage = MagicMock()
        mock_response.usage.prompt_tokens = 5
        mock_response.usage.total_tokens = 5

        client._embedding_client = MagicMock()
        client._embedding_client.embeddings.create.return_value = mock_response

        result = client.embed("Hello", return_result=True)

        assert isinstance(result, InferenceResult)
        assert result.request_type == "embed"
        assert result.content == [[0.1, 0.2, 0.3]]


class TestUnifiedClientSingleton:
    """Tests for singleton/instance caching."""

    def test_get_instance_creates_new(self):
        """Test get_instance creates new instance."""
        UnifiedInferenceClient.clear_instances()

        with patch.object(UnifiedInferenceClient, "create") as mock_create:
            mock_create.return_value = MagicMock(spec=UnifiedInferenceClient)
            _client = UnifiedInferenceClient.get_instance("test-key")  # noqa: F841

            mock_create.assert_called_once()

    def test_get_instance_returns_cached(self):
        """Test get_instance returns cached instance."""
        UnifiedInferenceClient.clear_instances()

        with patch.object(UnifiedInferenceClient, "create") as mock_create:
            mock_instance = MagicMock(spec=UnifiedInferenceClient)
            mock_create.return_value = mock_instance

            client1 = UnifiedInferenceClient.get_instance("test-key")
            client2 = UnifiedInferenceClient.get_instance("test-key")

            assert client1 is client2
            mock_create.assert_called_once()

    def test_clear_instances(self):
        """Test clear_instances removes all cached instances."""
        UnifiedInferenceClient.clear_instances()

        with patch.object(UnifiedInferenceClient, "create") as mock_create:
            mock_create.return_value = MagicMock(spec=UnifiedInferenceClient)
            UnifiedInferenceClient.get_instance("key1")
            UnifiedInferenceClient.get_instance("key2")

            assert len(UnifiedInferenceClient._instances) == 2

            UnifiedInferenceClient.clear_instances()

            assert len(UnifiedInferenceClient._instances) == 0
