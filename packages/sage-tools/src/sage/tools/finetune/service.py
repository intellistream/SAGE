#!/usr/bin/env python3
"""
Finetune CLI - Service Management
æœåŠ¡ç®¡ç†ï¼šè®­ç»ƒæ‰§è¡Œã€æ¨¡å‹åˆå¹¶ã€æœåŠ¡éƒ¨ç½²
"""

import json
import subprocess
from pathlib import Path

from rich.console import Console
from rich.panel import Panel

console = Console()


def start_training(config_path: Path, use_native: bool = True):
    """å¯åŠ¨è®­ç»ƒè¿‡ç¨‹

    Args:
        config_path: è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„
        use_native: æ˜¯å¦ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒæ¨¡å—ï¼ˆæ¨èï¼‰
    """
    try:
        if use_native:
            # ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒæ¨¡å—
            console.print("[cyan]ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒæ¨¡å—[/cyan]\n")

            # å¯¼å…¥è®­ç»ƒæ¨¡å—
            from sage.tools.finetune.trainer import train_from_meta

            # è¯»å–é…ç½®æ–‡ä»¶è·å–è¾“å‡ºç›®å½•
            with open(config_path) as f:
                config = json.load(f)

            # output_dir æ˜¯ checkpoints ç›®å½•ï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯å…¶çˆ¶ç›®å½•
            checkpoint_dir = Path(config.get("output_dir", "finetune_output"))
            if checkpoint_dir.name == "checkpoints":
                output_dir = checkpoint_dir.parent
            else:
                output_dir = checkpoint_dir

            # æ‰§è¡Œè®­ç»ƒ
            train_from_meta(output_dir)

        else:
            # å°è¯•ä½¿ç”¨ LLaMA-Factory (å¯èƒ½ä¸å…¼å®¹)
            cmd = ["llamafactory-cli", "train", str(config_path)]
            console.print("[yellow]âš ï¸  ä½¿ç”¨ LLaMA-Factory (å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜)[/yellow]")
            console.print(f"[cyan]æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}[/cyan]\n")

            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
            )

            for line in process.stdout:
                console.print(line, end="")

            process.wait()

            if process.returncode == 0:
                console.print("\n[green]âœ… è®­ç»ƒå®Œæˆï¼[/green]")
            else:
                console.print(f"\n[red]âŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : {process.returncode}[/red]")

    except ImportError as e:
        console.print(f"[red]âŒ å¯¼å…¥é”™è¯¯: {e}[/red]")
        console.print("[yellow]æç¤º:[/yellow]")
        console.print(
            "  â€¢ ç¡®ä¿å·²å®‰è£…å¾®è°ƒä¾èµ–: [cyan]pip install -e packages/sage-libs[finetune][/cyan]"
        )
    except FileNotFoundError as e:
        console.print(f"[red]âŒ æ‰¾ä¸åˆ°å‘½ä»¤: {e}[/red]")
        console.print("[yellow]æç¤º:[/yellow]")
        console.print("  â€¢ ä½¿ç”¨ SAGE åŸç”Ÿè„šæœ¬ (æ¨è): [cyan]--use-native[/cyan]")
        console.print("  â€¢ æˆ–å®‰è£… LLaMA-Factory: [cyan]pip install llmtuner[/cyan]")


def merge_lora_weights(
    checkpoint_path: Path, base_model: str, output_path: Path
) -> bool:
    """åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹

    Args:
        checkpoint_path: LoRA checkpoint è·¯å¾„
        base_model: åŸºç¡€æ¨¡å‹åç§°
        output_path: è¾“å‡ºè·¯å¾„

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        from peft import PeftModel
        from transformers import AutoModelForCausalLM, AutoTokenizer
    except ImportError:
        console.print("[red]âŒ ç¼ºå°‘ä¾èµ–ï¼Œè¯·å®‰è£…:[/red]")
        console.print("[cyan]pip install transformers peft[/cyan]")
        return False

    try:
        console.print("[cyan]â³ åŠ è½½åŸºç¡€æ¨¡å‹...[/cyan]")
        base = AutoModelForCausalLM.from_pretrained(
            base_model, torch_dtype="auto", device_map="cpu"  # åœ¨CPUä¸Šåˆå¹¶ï¼ŒèŠ‚çœæ˜¾å­˜
        )

        console.print("[cyan]â³ åŠ è½½ LoRA æƒé‡...[/cyan]")
        model = PeftModel.from_pretrained(base, str(checkpoint_path))

        console.print("[cyan]â³ åˆå¹¶æƒé‡...[/cyan]")
        merged_model = model.merge_and_unload()

        console.print("[cyan]â³ ä¿å­˜åˆå¹¶æ¨¡å‹...[/cyan]")
        merged_model.save_pretrained(str(output_path))

        # ä¿å­˜tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model)
        tokenizer.save_pretrained(str(output_path))

        console.print("\n[green]âœ… åˆå¹¶å®Œæˆï¼[/green]")
        console.print(f"ğŸ“ åˆå¹¶æ¨¡å‹å·²ä¿å­˜åˆ°: [cyan]{output_path}[/cyan]")
        return True

    except Exception as e:
        console.print(f"[red]âŒ åˆå¹¶å¤±è´¥: {e}[/red]")
        return False


def serve_model_with_vllm(
    model_path: Path,
    host: str = "0.0.0.0",
    port: int = 8000,
    gpu_memory_utilization: float = 0.9,
    daemon: bool = False,
    lora_path: Path | None = None,
    lora_name: str | None = None,
) -> subprocess.Popen | None:
    """ä½¿ç”¨ vLLM å¯åŠ¨æ¨¡å‹æœåŠ¡

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        host: æœåŠ¡ä¸»æœº
        port: æœåŠ¡ç«¯å£
        gpu_memory_utilization: GPU æ˜¾å­˜åˆ©ç”¨ç‡
        daemon: æ˜¯å¦åå°è¿è¡Œ
        lora_path: LoRA è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        lora_name: LoRA åç§°ï¼ˆå¯é€‰ï¼‰

    Returns:
        è¿›ç¨‹å¯¹è±¡ï¼ˆå¦‚æœæ˜¯daemonæ¨¡å¼ï¼‰
    """
    # æ£€æŸ¥ vLLM æ˜¯å¦å®‰è£…
    try:
        import vllm  # noqa: F401
    except ImportError:
        console.print("[yellow]âš ï¸  vLLM æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...[/yellow]")
        subprocess.run(["pip", "install", "vllm"], check=True)
        console.print("[green]âœ… vLLM å®‰è£…å®Œæˆ[/green]\n")

    # æ„å»ºå‘½ä»¤
    cmd = [
        "python",
        "-m",
        "vllm.entrypoints.openai.api_server",
        "--model",
        str(model_path),
        "--host",
        host,
        "--port",
        str(port),
        "--gpu-memory-utilization",
        str(gpu_memory_utilization),
    ]

    if lora_path and lora_name:
        cmd.extend(["--enable-lora", "--lora-modules", f"{lora_name}={lora_path}"])

    console.print(f"[cyan]ğŸ“¡ å¯åŠ¨å‘½ä»¤: {' '.join(cmd)}[/cyan]\n")
    console.print(
        Panel.fit(
            f"[bold green]ğŸ‰ æœåŠ¡å¯åŠ¨ä¸­...[/bold green]\n\n"
            f"ğŸ“ API åœ°å€: [cyan]http://{host}:{port}[/cyan]\n"
            f"ğŸ¤– æ¨¡å‹: [green]{model_path.name}[/green]\n\n"
            f"[bold]ä½¿ç”¨æ–¹å¼:[/bold]\n"
            f"  sage chat --backend compatible --base-url http://localhost:{port}/v1\n\n"
            f"[yellow]æŒ‰ Ctrl+C åœæ­¢æœåŠ¡[/yellow]",
            border_style="green",
        )
    )

    if daemon:
        # åå°è¿è¡Œ
        log_file = Path.cwd() / f"vllm_{model_path.name}.log"
        pid_file = Path.cwd() / f"vllm_{model_path.name}.pid"

        with open(log_file, "w") as f:
            process = subprocess.Popen(
                cmd, stdout=f, stderr=subprocess.STDOUT, start_new_session=True
            )

        with open(pid_file, "w") as f:
            f.write(str(process.pid))

        console.print("\n[green]âœ… æœåŠ¡å·²åœ¨åå°å¯åŠ¨[/green]")
        console.print(f"PID: {process.pid}")
        console.print(f"æ—¥å¿—: [cyan]{log_file}[/cyan]")
        console.print(f"\nåœæ­¢æœåŠ¡: [cyan]kill {process.pid}[/cyan]")

        return process
    else:
        # å‰å°è¿è¡Œ
        subprocess.run(cmd)
        return None
