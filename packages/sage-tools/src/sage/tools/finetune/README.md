# SAGE Finetune - è½»é‡çº§å¤§æ¨¡å‹å¾®è°ƒå·¥å…·

ä¸€ä¸ªç®€å•ã€é«˜æ•ˆã€æ˜“äºæ‰©å±•çš„å¤§æ¨¡å‹å¾®è°ƒå·¥å…·ï¼Œæ”¯æŒ LoRAã€é‡åŒ–è®­ç»ƒã€æ··åˆç²¾åº¦ç­‰ç‰¹æ€§ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **ä¸€é”®å¾®è°ƒ**: æ— éœ€å¤æ‚é…ç½®ï¼Œå¿«é€Ÿå¼€å§‹
- ğŸ’¾ **å†…å­˜ä¼˜åŒ–**: æ”¯æŒ 8-bit/4-bit é‡åŒ–ï¼ŒRTX 3060 ä¹Ÿèƒ½è®­ç»ƒå¤§æ¨¡å‹
- ğŸ”„ **æ— ç¼é›†æˆ**: å¾®è°ƒåçš„æ¨¡å‹å¯ç›´æ¥åœ¨ `sage chat` ä¸­ä½¿ç”¨
- âš¡ **è‡ªåŠ¨åŒ–**: è‡ªåŠ¨æ£€æµ‹ã€å¯åŠ¨ã€ç®¡ç†å¾®è°ƒæ¨¡å‹æœåŠ¡
- ğŸ“Š **å¯è§†åŒ–**: é›†æˆ TensorBoard å’Œ Wandb

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ å¾®è°ƒæ¨¡å‹

```bash
# å¿«é€Ÿå¾®è°ƒä¸€ä¸ªä»£ç ç†è§£æ¨¡å‹ï¼ˆçº¦ 10-30 åˆ†é’Ÿï¼‰
sage finetune quickstart code

# å¯ç”¨ä»»åŠ¡ç±»å‹: code | qa | chat | instruction | custom
# æ¨¡å‹ä¿å­˜åœ¨: ~/.sage/finetune_output/<ä»»åŠ¡ç±»å‹>/
```

### 2ï¸âƒ£ ä½¿ç”¨å¾®è°ƒæ¨¡å‹

```bash
# æ–¹å¼ 1: ç»Ÿä¸€å‘½ä»¤ï¼ˆæ¨èï¼‰
sage chat --backend finetune --finetune-model code

# æ–¹å¼ 2: å¿«æ·å‘½ä»¤
sage finetune chat code

# æ–¹å¼ 3: ç»“åˆ RAG æ£€ç´¢
sage chat --backend finetune --finetune-model code --index docs-public
```

**æŸ¥çœ‹å¯ç”¨æ¨¡å‹**:
```bash
ls ~/.sage/finetune_output/  # æ˜¾ç¤ºæ‰€æœ‰å·²å¾®è°ƒçš„æ¨¡å‹
```

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰é…ç½®å¾®è°ƒ

```python
from sage.tools.finetune import LoRATrainer, TrainingConfig, PresetConfigs

# ä½¿ç”¨é¢„è®¾é…ç½®
config = PresetConfigs.rtx_3060()
config.model_name = "Qwen/Qwen2.5-Coder-1.5B-Instruct"
config.data_path = "./my_data.json"
config.output_dir = "./output"

# åˆ›å»ºè®­ç»ƒå™¨
trainer = LoRATrainer(config)
trainer.train()
```

### CLI å®Œæ•´æµç¨‹

```bash
# 1. å‡†å¤‡æ•°æ®ï¼ˆJSON æ ¼å¼ï¼‰
# 2. åˆ›å»ºé…ç½®å¹¶å¼€å§‹è®­ç»ƒ
sage finetune start --task code --auto

# 3. ç›‘æ§è®­ç»ƒï¼ˆåœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ï¼‰
tensorboard --logdir ~/.sage/finetune_output/code/logs

# 4. åˆå¹¶ LoRA æƒé‡
sage finetune merge code

# 5. ä½¿ç”¨æ¨¡å‹èŠå¤©
sage chat --backend finetune --finetune-model code
```

---

## ğŸ› ï¸ Chat Backend é›†æˆ

### Backend ç±»å‹å¯¹æ¯”

| Backend | ç”¨é€” | ç‰¹ç‚¹ |
|---------|------|------|
| `mock` | æµ‹è¯• | æ— éœ€ APIï¼Œè¿”å›æ£€ç´¢æ‘˜è¦ |
| `openai` | OpenAI | å®˜æ–¹ APIï¼Œé«˜è´¨é‡ |
| `finetune` | **å¾®è°ƒæ¨¡å‹** | **æœ¬åœ°æ¨ç†ï¼Œè‡ªåŠ¨ç®¡ç†** |
| `vllm` | vLLM | è¿æ¥ç°æœ‰æœåŠ¡ |
| `ollama` | Ollama | æœ¬åœ°å°æ¨¡å‹ |

### è‡ªåŠ¨åŒ–æµç¨‹

ä½¿ç”¨ `finetune` backend æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ï¼š

1. âœ… æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
2. âœ… æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå¹¶ LoRA æƒé‡
3. âœ… æ£€æµ‹ vLLM æœåŠ¡çŠ¶æ€
4. âœ… å¯åŠ¨æœåŠ¡ï¼ˆå¦‚æœªè¿è¡Œï¼‰
5. âœ… è¿æ¥å¹¶å¼€å§‹å¯¹è¯
6. âœ… é€€å‡ºæ—¶æ¸…ç†èµ„æº

### é…ç½®é€‰é¡¹

```bash
# ç¯å¢ƒå˜é‡é…ç½®
export SAGE_CHAT_BACKEND="finetune"
export SAGE_CHAT_FINETUNE_MODEL="code"
export SAGE_CHAT_FINETUNE_PORT="8000"

# ç„¶åç›´æ¥è¿è¡Œ
sage chat
```

---

## âš™ï¸ é…ç½®ä¸å‚æ•°

### é¢„è®¾é…ç½®

### é¢„è®¾é…ç½®

æ ¹æ®æ˜¾å¡é€‰æ‹©ä¼˜åŒ–é…ç½®ï¼š

| æ˜¾å¡ | é¢„è®¾åç§° | ç‰¹ç‚¹ |
|------|---------|------|
| RTX 3060 (12GB) | `rtx_3060()` | 8-bit é‡åŒ–ï¼Œbatch_size=1 |
| RTX 4090 (24GB) | `rtx_4090()` | æ— é‡åŒ–ï¼Œbatch_size=4 |
| A100 (40GB+) | `a100()` | BF16ï¼Œbatch_size=8 |
| <8GB æ˜¾å­˜ | `minimal()` | 4-bit é‡åŒ–ï¼Œrank=4 |

### æ•°æ®æ ¼å¼

æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼š

**Alpaca æ ¼å¼**:
```json
[{"instruction": "...", "input": "...", "output": "..."}]
```

**QA æ ¼å¼**:
```json
[{"question": "...", "answer": "...", "context": "..."}]
```

**å¯¹è¯æ ¼å¼**:
```json
[{"conversations": [{"role": "user", "content": "..."}, ...]}]
```

### è®­ç»ƒé…ç½®ç¤ºä¾‹

```python
config = TrainingConfig(
    model_name="Qwen/Qwen2.5-Coder-7B-Instruct",
    data_path="./data.json",
    output_dir="./output",
    num_train_epochs=3,
    learning_rate=1e-4,
    load_in_8bit=True,
    lora=LoRAConfig(r=16, lora_alpha=32),
)
```

---

## ğŸ› å¸¸è§é—®é¢˜

### æ˜¾å­˜ä¸è¶³ (OOM)

æŒ‰ä¼˜å…ˆçº§å°è¯•ï¼š
1. å¯ç”¨é‡åŒ–: `load_in_8bit=True` æˆ– `load_in_4bit=True`
2. å‡å°åºåˆ—: `max_length=512`
3. å‡å° batch: `per_device_train_batch_size=1`
4. æ¢¯åº¦æ£€æŸ¥ç‚¹: `gradient_checkpointing=True`
5. å‡å° rank: `lora.r=4`

### æ¨¡å‹æœªæ‰¾åˆ°

```bash
# æŸ¥çœ‹å·²æœ‰æ¨¡å‹
ls ~/.sage/finetune_output/

# ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
sage chat --backend finetune --finetune-model <å®é™…æ¨¡å‹å>
```

### ç›‘æ§è®­ç»ƒ

```bash
# å®æ—¶ç›‘æ§æ˜¾å­˜
watch -n 1 nvidia-smi

# TensorBoard å¯è§†åŒ–
tensorboard --logdir ~/.sage/finetune_output/*/logs
```

---

## ğŸ“š å¼€å‘è€…èµ„æº

- **å¼€å‘è€…æ–‡æ¡£**: [docs/dev-notes/finetune/](../../../../../docs/dev-notes/finetune/)
- **API æ‰©å±•**: æ”¯æŒè‡ªå®šä¹‰ Trainerã€æ•°æ®å¤„ç†ç­‰
- **Issue**: https://github.com/intellistream/SAGE/issues

---

**MIT License** | SAGE ç”Ÿæ€ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿå¯ä½œä¸ºç‹¬ç«‹åŒ…ä½¿ç”¨

