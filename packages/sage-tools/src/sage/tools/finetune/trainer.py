"""
LoRA è®­ç»ƒå™¨æ¨¡å—

æä¾›ç®€å•æ˜“ç”¨çš„ LoRA å¾®è°ƒæ¥å£
"""

import sys
from pathlib import Path

import torch
from peft import LoraConfig as PeftLoraConfig
from peft import get_peft_model
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)

from .config import TrainingConfig
from .data import prepare_dataset


class LoRATrainer:
    """LoRA å¾®è°ƒè®­ç»ƒå™¨

    ç¤ºä¾‹:
        >>> config = TrainingConfig(
        ...     model_name="Qwen/Qwen2.5-Coder-1.5B-Instruct",
        ...     data_path="./data.json",
        ...     output_dir="./output",
        ... )
        >>> trainer = LoRATrainer(config)
        >>> trainer.train()
    """

    def __init__(self, config: TrainingConfig):
        """åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config
        self.model = None
        self.tokenizer = None
        self.dataset = None
        self.trainer = None

    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print("ğŸ¤– åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")

        # é‡åŒ–é…ç½®
        if self.config.load_in_8bit:
            print("âš ï¸  ä½¿ç”¨ 8-bit é‡åŒ–åŠ è½½ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
            load_kwargs = {
                "load_in_8bit": True,
                "device_map": "auto",
                "torch_dtype": torch.float16,
            }
        elif self.config.load_in_4bit:
            print("âš ï¸  ä½¿ç”¨ 4-bit é‡åŒ–åŠ è½½ï¼ˆæœ€å¤§åŒ–èŠ‚çœæ˜¾å­˜ï¼‰")
            from transformers import BitsAndBytesConfig

            load_kwargs = {
                "quantization_config": BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                ),
                "device_map": "auto",
            }
        else:
            print("ğŸ“¦ æ­£å¸¸åŠ è½½æ¨¡å‹")
            load_kwargs = {
                "torch_dtype": "auto",
                "device_map": "auto",
            }

        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(self.config.model_name, **load_kwargs)

        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)

        # è®¾ç½® pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {self.config.model_name}\n")

    def apply_lora(self):
        """åº”ç”¨ LoRA é€‚é…å™¨"""
        print("ğŸ”§ åº”ç”¨ LoRA é€‚é…å™¨...")

        # è½¬æ¢é…ç½®
        lora_config = PeftLoraConfig(
            r=self.config.lora.r,
            lora_alpha=self.config.lora.lora_alpha,
            target_modules=self.config.lora.target_modules,
            lora_dropout=self.config.lora.lora_dropout,
            bias=self.config.lora.bias,
            task_type=self.config.lora.task_type,
        )

        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        print()

    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡æ•°æ®é›†...")

        if self.config.data_path is None:
            raise ValueError("æœªæŒ‡å®šæ•°æ®è·¯å¾„ (data_path)")

        self.dataset = prepare_dataset(
            self.config.data_path,
            self.tokenizer,
            self.config.max_length,
        )
        print()

    def setup_trainer(self):
        """è®¾ç½® Trainer"""
        print("âš™ï¸  é…ç½®è®­ç»ƒå™¨...")

        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=str(self.config.checkpoint_dir),
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            lr_scheduler_type=self.config.lr_scheduler_type,
            warmup_ratio=self.config.warmup_ratio,
            fp16=self.config.fp16,
            bf16=self.config.bf16,
            gradient_checkpointing=self.config.gradient_checkpointing,
            optim=self.config.optim,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            save_total_limit=self.config.save_total_limit,
            logging_dir=str(self.config.log_dir),
            report_to=self.config.report_to,
            dataloader_num_workers=self.config.dataloader_num_workers,
            seed=self.config.seed,
        )

        # åˆ›å»º Trainer
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset,
            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),
        )

        print("âœ… è®­ç»ƒå™¨é…ç½®å®Œæˆ")
        print(f"   â€¢ æœ‰æ•ˆ batch size: {self.config.effective_batch_size}")
        print(f"   â€¢ æ€»æ ·æœ¬æ•°: {len(self.dataset)}")
        print(f"   â€¢ è®­ç»ƒè½®æ•°: {self.config.num_train_epochs}")
        print(
            f"   â€¢ é¢„è®¡æ­¥æ•°: ~{len(self.dataset) // self.config.effective_batch_size * self.config.num_train_epochs}"
        )
        print()

    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        # 1. åŠ è½½æ¨¡å‹
        if self.model is None:
            self.load_model_and_tokenizer()

        # 2. åº”ç”¨ LoRA
        if not hasattr(self.model, "peft_config"):
            self.apply_lora()

        # 3. å‡†å¤‡æ•°æ®
        if self.dataset is None:
            self.prepare_data()

        # 4. è®¾ç½®è®­ç»ƒå™¨
        if self.trainer is None:
            self.setup_trainer()

        # 5. å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...\n")
        print("ğŸ’¡ å»ºè®®:")
        print("  â€¢ åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥é¿å… VS Code å´©æºƒ")
        print("  â€¢ ç›‘æ§æ˜¾å­˜: watch -n 1 nvidia-smi")
        if self.config.load_in_8bit:
            print("  â€¢ å½“å‰ä½¿ç”¨ 8-bit é‡åŒ–")
        if self.config.gradient_checkpointing:
            print("  â€¢ å½“å‰å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        print()

        try:
            self.trainer.train()
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print("\nâŒ æ˜¾å­˜ä¸è¶³ (OOM) é”™è¯¯!")
                print("\nğŸ’¡ å»ºè®®:")
                print(
                    f"  1. å‡å° max_length (å½“å‰ {self.config.max_length} -> {self.config.max_length // 2})"
                )
                print(f"  2. å‡å° batch_size (å½“å‰ {self.config.per_device_train_batch_size})")
                if not self.config.load_in_8bit and not self.config.load_in_4bit:
                    print("  3. å¯ç”¨é‡åŒ–: load_in_8bit=True")
                if not self.config.gradient_checkpointing:
                    print("  4. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: gradient_checkpointing=True")
                print(f"  5. å‡å° LoRA rank (å½“å‰ {self.config.lora.r} -> 4)")
                sys.exit(1)
            else:
                raise

        # 6. ä¿å­˜æ¨¡å‹
        self.save_model()

        # 7. æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        self.print_completion_info()

    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")

        # ä¿å­˜ LoRA æƒé‡
        self.model.save_pretrained(str(self.config.lora_dir))
        self.tokenizer.save_pretrained(str(self.config.lora_dir))

        # ä¿å­˜é…ç½®
        self.config.save(self.config.output_dir / "training_config.json")

        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {self.config.lora_dir}")

    def print_completion_info(self):
        """æ‰“å°å®Œæˆä¿¡æ¯"""
        print(f"\n{'=' * 60}")
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"{'=' * 60}")
        print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"  â€¢ LoRA æƒé‡: {self.config.lora_dir}")
        print(f"  â€¢ æ£€æŸ¥ç‚¹: {self.config.checkpoint_dir}")
        print(f"  â€¢ è®­ç»ƒæ—¥å¿—: {self.config.log_dir}")
        print(f"  â€¢ è®­ç»ƒé…ç½®: {self.config.output_dir / 'training_config.json'}")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("  â€¢ æŸ¥çœ‹è®­ç»ƒæ›²çº¿:")
        print(f"    tensorboard --logdir {self.config.log_dir}")
        print("  â€¢ åˆå¹¶æƒé‡:")
        print(f"    sage finetune merge {self.config.output_dir.name}")
        print("  â€¢ æµ‹è¯•æ¨¡å‹:")
        print(f"    sage finetune chat {self.config.output_dir.name}")
        print()


def train_from_meta(output_dir: str | Path):
    """ä»å…ƒä¿¡æ¯æ–‡ä»¶è®­ç»ƒ

    è¿™ä¸ªå‡½æ•°ç”¨äºå…¼å®¹æ—§çš„ simple_finetune.py è„šæœ¬

    Args:
        output_dir: è¾“å‡ºç›®å½•ï¼ˆåŒ…å« finetune_meta.jsonï¼‰
    """
    output_dir = Path(output_dir)

    # è¯»å–å…ƒä¿¡æ¯
    meta_file = output_dir / "finetune_meta.json"
    if not meta_file.exists():
        print(f"âŒ æœªæ‰¾åˆ°å…ƒä¿¡æ¯æ–‡ä»¶: {meta_file}")
        print("è¯·å…ˆè¿è¡Œ: sage finetune quickstart")
        sys.exit(1)

    import json

    with open(meta_file) as f:
        meta = json.load(f)

    # åˆ›å»ºé…ç½®
    config = TrainingConfig(
        model_name=meta.get("model", "Qwen/Qwen2.5-Coder-1.5B-Instruct"),
        data_path=Path(meta.get("dataset")),
        output_dir=output_dir,
    )

    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
    trainer = LoRATrainer(config)
    trainer.train()


if __name__ == "__main__":
    # æ”¯æŒå‘½ä»¤è¡Œè°ƒç”¨
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python -m sage.tools.finetune.trainer <output_dir>")
        print("\nç¤ºä¾‹:")
        print("  python -m sage.tools.finetune.trainer finetune_output/code")
        sys.exit(1)

    train_from_meta(sys.argv[1])
