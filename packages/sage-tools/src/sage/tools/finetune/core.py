#!/usr/bin/env python3
"""
Finetune CLI - Core Logic
Ê†∏ÂøÉÈÄªËæëÔºöÊï∞ÊçÆÂáÜÂ§á„ÄÅÈÖçÁΩÆÁîüÊàê
"""

import json
from pathlib import Path
from typing import Optional, Dict, Any

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

from .models import FinetuneTask
from .utils import collect_sage_code_files

console = Console()


def prepare_training_data(
    task_type: FinetuneTask,
    root_dir: Path,
    output_dir: Path,
    format: str = "alpaca",
    custom_data_path: Optional[Path] = None,
    **kwargs
) -> Path:
    """ÂáÜÂ§áËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºàÊîØÊåÅÂ§öÁßç‰ªªÂä°Á±ªÂûãÔºâ"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    training_data = []
    
    if task_type == FinetuneTask.CODE_UNDERSTANDING:
        # ‰ª£Á†ÅÁêÜËß£‰ªªÂä° - Êî∂ÈõÜ‰ª£Á†ÅÊñá‰ª∂
        extensions = kwargs.get('extensions', ['.py', '.yaml', '.yml', '.toml', '.md', '.rst'])
        files = collect_sage_code_files(root_dir, extensions=extensions)
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task(
                f"üìù ÂáÜÂ§á‰ª£Á†ÅÁêÜËß£Êï∞ÊçÆ ({len(files)} ‰∏™Êñá‰ª∂)...",
                total=len(files)
            )
            
            for file_path in files:
                try:
                    content = file_path.read_text(encoding='utf-8')
                    rel_path = file_path.relative_to(root_dir)
                    
                    if format == "alpaca":
                        # Êñá‰ª∂ÂäüËÉΩËß£Èáä
                        training_data.append({
                            "instruction": f"ËØ∑Ëß£ÈáäÈ°πÁõÆ‰∏≠ {rel_path} Êñá‰ª∂ÁöÑÂäüËÉΩÂíåÂÆûÁé∞",
                            "input": "",
                            "output": content
                        })
                        
                        # ‰ª£Á†ÅÈóÆÁ≠î
                        if file_path.suffix == '.py':
                            training_data.append({
                                "instruction": f"È°πÁõÆ‰∏≠ {rel_path} Êñá‰ª∂ÂåÖÂê´Âì™‰∫õÁ±ªÂíåÂáΩÊï∞ÔºüËØ∑ËØ¶ÁªÜËØ¥Êòé„ÄÇ",
                                "input": content[:1500],
                                "output": f"ËøôÊòØ {rel_path} ÁöÑ‰ª£Á†ÅÂÆûÁé∞ÔºåÂåÖÂê´‰∫ÜÊ†∏ÂøÉÂäüËÉΩ„ÄÇËÆ©Êàë‰∏∫‰Ω†ËØ¶ÁªÜÂàÜÊûê..."
                            })
                            
                            # ‰ª£Á†Å‰øÆÊîπÂª∫ËÆÆ
                            training_data.append({
                                "instruction": f"Â¶Ç‰ΩïÊîπËøõ {rel_path} ‰∏≠ÁöÑ‰ª£Á†ÅÔºü",
                                "input": content[:1000],
                                "output": f"Âü∫‰∫é {rel_path} ÁöÑ‰ª£Á†ÅÂàÜÊûêÔºåÊàëÂª∫ËÆÆ‰ªé‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢ÊîπËøõ..."
                            })
                    
                    progress.update(task, advance=1)
                except Exception as e:
                    console.print(f"[yellow]‚ö†Ô∏è  Ë∑≥ËøáÊñá‰ª∂ {file_path}: {e}[/yellow]")
                    continue
    
    elif task_type == FinetuneTask.QA_PAIRS:
        # ÈóÆÁ≠îÂØπ‰ªªÂä°
        if custom_data_path and custom_data_path.exists():
            with open(custom_data_path, 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
            
            for item in qa_data:
                if format == "alpaca":
                    training_data.append({
                        "instruction": item.get("question", ""),
                        "input": item.get("context", ""),
                        "output": item.get("answer", "")
                    })
        else:
            console.print("[yellow]‚ö†Ô∏è  ÈúÄË¶ÅÊèê‰æõÈóÆÁ≠îÊï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑ[/yellow]")
    
    elif task_type == FinetuneTask.INSTRUCTION:
        # Êåá‰ª§ÂæÆË∞É‰ªªÂä°
        if custom_data_path and custom_data_path.exists():
            with open(custom_data_path, 'r', encoding='utf-8') as f:
                instruction_data = json.load(f)
            
            for item in instruction_data:
                if format == "alpaca":
                    training_data.append({
                        "instruction": item.get("instruction", ""),
                        "input": item.get("input", ""),
                        "output": item.get("output", "")
                    })
        else:
            console.print("[yellow]‚ö†Ô∏è  ÈúÄË¶ÅÊèê‰æõÊåá‰ª§Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑ[/yellow]")
    
    elif task_type == FinetuneTask.CHAT:
        # ÂØπËØùÂæÆË∞É‰ªªÂä°
        if custom_data_path and custom_data_path.exists():
            with open(custom_data_path, 'r', encoding='utf-8') as f:
                chat_data = json.load(f)
            
            for item in chat_data:
                if format == "chat":
                    training_data.append({
                        "conversations": item.get("conversations", [])
                    })
                elif format == "alpaca":
                    # ËΩ¨Êç¢‰∏∫alpacaÊ†ºÂºè
                    conversations = item.get("conversations", [])
                    if len(conversations) >= 2:
                        training_data.append({
                            "instruction": conversations[0].get("content", ""),
                            "input": "",
                            "output": conversations[1].get("content", "")
                        })
        else:
            console.print("[yellow]‚ö†Ô∏è  ÈúÄË¶ÅÊèê‰æõÂØπËØùÊï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑ[/yellow]")
    
    elif task_type == FinetuneTask.CUSTOM:
        # Ëá™ÂÆö‰πâÊï∞ÊçÆÈõÜ
        if custom_data_path and custom_data_path.exists():
            with open(custom_data_path, 'r', encoding='utf-8') as f:
                training_data = json.load(f)
            console.print(f"‚úÖ Â∑≤Âä†ËΩΩËá™ÂÆö‰πâÊï∞ÊçÆÈõÜ: {len(training_data)} Êù°")
        else:
            console.print("[red]‚ùå Ëá™ÂÆö‰πâ‰ªªÂä°ÈúÄË¶ÅÊèê‰æõÊï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑ[/red]")
            import typer
            raise typer.Exit(1)
    
    # ‰øùÂ≠òËÆ≠ÁªÉÊï∞ÊçÆ
    output_file = output_dir / f"training_data_{task_type.value}_{format}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, ensure_ascii=False, indent=2)
    
    console.print(f"\n‚úÖ ËÆ≠ÁªÉÊï∞ÊçÆÂ∑≤ÁîüÊàê: [cyan]{output_file}[/cyan]")
    console.print(f"   üìä ÂÖ± {len(training_data)} Êù°ËÆ≠ÁªÉÊ†∑Êú¨")
    
    return output_file


def generate_training_config(
    model_name: str,
    dataset_path: Path,
    output_dir: Path,
    framework: str = "llama-factory"
) -> Path:
    """ÁîüÊàêËÆ≠ÁªÉÈÖçÁΩÆÊñá‰ª∂"""
    
    config = {}
    
    if framework == "llama-factory":
        config = {
            "model_name_or_path": model_name,
            "stage": "sft",
            "do_train": True,
            "finetuning_type": "lora",
            "lora_target": "all",
            "dataset": str(dataset_path),
            "template": "qwen",
            "cutoff_len": 4096,
            "max_samples": 10000,
            "overwrite_cache": True,
            "preprocessing_num_workers": 16,
            "output_dir": str(output_dir / "checkpoints"),
            "logging_steps": 10,
            "save_steps": 500,
            "plot_loss": True,
            "overwrite_output_dir": True,
            "per_device_train_batch_size": 2,
            "gradient_accumulation_steps": 8,
            "learning_rate": 5e-5,
            "num_train_epochs": 3,
            "lr_scheduler_type": "cosine",
            "warmup_ratio": 0.1,
            "fp16": True,
            "lora_rank": 8,
            "lora_alpha": 16,
            "lora_dropout": 0.05,
        }
    elif framework == "unsloth":
        config = {
            "model_name": model_name,
            "max_seq_length": 4096,
            "load_in_4bit": True,
            "dataset": str(dataset_path),
            "dataset_text_field": "text",
            "packing": False,
            "output_dir": str(output_dir / "checkpoints"),
            "num_train_epochs": 3,
            "per_device_train_batch_size": 2,
            "gradient_accumulation_steps": 4,
            "warmup_steps": 10,
            "learning_rate": 2e-4,
            "fp16": True,
            "logging_steps": 1,
            "optim": "adamw_8bit",
            "weight_decay": 0.01,
            "lr_scheduler_type": "linear",
            "seed": 3407,
        }
    
    config_path = output_dir / f"{framework}_config.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2)
    
    return config_path
