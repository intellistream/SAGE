"""Predefined pipeline templates for the SAGE pipeline builder."""

from __future__ import annotations

from dataclasses import dataclass, field
from textwrap import dedent
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple


@dataclass(frozen=True)
class TemplateParameter:
    """Describes a configurable field for a pipeline template."""

    path: Tuple[str, ...]
    prompt: str
    default: Any
    value_type: type = str
    help_text: str = ""
    choices: Optional[Sequence[Any]] = None


@dataclass(frozen=True)
class PipelineTemplate:
    """A reusable pipeline blueprint with configuration and code skeleton."""

    key: str
    name: str
    description: str
    intent_keywords: Tuple[str, ...]
    default_pipeline_name: str
    default_pipeline_description: str
    config: Dict[str, Any]
    code_template: str
    parameters: Tuple[TemplateParameter, ...] = field(default_factory=tuple)

    def matches_intent(self, intent: str) -> bool:
        lowered = intent.lower()
        return any(keyword in lowered for keyword in self.intent_keywords)

    def clone_config(self) -> Dict[str, Any]:
        import copy

        return copy.deepcopy(self.config)


def _rag_qa_template() -> PipelineTemplate:
    code = dedent(
        '''\
"""Auto-generated SAGE pipeline.

This script was created by `sage pipeline build` and demonstrates a
dense-retrieval RAG pipeline. Adjust imports and configuration to match
your deployment environment before running in production.
"""

        from __future__ import annotations

        import argparse
        from pathlib import Path

        from sage.common.utils.config.loader import load_config
        from sage.core.api.local_environment import LocalEnvironment
        from sage.libs.io_utils.sink import TerminalSink
        from sage.libs.io_utils.source import FileSource
        from sage.libs.rag.generator import OpenAIGenerator
        from sage.libs.rag.promptor import QAPromptor
        from sage.libs.rag.retriever import ChromaRetriever


        def build_pipeline(config: dict):
            """Create the streaming pipeline using the provided configuration."""

            env = LocalEnvironment(config["pipeline"]["name"])

            query_stream = (
                env.from_source(FileSource, config["source"])
                .map(ChromaRetriever, config["retriever"])
                .map(QAPromptor, config["promptor"])
                .map(OpenAIGenerator, config["generator"])
                .sink(TerminalSink, config["sink"])
            )

            return env, query_stream


        def run(config_path: Path) -> None:
            config = load_config(str(config_path))
            env, _ = build_pipeline(config)

            env.submit()
            env.run_once()
            env.close()


        def main() -> None:
            parser = argparse.ArgumentParser(description="Run the generated SAGE pipeline")
            parser.add_argument(
                "--config",
                type=Path,
                default=Path(__file__).with_name("pipeline_config.yaml"),
                help="Path to the pipeline configuration file.",
            )
            args = parser.parse_args()
            run(args.config)


        if __name__ == "__main__":
            main()
        '''
    ).strip()

    config: Dict[str, Any] = {
        "pipeline": {
            "name": "rag_qa_pipeline",
            "description": "RAG pipeline generated by SAGE",
            "version": "1.0.0",
            "type": "local",
        },
        "source": {
            "platform": "local",
            "data_path": "examples/data/sample/question.txt",
            "encoding": "utf-8",
            "loop_reading": False,
        },
        "retriever": {
            "dimension": 384,
            "top_k": 4,
            "similarity_threshold": 0.75,
            "embedding": {
                "method": "default",
                "model": "sentence-transformers/all-MiniLM-L6-v2",
            },
            "chroma": {
                "collection": "sage_rag_pipeline",
                "persist_directory": "./storage/chroma",
                "knowledge_file": "examples/data/sample/knowledge.jsonl",
            },
        },
        "promptor": {
            "platform": "local",
            "use_short_answer": False,
        },
        "generator": {
            "method": "openai",
            "model_name": "qwen-turbo",
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": "${DASHSCOPE_API_KEY}",
            "seed": 42,
            "temperature": 0.6,
            "max_tokens": 1024,
        },
        "sink": {
            "platform": "local",
            "format": "terminal",
        },
    }

    parameters = (
        TemplateParameter(
            path=("pipeline", "name"),
            prompt="Pipeline 名称",
            default="rag_qa_pipeline",
        ),
        TemplateParameter(
            path=("source", "data_path"),
            prompt="问题数据文件路径",
            default="examples/data/sample/question.txt",
            help_text="每行一个问题，作为FileSource的输入",
        ),
        TemplateParameter(
            path=("retriever", "top_k"),
            prompt="Retriever top_k",
            default=4,
            value_type=int,
        ),
        TemplateParameter(
            path=("generator", "model_name"),
            prompt="生成模型名称",
            default="qwen-turbo",
        ),
        TemplateParameter(
            path=("generator", "base_url"),
            prompt="生成模型Base URL",
            default="https://dashscope.aliyuncs.com/compatible-mode/v1",
        ),
        TemplateParameter(
            path=("generator", "api_key"),
            prompt="生成模型API Key",
            default="${DASHSCOPE_API_KEY}",
        ),
    )

    return PipelineTemplate(
        key="rag-qa",
        name="RAG 问答流水线",
        description="使用Chroma向量检索和OpenAI兼容模型的问答Pipeline",
        intent_keywords=("rag", "qa", "问答", "retrieval", "知识库"),
        default_pipeline_name="rag_qa_pipeline",
        default_pipeline_description="Dense Retrieval QA pipeline",
        config=config,
        code_template=code,
        parameters=parameters,
    )


def _local_summarization_template() -> PipelineTemplate:
    code = dedent(
        '''\
"""Auto-generated summarization pipeline."""

        from __future__ import annotations

        import argparse
        from pathlib import Path

        from sage.common.utils.config.loader import load_config
        from sage.core.api.local_environment import LocalEnvironment
        from sage.libs.io_utils.sink import PrintSink
        from sage.libs.io_utils.source import FileSource
        from sage.libs.rag.promptor import SummarizationPromptor
        from sage.libs.rag.generator import HFGenerator


        def build_pipeline(config: dict):
            env = LocalEnvironment(config["pipeline"]["name"])

            data_stream = (
                env.from_source(FileSource, config["source"])
                .map(SummarizationPromptor, config["promptor"])
                .map(HFGenerator, config["generator"])
                .sink(PrintSink, config["sink"])
            )

            return env, data_stream


        def run(config_path: Path) -> None:
            config = load_config(str(config_path))
            env, _ = build_pipeline(config)

            env.submit()
            env.run_once()
            env.close()


        def main() -> None:
            parser = argparse.ArgumentParser(description="Run the generated summarization pipeline")
            parser.add_argument(
                "--config",
                type=Path,
                default=Path(__file__).with_name("pipeline_config.yaml"),
                help="Path to the pipeline configuration file.",
            )
            args = parser.parse_args()
            run(args.config)


        if __name__ == "__main__":
            main()
        '''
    ).strip()

    config: Dict[str, Any] = {
        "pipeline": {
            "name": "local_summarization_pipeline",
            "description": "Summarization pipeline generated by SAGE",
            "version": "1.0.0",
            "type": "local",
        },
        "source": {
            "platform": "local",
            "data_path": "examples/data/sample/documents.txt",
            "encoding": "utf-8",
            "loop_reading": False,
        },
        "promptor": {
            "platform": "local",
            "chunk_size": 512,
            "overlap": 128,
        },
        "generator": {
            "method": "hf",
            "model_name": "facebook/bart-large-cnn",
            "max_length": 512,
        },
        "sink": {
            "prefix": "摘要",
            "separator": " => ",
            "colored": True,
            "quiet": False,
        },
    }

    parameters = (
        TemplateParameter(
            path=("pipeline", "name"),
            prompt="Pipeline 名称",
            default="local_summarization_pipeline",
        ),
        TemplateParameter(
            path=("source", "data_path"),
            prompt="输入文档路径",
            default="examples/data/sample/documents.txt",
        ),
        TemplateParameter(
            path=("generator", "model_name"),
            prompt="HuggingFace 模型名称",
            default="facebook/bart-large-cnn",
        ),
    )

    return PipelineTemplate(
        key="summarization",
        name="本地摘要管道",
        description="使用本地HuggingFace模型进行文档摘要",
        intent_keywords=("summary", "summarize", "摘要", "compress"),
        default_pipeline_name="local_summarization_pipeline",
        default_pipeline_description="Summarize documents with HF models",
        config=config,
        code_template=code,
        parameters=parameters,
    )


def get_pipeline_templates() -> List[PipelineTemplate]:
    """Return all built-in pipeline templates."""

    return [
        _rag_qa_template(),
        _local_summarization_template(),
    ]


__all__ = [
    "PipelineTemplate",
    "TemplateParameter",
    "get_pipeline_templates",
]
