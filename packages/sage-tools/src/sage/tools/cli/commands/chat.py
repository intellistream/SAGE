#!/usr/bin/env python3
"""SAGE Chat CLI - Embedded programming assistant backed by SageDB."""
from __future__ import annotations

import hashlib
import json
import os
import re
import shutil
import tempfile
import textwrap
import urllib.request
import zipfile
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

import typer
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from sage.common.components.sage_embedding import get_embedding_model
from sage.common.components.sage_embedding.embedding_model import EmbeddingModel
from sage.common.config.output_paths import (
    find_sage_project_root,
    get_sage_paths,
)

# Âª∂ËøüÂØºÂÖ• sage_db ‰ª•ÂÖÅËÆ∏ CLI Âú®Ê≤°Êúâ C++ Êâ©Â±ïÁöÑÊÉÖÂÜµ‰∏ãÂêØÂä®
# from sage.middleware.components.sage_db.python.sage_db import SageDB, SageDBException
from sage.tools.cli.commands import pipeline as pipeline_builder
from sage.tools.cli.commands.pipeline_domain import load_domain_contexts
from sage.tools.cli.commands.pipeline_knowledge import get_default_knowledge_base
from sage.tools.cli.core.exceptions import CLIException

console = Console()

# sage_db ÈúÄË¶Å C++ Êâ©Â±ïÔºå‰ΩøÁî®Âª∂ËøüÂØºÂÖ•
SAGE_DB_AVAILABLE = False
SAGE_DB_IMPORT_ERROR: Optional[Exception] = None
SageDB = None  # type: ignore
SageDBException = Exception  # type: ignore


def _lazy_import_sage_db():
    """Âª∂ËøüÂØºÂÖ• sage_dbÔºåÂè™Âú®ÈúÄË¶ÅÊó∂ÂØºÂÖ•"""
    global SageDB, SageDBException, SAGE_DB_AVAILABLE, SAGE_DB_IMPORT_ERROR

    if SAGE_DB_AVAILABLE:
        return  # Â∑≤ÁªèÊàêÂäüÂØºÂÖ•

    try:
        from sage.middleware.components.sage_db.python.sage_db import SageDB as _SageDB
        from sage.middleware.components.sage_db.python.sage_db import (
            SageDBException as _SageDBException,
        )

        SageDB = _SageDB
        SageDBException = _SageDBException
        SAGE_DB_AVAILABLE = True
        SAGE_DB_IMPORT_ERROR = None
    except (ImportError, ModuleNotFoundError) as e:
        SAGE_DB_AVAILABLE = False
        SAGE_DB_IMPORT_ERROR = e


DEFAULT_INDEX_NAME = "docs-public"
DEFAULT_CHUNK_SIZE = 800
DEFAULT_CHUNK_OVERLAP = 160
DEFAULT_TOP_K = 4
DEFAULT_BACKEND = "mock"
DEFAULT_EMBEDDING_METHOD = "hash"
DEFAULT_FIXED_DIM = 384
DEFAULT_FINETUNE_MODEL = "sage_code_expert"
DEFAULT_FINETUNE_PORT = 8000
SUPPORTED_MARKDOWN_SUFFIXES = {".md", ".markdown", ".mdx"}

METHODS_REQUIRE_MODEL = {
    "hf",
    "openai",
    "jina",
    "cohere",
    "zhipu",
    "bedrock",
    "ollama",
    "siliconcloud",
    "nvidia_openai",
    "lollms",
}

GITHUB_DOCS_ZIP_URL = (
    "https://github.com/intellistream/SAGE-Pub/archive/refs/heads/main.zip"
)

app = typer.Typer(
    help="üß≠ ÂµåÂÖ•Âºè SAGE ÁºñÁ®ãÂä©Êâã (Docs + SageDB + LLM)",
    invoke_without_command=True,
)


@dataclass
class ChatManifest:
    """Metadata describing a built knowledge index."""

    index_name: str
    db_path: Path
    created_at: str
    source_dir: str
    embedding: Dict[str, object]
    chunk_size: int
    chunk_overlap: int
    num_documents: int
    num_chunks: int

    @property
    def embed_config(self) -> Dict[str, object]:
        return self.embedding


def ensure_sage_db() -> None:
    """Á°Æ‰øù SageDB Êâ©Â±ïÂèØÁî®ÔºåÂ¶ÇÊûú‰∏çÂèØÁî®ÂàôÊèêÂâçÈÄÄÂá∫„ÄÇ"""
    # Â∞ùËØïÂª∂ËøüÂØºÂÖ•
    _lazy_import_sage_db()

    if SAGE_DB_AVAILABLE:
        return

    message = (
        "[red]SageDB C++ Êâ©Â±ï‰∏çÂèØÁî®ÔºåÊó†Ê≥ï‰ΩøÁî® `sage chat`„ÄÇ[/red]\n"
        "ËØ∑ÂÖàÈÄöËøáÂëΩ‰ª§ `sage extensions install sage_db` ÊûÑÂª∫ SageDB ÁªÑ‰ª∂ÔºàÂ¶ÇÈúÄÈáçÊñ∞ÂÆâË£ÖÂèØÂä†‰∏ä --forceÔºâ„ÄÇ"
    )
    if SAGE_DB_IMPORT_ERROR:
        message += f"\nÂéüÂßãÈîôËØØ: {SAGE_DB_IMPORT_ERROR}"
    console.print(message)
    raise typer.Exit(code=1)


def resolve_index_root(index_root: Optional[str]) -> Path:
    if index_root:
        root = Path(index_root).expanduser().resolve()
        root.mkdir(parents=True, exist_ok=True)
        return root
    paths = get_sage_paths()
    cache_dir = paths.cache_dir / "chat"
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def default_source_dir() -> Path:
    project_root = find_sage_project_root()
    if not project_root:
        project_root = Path.cwd()
    candidate = project_root / "docs-public" / "docs_src"
    return candidate


def manifest_path(index_root: Path, index_name: str) -> Path:
    return index_root / f"{index_name}.manifest.json"


def db_file_path(index_root: Path, index_name: str) -> Path:
    return index_root / f"{index_name}.sagedb"


def load_manifest(index_root: Path, index_name: str) -> ChatManifest:
    path = manifest_path(index_root, index_name)
    if not path.exists():
        raise FileNotFoundError(
            f"Êú™ÊâæÂà∞Á¥¢Âºï manifest: {path}. ËØ∑ÂÖàËøêË°å `sage chat ingest`."
        )
    payload = json.loads(path.read_text(encoding="utf-8"))
    manifest = ChatManifest(
        index_name=index_name,
        db_path=Path(payload["db_path"]),
        created_at=payload["created_at"],
        source_dir=payload["source_dir"],
        embedding=payload["embedding"],
        chunk_size=payload["chunk_size"],
        chunk_overlap=payload["chunk_overlap"],
        num_documents=payload.get("num_documents", 0),
        num_chunks=payload.get("num_chunks", 0),
    )
    return manifest


def save_manifest(
    index_root: Path,
    index_name: str,
    manifest: ChatManifest,
) -> None:
    path = manifest_path(index_root, index_name)
    payload = {
        "index_name": manifest.index_name,
        "db_path": str(manifest.db_path),
        "created_at": manifest.created_at,
        "source_dir": manifest.source_dir,
        "embedding": manifest.embedding,
        "chunk_size": manifest.chunk_size,
        "chunk_overlap": manifest.chunk_overlap,
        "num_documents": manifest.num_documents,
        "num_chunks": manifest.num_chunks,
    }
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


def build_embedder(config: Dict[str, object]) -> Any:
    """ÊûÑÂª∫ embedder ÂÆû‰æãÔºà‰ΩøÁî®Êñ∞ÁöÑÁªü‰∏ÄÊé•Âè£Ôºâ

    Args:
        config: embedding ÈÖçÁΩÆ
            - method: ÊñπÊ≥ïÂêç (hash, hf, openai, mockembedder, ...)
            - params: ÊñπÊ≥ïÁâπÂÆöÂèÇÊï∞

    Returns:
        BaseEmbedding ÂÆû‰æã

    Examples:
        >>> config = {"method": "hash", "params": {"dim": 384}}
        >>> emb = build_embedder(config)
        >>> vec = emb.embed("test")
    """
    method = str(config.get("method", DEFAULT_EMBEDDING_METHOD))
    params = dict(config.get("params", {}))

    # Áªü‰∏Ä‰ΩøÁî®Êñ∞Êé•Âè£Ôºå‰∏çÈúÄË¶ÅÁâπÊÆäÂ§ÑÁêÜÔºÅ
    return get_embedding_model(method, **params)


def iter_markdown_files(source: Path) -> Iterable[Path]:
    for path in sorted(source.rglob("*")):
        if path.is_file() and path.suffix.lower() in SUPPORTED_MARKDOWN_SUFFIXES:
            yield path


_HEADING_PATTERN = re.compile(r"^(#{1,6})\s+(?P<title>.+?)\s*$")


def parse_markdown_sections(content: str) -> List[Dict[str, str]]:
    sections: List[Dict[str, str]] = []
    current_title = "Introduction"
    current_lines: List[str] = []

    for raw_line in content.splitlines():
        match = _HEADING_PATTERN.match(raw_line.strip())
        if match:
            if current_lines:
                sections.append(
                    {
                        "heading": current_title,
                        "content": "\n".join(current_lines).strip(),
                    }
                )
                current_lines = []
            current_title = match.group("title").strip()
        else:
            current_lines.append(raw_line)

    if current_lines:
        sections.append(
            {"heading": current_title, "content": "\n".join(current_lines).strip()}
        )

    return [section for section in sections if section["content"]]


def chunk_text(content: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    normalized = re.sub(r"\n{3,}", "\n\n", content).strip()
    if not normalized:
        return []

    start = 0
    length = len(normalized)
    step = max(1, chunk_size - chunk_overlap)
    chunks: List[str] = []

    while start < length:
        end = min(length, start + chunk_size)
        chunk = normalized[start:end]
        if end < length:
            boundary = max(chunk.rfind("\n"), chunk.rfind("„ÄÇ"), chunk.rfind("."))
            if boundary >= 0 and boundary > len(chunk) * 0.4:
                end = start + boundary
                chunk = normalized[start:end]
        chunks.append(chunk.strip())
        start += step

    return [c for c in chunks if c]


def slugify(text: str) -> str:
    slug = re.sub(r"[^\w\-]+", "-", text.lower()).strip("-")
    slug = re.sub(r"-+", "-", slug)
    return slug or "section"


def truncate_text(text: str, limit: int = 480) -> str:
    if len(text) <= limit:
        return text
    return text[: limit - 3] + "..."


def sanitize_metadata_value(value: str) -> str:
    cleaned = value.replace("\r", " ").replace("\n", " ")
    cleaned = cleaned.replace('"', "'")
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def ensure_docs_corpus(index_root: Path) -> Path:
    """Ensure we have a docs-public/docs_src directory available."""

    local_source = default_source_dir()
    if local_source.exists():
        return local_source

    cache_root = index_root / "remote_docs"
    docs_path = cache_root / "docs_src"
    if docs_path.exists():
        return docs_path

    cache_root.mkdir(parents=True, exist_ok=True)
    console.print(
        "üåê Êú™Ê£ÄÊµãÂà∞Êú¨Âú∞ docs-public/docs_srcÔºåÊ≠£Âú®‰∏ãËΩΩÂÆòÊñπÊñáÊ°£ÂåÖ...",
        style="cyan",
    )

    fd, tmp_path = tempfile.mkstemp(prefix="sage_docs_", suffix=".zip")
    os.close(fd)
    tmp_file = Path(tmp_path)
    try:
        urllib.request.urlretrieve(GITHUB_DOCS_ZIP_URL, tmp_file)
        with zipfile.ZipFile(tmp_file, "r") as zf:
            zf.extractall(cache_root)
    except Exception as exc:
        if tmp_file.exists():
            tmp_file.unlink()
        raise RuntimeError(f"‰∏ãËΩΩ docs-public ÊñáÊ°£Â§±Ë¥•: {exc}") from exc

    if tmp_file.exists():
        tmp_file.unlink()

    extracted_docs: Optional[Path] = None
    for candidate in cache_root.glob("**/docs_src"):
        if candidate.is_dir():
            extracted_docs = candidate
            break

    if extracted_docs is None:
        raise RuntimeError("‰∏ãËΩΩÁöÑÊñáÊ°£ÂåÖ‰∏≠Êú™ÊâæÂà∞ docs_src ÁõÆÂΩï")

    if docs_path.exists() and docs_path == extracted_docs:
        return docs_path

    if not docs_path.exists():
        docs_path.mkdir(parents=True, exist_ok=True)
        for item in extracted_docs.iterdir():
            shutil.move(str(item), docs_path / item.name)
    return docs_path


def bootstrap_default_index(
    index_root: Path, index_name: str
) -> Optional[ChatManifest]:
    try:
        source_dir = ensure_docs_corpus(index_root)
    except Exception as exc:
        console.print(f"[red]Êó†Ê≥ïÂáÜÂ§áÊñáÊ°£ËØ≠Êñô: {exc}[/red]")
        return None

    embedding_config: Dict[str, object] = {
        "method": DEFAULT_EMBEDDING_METHOD,
        "params": {"dim": DEFAULT_FIXED_DIM},
    }
    console.print(
        f"üöÄ Ê≠£Âú®ÂØºÂÖ• [cyan]{source_dir}[/cyan] ‰ª•ÂàùÂßãÂåñ `{index_name}` Á¥¢Âºï...",
        style="green",
    )
    manifest = ingest_source(
        source_dir=source_dir,
        index_root=index_root,
        index_name=index_name,
        chunk_size=DEFAULT_CHUNK_SIZE,
        chunk_overlap=DEFAULT_CHUNK_OVERLAP,
        embedding_config=embedding_config,
        max_files=None,
    )
    return manifest


def load_or_bootstrap_manifest(index_root: Path, index_name: str) -> ChatManifest:
    try:
        return load_manifest(index_root, index_name)
    except FileNotFoundError:
        console.print(
            "üîç Ê£ÄÊµãÂà∞Â∞öÊú™‰∏∫ `sage chat` ÂàùÂßãÂåñÁ¥¢Âºï„ÄÇ",
            style="yellow",
        )
        if not typer.confirm("ÊòØÂê¶Á´ãÂç≥ÂØºÂÖ• docs-public ÊñáÊ°£Ôºü", default=True):
            console.print(
                "üí° ÂèØ‰ΩøÁî® `sage chat ingest` ÊâãÂä®ÂØºÂÖ•ÂêéÂÜçÈáçËØï„ÄÇ",
                style="cyan",
            )
            raise typer.Exit(code=1)

        manifest = bootstrap_default_index(index_root, index_name)
        if manifest is None:
            raise typer.Exit(code=1)
        return manifest


def ingest_source(
    source_dir: Path,
    index_root: Path,
    index_name: str,
    chunk_size: int,
    chunk_overlap: int,
    embedding_config: Dict[str, object],
    max_files: Optional[int] = None,
) -> ChatManifest:
    ensure_sage_db()

    if not source_dir.exists():
        raise FileNotFoundError(f"ÊñáÊ°£ÁõÆÂΩï‰∏çÂ≠òÂú®: {source_dir}")

    embedder = build_embedder(embedding_config)
    db_path = db_file_path(index_root, index_name)
    if db_path.exists():
        db_path.unlink()

    db = SageDB(embedder.get_dim())
    total_chunks = 0
    total_docs = 0

    for idx, file_path in enumerate(iter_markdown_files(source_dir), start=1):
        if max_files is not None and idx > max_files:
            break

        rel_path = file_path.relative_to(source_dir)
        text = file_path.read_text(encoding="utf-8", errors="ignore")
        sections = parse_markdown_sections(text)
        if not sections:
            continue

        doc_title = sections[0]["heading"] if sections else file_path.stem

        for section_idx, section in enumerate(sections):
            section_chunks = chunk_text(section["content"], chunk_size, chunk_overlap)
            for chunk_idx, chunk in enumerate(section_chunks):
                vector = embedder.embed(chunk)
                metadata = {
                    "doc_path": sanitize_metadata_value(str(rel_path)),
                    "title": sanitize_metadata_value(doc_title),
                    "heading": sanitize_metadata_value(section["heading"]),
                    "anchor": sanitize_metadata_value(slugify(section["heading"])),
                    "chunk": str(chunk_idx),
                    "text": sanitize_metadata_value(truncate_text(chunk, limit=1200)),
                }
                db.add(vector, metadata)
                total_chunks += 1

        total_docs += 1
        console.print(
            f"üìÑ Â§ÑÁêÜÊñáÊ°£ {idx}: {rel_path} (sections={len(sections)})", style="cyan"
        )

    if total_chunks == 0:
        raise RuntimeError("Êú™Âú®ÊñáÊ°£‰∏≠ÁîüÊàê‰ªª‰Ωï chunkÔºåÊ£ÄÊü•Ê∫êÁõÆÂΩïÊàñ chunk ÂèÇÊï∞„ÄÇ")

    console.print(f"üß± ÂÖ±ÂÜôÂÖ•ÂêëÈáè: {total_chunks}")
    db.build_index()
    db.save(str(db_path))

    manifest = ChatManifest(
        index_name=index_name,
        db_path=db_path,
        created_at=datetime.utcnow().isoformat(),
        source_dir=str(source_dir),
        embedding=embedding_config,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        num_documents=total_docs,
        num_chunks=total_chunks,
    )
    save_manifest(index_root, index_name, manifest)
    console.print(
        Panel.fit(f"‚úÖ Á¥¢ÂºïÂ∑≤Êõ¥Êñ∞ -> {db_path}", title="INGEST", style="green")
    )
    return manifest


def open_database(manifest: ChatManifest) -> Any:
    ensure_sage_db()
    if not manifest.db_path.exists():
        prefix = manifest.db_path
        siblings = list(prefix.parent.glob(prefix.name + "*"))
        if not siblings:
            raise FileNotFoundError(
                f"Êú™ÊâæÂà∞Êï∞ÊçÆÂ∫ìÊñá‰ª∂ {manifest.db_path}„ÄÇËØ∑ÈáçÊñ∞ËøêË°å `sage chat ingest`."
            )
    embedder = build_embedder(manifest.embed_config)
    db = SageDB(embedder.get_dim())
    db.load(str(manifest.db_path))
    return db


def build_prompt(question: str, contexts: Sequence[str]) -> List[Dict[str, str]]:
    context_block = "\n\n".join(
        f"[{idx}] {textwrap.dedent(ctx).strip()}"
        for idx, ctx in enumerate(contexts, start=1)
        if ctx
    )
    system_instructions = textwrap.dedent(
        """
        You are SAGE ÂÜÖÂµåÁºñÁ®ãÂä©Êâã„ÄÇÂõûÁ≠îÁî®Êà∑ÂÖ≥‰∫é SAGE ÁöÑÈóÆÈ¢òÔºå‰æùÊçÆÊèê‰æõÁöÑ‰∏ä‰∏ãÊñáËøõË°åËß£Èáä„ÄÇ
        - Â¶ÇÊûú‰∏ä‰∏ãÊñá‰∏çË∂≥‰ª•ÂõûÁ≠îÔºåËØ∑Âù¶ËØöËØ¥ÊòéÂπ∂ÁªôÂá∫‰∏ã‰∏ÄÊ≠•Âª∫ËÆÆ„ÄÇ
        - ÂºïÁî®Êó∂‰ΩøÁî® [ÁºñÂè∑] Ë°®Á§∫„ÄÇ
        - ÂõûÁ≠î‰øùÊåÅÁÆÄÊ¥ÅÔºåÁõ¥Êé•ÁªôÂá∫Ê≠•È™§ÊàñÁ§∫‰æã‰ª£Á†Å„ÄÇ
        """
    ).strip()

    if context_block:
        system_instructions += f"\n\nÂ∑≤Ê£ÄÁ¥¢‰∏ä‰∏ãÊñá:\n{context_block}"

    return [
        {"role": "system", "content": system_instructions},
        {"role": "user", "content": question.strip()},
    ]


class ResponseGenerator:
    def __init__(
        self,
        backend: str,
        model: str,
        base_url: Optional[str],
        api_key: Optional[str],
        temperature: float = 0.2,
        finetune_model: Optional[str] = None,
        finetune_port: int = DEFAULT_FINETUNE_PORT,
    ) -> None:
        self.backend = backend.lower()
        self.model = model
        self.base_url = base_url
        self.api_key = api_key
        self.temperature = temperature
        self.finetune_model = finetune_model
        self.finetune_port = finetune_port
        self.vllm_process = None  # Áî®‰∫éËøΩË∏™ÂêØÂä®ÁöÑ vLLM ËøõÁ®ã

        if self.backend == "mock":
            self.client = None
        elif self.backend == "finetune":
            # ‰ΩøÁî®ÂæÆË∞ÉÊ®°Âûã
            self._setup_finetune_backend()
        else:
            try:
                from sage.libs.integrations.openaiclient import OpenAIClient

                kwargs = {"seed": 42}
                if base_url:
                    kwargs["base_url"] = base_url
                if api_key:
                    kwargs["api_key"] = api_key
                self.client = OpenAIClient(model_name=model, **kwargs)
            except Exception as exc:  # pragma: no cover - runtime check
                raise RuntimeError(f"Êó†Ê≥ïÂàùÂßãÂåñ OpenAIClient: {exc}") from exc

    def _setup_finetune_backend(self) -> None:
        """ËÆæÁΩÆÂæÆË∞ÉÊ®°Âûã backend"""
        import subprocess
        import time
        from pathlib import Path

        import requests

        model_name = self.finetune_model or DEFAULT_FINETUNE_MODEL
        port = self.finetune_port

        # ‰ΩøÁî® SAGE ÈÖçÁΩÆÁõÆÂΩï
        sage_config_dir = Path.home() / ".sage"
        finetune_output_dir = sage_config_dir / "finetune_output"

        # Ê£ÄÊü•ÂæÆË∞ÉÊ®°ÂûãÊòØÂê¶Â≠òÂú®
        finetune_dir = finetune_output_dir / model_name
        merged_path = finetune_dir / "merged_model"
        checkpoint_path = finetune_dir / "checkpoints"

        if not finetune_dir.exists():
            raise FileNotFoundError(
                f"ÂæÆË∞ÉÊ®°Âûã‰∏çÂ≠òÂú®: {model_name}\n"
                f"Ë∑ØÂæÑ: {finetune_dir}\n"
                f"ËØ∑ÂÖàËøêË°åÂæÆË∞ÉÊàñÊåáÂÆöÂÖ∂‰ªñÊ®°Âûã:\n"
                f"  sage finetune quickstart {model_name}"
            )

        # Ê£ÄÊü•ÊúçÂä°ÊòØÂê¶Â∑≤ËøêË°å
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=1)
            service_running = response.status_code == 200
        except:
            service_running = False

        if service_running:
            console.print(f"[green]‚úÖ vLLM ÊúçÂä°Â∑≤Âú®Á´ØÂè£ {port} ËøêË°å[/green]")
            model_to_use = self.model if self.model != "qwen-max" else None
        else:
            console.print(f"[yellow]‚è≥ Ê≠£Âú®ÂêØÂä®ÂæÆË∞ÉÊ®°Âûã vLLM ÊúçÂä°...[/yellow]")

            # Ê£ÄÊü•ÊòØÂê¶ÊúâÂêàÂπ∂Ê®°Âûã
            if not merged_path.exists():
                console.print(
                    f"[yellow]‚ö†Ô∏è  Êú™ÊâæÂà∞ÂêàÂπ∂Ê®°ÂûãÔºåÊ≠£Âú®Ëá™Âä®ÂêàÂπ∂ LoRA ÊùÉÈáç...[/yellow]"
                )

                # Ê£ÄÊü•ÊòØÂê¶Êúâ checkpoint
                if not checkpoint_path.exists():
                    raise FileNotFoundError(
                        f"Êú™ÊâæÂà∞Ê®°ÂûãÊàñ checkpoint: {model_name}\n"
                        f"ËØ∑Á°Æ‰øùÂ∑≤ÂÆåÊàêËÆ≠ÁªÉÊàñËøêË°å: sage finetune merge {model_name}"
                    )

                # Â∞ùËØïËá™Âä®ÂêàÂπ∂
                try:
                    console.print("[cyan]Ê≠£Âú®ÂêàÂπ∂ LoRA ÊùÉÈáç...[/cyan]")
                    from sage.tools.finetune.service import merge_lora_weights

                    # ËØªÂèñ meta Ëé∑ÂèñÂü∫Á°ÄÊ®°Âûã
                    meta_file = finetune_dir / "finetune_meta.json"
                    if meta_file.exists():
                        import json

                        with open(meta_file) as f:
                            meta = json.load(f)
                        base_model = meta.get("model", "")
                    else:
                        raise RuntimeError("Êú™ÊâæÂà∞ meta ‰ø°ÊÅØÊñá‰ª∂")

                    # ÊâæÂà∞ÊúÄÊñ∞ÁöÑ checkpoint
                    checkpoints = sorted(checkpoint_path.glob("checkpoint-*"))
                    if not checkpoints:
                        raise RuntimeError("Êú™ÊâæÂà∞ checkpoint")

                    latest_checkpoint = checkpoints[-1]
                    merge_lora_weights(latest_checkpoint, base_model, merged_path)
                    console.print("[green]‚úÖ ÊùÉÈáçÂêàÂπ∂ÂÆåÊàê[/green]")
                except Exception as merge_exc:
                    raise RuntimeError(
                        f"Ëá™Âä®ÂêàÂπ∂Â§±Ë¥•: {merge_exc}\n"
                        f"ËØ∑ÊâãÂä®ËøêË°å: sage finetune merge {model_name}"
                    ) from merge_exc

            # ÂêØÂä® vLLM ÊúçÂä°
            cmd = [
                "python",
                "-m",
                "vllm.entrypoints.openai.api_server",
                "--model",
                str(merged_path),
                "--host",
                "0.0.0.0",
                "--port",
                str(port),
                "--gpu-memory-utilization",
                "0.9",
            ]

            try:
                self.vllm_process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    universal_newlines=True,
                )
            except Exception as exc:
                raise RuntimeError(f"ÂêØÂä® vLLM ÊúçÂä°Â§±Ë¥•: {exc}") from exc

            # Á≠âÂæÖÊúçÂä°ÂêØÂä®
            console.print("[cyan]‚è≥ Á≠âÂæÖÊúçÂä°ÂêØÂä®ÔºàÊúÄÂ§ö 60 ÁßíÔºâ...[/cyan]")
            for i in range(60):
                try:
                    response = requests.get(
                        f"http://localhost:{port}/health", timeout=1
                    )
                    if response.status_code == 200:
                        console.print("[green]‚úÖ vLLM ÊúçÂä°ÂêØÂä®ÊàêÂäüÔºÅ[/green]\n")
                        break
                except:
                    pass
                time.sleep(1)
            else:
                if self.vllm_process:
                    self.vllm_process.terminate()
                raise RuntimeError("vLLM ÊúçÂä°ÂêØÂä®Ë∂ÖÊó∂Ôºà60ÁßíÔºâ")

            # ËØªÂèñÂÆûÈôÖÁöÑÊ®°ÂûãÂêçÁß∞
            meta_file = finetune_dir / "finetune_meta.json"
            if meta_file.exists():
                import json

                with open(meta_file) as f:
                    meta = json.load(f)
                model_to_use = meta.get("model", str(merged_path))
            else:
                model_to_use = str(merged_path)

        # ËÆæÁΩÆ OpenAI ÂÆ¢Êà∑Á´ØËøûÊé•Âà∞Êú¨Âú∞ vLLM
        try:
            from sage.libs.integrations.openaiclient import OpenAIClient

            self.client = OpenAIClient(
                model_name=model_to_use or str(merged_path),
                base_url=f"http://localhost:{port}/v1",
                api_key="EMPTY",
                seed=42,
            )
            self.model = model_to_use or str(merged_path)
            console.print(f"[green]‚úÖ Â∑≤ËøûÊé•Âà∞ÂæÆË∞ÉÊ®°Âûã: {model_name}[/green]\n")
        except Exception as exc:
            if self.vllm_process:
                self.vllm_process.terminate()
            raise RuntimeError(f"Êó†Ê≥ïËøûÊé•Âà∞ vLLM ÊúçÂä°: {exc}") from exc

    def cleanup(self) -> None:
        """Ê∏ÖÁêÜËµÑÊ∫êÔºàÂ¶ÇÊûúÂêØÂä®‰∫Ü vLLM ËøõÁ®ãÔºâ"""
        if self.vllm_process:
            try:
                console.print("\n[yellow]‚è≥ Ê≠£Âú®ÂÖ≥Èó≠ vLLM ÊúçÂä°...[/yellow]")
                self.vllm_process.terminate()
                self.vllm_process.wait(timeout=10)
                console.print("[green]‚úÖ vLLM ÊúçÂä°Â∑≤ÂÖ≥Èó≠[/green]")
            except:
                if self.vllm_process.poll() is None:
                    self.vllm_process.kill()
            self.vllm_process = None

    def answer(
        self,
        question: str,
        contexts: Sequence[str],
        references: Sequence[Dict[str, str]],
        stream: bool = False,
    ) -> str:
        if self.backend == "mock":
            return self._mock_answer(question, contexts, references)

        messages = build_prompt(question, contexts)
        try:
            response = self.client.generate(
                messages,
                max_tokens=768,
                temperature=self.temperature,
                stream=stream,
            )
            if isinstance(response, str):
                return response
            # Non-streaming ensures string; streaming not yet supported.
            return str(response)
        except Exception as exc:
            raise RuntimeError(f"Ë∞ÉÁî®ËØ≠Ë®ÄÊ®°ÂûãÂ§±Ë¥•: {exc}") from exc

    @staticmethod
    def _mock_answer(
        question: str,
        contexts: Sequence[str],
        references: Sequence[Dict[str, str]],
    ) -> str:
        if not contexts:
            return "ÊöÇÊó∂Ê≤°Êúâ‰ªéÁü•ËØÜÂ∫ìÊ£ÄÁ¥¢Âà∞Á≠îÊ°à„ÄÇËØ∑Â∞ùËØïÊîπÂÜôÊèêÈóÆÔºåÊàñËøêË°å `sage chat ingest` Êõ¥Êñ∞Á¥¢Âºï„ÄÇ"
        top_ref = references[0] if references else {"title": "ËµÑÊñô", "heading": ""}
        snippet = contexts[0].strip().replace("\n", " ")
        citation = top_ref.get("label", top_ref.get("title", "Docs"))
        return (
            f"Ê†πÊçÆ {citation} ÁöÑËØ¥ÊòéÔºö{snippet[:280]}...\n\n"
            "Â¶ÇÈúÄÊõ¥Â§öÁªÜËäÇÔºåÂèØ‰ª•ËæìÂÖ• `more` ÂÜçÊ¨°Ê£ÄÁ¥¢ÔºåÊàñ‰ΩøÁî® `--backend openai` ÂêØÁî®ÁúüÂÆûÊ®°Âûã„ÄÇ"
        )


PIPELINE_TRIGGER_VERBS = (
    "ÊûÑÂª∫",
    "ÁîüÊàê",
    "Êê≠Âª∫",
    "ÂàõÂª∫",
    "ËÆæËÆ°",
    "build",
    "create",
    "design",
    "orchestrate",
)

PIPELINE_TRIGGER_TERMS = (
    "pipeline",
    "Â∑•‰ΩúÊµÅ",
    "ÊµÅÁ®ã",
    "ÂõæË∞±",
    "Â§ßÊ®°ÂûãÂ∫îÁî®",
    "Â∫îÁî®",
    "app",
    "application",
    "workflow",
    "agent",
    "agents",
)

# Â∏∏ËßÅÂú∫ÊôØÊ®°Êùø
COMMON_SCENARIOS = {
    "qa": {
        "name": "ÈóÆÁ≠îÂä©Êâã",
        "goal": "ÊûÑÂª∫Âü∫‰∫éÊñáÊ°£ÁöÑÈóÆÁ≠îÁ≥ªÁªü",
        "data_sources": ["ÊñáÊ°£Áü•ËØÜÂ∫ì"],
        "latency_budget": "ÂÆûÊó∂ÂìçÂ∫î‰ºòÂÖà",
        "constraints": "",
    },
    "rag": {
        "name": "RAGÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê",
        "goal": "ÁªìÂêàÂêëÈáèÊ£ÄÁ¥¢ÂíåÂ§ßÊ®°ÂûãÁîüÊàêÁöÑÊô∫ËÉΩÈóÆÁ≠î",
        "data_sources": ["ÂêëÈáèÊï∞ÊçÆÂ∫ì", "ÊñáÊ°£Â∫ì"],
        "latency_budget": "ÂÆûÊó∂ÂìçÂ∫î‰ºòÂÖà",
        "constraints": "",
    },
    "chat": {
        "name": "ÂØπËØùÊú∫Âô®‰∫∫",
        "goal": "ÊîØÊåÅÂ§öËΩÆÂØπËØùÁöÑÊô∫ËÉΩÂä©Êâã",
        "data_sources": ["Áî®Êà∑ËæìÂÖ•", "ÂéÜÂè≤ÂØπËØù"],
        "latency_budget": "ÂÆûÊó∂ÂìçÂ∫î‰ºòÂÖà",
        "constraints": "ÊîØÊåÅÊµÅÂºèËæìÂá∫",
    },
    "batch": {
        "name": "ÊâπÈáèÂ§ÑÁêÜ",
        "goal": "ÊâπÈáèÂ§ÑÁêÜÊñáÊ°£ÊàñÊï∞ÊçÆ",
        "data_sources": ["Êñá‰ª∂Á≥ªÁªü", "Êï∞ÊçÆÂ∫ì"],
        "latency_budget": "ÊâπÂ§ÑÁêÜÂèØÊé•Âèó",
        "constraints": "",
    },
    "agent": {
        "name": "Êô∫ËÉΩ‰ª£ÁêÜ",
        "goal": "ÂÖ∑ÊúâÂ∑•ÂÖ∑Ë∞ÉÁî®ËÉΩÂäõÁöÑAI‰ª£ÁêÜ",
        "data_sources": ["Â∑•ÂÖ∑API", "Áü•ËØÜÂ∫ì"],
        "latency_budget": "ÂÆûÊó∂ÂìçÂ∫î‰ºòÂÖà",
        "constraints": "ÊîØÊåÅÂáΩÊï∞Ë∞ÉÁî®",
    },
}


def _get_scenario_template(scenario_key: str) -> Optional[Dict[str, str]]:
    """Ëé∑ÂèñÂú∫ÊôØÊ®°Êùø"""
    return COMMON_SCENARIOS.get(scenario_key.lower())


def _show_scenario_templates() -> None:
    """ÊòæÁ§∫ÂèØÁî®ÁöÑÂú∫ÊôØÊ®°Êùø"""
    console.print("\n[bold cyan]üìö ÂèØÁî®Âú∫ÊôØÊ®°ÊùøÔºö[/bold cyan]\n")
    for key, template in COMMON_SCENARIOS.items():
        console.print(
            f"  [yellow]{key:10}[/yellow] - {template['name']}: {template['goal']}"
        )
    console.print()


def _looks_like_pipeline_request(message: str) -> bool:
    text = message.strip()
    if not text:
        return False

    lowered = text.lower()
    if lowered.startswith("pipeline:") or lowered.startswith("workflow:"):
        return True

    has_verb = any(token in text for token in PIPELINE_TRIGGER_VERBS)
    has_term = any(token in text for token in PIPELINE_TRIGGER_TERMS)
    if has_verb and has_term:
        return True

    if (
        "llm" in lowered
        and "build" in lowered
        and ("app" in lowered or "application" in lowered)
    ):
        return True

    return False


def _default_pipeline_name(seed: str) -> str:
    headline = seed.strip().splitlines()[0] if seed.strip() else "LLM Â∫îÁî®"
    headline = re.sub(r"[„ÄÇÔºÅÔºü.!?]", " ", headline)
    tokens = [tok for tok in re.split(r"\s+", headline) if tok]
    if not tokens:
        return "LLM Â∫îÁî®"
    if len(tokens) == 1:
        word = tokens[0]
        return f"{word} Â∫îÁî®" if len(word) <= 10 else word[:10]
    trimmed = " ".join(tokens[:4])
    return trimmed[:32] if len(trimmed) > 32 else trimmed


def _normalize_list_field(raw_value: str) -> List[str]:
    if not raw_value.strip():
        return []
    return [item.strip() for item in re.split(r"[,Ôºå/Ôºõ;]", raw_value) if item.strip()]


def _validate_pipeline_config(plan: Dict[str, Any]) -> Tuple[bool, List[str]]:
    """
    È™åËØÅÁîüÊàêÁöÑ Pipeline ÈÖçÁΩÆÊòØÂê¶ÂêàÊ≥ï

    Returns:
        (is_valid, error_messages)
    """
    errors = []

    # Ê£ÄÊü•ÂøÖÈúÄÁöÑÈ°∂Â±ÇÂ≠óÊÆµ
    if "pipeline" not in plan:
        errors.append("Áº∫Â∞ë 'pipeline' Â≠óÊÆµ")
    else:
        pipeline = plan["pipeline"]
        if not isinstance(pipeline, dict):
            errors.append("'pipeline' ÂøÖÈ°ªÊòØÂ≠óÂÖ∏Á±ªÂûã")
        else:
            for required in ["name", "type"]:
                if required not in pipeline:
                    errors.append(f"'pipeline' Áº∫Â∞ëÂøÖÈúÄÂ≠óÊÆµ '{required}'")

    # Ê£ÄÊü• source
    if "source" not in plan:
        errors.append("Áº∫Â∞ë 'source' Â≠óÊÆµ")
    elif not isinstance(plan["source"], dict):
        errors.append("'source' ÂøÖÈ°ªÊòØÂ≠óÂÖ∏Á±ªÂûã")
    elif "class" not in plan["source"]:
        errors.append("'source' Áº∫Â∞ë 'class' Â≠óÊÆµ")

    # Ê£ÄÊü• sink
    if "sink" not in plan:
        errors.append("Áº∫Â∞ë 'sink' Â≠óÊÆµ")
    elif not isinstance(plan["sink"], dict):
        errors.append("'sink' ÂøÖÈ°ªÊòØÂ≠óÂÖ∏Á±ªÂûã")
    elif "class" not in plan["sink"]:
        errors.append("'sink' Áº∫Â∞ë 'class' Â≠óÊÆµ")

    # Ê£ÄÊü• stagesÔºàÂèØÈÄâ‰ΩÜÂ¶ÇÊûúÂ≠òÂú®ÈúÄË¶ÅÊòØÂàóË°®Ôºâ
    if "stages" in plan:
        stages = plan["stages"]
        if not isinstance(stages, list):
            errors.append("'stages' ÂøÖÈ°ªÊòØÂàóË°®Á±ªÂûã")
        else:
            for idx, stage in enumerate(stages):
                if not isinstance(stage, dict):
                    errors.append(f"stages[{idx}] ÂøÖÈ°ªÊòØÂ≠óÂÖ∏Á±ªÂûã")
                else:
                    for required in ["id", "kind", "class"]:
                        if required not in stage:
                            errors.append(f"stages[{idx}] Áº∫Â∞ëÂøÖÈúÄÂ≠óÊÆµ '{required}'")

    return (len(errors) == 0, errors)


def _check_class_imports(plan: Dict[str, Any]) -> List[str]:
    """
    Ê£ÄÊü•ÈÖçÁΩÆ‰∏≠ÁöÑÁ±ªÊòØÂê¶ÂèØ‰ª•ÂØºÂÖ•

    Returns:
        List of import warnings
    """
    warnings = []
    classes_to_check = []

    # Êî∂ÈõÜÊâÄÊúâÁ±ªÂêç
    if "source" in plan and isinstance(plan["source"], dict):
        if "class" in plan["source"]:
            classes_to_check.append(("source", plan["source"]["class"]))

    if "sink" in plan and isinstance(plan["sink"], dict):
        if "class" in plan["sink"]:
            classes_to_check.append(("sink", plan["sink"]["class"]))

    if "stages" in plan and isinstance(plan["stages"], list):
        for idx, stage in enumerate(plan["stages"]):
            if isinstance(stage, dict) and "class" in stage:
                classes_to_check.append((f"stages[{idx}]", stage["class"]))

    # Â∞ùËØïÂØºÂÖ•ÊØè‰∏™Á±ª
    for location, class_path in classes_to_check:
        if not class_path or not isinstance(class_path, str):
            continue

        try:
            # ÂàÜÂâ≤Ê®°ÂùóË∑ØÂæÑÂíåÁ±ªÂêç
            parts = class_path.rsplit(".", 1)
            if len(parts) != 2:
                warnings.append(f"{location}: Á±ªË∑ØÂæÑÊ†ºÂºè‰∏çÊ≠£Á°Æ '{class_path}'")
                continue

            module_path, class_name = parts

            # Â∞ùËØïÂØºÂÖ•Ôºà‰ΩÜ‰∏çÂÆûÈôÖÊâßË°åÔºåÂè™ÊòØÊ£ÄÊü•ËØ≠Ê≥ïÔºâ
            # Ê≥®ÊÑèÔºöËøôÈáåÂè™ÂÅöÂü∫Êú¨Ê£ÄÊü•Ôºå‰∏çÊâßË°åÁúüÂÆûÂØºÂÖ•‰ª•ÈÅøÂÖçÂâØ‰ΩúÁî®
            if not module_path or not class_name:
                warnings.append(f"{location}: Á±ªË∑ØÂæÑ '{class_path}' Êó†Êïà")

        except Exception as exc:
            warnings.append(f"{location}: Á±ª '{class_path}' ÂèØËÉΩÊó†Ê≥ïÂØºÂÖ• - {exc}")

    return warnings


class PipelineChatCoordinator:
    def __init__(
        self,
        backend: str,
        model: str,
        base_url: Optional[str],
        api_key: Optional[str],
    ) -> None:
        self.backend = backend
        self.model = model
        self.base_url = base_url
        self.api_key = api_key
        self.knowledge_top_k = 5
        self.show_knowledge = False
        self._domain_contexts: Tuple[str, ...] | None = None
        self._knowledge_base: Optional[Any] = None

    def detect(self, message: str) -> bool:
        return _looks_like_pipeline_request(message)

    def _ensure_api_key(self) -> bool:
        if self.backend == "mock":
            return True
        if self.api_key:
            return True

        console.print(
            "[yellow]ÂΩìÂâç‰ΩøÁî®ÁúüÂÆûÊ®°ÂûãÂêéÁ´ØÔºåÈúÄË¶ÅÊèê‰æõ API Key ÊâçËÉΩÁîüÊàê pipeline„ÄÇ[/yellow]"
        )
        provided = typer.prompt("ËØ∑ËæìÂÖ• LLM API Key (ÁïôÁ©∫ÂèñÊ∂à)", default="")
        if not provided.strip():
            console.print("Â∑≤ÂèñÊ∂à pipeline ÊûÑÂª∫ÊµÅÁ®ã„ÄÇ", style="yellow")
            return False

        self.api_key = provided.strip()
        return True

    def _ensure_contexts(self) -> Tuple[str, ...]:
        if self._domain_contexts is None:
            try:
                self._domain_contexts = tuple(load_domain_contexts(limit=4))
            except Exception as exc:  # pragma: no cover - defensive
                console.print(f"[yellow]Âä†ËΩΩÈªòËÆ§‰∏ä‰∏ãÊñáÂ§±Ë¥•: {exc}[/yellow]")
                self._domain_contexts = ()
        return self._domain_contexts

    def _ensure_knowledge_base(self) -> Optional[Any]:
        if self._knowledge_base is None:
            try:
                self._knowledge_base = get_default_knowledge_base()
            except Exception as exc:  # pragma: no cover - defensive
                console.print(
                    f"[yellow]ÂàùÂßãÂåñ pipeline Áü•ËØÜÂ∫ìÂ§±Ë¥•ÔºåÂ∞ÜË∑≥ËøáÊ£ÄÁ¥¢: {exc}[/yellow]"
                )
                self._knowledge_base = None
        return self._knowledge_base

    def _collect_requirements(self, initial_request: str) -> Dict[str, Any]:
        console.print("\n[bold cyan]üìã ÈúÄÊ±ÇÊî∂ÈõÜ[/bold cyan]", style="bold")
        console.print("ËØ∑Êèê‰æõ‰ª•‰∏ã‰ø°ÊÅØ‰ª•ÁîüÊàêÊúÄÈÄÇÂêàÁöÑ Pipeline ÈÖçÁΩÆÔºö\n", style="dim")

        # ËØ¢ÈóÆÊòØÂê¶‰ΩøÁî®Ê®°Êùø
        use_template = typer.confirm("ÊòØÂê¶‰ΩøÁî®È¢ÑËÆæÂú∫ÊôØÊ®°ÊùøÔºü", default=False)
        template_data = None

        if use_template:
            _show_scenario_templates()
            template_key = (
                typer.prompt(
                    "ÈÄâÊã©Âú∫ÊôØÊ®°Êùø (ËæìÂÖ•ÂÖ≥ÈîÆÂ≠óÔºåÂ¶Ç 'qa', 'rag', 'chat' Á≠â)", default="qa"
                )
                .strip()
                .lower()
            )
            template_data = _get_scenario_template(template_key)
            if template_data:
                console.print(
                    f"\n‚úÖ Â∑≤Âä†ËΩΩ [green]{template_data['name']}[/green] Ê®°Êùø\n"
                )
            else:
                console.print(
                    f"\n‚ö†Ô∏è  Êú™ÊâæÂà∞Ê®°Êùø '{template_key}'ÔºåÂ∞Ü‰ΩøÁî®Ëá™ÂÆö‰πâÈÖçÁΩÆ\n",
                    style="yellow",
                )

        default_name = _default_pipeline_name(initial_request)

        # ÊèêÁ§∫Áî®Êà∑ÂèØ‰ª•ÁÆÄÂåñËæìÂÖ•
        console.print(
            "üí° [dim]ÊèêÁ§∫ÔºöÁõ¥Êé•ÂõûËΩ¶Â∞Ü‰ΩøÁî®ÈªòËÆ§ÂÄºÔºàÂü∫‰∫éÊÇ®ÁöÑÊèèËø∞Ëá™Âä®Êé®Êñ≠Ôºâ[/dim]\n"
        )

        # Â¶ÇÊûúÊúâÊ®°ÊùøÔºå‰ΩøÁî®Ê®°ÊùøÁöÑÈªòËÆ§ÂÄº
        if template_data:
            name = typer.prompt("üìõ Pipeline ÂêçÁß∞", default=template_data["name"])
            goal = typer.prompt("üéØ Pipeline ÁõÆÊ†áÊèèËø∞", default=template_data["goal"])
            default_sources = ", ".join(template_data["data_sources"])
            default_latency = template_data["latency_budget"]
            default_constraints = template_data["constraints"]
        else:
            name = typer.prompt("üìõ Pipeline ÂêçÁß∞", default=default_name)
            goal = typer.prompt(
                "üéØ Pipeline ÁõÆÊ†áÊèèËø∞", default=initial_request.strip() or default_name
            )
            default_sources = "ÊñáÊ°£Áü•ËØÜÂ∫ì"
            default_latency = "ÂÆûÊó∂ÂìçÂ∫î‰ºòÂÖà"
            default_constraints = ""

        # Êèê‰æõÊõ¥ËØ¶ÁªÜÁöÑËØ¥Êòé
        console.print("\n[dim]Êï∞ÊçÆÊù•Ê∫êÁ§∫‰æãÔºöÊñáÊ°£Áü•ËØÜÂ∫ì„ÄÅÁî®Êà∑ËæìÂÖ•„ÄÅÊï∞ÊçÆÂ∫ì„ÄÅAPI Á≠â[/dim]")
        data_sources = typer.prompt(
            "üì¶ ‰∏ªË¶ÅÊï∞ÊçÆÊù•Ê∫ê (ÈÄóÂè∑ÂàÜÈöî)", default=default_sources
        )

        console.print(
            "\n[dim]Âª∂ËøüÈúÄÊ±ÇÁ§∫‰æãÔºöÂÆûÊó∂ÂìçÂ∫î‰ºòÂÖà„ÄÅÊâπÂ§ÑÁêÜÂèØÊé•Âèó„ÄÅÈ´òÂêûÂêêÈáè‰ºòÂÖà[/dim]"
        )
        latency = typer.prompt("‚ö° Âª∂Ëøü/ÂêûÂêêÈúÄÊ±Ç", default=default_latency)

        console.print(
            "\n[dim]Á∫¶ÊùüÊù°‰ª∂Á§∫‰æãÔºö‰ªÖ‰ΩøÁî®Êú¨Âú∞Ê®°Âûã„ÄÅÂÜÖÂ≠òÈôêÂà∂ 4GB„ÄÅÂøÖÈ°ªÊîØÊåÅÊµÅÂºèËæìÂá∫[/dim]"
        )
        constraints = typer.prompt("‚öôÔ∏è  ÁâπÊÆäÁ∫¶Êùü (ÂèØÁïôÁ©∫)", default=default_constraints)

        requirements: Dict[str, Any] = {
            "name": name.strip() or default_name,
            "goal": goal.strip() or initial_request.strip() or default_name,
            "data_sources": _normalize_list_field(data_sources),
            "latency_budget": latency.strip(),
            "constraints": constraints.strip(),
            "initial_prompt": initial_request.strip(),
        }

        # ÊòæÁ§∫Êî∂ÈõÜÂà∞ÁöÑÈúÄÊ±ÇÊëòË¶Å
        console.print("\n[bold green]‚úÖ ÈúÄÊ±ÇÊî∂ÈõÜÂÆåÊàê[/bold green]")
        summary_table = Table(show_header=False, box=None, padding=(0, 2))
        summary_table.add_row("ÂêçÁß∞:", f"[cyan]{requirements['name']}[/cyan]")
        summary_table.add_row("ÁõÆÊ†á:", f"[yellow]{requirements['goal']}[/yellow]")
        summary_table.add_row(
            "Êï∞ÊçÆÊ∫ê:", f"[magenta]{', '.join(requirements['data_sources'])}[/magenta]"
        )
        if requirements["latency_budget"]:
            summary_table.add_row("Âª∂ËøüÈúÄÊ±Ç:", requirements["latency_budget"])
        if requirements["constraints"]:
            summary_table.add_row("Á∫¶ÊùüÊù°‰ª∂:", requirements["constraints"])
        console.print(summary_table)
        console.print()

        return requirements

    def _build_config(self) -> pipeline_builder.BuilderConfig:
        domain_contexts = self._ensure_contexts() or ()
        knowledge_base = self._ensure_knowledge_base()
        return pipeline_builder.BuilderConfig(
            backend=self.backend,
            model=self.model,
            base_url=self.base_url,
            api_key=self.api_key,
            domain_contexts=domain_contexts,
            knowledge_base=knowledge_base,
            knowledge_top_k=self.knowledge_top_k,
            show_knowledge=self.show_knowledge,
        )

    def _generate_plan(self, requirements: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        console.print("\n[bold magenta]ü§ñ Ê≠£Âú®ÁîüÊàê Pipeline ÈÖçÁΩÆ...[/bold magenta]\n")

        config = self._build_config()
        generator = pipeline_builder.PipelinePlanGenerator(config)

        plan: Optional[Dict[str, Any]] = None
        feedback: Optional[str] = None

        for round_num in range(1, 7):  # ÊúÄÂ§ö 6 ËΩÆ
            console.print(f"[dim]>>> Á¨¨ {round_num} ËΩÆÁîüÊàê...[/dim]")

            try:
                plan = generator.generate(requirements, plan, feedback)
                console.print("[green]‚úì[/green] ÁîüÊàêÊàêÂäü\n")
            except pipeline_builder.PipelineBuilderError as exc:
                console.print(f"[red]‚úó ÁîüÊàêÂ§±Ë¥•: {exc}[/red]\n")

                # Êèê‰æõÊõ¥ËØ¶ÁªÜÁöÑÈîôËØØÂ§ÑÁêÜÂª∫ËÆÆ
                if "API" in str(exc) or "key" in str(exc).lower():
                    console.print("[yellow]üí° Âª∫ËÆÆÔºöÊ£ÄÊü• API Key ÊòØÂê¶Ê≠£Á°ÆÈÖçÁΩÆ[/yellow]")
                elif "timeout" in str(exc).lower():
                    console.print("[yellow]üí° Âª∫ËÆÆÔºöÁΩëÁªúÂèØËÉΩ‰∏çÁ®≥ÂÆöÔºåÂèØ‰ª•ÈáçËØï[/yellow]")
                elif "JSON" in str(exc) or "parse" in str(exc).lower():
                    console.print(
                        "[yellow]üí° Âª∫ËÆÆÔºöÊ®°ÂûãËæìÂá∫Ê†ºÂºèÂºÇÂ∏∏ÔºåÂ∞ùËØïÁÆÄÂåñÈúÄÊ±ÇÊèèËø∞[/yellow]"
                    )

                if not typer.confirm("\nÊòØÂê¶Â∞ùËØïÈáçÊñ∞ÁîüÊàêÔºü", default=True):
                    return None

                feedback = typer.prompt(
                    "ËØ∑Êèê‰æõÊõ¥Â§öÈúÄÊ±ÇÊàñ‰øÆÊîπÂª∫ËÆÆÔºàÁõ¥Êé•ÂõûËΩ¶‰ΩøÁî®ÂéüÈúÄÊ±ÇÈáçËØïÔºâ", default=""
                )
                if not feedback.strip():
                    feedback = None
                continue

            # ÊòæÁ§∫ÁîüÊàêÁöÑÈÖçÁΩÆ
            console.print("[bold cyan]üìÑ ÁîüÊàêÁöÑ Pipeline ÈÖçÁΩÆÔºö[/bold cyan]")
            pipeline_builder.render_pipeline_plan(plan)
            pipeline_builder.preview_pipeline_plan(plan)

            # È™åËØÅÈÖçÁΩÆ
            console.print("\n[dim]>>> Ê≠£Âú®È™åËØÅÈÖçÁΩÆ...[/dim]")
            is_valid, errors = _validate_pipeline_config(plan)

            if not is_valid:
                console.print("[red]‚ö†Ô∏è  ÈÖçÁΩÆÈ™åËØÅÂèëÁé∞ÈóÆÈ¢òÔºö[/red]")
                for error in errors:
                    console.print(f"  ‚Ä¢ [red]{error}[/red]")
                console.print(
                    "\n[yellow]Âª∫ËÆÆÔºöÂú®ÂèçÈ¶à‰∏≠ËØ¥ÊòéËøô‰∫õÈóÆÈ¢òÔºåËÆ©Ê®°ÂûãÈáçÊñ∞ÁîüÊàê[/yellow]"
                )

                if not typer.confirm(
                    "ÊòØÂê¶ÁªßÁª≠‰ΩøÁî®Ê≠§ÈÖçÁΩÆÔºàÂèØËÉΩÊó†Ê≥ïÊ≠£Â∏∏ËøêË°åÔºâÔºü", default=False
                ):
                    feedback = typer.prompt(
                        "ËØ∑ÊèèËø∞ÈúÄË¶Å‰øÆÊ≠£ÁöÑÈóÆÈ¢òÊàñÊèê‰æõÈ¢ùÂ§ñË¶ÅÊ±Ç",
                        default="ËØ∑‰øÆÂ§çÈÖçÁΩÆÈ™åËØÅ‰∏≠ÂèëÁé∞ÁöÑÈóÆÈ¢ò",
                    )
                    continue
            else:
                console.print("[green]‚úì[/green] ÈÖçÁΩÆÈ™åËØÅÈÄöËøá")

                # Ê£ÄÊü•Á±ªÂØºÂÖ•ÔºàË≠¶ÂëäÁ∫ßÂà´Ôºâ
                import_warnings = _check_class_imports(plan)
                if import_warnings:
                    console.print("\n[yellow]‚ö†Ô∏è  Ê£ÄÊµãÂà∞‰ª•‰∏ãÊΩúÂú®ÈóÆÈ¢òÔºö[/yellow]")
                    for warning in import_warnings[:5]:  # ÊúÄÂ§öÊòæÁ§∫5‰∏™
                        console.print(f"  ‚Ä¢ [yellow]{warning}[/yellow]")
                    if len(import_warnings) > 5:
                        console.print(
                            f"  [dim]... ËøòÊúâ {len(import_warnings) - 5} ‰∏™Ë≠¶Âëä[/dim]"
                        )
                    console.print(
                        "[dim]ÊèêÁ§∫ÔºöËøô‰∫õË≠¶Âëä‰∏ç‰∏ÄÂÆöÂØºËá¥ËøêË°åÂ§±Ë¥•Ôºå‰ΩÜÂª∫ËÆÆÊ£ÄÊü•[/dim]"
                    )

            if typer.confirm("\n‚ú® ÂØπËØ•ÈÖçÁΩÆÊª°ÊÑèÂêóÔºü", default=True):
                console.print("\n[bold green]üéâ Pipeline ÈÖçÁΩÆÂ∑≤Á°ÆËÆ§ÔºÅ[/bold green]\n")
                return plan

            console.print("\n[yellow]‚öôÔ∏è  ËøõÂÖ•‰ºòÂåñÊ®°Âºè...[/yellow]")
            feedback = typer.prompt(
                "ËØ∑ËæìÂÖ•ÈúÄË¶ÅË∞ÉÊï¥ÁöÑÂú∞ÊñπÔºà‰æãÂ¶ÇÔºö‰ΩøÁî®ÊµÅÂºèËæìÂá∫„ÄÅÊîπÁî®Êú¨Âú∞Ê®°Âûã„ÄÅÊ∑ªÂä†ÁõëÊéßÔºâ\nÁïôÁ©∫ÁªìÊùü",
                default="",
            )
            if not feedback.strip():
                console.print("[yellow]Êú™Êèê‰æõË∞ÉÊï¥ÊÑèËßÅÔºå‰øùÊåÅÂΩìÂâçÁâàÊú¨„ÄÇ[/yellow]")
                return plan

        # ËææÂà∞ÊúÄÂ§ßËΩÆÊï∞
        console.print("\n[yellow]‚ö†Ô∏è  Â∑≤ËææÂà∞ÊúÄÂ§ß‰ºòÂåñËΩÆÊï∞Ôºà6 ËΩÆÔºâ[/yellow]")
        if plan and typer.confirm("ÊòØÂê¶‰ΩøÁî®ÊúÄÂêé‰∏ÄÊ¨°ÁîüÊàêÁöÑÈÖçÁΩÆÔºü", default=True):
            return plan

        return plan

    def handle(self, message: str) -> bool:
        if not self.detect(message):
            return False

        if not self._ensure_api_key():
            return True

        # ÊòæÁ§∫Ê¨¢Ëøé‰ø°ÊÅØÂíåÂäüËÉΩËØ¥Êòé
        console.print("\n" + "=" * 70)
        console.print(
            "[bold magenta]üöÄ SAGE Pipeline Builder - Êô∫ËÉΩÁºñÊéíÂä©Êâã[/bold magenta]"
        )
        console.print("=" * 70)
        console.print(
            """
[dim]ÂäüËÉΩËØ¥ÊòéÔºö
  ‚Ä¢ Âü∫‰∫éÊÇ®ÁöÑÊèèËø∞Ëá™Âä®ÁîüÊàê SAGE Pipeline ÈÖçÁΩÆ
  ‚Ä¢ ‰ΩøÁî® RAG ÊäÄÊúØÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÊ°£ÂíåÁ§∫‰æã
  ‚Ä¢ ÊîØÊåÅÂ§öËΩÆÂØπËØù‰ºòÂåñÈÖçÁΩÆ
  ‚Ä¢ ÂèØÁõ¥Êé•ËøêË°åÁîüÊàêÁöÑ Pipeline
[/dim]
        """
        )

        console.print("üéØ [cyan]Ê£ÄÊµãÂà∞ Pipeline ÊûÑÂª∫ËØ∑Ê±Ç[/cyan]")
        console.print(f"üìù ÊÇ®ÁöÑÈúÄÊ±Ç: [yellow]{message}[/yellow]\n")

        # Êî∂ÈõÜÈúÄÊ±Ç
        requirements = self._collect_requirements(message)

        # ÁîüÊàêÈÖçÁΩÆ
        plan = self._generate_plan(requirements)
        if plan is None:
            console.print("[yellow]‚ö†Ô∏è  Êú™ÁîüÊàêÂèØÁî®ÁöÑ Pipeline ÈÖçÁΩÆ„ÄÇ[/yellow]")
            return True

        # ‰øùÂ≠òÈÖçÁΩÆ
        console.print("[bold cyan]üíæ ‰øùÂ≠òÈÖçÁΩÆÊñá‰ª∂[/bold cyan]")
        destination = typer.prompt(
            "‰øùÂ≠òÂà∞Êñá‰ª∂ (Áõ¥Êé•ÂõûËΩ¶‰ΩøÁî®ÈªòËÆ§ËæìÂá∫ÁõÆÂΩï)", default=""
        ).strip()
        output_path: Optional[Path] = (
            Path(destination).expanduser() if destination else None
        )
        overwrite = False
        if output_path and output_path.exists():
            overwrite = typer.confirm("‚ö†Ô∏è  ÁõÆÊ†áÊñá‰ª∂Â∑≤Â≠òÂú®ÔºåÊòØÂê¶Ë¶ÜÁõñÔºü", default=False)
            if not overwrite:
                console.print("[yellow]Â∑≤ÂèñÊ∂à‰øùÂ≠òÔºåÁªìÊùüÊú¨Ê¨°ÊûÑÂª∫„ÄÇ[/yellow]")
                return True

        saved_path = pipeline_builder.save_pipeline_plan(plan, output_path, overwrite)
        console.print(f"\n‚úÖ Pipeline ÈÖçÁΩÆÂ∑≤‰øùÂ≠òËá≥ [green]{saved_path}[/green]\n")

        # ËØ¢ÈóÆÊòØÂê¶ËøêË°å
        if not typer.confirm("‚ñ∂Ô∏è  ÊòØÂê¶Á´ãÂç≥ËøêË°åËØ• PipelineÔºü", default=True):
            console.print("\n[dim]ÊèêÁ§∫ÔºöÁ®çÂêéÂèØ‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ËøêË°åÔºö[/dim]")
            console.print(f"[cyan]  sage pipeline run {saved_path}[/cyan]\n")
            return True

        # ÈÖçÁΩÆËøêË°åÂèÇÊï∞
        console.print("\n[bold cyan]‚öôÔ∏è  ÈÖçÁΩÆËøêË°åÂèÇÊï∞[/bold cyan]")
        autostop = typer.confirm("Êèê‰∫§ÂêéÁ≠âÂæÖÊâßË°åÂÆåÊàê (autostop)?", default=True)

        pipeline_type = (plan.get("pipeline", {}).get("type") or "local").lower()
        host: Optional[str] = None
        port_value: Optional[int] = None

        if pipeline_type == "remote":
            console.print(
                "\n[yellow]Ê£ÄÊµãÂà∞ËøúÁ®ã PipelineÔºåÈúÄË¶ÅÈÖçÁΩÆ JobManager ËøûÊé•‰ø°ÊÅØ[/yellow]"
            )
            host = (
                typer.prompt("ËøúÁ®ã JobManager host", default="127.0.0.1").strip()
                or None
            )
            port_text = typer.prompt("ËøúÁ®ã JobManager Á´ØÂè£", default="19001").strip()
            try:
                port_value = int(port_text)
            except ValueError:
                console.print("[yellow]Á´ØÂè£Ê†ºÂºè‰∏çÂêàÊ≥ïÔºå‰ΩøÁî®ÈªòËÆ§ 19001„ÄÇ[/yellow]")
                port_value = 19001

        # ËøêË°å Pipeline
        console.print("\n[bold green]üöÄ Ê≠£Âú®ÂêØÂä® Pipeline...[/bold green]\n")
        try:
            job_id = pipeline_builder.execute_pipeline_plan(
                plan,
                autostop=autostop,
                host=host,
                port=port_value,
                console_override=console,
            )
            if job_id:
                console.print(
                    f"\n[bold green]‚úÖ Pipeline Â∑≤Êèê‰∫§ÔºåJob ID: {job_id}[/bold green]"
                )
            else:
                console.print("\n[bold green]‚úÖ Pipeline ÊâßË°åÂÆåÊàê[/bold green]")
        except Exception as exc:
            console.print(f"\n[red]‚ùå Pipeline ÊâßË°åÂ§±Ë¥•: {exc}[/red]")
            console.print("\n[dim]ÊèêÁ§∫ÔºöËØ∑Ê£ÄÊü•ÈÖçÁΩÆÊñá‰ª∂Âíå‰æùËµñÈ°πÊòØÂê¶Ê≠£Á°Æ[/dim]")

        console.print("\n" + "=" * 70 + "\n")
        return True


def render_references(references: Sequence[Dict[str, str]]) -> Table:
    table = Table(title="Áü•ËØÜÂºïÁî®", show_header=True, header_style="bold cyan")
    table.add_column("#", justify="right", width=3)
    table.add_column("ÊñáÊ°£")
    table.add_column("ËäÇ")
    table.add_column("ÂæóÂàÜ", justify="right", width=7)

    for idx, ref in enumerate(references, start=1):
        table.add_row(
            str(idx),
            ref.get("title", "Êú™Áü•"),
            ref.get("heading", "-"),
            f"{ref.get('score', 0.0):.4f}",
        )
    return table


def retrieve_context(
    db: Any,
    embedder: EmbeddingModel,
    question: str,
    top_k: int,
) -> Dict[str, object]:
    query_vector = embedder.embed(question)
    results = db.search(query_vector, top_k, True)

    contexts: List[str] = []
    references: List[Dict[str, str]] = []
    for item in results:
        metadata = dict(item.metadata) if hasattr(item, "metadata") else {}
        contexts.append(metadata.get("text", ""))
        references.append(
            {
                "title": metadata.get("title", metadata.get("doc_path", "Êú™Áü•")),
                "heading": metadata.get("heading", ""),
                "path": metadata.get("doc_path", ""),
                "anchor": metadata.get("anchor", ""),
                "score": float(getattr(item, "score", 0.0)),
                "label": f"[{metadata.get('doc_path', '?')}]",
            }
        )
    return {"contexts": contexts, "references": references}


def interactive_chat(
    manifest: ChatManifest,
    index_root: Path,
    top_k: int,
    backend: str,
    model: str,
    base_url: Optional[str],
    api_key: Optional[str],
    ask: Optional[str],
    stream: bool,
    finetune_model: Optional[str] = None,
    finetune_port: int = DEFAULT_FINETUNE_PORT,
) -> None:
    embedder: Optional[Any] = None
    db: Optional[Any] = None
    generator = ResponseGenerator(
        backend,
        model,
        base_url,
        api_key,
        finetune_model=finetune_model,
        finetune_port=finetune_port,
    )
    pipeline_coordinator = PipelineChatCoordinator(backend, model, base_url, api_key)

    console.print(
        Panel(
            f"Á¥¢Âºï: [cyan]{manifest.index_name}[/cyan]\n"
            f"Êù•Ê∫ê: [green]{manifest.source_dir}[/green]\n"
            f"ÊñáÊ°£Êï∞: {manifest.num_documents}  ChunkÊï∞: {manifest.num_chunks}\n"
            f"Embedding: {manifest.embed_config}",
            title="SAGE Chat ÂáÜÂ§áÂ∞±Áª™",
        )
    )

    def ensure_retriever() -> Tuple[Any, Any]:
        nonlocal embedder, db
        if embedder is None:
            embedder = build_embedder(manifest.embed_config)
        if db is None:
            db = open_database(manifest)
        return db, embedder

    def answer_once(query: str) -> None:
        current_db, current_embedder = ensure_retriever()
        payload = retrieve_context(current_db, current_embedder, query, top_k)
        contexts = payload["contexts"]
        references = payload["references"]

        try:
            reply = generator.answer(query, contexts, references, stream=stream)
        except Exception as exc:
            console.print(f"[red]ÁîüÊàêÂõûÁ≠îÂ§±Ë¥•: {exc}[/red]")
            return

        context_table = render_references(references)
        console.print(context_table)

        if stream:
            text = Text()
            with Live(Panel(text, title="ÂõûÁ≠î"), auto_refresh=False) as live:
                text.append(reply)
                live.refresh()
        else:
            console.print(Panel(Markdown(reply), title="ÂõûÁ≠î", style="bold green"))

    if ask:
        if pipeline_coordinator.handle(ask):
            return
        answer_once(ask)
        return

    # ÊòæÁ§∫Â∏ÆÂä©‰ø°ÊÅØ
    console.print("\n[bold cyan]üß≠ SAGE Chat ‰ΩøÁî®ÊåáÂçó[/bold cyan]")

    # ÊòæÁ§∫ÂΩìÂâçÈÖçÁΩÆ
    if backend == "finetune":
        console.print(
            f"[green]‚úÖ ‰ΩøÁî®ÂæÆË∞ÉÊ®°Âûã: {finetune_model or DEFAULT_FINETUNE_MODEL}[/green]"
        )
        console.print(f"[dim]Á´ØÂè£: {finetune_port}[/dim]\n")

    console.print(
        """
[dim]Âü∫Êú¨ÂëΩ‰ª§Ôºö
  ‚Ä¢ Áõ¥Êé•ËæìÂÖ•ÈóÆÈ¢òËé∑ÂèñÊñáÊ°£Áõ∏ÂÖ≥ÂõûÁ≠î
  ‚Ä¢ ËæìÂÖ•ÂåÖÂê´ 'pipeline'„ÄÅ'ÊûÑÂª∫Â∫îÁî®' Á≠âÂÖ≥ÈîÆËØçËß¶Âèë Pipeline ÊûÑÂª∫Ê®°Âºè
  ‚Ä¢ ËæìÂÖ• 'help' ÊòæÁ§∫Â∏ÆÂä©‰ø°ÊÅØ
  ‚Ä¢ ËæìÂÖ• 'templates' Êü•ÁúãÂèØÁî®Âú∫ÊôØÊ®°Êùø
  ‚Ä¢ ËæìÂÖ• 'exit'„ÄÅ'quit' Êàñ 'q' ÈÄÄÂá∫ÂØπËØù
[/dim]
    """
    )

    try:
        while True:
            try:
                question = typer.prompt("ü§ñ ‰Ω†ÁöÑÈóÆÈ¢ò")
            except (EOFError, KeyboardInterrupt):
                console.print("\nÂÜçËßÅ üëã", style="cyan")
                break
            if not question.strip():
                continue

            question_lower = question.lower().strip()

            # Â§ÑÁêÜÁâπÊÆäÂëΩ‰ª§
            if question_lower in {"exit", "quit", "q"}:
                console.print("ÂÜçËßÅ üëã", style="cyan")
                break
            elif question_lower in {"help", "Â∏ÆÂä©", "?"}:
                console.print("\n[bold cyan]üìö Â∏ÆÂä©‰ø°ÊÅØ[/bold cyan]")
                console.print(
                    """
[dim]ÂäüËÉΩËØ¥ÊòéÔºö
  1. ÊñáÊ°£ÈóÆÁ≠îÔºöÁõ¥Êé•ÊèêÈóÆÂÖ≥‰∫é SAGE ÁöÑÈóÆÈ¢ò
  2. Pipeline ÊûÑÂª∫ÔºöÊèèËø∞‰Ω†ÊÉ≥Ë¶ÅÁöÑÂ∫îÁî®Âú∫ÊôØ

Á§∫‰æãÈóÆÈ¢òÔºö
  ‚Ä¢ "SAGE Â¶Ç‰ΩïÈÖçÁΩÆ RAGÔºü"
  ‚Ä¢ "ËØ∑Â∏ÆÊàëÊûÑÂª∫‰∏Ä‰∏™ÈóÆÁ≠îÂ∫îÁî®"
  ‚Ä¢ "Â¶Ç‰Ωï‰ΩøÁî®ÂêëÈáèÊï∞ÊçÆÂ∫ìÔºü"
  ‚Ä¢ "build a chat pipeline with streaming"

ÁâπÊÆäÂëΩ‰ª§Ôºö
  ‚Ä¢ help      - ÊòæÁ§∫Ê≠§Â∏ÆÂä©‰ø°ÊÅØ
  ‚Ä¢ templates - Êü•ÁúãÂèØÁî®Âú∫ÊôØÊ®°Êùø
  ‚Ä¢ exit/quit - ÈÄÄÂá∫ÂØπËØù
[/dim]
                """
                )
                continue
            elif question_lower in {"templates", "Ê®°Êùø"}:
                _show_scenario_templates()
                console.print(
                    "[dim]ÊèêÁ§∫ÔºöÂú® Pipeline ÊûÑÂª∫ÊµÅÁ®ã‰∏≠ÂèØ‰ª•ÈÄâÊã©‰ΩøÁî®Ëøô‰∫õÊ®°Êùø[/dim]\n"
                )
                continue

            # Â§ÑÁêÜ Pipeline ÊûÑÂª∫ËØ∑Ê±Ç
            if pipeline_coordinator.handle(question):
                continue

            # Â§ÑÁêÜÊôÆÈÄöÈóÆÁ≠î
            answer_once(question)
    finally:
        # Ê∏ÖÁêÜËµÑÊ∫ê
        generator.cleanup()


@app.callback()
def main(
    ctx: typer.Context,
    index_name: str = typer.Option(
        DEFAULT_INDEX_NAME,
        "--index",
        "-i",
        help="Á¥¢ÂºïÂêçÁß∞ÔºåÁî®‰∫éËØªÂèñ manifest Âíå SageDB Êñá‰ª∂",
    ),
    ask: Optional[str] = typer.Option(
        None,
        "--ask",
        "-q",
        help="Áõ¥Êé•ÊèêÈóÆÂπ∂ÈÄÄÂá∫ÔºåËÄå‰∏çÊòØËøõÂÖ•‰∫§‰∫íÊ®°Âºè",
    ),
    top_k: int = typer.Option(
        DEFAULT_TOP_K,
        "--top-k",
        "-k",
        min=1,
        max=20,
        help="Ê£ÄÁ¥¢Êó∂ËøîÂõûÁöÑÂèÇËÄÉÊñáÊ°£Êï∞Èáè",
    ),
    backend: str = typer.Option(
        DEFAULT_BACKEND,
        "--backend",
        help="ÂõûÁ≠îÁîüÊàêÂêéÁ´Ø: mock / openai / compatible / finetune / vllm / ollama",
    ),
    model: str = typer.Option(
        "qwen-max",
        "--model",
        help="ÂõûÁ≠îÁîüÊàêÊ®°ÂûãÂêçÁß∞Ôºàfinetune backend ‰ºöËá™Âä®‰ªéÈÖçÁΩÆËØªÂèñÔºâ",
    ),
    base_url: Optional[str] = typer.Option(
        None,
        "--base-url",
        help="LLM API base_url (‰æãÂ¶Ç vLLM ÊàñÂÖºÂÆπ OpenAI ÁöÑÊé•Âè£)",
    ),
    api_key: Optional[str] = typer.Option(
        lambda: os.environ.get("TEMP_GENERATOR_API_KEY"),
        "--api-key",
        help="LLM API Key (ÈªòËÆ§ËØªÂèñÁéØÂ¢ÉÂèòÈáè TEMP_GENERATOR_API_KEY)",
    ),
    finetune_model: Optional[str] = typer.Option(
        DEFAULT_FINETUNE_MODEL,
        "--finetune-model",
        help="‰ΩøÁî® finetune backend Êó∂ÁöÑÂæÆË∞ÉÊ®°ÂûãÂêçÁß∞Ôºà~/.sage/finetune_output/ ‰∏ãÁöÑÁõÆÂΩïÂêçÔºâ",
    ),
    finetune_port: int = typer.Option(
        DEFAULT_FINETUNE_PORT,
        "--finetune-port",
        help="finetune backend ‰ΩøÁî®ÁöÑ vLLM ÊúçÂä°Á´ØÂè£",
    ),
    index_root: Optional[str] = typer.Option(
        None,
        "--index-root",
        help="Á¥¢ÂºïËæìÂá∫ÁõÆÂΩï (Êú™Êèê‰æõÂàô‰ΩøÁî® ~/.sage/cache/chat)",
    ),
    stream: bool = typer.Option(False, "--stream", help="ÂêØÁî®ÊµÅÂºèËæìÂá∫ (‰ªÖÂΩìÂêéÁ´ØÊîØÊåÅ)"),
) -> None:
    if ctx.invoked_subcommand is not None:
        return

    ensure_sage_db()
    root = resolve_index_root(index_root)
    manifest = load_or_bootstrap_manifest(root, index_name)

    interactive_chat(
        manifest=manifest,
        index_root=root,
        top_k=top_k,
        backend=backend,
        model=model,
        base_url=base_url,
        api_key=api_key,
        ask=ask,
        stream=stream,
        finetune_model=finetune_model,
        finetune_port=finetune_port,
    )


@app.command("ingest")
def ingest(
    source_dir: Optional[Path] = typer.Option(
        None,
        "--source",
        "-s",
        exists=True,
        file_okay=False,
        dir_okay=True,
        resolve_path=True,
        help="ÊñáÊ°£Êù•Ê∫êÁõÆÂΩï (ÈªòËÆ§ docs-public/docs_src)",
    ),
    index_name: str = typer.Option(
        DEFAULT_INDEX_NAME, "--index", "-i", help="Á¥¢ÂºïÂêçÁß∞"
    ),
    chunk_size: int = typer.Option(
        DEFAULT_CHUNK_SIZE,
        "--chunk-size",
        help="chunk Â≠óÁ¨¶ÈïøÂ∫¶",
        min=128,
        max=4096,
    ),
    chunk_overlap: int = typer.Option(
        DEFAULT_CHUNK_OVERLAP,
        "--chunk-overlap",
        help="chunk ‰πãÈó¥ÁöÑÈáçÂè†Â≠óÁ¨¶Êï∞",
        min=0,
        max=1024,
    ),
    embedding_method: str = typer.Option(
        DEFAULT_EMBEDDING_METHOD,
        "--embedding-method",
        help="Embedding ÊñπÊ≥ï (mockembedder/hf/openai/...)",
    ),
    embedding_model: Optional[str] = typer.Option(
        None,
        "--embedding-model",
        help="Embedding Ê®°ÂûãÂêçÁß∞ (ÊñπÊ≥ïÈúÄË¶ÅÊó∂Êèê‰æõ)",
    ),
    fixed_dim: int = typer.Option(
        DEFAULT_FIXED_DIM,
        "--fixed-dim",
        help="mockembedder ‰ΩøÁî®ÁöÑÁª¥Â∫¶",
        min=64,
        max=2048,
    ),
    max_files: Optional[int] = typer.Option(
        None,
        "--max-files",
        help="‰ªÖÂ§ÑÁêÜÊåáÂÆöÊï∞ÈáèÁöÑÊñá‰ª∂ (ÊµãËØï/Ë∞ÉËØïÁî®)",
    ),
    index_root: Optional[str] = typer.Option(
        None,
        "--index-root",
        help="Á¥¢ÂºïËæìÂá∫ÁõÆÂΩï (Êú™Êèê‰æõÂàô‰ΩøÁî® ~/.sage/cache/chat)",
    ),
) -> None:
    ensure_sage_db()
    root = resolve_index_root(index_root)
    target_source = source_dir or default_source_dir()

    needs_model = embedding_method in METHODS_REQUIRE_MODEL
    if needs_model and not embedding_model:
        raise typer.BadParameter(f"{embedding_method} ÊñπÊ≥ïÈúÄË¶ÅÊåáÂÆö --embedding-model")

    # ÊûÑÂª∫ embedding ÈÖçÁΩÆÔºàÊñ∞Êé•Âè£‰ºöËá™Âä®Â§ÑÁêÜÈªòËÆ§ÂÄºÔºâ
    embedding_config: Dict[str, object] = {"method": embedding_method, "params": {}}

    # ËÆæÁΩÆÊñπÊ≥ïÁâπÂÆöÂèÇÊï∞
    if embedding_method == "mockembedder":
        embedding_config["params"]["fixed_dim"] = fixed_dim
    elif embedding_method == "hash":
        embedding_config["params"]["dim"] = fixed_dim

    # ËÆæÁΩÆÊ®°ÂûãÂêçÁß∞ÔºàÂ¶ÇÊûúÊèê‰æõÔºâ
    if embedding_model:
        embedding_config["params"]["model"] = embedding_model

    console.print(
        Panel(
            f"Á¥¢ÂºïÂêçÁß∞: [cyan]{index_name}[/cyan]\n"
            f"ÊñáÊ°£ÁõÆÂΩï: [green]{target_source}[/green]\n"
            f"Á¥¢ÂºïÁõÆÂΩï: [magenta]{root}[/magenta]\n"
            f"Embedding: {embedding_config}",
            title="SAGE Chat Ingest",
        )
    )

    manifest = ingest_source(
        source_dir=target_source,
        index_root=root,
        index_name=index_name,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        embedding_config=embedding_config,
        max_files=max_files,
    )


@app.command("show")
def show_manifest(
    index_name: str = typer.Option(DEFAULT_INDEX_NAME, "--index", "-i"),
    index_root: Optional[str] = typer.Option(None, "--index-root", help="Á¥¢ÂºïÊâÄÂú®ÁõÆÂΩï"),
) -> None:
    ensure_sage_db()
    root = resolve_index_root(index_root)
    try:
        manifest = load_manifest(root, index_name)
    except FileNotFoundError as exc:
        console.print(f"[red]{exc}[/red]")
        raise typer.Exit(code=1)

    table = Table(title=f"SAGE Chat Á¥¢Âºï: {index_name}")
    table.add_column("Â±ûÊÄß", style="cyan")
    table.add_column("ÂÄº", style="green")
    table.add_row("Á¥¢ÂºïË∑ØÂæÑ", str(manifest.db_path))
    table.add_row("ÂàõÂª∫Êó∂Èó¥", manifest.created_at)
    table.add_row("ÊñáÊ°£ÁõÆÂΩï", manifest.source_dir)
    table.add_row("ÊñáÊ°£Êï∞Èáè", str(manifest.num_documents))
    table.add_row("Chunk Êï∞Èáè", str(manifest.num_chunks))
    table.add_row("Embedding", json.dumps(manifest.embedding, ensure_ascii=False))
    table.add_row(
        "Chunk ÈÖçÁΩÆ", f"size={manifest.chunk_size}, overlap={manifest.chunk_overlap}"
    )
    console.print(table)


__all__ = ["app"]
