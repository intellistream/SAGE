"""
SAGE Studio Backend API

A simple FastAPI backend service that provides real SAGE data to the Studio frontend.
"""

import importlib
import inspect
import json
import os
import sys
from datetime import datetime
from pathlib import Path

import uvicorn
from fastapi import FastAPI, File, HTTPException, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from sage.studio.services.chat_pipeline_recommender import generate_pipeline_recommendation


def _convert_pipeline_to_job(
    pipeline_data: dict, pipeline_id: str, file_path: Path | None = None
) -> dict:
    """å°†æ‹“æ‰‘å›¾æ•°æ®è½¬æ¢ä¸º Job æ ¼å¼"""
    from datetime import datetime

    # ä»æ‹“æ‰‘å›¾æ•°æ®ä¸­æå–ä¿¡æ¯
    name = pipeline_data.get("name", f"æ‹“æ‰‘å›¾ {pipeline_id}")
    description = pipeline_data.get("description", "")
    nodes = pipeline_data.get("nodes", [])
    edges = pipeline_data.get("edges", [])

    # åˆ›å»ºæ“ä½œç¬¦åˆ—è¡¨
    operators = []
    for i, node in enumerate(nodes):
        # æ„å»ºä¸‹æ¸¸è¿æ¥
        downstream = []
        for edge in edges:
            if edge.get("source") == node.get("id"):
                # æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹çš„ç´¢å¼•
                target_node = next((n for n in nodes if n.get("id") == edge.get("target")), None)
                if target_node:
                    target_index = next(
                        (j for j, n in enumerate(nodes) if n.get("id") == edge.get("target")),
                        None,
                    )
                    if target_index is not None:
                        downstream.append(target_index)

        operator = {
            "id": i,
            "name": node.get("name", f"Operator_{i}"),
            "numOfInstances": 1,
            "downstream": downstream,
        }
        operators.append(operator)

    # ä»æ–‡ä»¶åæˆ–æ–‡ä»¶å…ƒæ•°æ®ä¸­æå–åˆ›å»ºæ—¶é—´
    create_time = None

    # æ–¹æ³•1: ä»æ–‡ä»¶åè§£ææ—¶é—´æˆ³ (pipeline_1759908680.json)
    if pipeline_id.startswith("pipeline_"):
        try:
            timestamp_str = pipeline_id.replace("pipeline_", "")
            timestamp = int(timestamp_str)
            create_time = datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
        except (ValueError, OSError) as e:
            print(f"Failed to parse timestamp from pipeline_id {pipeline_id}: {e}")

    # æ–¹æ³•2: å¦‚æœè§£æå¤±è´¥,ä½¿ç”¨æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
    if create_time is None and file_path and file_path.exists():
        try:
            mtime = file_path.stat().st_mtime
            create_time = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M:%S")
        except Exception as e:
            print(f"Failed to get file mtime for {file_path}: {e}")

    # æ–¹æ³•3: å…œåº•ä½¿ç”¨å½“å‰æ—¶é—´
    if create_time is None:
        create_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    job = {
        "jobId": pipeline_id,
        "name": name,
        "description": description,  # æ·»åŠ æè¿°å­—æ®µ
        "isRunning": False,  # æ‹“æ‰‘å›¾é»˜è®¤ä¸åœ¨è¿è¡Œ
        "nthreads": "1",
        "cpu": "0%",
        "ram": "0GB",
        "startTime": create_time,
        "duration": "00:00:00",
        "nevents": 0,
        "minProcessTime": 0,
        "maxProcessTime": 0,
        "meanProcessTime": 0,
        "latency": 0,
        "throughput": 0,
        "ncore": 1,
        "periodicalThroughput": [0],
        "periodicalLatency": [0],
        "totalTimeBreakdown": {
            "totalTime": 0,
            "serializeTime": 0,
            "persistTime": 0,
            "streamProcessTime": 0,
            "overheadTime": 0,
        },
        "schedulerTimeBreakdown": {
            "overheadTime": 0,
            "streamTime": 0,
            "totalTime": 0,
            "txnTime": 0,
        },
        "operators": operators,
        # æ·»åŠ  config å­—æ®µï¼Œä¿ç•™åŸå§‹çš„ React Flow æ ¼å¼æ•°æ®
        "config": {
            "name": name,
            "description": description,
            "nodes": nodes,
            "edges": edges,
        },
    }

    return job


def _get_sage_dir() -> Path:
    """è·å– SAGE ç›®å½•è·¯å¾„"""
    # é¦–å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡
    env_dir = os.environ.get("SAGE_OUTPUT_DIR")
    if env_dir:
        sage_dir = Path(env_dir)
    else:
        # æ£€æŸ¥æ˜¯å¦åœ¨å¼€å‘ç¯å¢ƒä¸­
        current_dir = Path.cwd()
        if (current_dir / "packages" / "sage-common").exists():
            sage_dir = current_dir / ".sage"
        else:
            sage_dir = Path.home() / ".sage"

    sage_dir.mkdir(parents=True, exist_ok=True)
    return sage_dir


# Pydantic æ¨¡å‹å®šä¹‰
class Job(BaseModel):
    jobId: str
    name: str
    description: str | None = ""  # æ·»åŠ æè¿°å­—æ®µ
    isRunning: bool
    nthreads: str
    cpu: str
    ram: str
    startTime: str
    duration: str
    nevents: int
    minProcessTime: int
    maxProcessTime: int
    meanProcessTime: int
    latency: int
    throughput: int
    ncore: int
    periodicalThroughput: list[int]
    periodicalLatency: list[int]
    totalTimeBreakdown: dict
    schedulerTimeBreakdown: dict
    operators: list[dict]
    config: dict | None = None  # æ·»åŠ  config å­—æ®µï¼Œç”¨äºå­˜å‚¨ React Flow æ ¼å¼çš„èŠ‚ç‚¹å’Œè¾¹æ•°æ®


class ParameterConfig(BaseModel):
    """èŠ‚ç‚¹å‚æ•°é…ç½®"""

    name: str
    label: str
    type: str  # text, textarea, number, select, password, json
    required: bool | None = False
    defaultValue: str | int | float | dict | list | None = None  # æ”¯æŒ JSON å¯¹è±¡å’Œæ•°ç»„
    placeholder: str | None = None
    description: str | None = None
    options: list[str] | None = None
    min: int | float | None = None
    max: int | float | None = None
    step: int | float | None = None


class OperatorInfo(BaseModel):
    id: int
    name: str
    description: str
    code: str
    isCustom: bool
    parameters: list[ParameterConfig] | None = []  # æ·»åŠ å‚æ•°é…ç½®å­—æ®µ


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="SAGE Studio Backend",
    description="Backend API service for SAGE Studio frontend",
    version="1.0.0",
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",  # Vite å¼€å‘æœåŠ¡å™¨é»˜è®¤ç«¯å£
        "http://localhost:4173",  # Vite preview æœåŠ¡å™¨é»˜è®¤ç«¯å£
        "http://0.0.0.0:5173",
        "http://0.0.0.0:4173",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def _read_sage_data_from_files():
    """ä» .sage ç›®å½•çš„æ–‡ä»¶ä¸­è¯»å–å®é™…çš„ SAGE æ•°æ®"""
    sage_dir = _get_sage_dir()
    data = {"jobs": [], "operators": [], "pipelines": []}

    try:
        # è¯»å–ä½œä¸šä¿¡æ¯
        states_dir = sage_dir / "states"
        if states_dir.exists():
            for job_file in states_dir.glob("*.json"):
                try:
                    with open(job_file, encoding="utf-8") as f:
                        job_data = json.load(f)
                        data["jobs"].append(job_data)
                except Exception as e:
                    print(f"Error reading job file {job_file}: {e}")

        # è¯»å–ä¿å­˜çš„æ‹“æ‰‘å›¾å¹¶è½¬æ¢ä¸º Job æ ¼å¼
        pipelines_dir = sage_dir / "pipelines"
        if pipelines_dir.exists():
            for pipeline_file in pipelines_dir.glob("pipeline_*.json"):
                try:
                    with open(pipeline_file, encoding="utf-8") as f:
                        pipeline_data = json.load(f)
                        # å°†æ‹“æ‰‘å›¾è½¬æ¢ä¸º Job æ ¼å¼ï¼Œä¼ é€’æ–‡ä»¶è·¯å¾„ä»¥æå–çœŸå®åˆ›å»ºæ—¶é—´
                        job_from_pipeline = _convert_pipeline_to_job(
                            pipeline_data, pipeline_file.stem, pipeline_file
                        )
                        data["jobs"].append(job_from_pipeline)
                except Exception as e:
                    print(f"Error reading pipeline file {pipeline_file}: {e}")

        # è¯»å–æ“ä½œç¬¦ä¿¡æ¯
        operators_file = sage_dir / "output" / "operators.json"
        if operators_file.exists():
            try:
                with open(operators_file, encoding="utf-8") as f:
                    operators_data = json.load(f)
                    data["operators"] = operators_data
            except Exception as e:
                print(f"Error reading operators file: {e}")

        # è¯»å–ç®¡é“ä¿¡æ¯
        pipelines_file = sage_dir / "output" / "pipelines.json"
        if pipelines_file.exists():
            try:
                with open(pipelines_file) as f:
                    pipelines_data = json.load(f)
                    data["pipelines"] = pipelines_data
            except Exception as e:
                print(f"Error reading pipelines file: {e}")

    except Exception as e:
        print(f"Error reading SAGE data: {e}")

    return data


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "SAGE Studio Backend API", "status": "running"}


@app.get("/api/jobs/all", response_model=list[Job])
async def get_all_jobs():
    """è·å–æ‰€æœ‰ä½œä¸šä¿¡æ¯"""
    try:
        sage_data = _read_sage_data_from_files()
        jobs = sage_data.get("jobs", [])

        print(f"DEBUG: Read {len(jobs)} jobs from files")
        print(f"DEBUG: sage_data = {sage_data}")

        # å¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼Œè¿”å›ä¸€äº›ç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºå¼€å‘ï¼‰
        if not jobs:
            print("DEBUG: No real jobs found, using fallback data")
            jobs = [
                {
                    "jobId": "job_001",
                    "name": "RAGé—®ç­”ç®¡é“ç¤ºä¾‹",
                    "isRunning": False,
                    "nthreads": "4",
                    "cpu": "0%",
                    "ram": "0GB",
                    "startTime": "2025-08-18 10:30:00",
                    "duration": "00:45:12",
                    "nevents": 1000,
                    "minProcessTime": 10,
                    "maxProcessTime": 500,
                    "meanProcessTime": 150,
                    "latency": 200,
                    "throughput": 800,
                    "ncore": 4,
                    "periodicalThroughput": [750, 800, 820, 785, 810],
                    "periodicalLatency": [180, 200, 190, 210, 195],
                    "totalTimeBreakdown": {
                        "totalTime": 2712000,
                        "serializeTime": 50000,
                        "persistTime": 100000,
                        "streamProcessTime": 2500000,
                        "overheadTime": 62000,
                    },
                    "schedulerTimeBreakdown": {
                        "overheadTime": 50000,
                        "streamTime": 2600000,
                        "totalTime": 2712000,
                        "txnTime": 62000,
                    },
                    "operators": [
                        {
                            "id": 1,
                            "name": "FileSource",
                            "numOfInstances": 1,
                            "throughput": 800,
                            "latency": 50,
                            "explorationStrategy": "greedy",
                            "schedulingGranularity": "batch",
                            "abortHandling": "rollback",
                            "numOfTD": 10,
                            "numOfLD": 5,
                            "numOfPD": 2,
                            "lastBatch": 999,
                            "downstream": [2],
                        }
                    ],
                }
            ]

        return jobs
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šä¿¡æ¯å¤±è´¥: {str(e)}")


def _get_studio_operators_dir() -> Path:
    """è·å– Studio operators æ•°æ®ç›®å½•è·¯å¾„"""
    current_file = Path(__file__)
    # ä» api.py æ–‡ä»¶è·¯å¾„æ‰¾åˆ° studio æ ¹ç›®å½•:
    # ../../../ ä» backend/ åˆ° studio/
    studio_root = current_file.parent.parent.parent
    operators_dir = studio_root / "data" / "operators"
    return operators_dir


def _load_operator_class_source(module_path: str, class_name: str) -> str:
    """åŠ¨æ€åŠ è½½operatorç±»å¹¶è·å–å…¶æºä»£ç """
    try:
        # æ·»åŠ SAGEé¡¹ç›®è·¯å¾„åˆ°sys.path
        sage_root = Path(__file__).parent.parent.parent.parent.parent.parent
        if str(sage_root) not in sys.path:
            sys.path.insert(0, str(sage_root))

        # åŠ¨æ€å¯¼å…¥æ¨¡å—
        module = importlib.import_module(module_path)

        # è·å–ç±»
        operator_class = getattr(module, class_name)

        # è·å–æºä»£ç 
        source_code = inspect.getsource(operator_class)

        return source_code

    except Exception as e:
        print(f"Error loading operator class {module_path}.{class_name}: {e}")
        return f"# Error loading source code for {class_name}\n# {str(e)}"


def _read_real_operators():
    """ä» studio data ç›®å½•è¯»å–çœŸå®çš„æ“ä½œç¬¦æ•°æ®å¹¶åŠ¨æ€åŠ è½½æºä»£ç """
    operators = []
    operators_dir = _get_studio_operators_dir()

    if not operators_dir.exists():
        print(f"Operators directory not found: {operators_dir}")
        return []

    try:
        # è¯»å–æ‰€æœ‰ JSON æ–‡ä»¶
        for json_file in operators_dir.glob("*.json"):
            try:
                with open(json_file, encoding="utf-8") as f:
                    operator_data = json.load(f)

                    # æ£€æŸ¥æ˜¯å¦æœ‰module_pathå’Œclass_nameå­—æ®µ
                    if "module_path" in operator_data and "class_name" in operator_data:
                        # åŠ¨æ€åŠ è½½æºä»£ç 
                        source_code = _load_operator_class_source(
                            operator_data["module_path"], operator_data["class_name"]
                        )
                        operator_data["code"] = source_code
                    else:
                        # å¦‚æœæ²¡æœ‰æ¨¡å—è·¯å¾„ä¿¡æ¯ï¼Œä½¿ç”¨ç©ºä»£ç 
                        operator_data["code"] = ""

                    # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
                    required_fields = ["id", "name", "description", "isCustom"]
                    if all(key in operator_data for key in required_fields):
                        # æ¸…ç†ä¸éœ€è¦çš„å­—æ®µ
                        clean_data = {
                            "id": operator_data["id"],
                            "name": operator_data["name"],
                            "description": operator_data["description"],
                            "code": operator_data.get("code", ""),
                            "isCustom": operator_data["isCustom"],
                            "parameters": operator_data.get("parameters", []),  # æ·»åŠ å‚æ•°é…ç½®
                        }
                        operators.append(clean_data)
                        print(
                            f"Loaded operator: {operator_data['name']} with {len(clean_data['parameters'])} parameters"
                        )
                    else:
                        print(f"Invalid operator data in {json_file}")

            except Exception as e:
                print(f"Error reading operator file {json_file}: {e}")

    except Exception as e:
        print(f"Error reading operators directory: {e}")

    return operators


@app.get("/api/operators", response_model=list[OperatorInfo])
async def get_operators():
    """è·å–æ‰€æœ‰æ“ä½œç¬¦ä¿¡æ¯"""
    try:
        # é¦–å…ˆå°è¯•è¯»å–çœŸå®çš„æ“ä½œç¬¦æ•°æ®
        operators = _read_real_operators()

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®æ•°æ®ï¼Œä½¿ç”¨åå¤‡æ•°æ®
        if not operators:
            print("No real operator data found, using fallback data")
            operators = [
                {
                    "id": 1,
                    "name": "FileSource",
                    "description": "ä»æ–‡ä»¶è¯»å–æ•°æ®çš„æºæ“ä½œç¬¦",
                    "code": "class FileSource:\n    def __init__(self, file_path):\n        self.file_path = file_path\n    \n    def read_data(self):\n        with open(self.file_path, 'r') as f:\n            return f.read()",
                    "isCustom": True,
                },
                {
                    "id": 2,
                    "name": "SimpleRetriever",
                    "description": "ç®€å•çš„æ£€ç´¢æ“ä½œç¬¦",
                    "code": "class SimpleRetriever:\n    def __init__(self, top_k=5):\n        self.top_k = top_k\n    \n    def retrieve(self, query):\n        return query[:self.top_k]",
                    "isCustom": True,
                },
            ]

        print(f"Returning {len(operators)} operators")
        return operators
    except Exception as e:
        print(f"Error in get_operators: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ“ä½œç¬¦ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.get("/api/operators/list")
async def get_operators_list(page: int = 1, size: int = 10, search: str = ""):
    """è·å–æ“ä½œç¬¦åˆ—è¡¨ - æ”¯æŒåˆ†é¡µå’Œæœç´¢"""
    try:
        # è·å–æ‰€æœ‰æ“ä½œç¬¦
        all_operators = _read_real_operators()

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®æ•°æ®ï¼Œä½¿ç”¨åå¤‡æ•°æ®
        if not all_operators:
            print("No real operator data found, using fallback data")
            all_operators = [
                {
                    "id": 1,
                    "name": "FileSource",
                    "description": "ä»æ–‡ä»¶è¯»å–æ•°æ®çš„æºæ“ä½œç¬¦",
                    "code": "class FileSource:\n    def __init__(self, file_path):\n        self.file_path = file_path\n    \n    def read_data(self):\n        with open(self.file_path, 'r') as f:\n            return f.read()",
                    "isCustom": True,
                },
                {
                    "id": 2,
                    "name": "SimpleRetriever",
                    "description": "ç®€å•çš„æ£€ç´¢æ“ä½œç¬¦",
                    "code": "class SimpleRetriever:\n    def __init__(self, top_k=5):\n        self.top_k = top_k\n    \n    def retrieve(self, query):\n        return query[:self.top_k]",
                    "isCustom": True,
                },
            ]

        # æœç´¢è¿‡æ»¤
        if search:
            filtered_operators = [
                op
                for op in all_operators
                if search.lower() in op["name"].lower()
                or search.lower() in op["description"].lower()
            ]
        else:
            filtered_operators = all_operators

        # åˆ†é¡µè®¡ç®—
        total = len(filtered_operators)
        start = (page - 1) * size
        end = start + size
        items = filtered_operators[start:end]

        result = {"items": items, "total": total}

        print(f"Returning page {page} with {len(items)} operators (total: {total})")
        return result

    except Exception as e:
        print(f"Error in get_operators_list: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ“ä½œç¬¦åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/api/pipelines")
async def get_pipelines():
    """è·å–æ‰€æœ‰ç®¡é“ä¿¡æ¯"""
    try:
        sage_data = _read_sage_data_from_files()
        pipelines = sage_data.get("pipelines", [])

        # å¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼Œè¿”å›ä¸€äº›ç¤ºä¾‹æ•°æ®
        if not pipelines:
            pipelines = [
                {
                    "id": "pipeline_001",
                    "name": "ç¤ºä¾‹RAGç®¡é“",
                    "description": "æ¼”ç¤ºRAGé—®ç­”ç³»ç»Ÿçš„æ•°æ®å¤„ç†ç®¡é“",
                    "status": "running",
                    "operators": [
                        {
                            "id": "source1",
                            "type": "FileSource",
                            "config": {"file_path": "/data/documents.txt"},
                        },
                        {
                            "id": "retriever1",
                            "type": "SimpleRetriever",
                            "config": {"top_k": 5},
                        },
                        {
                            "id": "sink1",
                            "type": "TerminalSink",
                            "config": {"format": "json"},
                        },
                    ],
                    "connections": [
                        {"from": "source1", "to": "retriever1"},
                        {"from": "retriever1", "to": "sink1"},
                    ],
                }
            ]

        return {"pipelines": pipelines}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç®¡é“ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.post("/api/pipeline/submit")
async def submit_pipeline(topology_data: dict):
    """æäº¤æ‹“æ‰‘å›¾/ç®¡é“é…ç½®"""
    try:
        print(f"Received pipeline submission: {topology_data}")

        # è¿™é‡Œå¯ä»¥æ·»åŠ ä¿å­˜åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“çš„é€»è¾‘
        sage_dir = _get_sage_dir()
        pipelines_dir = sage_dir / "pipelines"
        pipelines_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        import time

        timestamp = int(time.time())
        pipeline_file = pipelines_dir / f"pipeline_{timestamp}.json"

        # ä¿å­˜æ‹“æ‰‘æ•°æ®åˆ°æ–‡ä»¶
        with open(pipeline_file, "w", encoding="utf-8") as f:
            json.dump(topology_data, f, indent=2, ensure_ascii=False)

        return {
            "status": "success",
            "message": "æ‹“æ‰‘å›¾æäº¤æˆåŠŸ",
            "pipeline_id": f"pipeline_{timestamp}",
            "file_path": str(pipeline_file),
        }
    except Exception as e:
        print(f"Error submitting pipeline: {e}")
        raise HTTPException(status_code=500, detail=f"æäº¤æ‹“æ‰‘å›¾å¤±è´¥: {str(e)}")


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "service": "SAGE Studio Backend"}


# ==================== ç®¡é“è¯¦æƒ…ç›¸å…³ç«¯ç‚¹ ====================
# ç”¨äºæ”¯æŒå‰ç«¯ View Details åŠŸèƒ½çš„å ä½ç¬¦ç«¯ç‚¹

# å…¨å±€çŠ¶æ€å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ•°æ®åº“ï¼‰
job_runtime_status = {}  # {job_id: {status, use_ray, isRunning, ...}}
job_logs = {}  # {job_id: [log_lines]}
job_configs_cache = {}  # {pipeline_id: yaml_config}
user_queries = {}  # {job_id: [(query, answer), ...]}


@app.get("/jobInfo/get/{job_id}")
async def get_job_detail(job_id: str):
    """è·å–ä½œä¸šè¯¦ç»†ä¿¡æ¯ - åŒ…å«æ“ä½œç¬¦æ‹“æ‰‘ç»“æ„"""
    try:
        # é¦–å…ˆå°è¯•ä»å·²ä¿å­˜çš„æ•°æ®ä¸­æŸ¥æ‰¾
        sage_data = _read_sage_data_from_files()
        jobs = sage_data.get("jobs", [])

        # æŸ¥æ‰¾åŒ¹é…çš„ä½œä¸š
        job = next((j for j in jobs if j.get("jobId") == job_id), None)

        if job:
            return job

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®é™…æ•°æ®ï¼Œè¿”å›å ä½ç¬¦æ•°æ®ï¼ˆç”¨äºå¼€å‘ï¼‰
        print(f"Job {job_id} not found in saved data, returning placeholder")
        return {
            "jobId": job_id,
            "name": f"ç®¡é“ {job_id}",
            "isRunning": False,
            "nthreads": "4",
            "cpu": "0%",
            "ram": "0GB",
            "startTime": "2025-10-10 15:00:00",
            "duration": "00:00:00",
            "nevents": 0,
            "minProcessTime": 0,
            "maxProcessTime": 0,
            "meanProcessTime": 0,
            "latency": 0,
            "throughput": 0,
            "ncore": 4,
            "periodicalThroughput": [0],
            "periodicalLatency": [0],
            "totalTimeBreakdown": {
                "totalTime": 0,
                "serializeTime": 0,
                "persistTime": 0,
                "streamProcessTime": 0,
                "overheadTime": 0,
            },
            "schedulerTimeBreakdown": {
                "overheadTime": 0,
                "streamTime": 0,
                "totalTime": 0,
                "txnTime": 0,
            },
            "operators": [
                {
                    "id": 1,
                    "name": "FileSource",
                    "numOfInstances": 1,
                    "throughput": 0,
                    "latency": 0,
                    "explorationStrategy": "greedy",
                    "schedulingGranularity": "batch",
                    "abortHandling": "rollback",
                    "numOfTD": 0,
                    "numOfLD": 0,
                    "numOfPD": 0,
                    "lastBatch": 0,
                    "downstream": [2],
                },
                {
                    "id": 2,
                    "name": "TerminalSink",
                    "numOfInstances": 1,
                    "throughput": 0,
                    "latency": 0,
                    "explorationStrategy": "greedy",
                    "schedulingGranularity": "batch",
                    "abortHandling": "rollback",
                    "numOfTD": 0,
                    "numOfLD": 0,
                    "numOfPD": 0,
                    "lastBatch": 0,
                    "downstream": [],
                },
            ],
        }
    except Exception as e:
        print(f"Error getting job detail: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šè¯¦æƒ…å¤±è´¥: {str(e)}")


@app.get("/api/signal/status/{job_id}")
async def get_job_status(job_id: str):
    """è·å–ä½œä¸šè¿è¡ŒçŠ¶æ€"""
    try:
        # ä»å†…å­˜ä¸­è·å–çŠ¶æ€
        status = job_runtime_status.get(
            job_id,
            {
                "job_id": job_id,
                "status": "idle",
                "use_ray": False,
                "isRunning": False,
            },
        )
        return status
    except Exception as e:
        print(f"Error getting job status: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šçŠ¶æ€å¤±è´¥: {str(e)}")


@app.post("/api/signal/start/{job_id}")
async def start_job(job_id: str):
    """å¯åŠ¨ä½œä¸š"""
    try:
        # æ›´æ–°è¿è¡ŒçŠ¶æ€
        job_runtime_status[job_id] = {
            "job_id": job_id,
            "status": "running",
            "use_ray": False,
            "isRunning": True,
        }

        # åˆå§‹åŒ–æ—¥å¿—
        if job_id not in job_logs:
            job_logs[job_id] = []

        job_logs[job_id].append(f"[SYSTEM] Job {job_id} started at 2025-10-10 15:30:00")

        return {"status": "success", "message": f"ä½œä¸š {job_id} å·²å¯åŠ¨"}
    except Exception as e:
        print(f"Error starting job: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨ä½œä¸šå¤±è´¥: {str(e)}")


@app.post("/api/signal/stop/{job_id}/{duration}")
async def stop_job(job_id: str, duration: str):
    """åœæ­¢ä½œä¸š"""
    try:
        # æ›´æ–°è¿è¡ŒçŠ¶æ€
        job_runtime_status[job_id] = {
            "job_id": job_id,
            "status": "stopped",
            "use_ray": False,
            "isRunning": False,
        }

        # æ·»åŠ åœæ­¢æ—¥å¿—
        if job_id in job_logs:
            job_logs[job_id].append(f"[SYSTEM] Job {job_id} stopped after {duration}")

        return {"status": "success", "message": f"ä½œä¸š {job_id} å·²åœæ­¢"}
    except Exception as e:
        print(f"Error stopping job: {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢ä½œä¸šå¤±è´¥: {str(e)}")


@app.get("/api/signal/sink/{job_id}")
async def get_job_logs(job_id: str, offset: int = 0):
    """è·å–ä½œä¸šæ—¥å¿—ï¼ˆå¢é‡ï¼‰"""
    try:
        # è·å–è¯¥ä½œä¸šçš„æ—¥å¿—
        logs = job_logs.get(job_id, [])

        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆoffset=0ï¼‰ä¸”æ²¡æœ‰æ—¥å¿—ï¼Œè¿”å›ç§å­æ¶ˆæ¯
        if offset == 0 and len(logs) == 0:
            seed_line = (
                f"[SYSTEM] Console ready for {job_id}. Click Start or submit a FileSource query."
            )
            job_logs[job_id] = [seed_line]
            return {"offset": 1, "lines": [seed_line]}

        # è¿”å›ä» offset å¼€å§‹çš„æ–°æ—¥å¿—
        new_logs = logs[offset:]
        new_offset = len(logs)

        return {"offset": new_offset, "lines": new_logs}
    except Exception as e:
        print(f"Error getting job logs: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šæ—¥å¿—å¤±è´¥: {str(e)}")


@app.get("/batchInfo/get/all/{job_id}/{operator_id}")
async def get_all_batches(job_id: str, operator_id: str):
    """è·å–æ“ä½œç¬¦çš„æ‰€æœ‰æ‰¹æ¬¡ä¿¡æ¯"""
    try:
        # operator_id å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "s1", "r1"ï¼‰æˆ–æ•°å­—
        # è¿”å›ç©ºæ•°ç»„ä½œä¸ºå ä½ç¬¦
        # å®é™…å®ç°éœ€è¦ä» SAGE è¿è¡Œæ—¶è·å–æ‰¹æ¬¡ç»Ÿè®¡æ•°æ®
        print(f"Getting batches for job={job_id}, operator={operator_id}")
        return []
    except Exception as e:
        print(f"Error getting batches: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ‰¹æ¬¡ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.get("/batchInfo/get/{job_id}/{batch_id}/{operator_id}")
async def get_batch_detail(job_id: str, batch_id: int, operator_id: str):
    """è·å–å•ä¸ªæ‰¹æ¬¡çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        # operator_id å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "s1", "r1"ï¼‰æˆ–æ•°å­—
        # è¿”å›å ä½ç¬¦æ‰¹æ¬¡æ•°æ®
        return {
            "batchId": batch_id,
            "operatorId": operator_id,
            "processTime": 0,
            "tupleCount": 0,
            "timestamp": "2025-10-10 15:30:00",
        }
    except Exception as e:
        print(f"Error getting batch detail: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ‰¹æ¬¡è¯¦æƒ…å¤±è´¥: {str(e)}")


@app.get("/jobInfo/config/{pipeline_id}")
async def get_pipeline_config(pipeline_id: str):
    """è·å–ç®¡é“é…ç½®ï¼ˆYAMLæ ¼å¼ï¼‰"""
    try:
        # å°è¯•ä»ç¼“å­˜è·å–
        if pipeline_id in job_configs_cache:
            return {"config": job_configs_cache[pipeline_id]}

        # è¿”å›é»˜è®¤é…ç½®æ¨¡æ¿
        default_config = """# SAGE Pipeline Configuration
name: Example RAG Pipeline
version: 1.0.0

operators:
  - name: FileSource
    type: source
    config:
      file_path: /data/documents.txt

  - name: SimpleRetriever
    type: retriever
    config:
      top_k: 5

  - name: TerminalSink
    type: sink
    config:
      output_path: /tmp/output.txt
"""
        return {"config": default_config}
    except Exception as e:
        print(f"Error getting pipeline config: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç®¡é“é…ç½®å¤±è´¥: {str(e)}")


@app.put("/jobInfo/config/update/{pipeline_id}")
async def update_pipeline_config(pipeline_id: str, config: dict):
    """æ›´æ–°ç®¡é“é…ç½®"""
    try:
        # ä¿å­˜é…ç½®åˆ°ç¼“å­˜
        config_yaml = config.get("config", "")
        job_configs_cache[pipeline_id] = config_yaml

        # å¯é€‰ï¼šä¿å­˜åˆ°æ–‡ä»¶
        sage_dir = _get_sage_dir()
        config_dir = sage_dir / "configs"
        config_dir.mkdir(exist_ok=True)

        config_file = config_dir / f"{pipeline_id}.yaml"
        with open(config_file, "w", encoding="utf-8") as f:
            f.write(config_yaml)

        return {
            "status": "success",
            "message": "é…ç½®æ›´æ–°æˆåŠŸ",
            "file_path": str(config_file),
        }
    except Exception as e:
        print(f"Error updating pipeline config: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ç®¡é“é…ç½®å¤±è´¥: {str(e)}")


# ==================== Playground API ====================


def _load_flow_data(flow_id: str) -> dict | None:
    """åŠ è½½ Flow æ•°æ®"""
    sage_dir = _get_sage_dir()
    pipelines_dir = sage_dir / "pipelines"

    print(f"ğŸ” Looking for flow: {flow_id}")
    print(f"ğŸ“ Sage dir: {sage_dir}")
    print(f"ğŸ“ Pipelines dir: {pipelines_dir}")
    print(f"ğŸ“ Pipelines dir exists: {pipelines_dir.exists()}")

    # å°è¯•åŠ è½½ pipeline æ–‡ä»¶
    flow_file = pipelines_dir / f"{flow_id}.json"
    print(f"ğŸ“„ Flow file path: {flow_file}")
    print(f"ğŸ“„ Flow file exists: {flow_file.exists()}")

    if flow_file.exists():
        with open(flow_file, encoding="utf-8") as f:
            data = json.load(f)
            print(f"âœ… Loaded flow: {data.get('name', 'Unnamed')}")
            return data

    print("âŒ Flow file not found")
    return None


def _convert_to_flow_definition(flow_data: dict, flow_id: str):
    """å°†å‰ç«¯ Flow æ•°æ®è½¬æ¢ä¸º FlowDefinition"""
    import sys
    from pathlib import Path

    # æ·»åŠ  sage-studio åˆ° Python è·¯å¾„
    studio_path = Path(__file__).parent.parent.parent.parent
    if str(studio_path) not in sys.path:
        sys.path.insert(0, str(studio_path))

    from sage.studio.models import (  # type: ignore[import-not-found]
        VisualConnection,
        VisualNode,
        VisualPipeline,
    )
    from sage.studio.services.node_registry import (  # type: ignore[import-not-found]
        convert_node_type_to_snake_case,
    )

    name = flow_data.get("name", "Unnamed Flow")
    description = flow_data.get("description", "")
    nodes_data = flow_data.get("nodes", [])
    edges_data = flow_data.get("edges", [])

    # è½¬æ¢èŠ‚ç‚¹
    nodes = []
    for node_data in nodes_data:
        # è·å–èŠ‚ç‚¹ç±»å‹å¹¶è½¬æ¢ä¸º snake_case
        node_id = node_data.get("data", {}).get("nodeId", "unknown")
        node_type = convert_node_type_to_snake_case(node_id)

        print(f"ğŸ”„ Converting node: {node_id} â†’ {node_type}")

        node = VisualNode(
            id=node_data.get("id", ""),
            type=node_type,  # ä½¿ç”¨è½¬æ¢åçš„ç±»å‹
            label=node_data.get("data", {}).get("label", "Unnamed Node"),
            position=node_data.get("position", {"x": 0, "y": 0}),
            config=node_data.get("data", {}).get("properties", {}),
        )
        nodes.append(node)

    # è½¬æ¢è¿æ¥
    connections = []
    for edge_data in edges_data:
        connection = VisualConnection(
            id=edge_data.get("id", f"{edge_data.get('source')}-{edge_data.get('target')}"),
            source_node_id=edge_data.get("source", ""),
            source_port="output",  # é»˜è®¤è¾“å‡ºç«¯å£
            target_node_id=edge_data.get("target", ""),
            target_port="input",  # é»˜è®¤è¾“å…¥ç«¯å£
        )
        connections.append(connection)

    return VisualPipeline(
        id=flow_id,
        name=name,
        description=description,
        nodes=nodes,
        connections=connections,
    )


class PlaygroundExecuteRequest(BaseModel):
    """Playground æ‰§è¡Œè¯·æ±‚"""

    flowId: str
    input: str
    sessionId: str = "default"
    stream: bool = False


class AgentStep(BaseModel):
    """Agent æ‰§è¡Œæ­¥éª¤"""

    step: int
    type: str  # reasoning, tool_call, response
    content: str
    timestamp: str
    duration: int | None = None
    toolName: str | None = None
    toolInput: dict | None = None
    toolOutput: dict | None = None


class PlaygroundExecuteResponse(BaseModel):
    """Playground æ‰§è¡Œå“åº”"""

    output: str
    status: str
    agentSteps: list[AgentStep] | None = None


@app.post("/api/playground/execute", response_model=PlaygroundExecuteResponse)
async def execute_playground(request: PlaygroundExecuteRequest):
    """æ‰§è¡Œ Playground Flow - ä½¿ç”¨çœŸå®çš„ SAGE Pipeline"""
    try:
        from datetime import datetime

        print(f"ğŸ¯ Executing playground - flowId: {request.flowId}, sessionId: {request.sessionId}")
        print(f"ğŸ“ Input: {request.input}")

        # 1. åŠ è½½ Flow å®šä¹‰
        flow_data = _load_flow_data(request.flowId)
        if not flow_data:
            raise HTTPException(status_code=404, detail=f"Flow not found: {request.flowId}")

        nodes_config = flow_data.get("nodes", [])
        if not nodes_config:
            return PlaygroundExecuteResponse(
                output="âŒ è¯·å…ˆåœ¨ç”»å¸ƒä¸­åˆ›å»ºèŠ‚ç‚¹",
                status="error",
                agentSteps=None,
            )

        # 2. å‡†å¤‡æ“ä½œç¬¦é…ç½®
        operator_configs = []
        for node in nodes_config:
            node_data = node.get("data", {})
            node_type = node_data.get("nodeId", node_data.get("type", "Unknown"))
            node_config = node_data.get("config", {})

            operator_configs.append({"type": node_type, "config": node_config})

            print(f"ğŸ“¦ èŠ‚ç‚¹é…ç½®: {node_type} - {node_config}")

        # 3. ä½¿ç”¨ PlaygroundExecutor æ‰§è¡Œ
        try:
            from sage.studio.services.playground_executor import get_playground_executor

            executor = get_playground_executor()
            execution_result = executor.execute_simple_query(
                user_input=request.input,
                operator_configs=operator_configs,
                flow_id=request.flowId,  # ä¼ é€’ flow_id ç”¨äºæ—¥å¿—
            )

            # 4. ç”Ÿæˆæ‰§è¡Œæ­¥éª¤
            agent_steps = []
            for idx, op_config in enumerate(operator_configs, start=1):
                agent_steps.append(
                    AgentStep(
                        step=idx,
                        type="tool_call",
                        content=f"æ‰§è¡ŒèŠ‚ç‚¹: {op_config['type']}",
                        timestamp=datetime.now().isoformat(),
                        toolName=op_config["type"],
                        toolInput=op_config["config"],
                        toolOutput={"status": "completed"},
                    )
                )

            # 5. æ·»åŠ æ—¥å¿—æ­¥éª¤ï¼ˆå¦‚æœæœ‰æ—¥å¿—ï¼‰
            if execution_result.get("logs"):
                for log in execution_result["logs"][-5:]:  # æœ€å5æ¡æ—¥å¿—
                    agent_steps.append(
                        AgentStep(
                            step=len(agent_steps) + 1,
                            type="reasoning",
                            content=f"[{log['level']}] {log['message']}",
                            timestamp=log["timestamp"],
                        )
                    )

            response = PlaygroundExecuteResponse(
                output=execution_result["output"],
                status=execution_result["status"],
                agentSteps=agent_steps if agent_steps else None,
            )

            # è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°è¿”å›çš„æ•°æ®
            print("âœ… API Response prepared:")
            print(f"   - Status: {response.status}")
            print(f"   - Output length: {len(response.output) if response.output else 0}")
            print(f"   - Output preview: {response.output[:200] if response.output else 'EMPTY'}")
            print(f"   - Agent steps: {len(response.agentSteps) if response.agentSteps else 0}")

            return response

        except ImportError as e:
            return PlaygroundExecuteResponse(
                output=f"""âŒ SAGE æ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}

è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–:
  pip install -e packages/sage-kernel
  pip install -e packages/sage-common
  pip install -e packages/sage-middleware

æˆ–ä½¿ç”¨ Python è„šæœ¬æµ‹è¯•:
  python /home/gyy/SAGE/run_rag_test.py
""",
                status="error",
                agentSteps=None,
            )

    except HTTPException:
        raise
    except Exception as e:
        import traceback

        print(f"âŒ Error executing playground: {e}")
        print(traceback.format_exc())

        # è¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        return PlaygroundExecuteResponse(
            output=f"æ‰§è¡Œå‡ºé”™: {str(e)}", status="error", agentSteps=None
        )


# ==================== MVP å¢å¼ºåŠŸèƒ½ ====================


# 1. èŠ‚ç‚¹è¾“å‡ºé¢„è§ˆ
@app.get("/api/node/{flow_id}/{node_id}/output")
async def get_node_output(flow_id: str, node_id: str):
    """è·å–èŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®"""
    try:
        # ä»ç¼“å­˜æˆ–çŠ¶æ€å­˜å‚¨ä¸­è·å–èŠ‚ç‚¹è¾“å‡º
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä» SAGE è¿è¡Œæ—¶è·å–
        sage_dir = _get_sage_dir()
        states_dir = sage_dir / "states" / flow_id

        if not states_dir.exists():
            raise HTTPException(404, "Flow å°šæœªæ‰§è¡Œæˆ–è¾“å‡ºä¸å¯ç”¨")

        # æŸ¥æ‰¾èŠ‚ç‚¹è¾“å‡ºæ–‡ä»¶
        output_file = states_dir / f"{node_id}_output.json"
        if not output_file.exists():
            raise HTTPException(404, "èŠ‚ç‚¹è¾“å‡ºä¸å¯ç”¨")

        import json

        with open(output_file, encoding="utf-8") as f:
            output_data = json.load(f)

        return output_data
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error getting node output: {e}")
        raise HTTPException(500, f"è·å–èŠ‚ç‚¹è¾“å‡ºå¤±è´¥: {str(e)}")


# 2. Flow å¯¼å…¥/å¯¼å‡º
@app.get("/api/flows/{flow_id}/export")
async def export_flow(flow_id: str):
    """å¯¼å‡º Flow ä¸º JSON æ–‡ä»¶"""
    try:
        flow_data = _load_flow_data(flow_id)
        if not flow_data:
            raise HTTPException(404, f"Flow not found: {flow_id}")

        import json

        from fastapi.responses import Response

        # æ·»åŠ å¯¼å‡ºå…ƒæ•°æ®
        export_data = {
            "version": "1.0.0",
            "exportTime": str(datetime.now()),
            "flowId": flow_id,
            "flow": flow_data,
        }

        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        return Response(
            content=json_str,
            media_type="application/json",
            headers={"Content-Disposition": f'attachment; filename="{flow_id}.sage-flow.json"'},
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"å¯¼å‡ºå¤±è´¥: {str(e)}")


@app.post("/api/flows/import")
async def import_flow(file: UploadFile = File(...)):
    """å¯¼å…¥ Flow JSON æ–‡ä»¶"""
    try:
        import json
        from datetime import datetime

        # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶
        content = await file.read()
        import_data = json.loads(content)

        # éªŒè¯æ ¼å¼
        if "flow" not in import_data:
            raise HTTPException(400, "æ— æ•ˆçš„ Flow æ–‡ä»¶æ ¼å¼")

        flow_data = import_data["flow"]

        # ç”Ÿæˆæ–°çš„ flow_id
        timestamp = int(datetime.now().timestamp() * 1000)
        new_flow_id = f"pipeline_{timestamp}"

        # ä¿å­˜åˆ°æœ¬åœ°
        sage_dir = _get_sage_dir()
        pipelines_dir = sage_dir / "pipelines"
        pipelines_dir.mkdir(parents=True, exist_ok=True)

        flow_file = pipelines_dir / f"{new_flow_id}.json"
        with open(flow_file, "w", encoding="utf-8") as f:
            json.dump(flow_data, f, indent=2, ensure_ascii=False)

        return {
            "flowId": new_flow_id,
            "name": flow_data.get("name", "Imported Flow"),
            "message": "Flow å¯¼å…¥æˆåŠŸ",
        }
    except json.JSONDecodeError:
        raise HTTPException(400, "æ— æ•ˆçš„ JSON æ–‡ä»¶")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"å¯¼å…¥å¤±è´¥: {str(e)}")


# 3. ç¯å¢ƒå˜é‡ç®¡ç†
@app.get("/api/env")
async def get_env_vars():
    """è·å–ç¯å¢ƒå˜é‡"""
    try:
        sage_dir = _get_sage_dir()
        env_file = sage_dir / ".env.json"

        if not env_file.exists():
            return {}

        import json

        with open(env_file, encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading env vars: {e}")
        return {}


@app.put("/api/env")
async def update_env_vars(vars: dict):
    """æ›´æ–°ç¯å¢ƒå˜é‡"""
    try:
        import json

        sage_dir = _get_sage_dir()
        env_file = sage_dir / ".env.json"

        # åŠ å¯†æ•æ„Ÿä¿¡æ¯ï¼ˆç®€åŒ–å®ç°ï¼Œå®é™…åº”ä½¿ç”¨åŠ å¯†åº“ï¼‰
        with open(env_file, "w", encoding="utf-8") as f:
            json.dump(vars, f, indent=2, ensure_ascii=False)

        return {"message": "ç¯å¢ƒå˜é‡å·²æ›´æ–°"}
    except Exception as e:
        raise HTTPException(500, f"æ›´æ–°å¤±è´¥: {str(e)}")


@app.get("/api/logs/{flow_id}")
async def get_logs(flow_id: str, last_id: int = 0):
    """è·å–æµç¨‹æ‰§è¡Œæ—¥å¿—ï¼ˆå¢é‡è·å–ï¼‰

    Args:
        flow_id: æµç¨‹ID
        last_id: ä¸Šæ¬¡è·å–çš„æœ€åä¸€æ¡æ—¥å¿—IDï¼Œç”¨äºå¢é‡è·å–

    Returns:
        æ—¥å¿—æ¡ç›®åˆ—è¡¨
    """
    try:
        sage_dir = _get_sage_dir()
        log_file = sage_dir / "logs" / f"{flow_id}.log"

        if not log_file.exists():
            return {"logs": [], "last_id": 0}

        # è¯»å–æ—¥å¿—æ–‡ä»¶
        logs = []
        with open(log_file, encoding="utf-8") as f:
            for idx, line in enumerate(f, start=1):
                if idx > last_id:  # åªè¿”å›æ–°æ—¥å¿—
                    # ç®€å•çš„æ—¥å¿—è§£æï¼ˆæ ¼å¼: [timestamp] [level] [node_id] messageï¼‰
                    try:
                        parts = line.strip().split("] ", 3)
                        if len(parts) >= 3:
                            timestamp = parts[0].replace("[", "")
                            level = parts[1].replace("[", "")
                            node_id = parts[2].replace("[", "") if len(parts) == 4 else None
                            message = parts[-1]

                            logs.append(
                                {
                                    "id": idx,
                                    "timestamp": timestamp,
                                    "level": level,
                                    "message": message,
                                    "nodeId": node_id,
                                }
                            )
                    except Exception:
                        # è§£æå¤±è´¥ï¼Œè·³è¿‡è¿™è¡Œ
                        continue

        return {"logs": logs, "last_id": last_id + len(logs)}
    except Exception as e:
        raise HTTPException(500, f"è·å–æ—¥å¿—å¤±è´¥: {str(e)}")


# ==================== Chat Mode API (æ–°å¢) ====================


class ChatRequest(BaseModel):
    """Chat æ¨¡å¼è¯·æ±‚"""

    message: str
    session_id: str | None = None
    model: str = "sage-default"
    stream: bool = False


class ChatResponse(BaseModel):
    """Chat æ¨¡å¼å“åº”"""

    content: str
    session_id: str
    timestamp: str


class ChatSessionSummary(BaseModel):
    """Chat ä¼šè¯æ‘˜è¦"""

    id: str
    title: str
    created_at: str
    last_active: str
    message_count: int


class ChatSessionDetail(ChatSessionSummary):
    messages: list[dict]
    metadata: dict | None = None


class ChatSessionCreateRequest(BaseModel):
    title: str | None = None


class ChatSessionTitleUpdate(BaseModel):
    title: str


@app.post("/api/chat/message", response_model=ChatResponse)
async def send_chat_message(request: ChatRequest):
    """
    å‘é€èŠå¤©æ¶ˆæ¯ï¼ˆè°ƒç”¨ sage-gatewayï¼‰

    æ³¨æ„ï¼šéœ€è¦ sage-gateway æœåŠ¡è¿è¡Œåœ¨ localhost:8000
    """
    from datetime import datetime

    import httpx

    try:
        # è°ƒç”¨ sage-gateway çš„ OpenAI å…¼å®¹æ¥å£
        async with httpx.AsyncClient(timeout=30.0) as client:
            gateway_response = await client.post(
                "http://localhost:8000/v1/chat/completions",
                json={
                    "model": request.model,
                    "messages": [{"role": "user", "content": request.message}],
                    "stream": False,
                    "session_id": request.session_id,
                },
            )

            if gateway_response.status_code != 200:
                raise HTTPException(
                    status_code=gateway_response.status_code,
                    detail=f"Gateway error: {gateway_response.text}",
                )

            data = gateway_response.json()

            # æå–å“åº”å†…å®¹
            assistant_message = data["choices"][0]["message"]["content"]
            session_id = data.get("id", request.session_id or "default")

            return ChatResponse(
                content=assistant_message,
                session_id=session_id,
                timestamp=datetime.now().isoformat(),
            )

    except httpx.ConnectError:
        raise HTTPException(
            status_code=503,
            detail="æ— æ³•è¿æ¥åˆ° SAGE Gateway (localhost:8000)ã€‚è¯·ç¡®ä¿ gateway æœåŠ¡å·²å¯åŠ¨ã€‚",
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat è¯·æ±‚å¤±è´¥: {str(e)}")


@app.get("/api/chat/sessions", response_model=list[ChatSessionSummary])
async def list_chat_sessions():
    """è·å–æ‰€æœ‰èŠå¤©ä¼šè¯"""
    import httpx

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get("http://localhost:8000/sessions")
            data = response.json()
            return data.get("sessions", [])
    except httpx.ConnectError:
        raise HTTPException(
            status_code=503,
            detail="æ— æ³•è¿æ¥åˆ° SAGE Gateway",
        )


@app.post("/api/chat/sessions", response_model=ChatSessionDetail)
async def create_chat_session(payload: ChatSessionCreateRequest):
    """åˆ›å»ºæ–°çš„èŠå¤©ä¼šè¯"""
    import httpx

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(
                "http://localhost:8000/sessions", json=payload.model_dump()
            )
            if response.status_code >= 400:
                raise HTTPException(status_code=response.status_code, detail=response.text)
            return response.json()
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="æ— æ³•è¿æ¥åˆ° SAGE Gateway")


@app.get("/api/chat/sessions/{session_id}", response_model=ChatSessionDetail)
async def get_chat_session(session_id: str):
    """è·å–å•ä¸ªä¼šè¯è¯¦æƒ…"""
    import httpx

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"http://localhost:8000/sessions/{session_id}")
            if response.status_code == 404:
                raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
            response.raise_for_status()
            return response.json()
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="æ— æ³•è¿æ¥åˆ° SAGE Gateway")


@app.post("/api/chat/sessions/{session_id}/clear")
async def clear_chat_session(session_id: str):
    """æ¸…ç©ºä¼šè¯å†å²"""
    import httpx

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(f"http://localhost:8000/sessions/{session_id}/clear")
            if response.status_code == 404:
                raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
            response.raise_for_status()
            return response.json()
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="æ— æ³•è¿æ¥åˆ° SAGE Gateway")


@app.patch("/api/chat/sessions/{session_id}/title", response_model=ChatSessionSummary)
async def update_chat_session_title(session_id: str, payload: ChatSessionTitleUpdate):
    """æ›´æ–°ä¼šè¯æ ‡é¢˜"""
    import httpx

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.patch(
                f"http://localhost:8000/sessions/{session_id}/title",
                json=payload.model_dump(),
            )
            if response.status_code == 404:
                raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
            response.raise_for_status()
            # æ›´æ–°åé‡æ–°è·å–ä¸€æ¬¡ä¼šè¯æ‘˜è¦ï¼Œé¿å…ç¼ºå­—æ®µ
            detail_resp = await client.get(f"http://localhost:8000/sessions/{session_id}")
            detail_resp.raise_for_status()
            detail = detail_resp.json()
            return ChatSessionSummary(
                id=detail["id"],
                title=detail.get("metadata", {}).get("title", payload.title),
                created_at=detail.get("created_at"),
                last_active=detail.get("last_active"),
                message_count=len(detail.get("messages", [])),
            )
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="æ— æ³•è¿æ¥åˆ° SAGE Gateway")


@app.delete("/api/chat/sessions/{session_id}")
async def delete_chat_session(session_id: str):
    """åˆ é™¤èŠå¤©ä¼šè¯"""
    import httpx

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.delete(f"http://localhost:8000/sessions/{session_id}")
            return response.json()
    except httpx.ConnectError:
        raise HTTPException(
            status_code=503,
            detail="æ— æ³•è¿æ¥åˆ° SAGE Gateway",
        )


@app.post("/api/chat/sessions/{session_id}/convert")
async def convert_chat_session(session_id: str):
    """æ ¹æ®èŠå¤©è®°å½•ç”Ÿæˆ Pipeline å»ºè®®"""
    import httpx

    try:
        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.get(f"http://localhost:8000/sessions/{session_id}")
            if response.status_code == 404:
                raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨ï¼Œæ— æ³•è½¬æ¢")
            response.raise_for_status()
            session = response.json()

        recommendation = generate_pipeline_recommendation(session)
        return recommendation
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="æ— æ³•è¿æ¥åˆ° SAGE Gateway")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)  # ä¿®æ”¹ä¸ºç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£
