"""
SAGE Studio Backend API

A simple FastAPI backend service that provides real SAGE data to the Studio frontend.
"""

import importlib
import inspect
import json
import os
import sys
from pathlib import Path
from typing import List, Optional

import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel


def _convert_pipeline_to_job(pipeline_data: dict, pipeline_id: str, file_path: Path = None) -> dict:
    """å°†æ‹“æ‰‘å›¾æ•°æ®è½¬æ¢ä¸º Job æ ¼å¼"""
    from datetime import datetime

    # ä»æ‹“æ‰‘å›¾æ•°æ®ä¸­æå–ä¿¡æ¯
    name = pipeline_data.get("name", f"æ‹“æ‰‘å›¾ {pipeline_id}")
    description = pipeline_data.get("description", "")
    nodes = pipeline_data.get("nodes", [])
    edges = pipeline_data.get("edges", [])

    # åˆ›å»ºæ“ä½œç¬¦åˆ—è¡¨
    operators = []
    for i, node in enumerate(nodes):
        # æ„å»ºä¸‹æ¸¸è¿æ¥
        downstream = []
        for edge in edges:
            if edge.get("source") == node.get("id"):
                # æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹çš„ç´¢å¼•
                target_node = next(
                    (n for n in nodes if n.get("id") == edge.get("target")), None
                )
                if target_node:
                    target_index = next(
                        (
                            j
                            for j, n in enumerate(nodes)
                            if n.get("id") == edge.get("target")
                        ),
                        None,
                    )
                    if target_index is not None:
                        downstream.append(target_index)

        operator = {
            "id": i,
            "name": node.get("name", f"Operator_{i}"),
            "numOfInstances": 1,
            "downstream": downstream,
        }
        operators.append(operator)

    # ä»æ–‡ä»¶åæˆ–æ–‡ä»¶å…ƒæ•°æ®ä¸­æå–åˆ›å»ºæ—¶é—´
    create_time = None
    
    # æ–¹æ³•1: ä»æ–‡ä»¶åè§£ææ—¶é—´æˆ³ (pipeline_1759908680.json)
    if pipeline_id.startswith("pipeline_"):
        try:
            timestamp_str = pipeline_id.replace("pipeline_", "")
            timestamp = int(timestamp_str)
            create_time = datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
        except (ValueError, OSError) as e:
            print(f"Failed to parse timestamp from pipeline_id {pipeline_id}: {e}")
    
    # æ–¹æ³•2: å¦‚æœè§£æå¤±è´¥,ä½¿ç”¨æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
    if create_time is None and file_path and file_path.exists():
        try:
            mtime = file_path.stat().st_mtime
            create_time = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M:%S")
        except Exception as e:
            print(f"Failed to get file mtime for {file_path}: {e}")
    
    # æ–¹æ³•3: å…œåº•ä½¿ç”¨å½“å‰æ—¶é—´
    if create_time is None:
        create_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    job = {
        "jobId": pipeline_id,
        "name": name,
        "description": description,  # æ·»åŠ æè¿°å­—æ®µ
        "isRunning": False,  # æ‹“æ‰‘å›¾é»˜è®¤ä¸åœ¨è¿è¡Œ
        "nthreads": "1",
        "cpu": "0%",
        "ram": "0GB",
        "startTime": create_time,
        "duration": "00:00:00",
        "nevents": 0,
        "minProcessTime": 0,
        "maxProcessTime": 0,
        "meanProcessTime": 0,
        "latency": 0,
        "throughput": 0,
        "ncore": 1,
        "periodicalThroughput": [0],
        "periodicalLatency": [0],
        "totalTimeBreakdown": {
            "totalTime": 0,
            "serializeTime": 0,
            "persistTime": 0,
            "streamProcessTime": 0,
            "overheadTime": 0,
        },
        "schedulerTimeBreakdown": {
            "overheadTime": 0,
            "streamTime": 0,
            "totalTime": 0,
            "txnTime": 0,
        },
        "operators": operators,
        # æ·»åŠ  config å­—æ®µï¼Œä¿ç•™åŸå§‹çš„ React Flow æ ¼å¼æ•°æ®
        "config": {
            "name": name,
            "description": description,
            "nodes": nodes,
            "edges": edges,
        },
    }

    return job


def _get_sage_dir() -> Path:
    """è·å– SAGE ç›®å½•è·¯å¾„"""
    # é¦–å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡
    env_dir = os.environ.get("SAGE_OUTPUT_DIR")
    if env_dir:
        sage_dir = Path(env_dir)
    else:
        # æ£€æŸ¥æ˜¯å¦åœ¨å¼€å‘ç¯å¢ƒä¸­
        current_dir = Path.cwd()
        if (current_dir / "packages" / "sage-common").exists():
            sage_dir = current_dir / ".sage"
        else:
            sage_dir = Path.home() / ".sage"

    sage_dir.mkdir(parents=True, exist_ok=True)
    return sage_dir


# Pydantic æ¨¡å‹å®šä¹‰
class Job(BaseModel):
    jobId: str
    name: str
    description: Optional[str] = ""  # æ·»åŠ æè¿°å­—æ®µ
    isRunning: bool
    nthreads: str
    cpu: str
    ram: str
    startTime: str
    duration: str
    nevents: int
    minProcessTime: int
    maxProcessTime: int
    meanProcessTime: int
    latency: int
    throughput: int
    ncore: int
    periodicalThroughput: List[int]
    periodicalLatency: List[int]
    totalTimeBreakdown: dict
    schedulerTimeBreakdown: dict
    operators: List[dict]
    config: Optional[dict] = None  # æ·»åŠ  config å­—æ®µï¼Œç”¨äºå­˜å‚¨ React Flow æ ¼å¼çš„èŠ‚ç‚¹å’Œè¾¹æ•°æ®


class OperatorInfo(BaseModel):
    id: int
    name: str
    description: str
    code: str
    isCustom: bool


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="SAGE Studio Backend",
    description="Backend API service for SAGE Studio frontend",
    version="1.0.0",
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:4200"],  # Studio é»˜è®¤ç«¯å£
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def _read_sage_data_from_files():
    """ä» .sage ç›®å½•çš„æ–‡ä»¶ä¸­è¯»å–å®é™…çš„ SAGE æ•°æ®"""
    sage_dir = _get_sage_dir()
    data = {"jobs": [], "operators": [], "pipelines": []}

    try:
        # è¯»å–ä½œä¸šä¿¡æ¯
        states_dir = sage_dir / "states"
        if states_dir.exists():
            for job_file in states_dir.glob("*.json"):
                try:
                    with open(job_file, "r", encoding="utf-8") as f:
                        job_data = json.load(f)
                        data["jobs"].append(job_data)
                except Exception as e:
                    print(f"Error reading job file {job_file}: {e}")

        # è¯»å–ä¿å­˜çš„æ‹“æ‰‘å›¾å¹¶è½¬æ¢ä¸º Job æ ¼å¼
        pipelines_dir = sage_dir / "pipelines"
        if pipelines_dir.exists():
            for pipeline_file in pipelines_dir.glob("pipeline_*.json"):
                try:
                    with open(pipeline_file, "r", encoding="utf-8") as f:
                        pipeline_data = json.load(f)
                        # å°†æ‹“æ‰‘å›¾è½¬æ¢ä¸º Job æ ¼å¼ï¼Œä¼ é€’æ–‡ä»¶è·¯å¾„ä»¥æå–çœŸå®åˆ›å»ºæ—¶é—´
                        job_from_pipeline = _convert_pipeline_to_job(
                            pipeline_data, pipeline_file.stem, pipeline_file
                        )
                        data["jobs"].append(job_from_pipeline)
                except Exception as e:
                    print(f"Error reading pipeline file {pipeline_file}: {e}")

        # è¯»å–æ“ä½œç¬¦ä¿¡æ¯
        operators_file = sage_dir / "output" / "operators.json"
        if operators_file.exists():
            try:
                with open(operators_file, "r", encoding="utf-8") as f:
                    operators_data = json.load(f)
                    data["operators"] = operators_data
            except Exception as e:
                print(f"Error reading operators file: {e}")

        # è¯»å–ç®¡é“ä¿¡æ¯
        pipelines_file = sage_dir / "output" / "pipelines.json"
        if pipelines_file.exists():
            try:
                with open(pipelines_file, "r") as f:
                    pipelines_data = json.load(f)
                    data["pipelines"] = pipelines_data
            except Exception as e:
                print(f"Error reading pipelines file: {e}")

    except Exception as e:
        print(f"Error reading SAGE data: {e}")

    return data


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "SAGE Studio Backend API", "status": "running"}


@app.get("/api/jobs/all", response_model=List[Job])
async def get_all_jobs():
    """è·å–æ‰€æœ‰ä½œä¸šä¿¡æ¯"""
    try:
        sage_data = _read_sage_data_from_files()
        jobs = sage_data.get("jobs", [])

        print(f"DEBUG: Read {len(jobs)} jobs from files")
        print(f"DEBUG: sage_data = {sage_data}")

        # å¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼Œè¿”å›ä¸€äº›ç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºå¼€å‘ï¼‰
        if not jobs:
            print("DEBUG: No real jobs found, using fallback data")
            jobs = [
                {
                    "jobId": "job_001",
                    "name": "RAGé—®ç­”ç®¡é“ç¤ºä¾‹",
                    "isRunning": False,
                    "nthreads": "4",
                    "cpu": "0%",
                    "ram": "0GB",
                    "startTime": "2025-08-18 10:30:00",
                    "duration": "00:45:12",
                    "nevents": 1000,
                    "minProcessTime": 10,
                    "maxProcessTime": 500,
                    "meanProcessTime": 150,
                    "latency": 200,
                    "throughput": 800,
                    "ncore": 4,
                    "periodicalThroughput": [750, 800, 820, 785, 810],
                    "periodicalLatency": [180, 200, 190, 210, 195],
                    "totalTimeBreakdown": {
                        "totalTime": 2712000,
                        "serializeTime": 50000,
                        "persistTime": 100000,
                        "streamProcessTime": 2500000,
                        "overheadTime": 62000,
                    },
                    "schedulerTimeBreakdown": {
                        "overheadTime": 50000,
                        "streamTime": 2600000,
                        "totalTime": 2712000,
                        "txnTime": 62000,
                    },
                    "operators": [
                        {
                            "id": 1,
                            "name": "FileSource",
                            "numOfInstances": 1,
                            "throughput": 800,
                            "latency": 50,
                            "explorationStrategy": "greedy",
                            "schedulingGranularity": "batch",
                            "abortHandling": "rollback",
                            "numOfTD": 10,
                            "numOfLD": 5,
                            "numOfPD": 2,
                            "lastBatch": 999,
                            "downstream": [2],
                        }
                    ],
                }
            ]

        return jobs
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šä¿¡æ¯å¤±è´¥: {str(e)}")


def _get_studio_operators_dir() -> Path:
    """è·å– Studio operators æ•°æ®ç›®å½•è·¯å¾„"""
    current_file = Path(__file__)
    # ä» api.py æ–‡ä»¶è·¯å¾„æ‰¾åˆ° studio æ ¹ç›®å½•:
    # ../../../ ä» backend/ åˆ° studio/
    studio_root = current_file.parent.parent.parent
    operators_dir = studio_root / "data" / "operators"
    return operators_dir


def _load_operator_class_source(module_path: str, class_name: str) -> str:
    """åŠ¨æ€åŠ è½½operatorç±»å¹¶è·å–å…¶æºä»£ç """
    try:
        # æ·»åŠ SAGEé¡¹ç›®è·¯å¾„åˆ°sys.path
        sage_root = Path(__file__).parent.parent.parent.parent.parent.parent
        if str(sage_root) not in sys.path:
            sys.path.insert(0, str(sage_root))

        # åŠ¨æ€å¯¼å…¥æ¨¡å—
        module = importlib.import_module(module_path)

        # è·å–ç±»
        operator_class = getattr(module, class_name)

        # è·å–æºä»£ç 
        source_code = inspect.getsource(operator_class)

        return source_code

    except Exception as e:
        print(f"Error loading operator class {module_path}.{class_name}: {e}")
        return f"# Error loading source code for {class_name}\n# {str(e)}"


def _read_real_operators():
    """ä» studio data ç›®å½•è¯»å–çœŸå®çš„æ“ä½œç¬¦æ•°æ®å¹¶åŠ¨æ€åŠ è½½æºä»£ç """
    operators = []
    operators_dir = _get_studio_operators_dir()

    if not operators_dir.exists():
        print(f"Operators directory not found: {operators_dir}")
        return []

    try:
        # è¯»å–æ‰€æœ‰ JSON æ–‡ä»¶
        for json_file in operators_dir.glob("*.json"):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    operator_data = json.load(f)

                    # æ£€æŸ¥æ˜¯å¦æœ‰module_pathå’Œclass_nameå­—æ®µ
                    if "module_path" in operator_data and "class_name" in operator_data:
                        # åŠ¨æ€åŠ è½½æºä»£ç 
                        source_code = _load_operator_class_source(
                            operator_data["module_path"], operator_data["class_name"]
                        )
                        operator_data["code"] = source_code
                    else:
                        # å¦‚æœæ²¡æœ‰æ¨¡å—è·¯å¾„ä¿¡æ¯ï¼Œä½¿ç”¨ç©ºä»£ç 
                        operator_data["code"] = ""

                    # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
                    required_fields = ["id", "name", "description", "isCustom"]
                    if all(key in operator_data for key in required_fields):
                        # æ¸…ç†ä¸éœ€è¦çš„å­—æ®µ
                        clean_data = {
                            "id": operator_data["id"],
                            "name": operator_data["name"],
                            "description": operator_data["description"],
                            "code": operator_data.get("code", ""),
                            "isCustom": operator_data["isCustom"],
                        }
                        operators.append(clean_data)
                        print(f"Loaded operator: {operator_data['name']}")
                    else:
                        print(f"Invalid operator data in {json_file}")

            except Exception as e:
                print(f"Error reading operator file {json_file}: {e}")

    except Exception as e:
        print(f"Error reading operators directory: {e}")

    return operators


@app.get("/api/operators", response_model=List[OperatorInfo])
async def get_operators():
    """è·å–æ‰€æœ‰æ“ä½œç¬¦ä¿¡æ¯"""
    try:
        # é¦–å…ˆå°è¯•è¯»å–çœŸå®çš„æ“ä½œç¬¦æ•°æ®
        operators = _read_real_operators()

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®æ•°æ®ï¼Œä½¿ç”¨åå¤‡æ•°æ®
        if not operators:
            print("No real operator data found, using fallback data")
            operators = [
                {
                    "id": 1,
                    "name": "FileSource",
                    "description": "ä»æ–‡ä»¶è¯»å–æ•°æ®çš„æºæ“ä½œç¬¦",
                    "code": "class FileSource:\n    def __init__(self, file_path):\n        self.file_path = file_path\n    \n    def read_data(self):\n        with open(self.file_path, 'r') as f:\n            return f.read()",
                    "isCustom": True,
                },
                {
                    "id": 2,
                    "name": "SimpleRetriever",
                    "description": "ç®€å•çš„æ£€ç´¢æ“ä½œç¬¦",
                    "code": "class SimpleRetriever:\n    def __init__(self, top_k=5):\n        self.top_k = top_k\n    \n    def retrieve(self, query):\n        return query[:self.top_k]",
                    "isCustom": True,
                },
            ]

        print(f"Returning {len(operators)} operators")
        return operators
    except Exception as e:
        print(f"Error in get_operators: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ“ä½œç¬¦ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.get("/api/operators/list")
async def get_operators_list(page: int = 1, size: int = 10, search: str = ""):
    """è·å–æ“ä½œç¬¦åˆ—è¡¨ - æ”¯æŒåˆ†é¡µå’Œæœç´¢"""
    try:
        # è·å–æ‰€æœ‰æ“ä½œç¬¦
        all_operators = _read_real_operators()

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®æ•°æ®ï¼Œä½¿ç”¨åå¤‡æ•°æ®
        if not all_operators:
            print("No real operator data found, using fallback data")
            all_operators = [
                {
                    "id": 1,
                    "name": "FileSource",
                    "description": "ä»æ–‡ä»¶è¯»å–æ•°æ®çš„æºæ“ä½œç¬¦",
                    "code": "class FileSource:\n    def __init__(self, file_path):\n        self.file_path = file_path\n    \n    def read_data(self):\n        with open(self.file_path, 'r') as f:\n            return f.read()",
                    "isCustom": True,
                },
                {
                    "id": 2,
                    "name": "SimpleRetriever",
                    "description": "ç®€å•çš„æ£€ç´¢æ“ä½œç¬¦",
                    "code": "class SimpleRetriever:\n    def __init__(self, top_k=5):\n        self.top_k = top_k\n    \n    def retrieve(self, query):\n        return query[:self.top_k]",
                    "isCustom": True,
                },
            ]

        # æœç´¢è¿‡æ»¤
        if search:
            filtered_operators = [
                op
                for op in all_operators
                if search.lower() in op["name"].lower()
                or search.lower() in op["description"].lower()
            ]
        else:
            filtered_operators = all_operators

        # åˆ†é¡µè®¡ç®—
        total = len(filtered_operators)
        start = (page - 1) * size
        end = start + size
        items = filtered_operators[start:end]

        result = {"items": items, "total": total}

        print(f"Returning page {page} with {len(items)} operators (total: {total})")
        return result

    except Exception as e:
        print(f"Error in get_operators_list: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ“ä½œç¬¦åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/api/pipelines")
async def get_pipelines():
    """è·å–æ‰€æœ‰ç®¡é“ä¿¡æ¯"""
    try:
        sage_data = _read_sage_data_from_files()
        pipelines = sage_data.get("pipelines", [])

        # å¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼Œè¿”å›ä¸€äº›ç¤ºä¾‹æ•°æ®
        if not pipelines:
            pipelines = [
                {
                    "id": "pipeline_001",
                    "name": "ç¤ºä¾‹RAGç®¡é“",
                    "description": "æ¼”ç¤ºRAGé—®ç­”ç³»ç»Ÿçš„æ•°æ®å¤„ç†ç®¡é“",
                    "status": "running",
                    "operators": [
                        {
                            "id": "source1",
                            "type": "FileSource",
                            "config": {"file_path": "/data/documents.txt"},
                        },
                        {
                            "id": "retriever1",
                            "type": "SimpleRetriever",
                            "config": {"top_k": 5},
                        },
                        {
                            "id": "sink1",
                            "type": "TerminalSink",
                            "config": {"format": "json"},
                        },
                    ],
                    "connections": [
                        {"from": "source1", "to": "retriever1"},
                        {"from": "retriever1", "to": "sink1"},
                    ],
                }
            ]

        return {"pipelines": pipelines}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç®¡é“ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.post("/api/pipeline/submit")
async def submit_pipeline(topology_data: dict):
    """æäº¤æ‹“æ‰‘å›¾/ç®¡é“é…ç½®"""
    try:
        print(f"Received pipeline submission: {topology_data}")

        # è¿™é‡Œå¯ä»¥æ·»åŠ ä¿å­˜åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“çš„é€»è¾‘
        sage_dir = _get_sage_dir()
        pipelines_dir = sage_dir / "pipelines"
        pipelines_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        import time

        timestamp = int(time.time())
        pipeline_file = pipelines_dir / f"pipeline_{timestamp}.json"

        # ä¿å­˜æ‹“æ‰‘æ•°æ®åˆ°æ–‡ä»¶
        with open(pipeline_file, "w", encoding="utf-8") as f:
            json.dump(topology_data, f, indent=2, ensure_ascii=False)

        return {
            "status": "success",
            "message": "æ‹“æ‰‘å›¾æäº¤æˆåŠŸ",
            "pipeline_id": f"pipeline_{timestamp}",
            "file_path": str(pipeline_file),
        }
    except Exception as e:
        print(f"Error submitting pipeline: {e}")
        raise HTTPException(status_code=500, detail=f"æäº¤æ‹“æ‰‘å›¾å¤±è´¥: {str(e)}")


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "service": "SAGE Studio Backend"}


# ==================== ç®¡é“è¯¦æƒ…ç›¸å…³ç«¯ç‚¹ ====================
# ç”¨äºæ”¯æŒå‰ç«¯ View Details åŠŸèƒ½çš„å ä½ç¬¦ç«¯ç‚¹

# å…¨å±€çŠ¶æ€å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ•°æ®åº“ï¼‰
job_runtime_status = {}  # {job_id: {status, use_ray, isRunning, ...}}
job_logs = {}  # {job_id: [log_lines]}
job_configs_cache = {}  # {pipeline_id: yaml_config}
user_queries = {}  # {job_id: [(query, answer), ...]}


@app.get("/jobInfo/get/{job_id}")
async def get_job_detail(job_id: str):
    """è·å–ä½œä¸šè¯¦ç»†ä¿¡æ¯ - åŒ…å«æ“ä½œç¬¦æ‹“æ‰‘ç»“æ„"""
    try:
        # é¦–å…ˆå°è¯•ä»å·²ä¿å­˜çš„æ•°æ®ä¸­æŸ¥æ‰¾
        sage_data = _read_sage_data_from_files()
        jobs = sage_data.get("jobs", [])
        
        # æŸ¥æ‰¾åŒ¹é…çš„ä½œä¸š
        job = next((j for j in jobs if j.get("jobId") == job_id), None)
        
        if job:
            return job
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®é™…æ•°æ®ï¼Œè¿”å›å ä½ç¬¦æ•°æ®ï¼ˆç”¨äºå¼€å‘ï¼‰
        print(f"Job {job_id} not found in saved data, returning placeholder")
        return {
            "jobId": job_id,
            "name": f"ç®¡é“ {job_id}",
            "isRunning": False,
            "nthreads": "4",
            "cpu": "0%",
            "ram": "0GB",
            "startTime": "2025-10-10 15:00:00",
            "duration": "00:00:00",
            "nevents": 0,
            "minProcessTime": 0,
            "maxProcessTime": 0,
            "meanProcessTime": 0,
            "latency": 0,
            "throughput": 0,
            "ncore": 4,
            "periodicalThroughput": [0],
            "periodicalLatency": [0],
            "totalTimeBreakdown": {
                "totalTime": 0,
                "serializeTime": 0,
                "persistTime": 0,
                "streamProcessTime": 0,
                "overheadTime": 0,
            },
            "schedulerTimeBreakdown": {
                "overheadTime": 0,
                "streamTime": 0,
                "totalTime": 0,
                "txnTime": 0,
            },
            "operators": [
                {
                    "id": 1,
                    "name": "FileSource",
                    "numOfInstances": 1,
                    "throughput": 0,
                    "latency": 0,
                    "explorationStrategy": "greedy",
                    "schedulingGranularity": "batch",
                    "abortHandling": "rollback",
                    "numOfTD": 0,
                    "numOfLD": 0,
                    "numOfPD": 0,
                    "lastBatch": 0,
                    "downstream": [2],
                },
                {
                    "id": 2,
                    "name": "TerminalSink",
                    "numOfInstances": 1,
                    "throughput": 0,
                    "latency": 0,
                    "explorationStrategy": "greedy",
                    "schedulingGranularity": "batch",
                    "abortHandling": "rollback",
                    "numOfTD": 0,
                    "numOfLD": 0,
                    "numOfPD": 0,
                    "lastBatch": 0,
                    "downstream": [],
                },
            ],
        }
    except Exception as e:
        print(f"Error getting job detail: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šè¯¦æƒ…å¤±è´¥: {str(e)}")


@app.get("/api/signal/status/{job_id}")
async def get_job_status(job_id: str):
    """è·å–ä½œä¸šè¿è¡ŒçŠ¶æ€"""
    try:
        # ä»å†…å­˜ä¸­è·å–çŠ¶æ€
        status = job_runtime_status.get(
            job_id,
            {
                "job_id": job_id,
                "status": "idle",
                "use_ray": False,
                "isRunning": False,
            },
        )
        return status
    except Exception as e:
        print(f"Error getting job status: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šçŠ¶æ€å¤±è´¥: {str(e)}")


@app.post("/api/signal/start/{job_id}")
async def start_job(job_id: str):
    """å¯åŠ¨ä½œä¸š"""
    try:
        # æ›´æ–°è¿è¡ŒçŠ¶æ€
        job_runtime_status[job_id] = {
            "job_id": job_id,
            "status": "running",
            "use_ray": False,
            "isRunning": True,
        }
        
        # åˆå§‹åŒ–æ—¥å¿—
        if job_id not in job_logs:
            job_logs[job_id] = []
        
        job_logs[job_id].append(f"[SYSTEM] Job {job_id} started at 2025-10-10 15:30:00")
        
        return {"status": "success", "message": f"ä½œä¸š {job_id} å·²å¯åŠ¨"}
    except Exception as e:
        print(f"Error starting job: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨ä½œä¸šå¤±è´¥: {str(e)}")


@app.post("/api/signal/stop/{job_id}/{duration}")
async def stop_job(job_id: str, duration: str):
    """åœæ­¢ä½œä¸š"""
    try:
        # æ›´æ–°è¿è¡ŒçŠ¶æ€
        job_runtime_status[job_id] = {
            "job_id": job_id,
            "status": "stopped",
            "use_ray": False,
            "isRunning": False,
        }
        
        # æ·»åŠ åœæ­¢æ—¥å¿—
        if job_id in job_logs:
            job_logs[job_id].append(
                f"[SYSTEM] Job {job_id} stopped after {duration}"
            )
        
        return {"status": "success", "message": f"ä½œä¸š {job_id} å·²åœæ­¢"}
    except Exception as e:
        print(f"Error stopping job: {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢ä½œä¸šå¤±è´¥: {str(e)}")


@app.get("/api/signal/sink/{job_id}")
async def get_job_logs(job_id: str, offset: int = 0):
    """è·å–ä½œä¸šæ—¥å¿—ï¼ˆå¢é‡ï¼‰"""
    try:
        # è·å–è¯¥ä½œä¸šçš„æ—¥å¿—
        logs = job_logs.get(job_id, [])
        
        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆoffset=0ï¼‰ä¸”æ²¡æœ‰æ—¥å¿—ï¼Œè¿”å›ç§å­æ¶ˆæ¯
        if offset == 0 and len(logs) == 0:
            seed_line = f"[SYSTEM] Console ready for {job_id}. Click Start or submit a FileSource query."
            job_logs[job_id] = [seed_line]
            return {"offset": 1, "lines": [seed_line]}
        
        # è¿”å›ä» offset å¼€å§‹çš„æ–°æ—¥å¿—
        new_logs = logs[offset:]
        new_offset = len(logs)
        
        return {"offset": new_offset, "lines": new_logs}
    except Exception as e:
        print(f"Error getting job logs: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šæ—¥å¿—å¤±è´¥: {str(e)}")


@app.get("/batchInfo/get/all/{job_id}/{operator_id}")
async def get_all_batches(job_id: str, operator_id: str):
    """è·å–æ“ä½œç¬¦çš„æ‰€æœ‰æ‰¹æ¬¡ä¿¡æ¯"""
    try:
        # operator_id å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "s1", "r1"ï¼‰æˆ–æ•°å­—
        # è¿”å›ç©ºæ•°ç»„ä½œä¸ºå ä½ç¬¦
        # å®é™…å®ç°éœ€è¦ä» SAGE è¿è¡Œæ—¶è·å–æ‰¹æ¬¡ç»Ÿè®¡æ•°æ®
        print(f"Getting batches for job={job_id}, operator={operator_id}")
        return []
    except Exception as e:
        print(f"Error getting batches: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ‰¹æ¬¡ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.get("/batchInfo/get/{job_id}/{batch_id}/{operator_id}")
async def get_batch_detail(job_id: str, batch_id: int, operator_id: str):
    """è·å–å•ä¸ªæ‰¹æ¬¡çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        # operator_id å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "s1", "r1"ï¼‰æˆ–æ•°å­—
        # è¿”å›å ä½ç¬¦æ‰¹æ¬¡æ•°æ®
        return {
            "batchId": batch_id,
            "operatorId": operator_id,
            "processTime": 0,
            "tupleCount": 0,
            "timestamp": "2025-10-10 15:30:00",
        }
    except Exception as e:
        print(f"Error getting batch detail: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ‰¹æ¬¡è¯¦æƒ…å¤±è´¥: {str(e)}")


@app.get("/jobInfo/config/{pipeline_id}")
async def get_pipeline_config(pipeline_id: str):
    """è·å–ç®¡é“é…ç½®ï¼ˆYAMLæ ¼å¼ï¼‰"""
    try:
        # å°è¯•ä»ç¼“å­˜è·å–
        if pipeline_id in job_configs_cache:
            return {"config": job_configs_cache[pipeline_id]}
        
        # è¿”å›é»˜è®¤é…ç½®æ¨¡æ¿
        default_config = """# SAGE Pipeline Configuration
name: Example RAG Pipeline
version: 1.0.0

operators:
  - name: FileSource
    type: source
    config:
      file_path: /data/documents.txt
  
  - name: SimpleRetriever
    type: retriever
    config:
      top_k: 5
  
  - name: TerminalSink
    type: sink
    config:
      output_path: /tmp/output.txt
"""
        return {"config": default_config}
    except Exception as e:
        print(f"Error getting pipeline config: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç®¡é“é…ç½®å¤±è´¥: {str(e)}")


@app.put("/jobInfo/config/update/{pipeline_id}")
async def update_pipeline_config(pipeline_id: str, config: dict):
    """æ›´æ–°ç®¡é“é…ç½®"""
    try:
        # ä¿å­˜é…ç½®åˆ°ç¼“å­˜
        config_yaml = config.get("config", "")
        job_configs_cache[pipeline_id] = config_yaml
        
        # å¯é€‰ï¼šä¿å­˜åˆ°æ–‡ä»¶
        sage_dir = _get_sage_dir()
        config_dir = sage_dir / "configs"
        config_dir.mkdir(exist_ok=True)
        
        config_file = config_dir / f"{pipeline_id}.yaml"
        with open(config_file, "w", encoding="utf-8") as f:
            f.write(config_yaml)
        
        return {
            "status": "success",
            "message": "é…ç½®æ›´æ–°æˆåŠŸ",
            "file_path": str(config_file),
        }
    except Exception as e:
        print(f"Error updating pipeline config: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ç®¡é“é…ç½®å¤±è´¥: {str(e)}")


# ==================== Playground API ====================

def _load_flow_data(flow_id: str) -> Optional[dict]:
    """åŠ è½½ Flow æ•°æ®"""
    sage_dir = _get_sage_dir()
    pipelines_dir = sage_dir / "pipelines"
    
    print(f"ğŸ” Looking for flow: {flow_id}")
    print(f"ğŸ“ Sage dir: {sage_dir}")
    print(f"ğŸ“ Pipelines dir: {pipelines_dir}")
    print(f"ğŸ“ Pipelines dir exists: {pipelines_dir.exists()}")
    
    # å°è¯•åŠ è½½ pipeline æ–‡ä»¶
    flow_file = pipelines_dir / f"{flow_id}.json"
    print(f"ğŸ“„ Flow file path: {flow_file}")
    print(f"ğŸ“„ Flow file exists: {flow_file.exists()}")
    
    if flow_file.exists():
        with open(flow_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            print(f"âœ… Loaded flow: {data.get('name', 'Unnamed')}")
            return data
    
    print(f"âŒ Flow file not found")
    return None


def _convert_to_flow_definition(flow_data: dict, flow_id: str):
    """å°†å‰ç«¯ Flow æ•°æ®è½¬æ¢ä¸º FlowDefinition"""
    import sys
    from pathlib import Path
    
    # æ·»åŠ  sage-studio åˆ° Python è·¯å¾„
    studio_path = Path(__file__).parent.parent.parent.parent
    if str(studio_path) not in sys.path:
        sys.path.insert(0, str(studio_path))
    
    from sage.studio.models import VisualPipeline, VisualNode, VisualConnection
    
    name = flow_data.get("name", "Unnamed Flow")
    description = flow_data.get("description", "")
    nodes_data = flow_data.get("nodes", [])
    edges_data = flow_data.get("edges", [])
    
    # è½¬æ¢èŠ‚ç‚¹
    nodes = []
    for node_data in nodes_data:
        node = VisualNode(
            id=node_data.get("id", ""),
            type=node_data.get("data", {}).get("nodeId", "unknown"),
            label=node_data.get("data", {}).get("label", "Unnamed Node"),
            position=node_data.get("position", {"x": 0, "y": 0}),
            config=node_data.get("data", {}).get("properties", {})
        )
        nodes.append(node)
    
    # è½¬æ¢è¿æ¥
    connections = []
    for edge_data in edges_data:
        connection = VisualConnection(
            id=edge_data.get("id", f"{edge_data.get('source')}-{edge_data.get('target')}"),
            source_node_id=edge_data.get("source", ""),
            source_port="output",  # é»˜è®¤è¾“å‡ºç«¯å£
            target_node_id=edge_data.get("target", ""),
            target_port="input"  # é»˜è®¤è¾“å…¥ç«¯å£
        )
        connections.append(connection)
    
    return VisualPipeline(
        id=flow_id,
        name=name,
        description=description,
        nodes=nodes,
        connections=connections
    )


class PlaygroundExecuteRequest(BaseModel):
    """Playground æ‰§è¡Œè¯·æ±‚"""
    flowId: str
    input: str
    sessionId: str = "default"
    stream: bool = False


class AgentStep(BaseModel):
    """Agent æ‰§è¡Œæ­¥éª¤"""
    step: int
    type: str  # reasoning, tool_call, response
    content: str
    timestamp: str
    duration: Optional[int] = None
    toolName: Optional[str] = None
    toolInput: Optional[dict] = None
    toolOutput: Optional[dict] = None


class PlaygroundExecuteResponse(BaseModel):
    """Playground æ‰§è¡Œå“åº”"""
    output: str
    status: str
    agentSteps: Optional[List[AgentStep]] = None


@app.post("/api/playground/execute", response_model=PlaygroundExecuteResponse)
async def execute_playground(request: PlaygroundExecuteRequest):
    """æ‰§è¡Œ Playground Flow - ä½¿ç”¨ SAGE å¼•æ“"""
    try:
        from datetime import datetime
        import sys
        from pathlib import Path
        import time
        
        # æ·»åŠ  sage-studio åˆ° Python è·¯å¾„
        studio_path = Path(__file__).parent.parent.parent.parent
        if str(studio_path) not in sys.path:
            sys.path.insert(0, str(studio_path))
        
        from sage.studio.models import PipelineStatus, NodeStatus
        from sage.studio.services import get_pipeline_builder
        
        print(f"ğŸ¯ Executing playground - flowId: {request.flowId}, sessionId: {request.sessionId}")
        print(f"ğŸ“ Input: {request.input}")
        
        # 1. åŠ è½½ Flow å®šä¹‰
        flow_data = _load_flow_data(request.flowId)
        if not flow_data:
            raise HTTPException(status_code=404, detail=f"Flow not found: {request.flowId}")
        
        # 2. è½¬æ¢ä¸º VisualPipeline
        visual_pipeline = _convert_to_flow_definition(flow_data, request.flowId)
        
        # 3. ä½¿ç”¨ PipelineBuilder æ„å»º SAGE Pipeline
        builder = get_pipeline_builder()
        sage_env = builder.build(visual_pipeline)
        
        # 4. æ‰§è¡Œ Pipeline
        start_time = time.time()
        job = sage_env.execute()
        
        # ç­‰å¾…æ‰§è¡Œå®Œæˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥å¼‚æ­¥å¤„ç†ï¼‰
        # TODO: å®ç°çœŸæ­£çš„å¼‚æ­¥æ‰§è¡Œå’ŒçŠ¶æ€è½®è¯¢
        import asyncio
        await asyncio.sleep(0.1)  # è®©å‡ºæ§åˆ¶æƒ
        
        execution_time = time.time() - start_time
        
        # 5. ç”Ÿæˆæ‰§è¡Œæ­¥éª¤ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        agent_steps = []
        for idx, node in enumerate(visual_pipeline.nodes, start=1):
            agent_steps.append(AgentStep(
                step=idx,
                type="tool_call",
                content=f"æ‰§è¡ŒèŠ‚ç‚¹: {node.label}",
                timestamp=datetime.now().isoformat(),
                duration=int(execution_time * 1000 / len(visual_pipeline.nodes)),
                toolName=node.label,
                toolInput=node.config,
                toolOutput={"status": "completed"}
            ))
        
        # 6. ç”Ÿæˆè¾“å‡º
        output_text = f"Pipeline æ‰§è¡ŒæˆåŠŸï¼æ€»è€—æ—¶: {execution_time:.2f}ç§’"
        
        print(f"âœ… Playground execution completed: {PipelineStatus.COMPLETED.value}")
        
        return PlaygroundExecuteResponse(
            output=output_text,
            status=PipelineStatus.COMPLETED.value,
            agentSteps=agent_steps if agent_steps else None
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"âŒ Error executing playground: {e}")
        print(traceback.format_exc())
        
        # è¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        return PlaygroundExecuteResponse(
            output=f"æ‰§è¡Œå‡ºé”™: {str(e)}",
            status="failed",
            agentSteps=None
        )


if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8080)
