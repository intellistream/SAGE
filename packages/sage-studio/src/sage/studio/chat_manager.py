"""Chat Mode Manager - Studio Manager with integrated LLM support"""

from __future__ import annotations

import os
import signal
import subprocess
import sys
import time
from pathlib import Path

import psutil
import requests
from rich.console import Console
from rich.table import Table

from .studio_manager import StudioManager

console = Console()


class ChatModeManager(StudioManager):
    """Studio Manager with integrated local LLM support.

    Extends StudioManager to add sageLLM integration for local LLM services.
    This is now the default manager - no need for backward compatibility.
    """

    def __init__(self):
        super().__init__()

        # Local LLM service management (via sageLLM)
        self.llm_service = None  # Will be VLLMService or other sageLLM service
        # Default to enabling LLM with a small model
        self.llm_enabled = os.getenv("SAGE_STUDIO_LLM", "true").lower() in ("true", "1", "yes")
        # Use Qwen2.5-0.5B as default - very small and fast
        self.llm_model = os.getenv("SAGE_STUDIO_LLM_MODEL", "Qwen/Qwen2.5-0.5B-Instruct")
        self.llm_port = 8001  # OpenAI-compatible API port

    # ------------------------------------------------------------------
    # Fine-tuned Model Discovery
    # ------------------------------------------------------------------
    def list_finetuned_models(self) -> list[dict]:
        """List available fine-tuned models from Studio's finetune manager.

        Returns:
            List of fine-tuned model info dictionaries
        """
        try:
            from sage.studio.services.finetune_manager import finetune_manager

            models = []
            for task in finetune_manager.tasks.values():
                if task.status.value == "completed":
                    # Check for merged model (preferred) or LoRA checkpoint
                    output_path = Path(task.output_dir)
                    merged_path = output_path / "merged_model"
                    lora_path = output_path / "lora"

                    model_path = None
                    model_type = None

                    if merged_path.exists():
                        model_path = str(merged_path)
                        model_type = "merged"
                    elif lora_path.exists():
                        model_path = str(lora_path)
                        model_type = "lora"

                    if model_path:
                        models.append(
                            {
                                "path": model_path,
                                "name": task.task_id,
                                "base_model": task.model_name,
                                "type": model_type,
                                "completed_at": task.completed_at,
                            }
                        )

            return models
        except ImportError:
            console.print("[yellow]âš ï¸  FinetuneManager not available[/yellow]")
            return []

    def get_finetuned_model_path(self, model_name: str) -> str | None:
        """Get path of a fine-tuned model by name.

        Args:
            model_name: Task ID or model name

        Returns:
            Path to the fine-tuned model, or None if not found
        """
        models = self.list_finetuned_models()
        for model in models:
            if model["name"] == model_name or model_name in model["path"]:
                return model["path"]
        return None

    # ------------------------------------------------------------------
    # Local LLM Service helpers (via sageLLM)
    # ------------------------------------------------------------------
    def _start_llm_service(self, model: str | None = None, use_finetuned: bool = False) -> bool:
        """Start local LLM service via sageLLM.

        Uses sageLLM's API server to start a local LLM HTTP server.
        The server provides OpenAI-compatible API at http://localhost:{port}/v1

        Args:
            model: Model name/path to load (can be HF model or local path)
            use_finetuned: If True, try to use a fine-tuned model

        Returns:
            True if started successfully, False otherwise
        """
        try:
            from sage.common.components.sage_llm import LLMAPIServer, LLMServerConfig
        except ImportError:
            console.print(
                "[yellow]âš ï¸  sageLLM API Server ä¸å¯ç”¨ï¼Œè·³è¿‡æœ¬åœ° LLM å¯åŠ¨[/yellow]\n"
                "æç¤ºï¼šç¡®ä¿å·²å®‰è£… sage-common åŒ…"
            )
            return False

        # Determine which model to use
        model_name = model or self.llm_model

        # If use_finetuned is requested, try to use a fine-tuned model
        if use_finetuned and not model:
            finetuned_models = self.list_finetuned_models()
            if finetuned_models:
                # Use the most recent fine-tuned model
                latest_model = sorted(
                    finetuned_models, key=lambda m: m["completed_at"] or "", reverse=True
                )[0]
                model_name = latest_model["path"]
                console.print(f"[cyan]ğŸ“ ä½¿ç”¨å¾®è°ƒæ¨¡å‹: {latest_model['name']}[/cyan]")
                console.print(f"   åŸºç¡€æ¨¡å‹: {latest_model['base_model']}")
                console.print(f"   ç±»å‹: {latest_model['type']}")
            else:
                console.print("[yellow]âš ï¸  æœªæ‰¾åˆ°å¯ç”¨çš„å¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹[/yellow]")

        # Check if this is a local path (fine-tuned model)
        is_local_path = Path(model_name).exists()

        console.print("[blue]ğŸš€ å¯åŠ¨æœ¬åœ° LLM æœåŠ¡ï¼ˆé€šè¿‡ sageLLMï¼‰...[/blue]")
        console.print(f"   æ¨¡å‹: {model_name}")
        console.print(f"   ç«¯å£: {self.llm_port}")

        # Resolve model path - use cache if available
        resolved_model_path = model_name
        if not is_local_path:
            try:
                from sage.common.model_registry import vllm_registry

                try:
                    cached_path = vllm_registry.get_model_path(model_name)
                    console.print(f"   [green]âœ“[/green] ä½¿ç”¨æœ¬åœ°ç¼“å­˜: {cached_path}")
                    resolved_model_path = str(cached_path)  # Convert Path to string for vLLM
                except Exception:
                    console.print("   [yellow]âš ï¸  æ¨¡å‹æœªç¼“å­˜ï¼Œå°†ä» HuggingFace ä¸‹è½½...[/yellow]")
                    console.print(
                        f"   ä¸‹è½½ä½ç½®: ~/.sage/models/vllm/{model_name.replace('/', '__')}/"
                    )
                    # Keep original model_name, vLLM will download it
            except ImportError:
                pass  # Registry not available, will download during setup
        else:
            console.print("   [green]âœ“[/green] ä½¿ç”¨æœ¬åœ°å¾®è°ƒæ¨¡å‹")

        try:
            # Create sageLLM API server configuration
            # Use resolved_model_path which points to local cache if available
            config = LLMServerConfig(
                model=resolved_model_path,  # Use cached path to avoid re-download
                backend="vllm",  # Default to vLLM, can be made configurable
                host="0.0.0.0",
                port=self.llm_port,
                gpu_memory_utilization=float(os.getenv("SAGE_STUDIO_LLM_GPU_MEMORY", "0.9")),
                max_model_len=4096,
                tensor_parallel_size=int(os.getenv("SAGE_STUDIO_LLM_TENSOR_PARALLEL", "1")),
                disable_log_stats=True,
            )

            # Create and start API server
            self.llm_service = LLMAPIServer(config)
            success = self.llm_service.start(background=True)

            if success:
                console.print("[green]âœ… æœ¬åœ° LLM æœåŠ¡å·²å¯åŠ¨[/green]")

                # Set environment variables for IntelligentLLMClient
                os.environ["SAGE_CHAT_BASE_URL"] = f"http://127.0.0.1:{self.llm_port}/v1"
                # Set model name to match what vLLM is actually serving
                # vLLM registers the model with the path we pass (resolved_model_path)
                os.environ["SAGE_CHAT_MODEL"] = resolved_model_path

                return True
            else:
                console.print("[red]âŒ LLM æœåŠ¡å¯åŠ¨å¤±è´¥[/red]")
                return False

        except Exception as exc:
            console.print(f"[red]âŒ å¯åŠ¨ LLM æœåŠ¡å¤±è´¥: {exc}[/red]")
            console.print("[yellow]ğŸ’¡ æç¤ºï¼šå®‰è£…æ¨ç†å¼•æ“åå¯ä½¿ç”¨æœ¬åœ°æœåŠ¡[/yellow]")
            console.print("   ç¤ºä¾‹ï¼špip install vllm  # å®‰è£… vLLM å¼•æ“")
            return False

    def _stop_llm_service(self) -> bool:
        """Stop local LLM service."""
        # First, try to stop via self.llm_service if it exists
        if self.llm_service is not None:
            console.print("[blue]ğŸ›‘ åœæ­¢æœ¬åœ° LLM æœåŠ¡...[/blue]")
            try:
                self.llm_service.stop()
                self.llm_service = None
                console.print("[green]âœ… æœ¬åœ° LLM æœåŠ¡å·²åœæ­¢[/green]")
                return True
            except Exception as exc:
                console.print(f"[red]âŒ åœæ­¢ LLM æœåŠ¡å¤±è´¥: {exc}[/red]")
                return False

        # If llm_service is None, check if there's an orphaned vLLM process
        # This handles restart scenarios where the old process wasn't tracked
        import subprocess

        try:
            # Check for vLLM processes on port 8001
            result = subprocess.run(
                ["lsof", "-ti", ":8001"],
                capture_output=True,
                text=True,
                timeout=5,
            )
            if result.returncode == 0 and result.stdout.strip():
                pid = int(result.stdout.strip().split()[0])
                console.print(f"[yellow]å‘ç°é—ç•™çš„ LLM è¿›ç¨‹ (PID: {pid})ï¼Œæ­£åœ¨æ¸…ç†...[/yellow]")
                subprocess.run(["kill", str(pid)], timeout=5)
                import time

                # Wait for port to actually be freed (up to 10 seconds)
                for i in range(10):
                    time.sleep(1)
                    check_result = subprocess.run(
                        ["lsof", "-ti", ":8001"],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if check_result.returncode != 0 or not check_result.stdout.strip():
                        # Port is free
                        console.print("[green]âœ… é—ç•™ LLM è¿›ç¨‹å·²æ¸…ç†[/green]")
                        return True

                # Timeout - force kill
                console.print("[yellow]âš ï¸  è¿›ç¨‹æœªå“åº” SIGTERMï¼Œä½¿ç”¨ SIGKILL å¼ºåˆ¶ç»ˆæ­¢...[/yellow]")
                subprocess.run(["kill", "-9", str(pid)], timeout=5)
                time.sleep(2)
                console.print("[green]âœ… é—ç•™ LLM è¿›ç¨‹å·²æ¸…ç†[/green]")
                return True
        except Exception as exc:
            console.print(f"[yellow]æ£€æŸ¥é—ç•™è¿›ç¨‹å¤±è´¥: {exc}[/yellow]")

        console.print("[yellow]æœ¬åœ° LLM æœåŠ¡æœªè¿è¡Œ[/yellow]")
        return True

    # ------------------------------------------------------------------
    # Gateway helpers
    # ------------------------------------------------------------------
    def _is_gateway_running(self) -> int | None:
        if not self.gateway_pid_file.exists():
            return None

        try:
            pid = int(self.gateway_pid_file.read_text().strip())
        except Exception:
            return None

        if psutil.pid_exists(pid):
            return pid

        # æ¸…ç†è„ PID æ–‡ä»¶
        try:
            self.gateway_pid_file.unlink()
        except OSError:
            pass
        return None

    def _start_gateway(self, port: int | None = None) -> bool:
        if self._is_gateway_running():
            console.print("[green]âœ… sage-gateway å·²è¿è¡Œ[/green]")
            return True

        # Skip slow import check - just try to start directly
        # If gateway is not installed, subprocess will fail anyway
        gateway_port = port or self.gateway_port
        env = os.environ.copy()
        env.setdefault("SAGE_GATEWAY_PORT", str(gateway_port))

        console.print(f"[blue]ğŸš€ å¯åŠ¨ sage-gateway (ç«¯å£: {gateway_port})...[/blue]")
        try:
            log_handle = open(self.gateway_log_file, "w")
            process = subprocess.Popen(
                [sys.executable, "-m", "sage.gateway.server"],
                stdout=log_handle,
                stderr=subprocess.STDOUT,
                preexec_fn=os.setsid if os.name != "nt" else None,
                env=env,
            )
            self.gateway_pid_file.write_text(str(process.pid))
        except Exception as exc:
            console.print(f"[red]âŒ å¯åŠ¨ gateway å¤±è´¥: {exc}")
            console.print(
                "[yellow]æç¤º: è¯·ç¡®ä¿å·²å®‰è£… sage-gateway: "
                "pip install -e packages/sage-gateway[/yellow]"
            )
            return False

        # ç­‰å¾…æœåŠ¡å°±ç»ª
        url = f"http://localhost:{gateway_port}/health"
        for _ in range(20):
            try:
                response = requests.get(url, timeout=1)
                if response.status_code == 200:
                    console.print("[green]âœ… gateway å·²å°±ç»ª[/green]")
                    return True
            except requests.RequestException:
                time.sleep(0.5)
        console.print("[yellow]âš ï¸ gateway ä»åœ¨å¯åŠ¨ï¼Œè¯·ç¨åæ£€æŸ¥[/yellow]")
        return True

    def _stop_gateway(self) -> bool:
        pid = self._is_gateway_running()
        if not pid:
            console.print("[yellow]gateway æœªè¿è¡Œ[/yellow]")
            return True

        console.print("[blue]ğŸ›‘ åœæ­¢ sage-gateway...[/blue]")
        try:
            if os.name == "nt":
                subprocess.run(["taskkill", "/F", "/PID", str(pid)], check=True)
            else:
                os.killpg(os.getpgid(pid), signal.SIGTERM)
                time.sleep(1)
                if psutil.pid_exists(pid):
                    os.killpg(os.getpgid(pid), signal.SIGKILL)

            self.gateway_pid_file.unlink(missing_ok=True)
            console.print("[green]âœ… gateway å·²åœæ­¢[/green]")
            return True
        except Exception as exc:
            console.print(f"[red]âŒ åœæ­¢ gateway å¤±è´¥: {exc}")
            return False

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    def start(
        self,
        frontend_port: int | None = None,
        backend_port: int | None = None,
        gateway_port: int | None = None,
        host: str = "localhost",
        dev: bool = True,
        llm: bool | None = None,
        llm_model: str | None = None,
        use_finetuned: bool = False,
    ) -> bool:
        """Start Studio Chat Mode services.

        Args:
            frontend_port: Studio frontend port
            backend_port: Studio backend port
            gateway_port: Gateway API port (default: 8000)
            host: Host to bind to
            dev: Run in development mode
            llm: Enable local LLM service via sageLLM (default: from SAGE_STUDIO_LLM env)
            llm_model: Model to load (default: from SAGE_STUDIO_LLM_MODEL env)
            use_finetuned: Use latest fine-tuned model (overrides llm_model if True)

        Returns:
            True if all services started successfully
        """
        if gateway_port:
            self.gateway_port = gateway_port

        # Determine if local LLM should be started
        start_llm = llm if llm is not None else self.llm_enabled

        # Start local LLM service first (if enabled)
        if start_llm:
            model = llm_model or self.llm_model if not use_finetuned else None
            llm_started = self._start_llm_service(model=model, use_finetuned=use_finetuned)
            if llm_started:
                console.print(
                    "[green]ğŸ’¡ Gateway å°†è‡ªåŠ¨ä½¿ç”¨æœ¬åœ° LLM æœåŠ¡ï¼ˆé€šè¿‡ IntelligentLLMClient è‡ªåŠ¨æ£€æµ‹ï¼‰[/green]"
                )
            else:
                console.print(
                    "[yellow]âš ï¸  æœ¬åœ° LLM æœªå¯åŠ¨ï¼ŒGateway å°†ä½¿ç”¨äº‘ç«¯ APIï¼ˆå¦‚å·²é…ç½®ï¼‰[/yellow]"
                )

        # Start Gateway
        if not self._start_gateway(port=self.gateway_port):
            return False

        # Start Studio UI (use parent class method)
        console.print("[blue]âš™ï¸ å¯åŠ¨ Studio æœåŠ¡...[/blue]")
        success = super().start(
            port=frontend_port,
            host=host,
            dev=dev,
            backend_port=backend_port,
            auto_gateway=False,  # We manage gateway ourselves
        )

        if success:
            console.print("\n" + "=" * 70)
            console.print("[green]ğŸ‰ Chat æ¨¡å¼å°±ç»ªï¼[/green]")
            if start_llm and self.llm_service:
                console.print("[green]ğŸ¤– æœ¬åœ° LLM: ç”± sageLLM ç®¡ç†[/green]")
            console.print(f"[green]ğŸŒ Gateway API: http://localhost:{self.gateway_port}[/green]")
            console.print("[green]ğŸ’¬ æ‰“å¼€é¡¶éƒ¨ Chat æ ‡ç­¾å³å¯ä½“éªŒ[/green]")
            console.print("=" * 70)

        return success

    def stop(self) -> bool:
        """Stop all Studio Chat Mode services."""
        frontend_backend = super().stop(stop_gateway=False)  # Don't stop gateway via parent
        gateway = self._stop_gateway()
        llm = self._stop_llm_service()
        return frontend_backend and gateway and llm

    def status(self):
        """Display status of all Studio Chat Mode services."""
        super().status()  # Show Studio status first

        # Local LLM Service status (via sageLLM)
        llm_table = Table(title="æœ¬åœ° LLM æœåŠ¡çŠ¶æ€ï¼ˆsageLLMï¼‰")
        llm_table.add_column("å±æ€§", style="cyan", width=14)
        llm_table.add_column("å€¼", style="white")

        if self.llm_service:
            llm_table.add_row("çŠ¶æ€", "[green]è¿è¡Œä¸­[/green]")
            llm_table.add_row("å¼•æ“", "sageLLM (å¯é…ç½®ä¸åŒ vendor)")
            llm_table.add_row("æ¨¡å‹", self.llm_model)
            llm_table.add_row("è¯´æ˜", "ç”± IntelligentLLMClient è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨")
        else:
            llm_table.add_row("çŠ¶æ€", "[red]æœªè¿è¡Œ[/red]")
            llm_table.add_row("æç¤º", "ä½¿ç”¨ --llm å¯åŠ¨æœ¬åœ°æœåŠ¡")
            llm_table.add_row("è¯´æ˜", "æ”¯æŒé€šè¿‡ sageLLM é…ç½®ä¸åŒæ¨ç†å¼•æ“")

        console.print(llm_table)

        # Gateway status
        table = Table(title="sage-gateway çŠ¶æ€")
        table.add_column("å±æ€§", style="cyan", width=14)
        table.add_column("å€¼", style="white")

        pid = self._is_gateway_running()
        if pid:
            table.add_row("çŠ¶æ€", "[green]è¿è¡Œä¸­[/green]")
            table.add_row("PID", str(pid))
            url = f"http://localhost:{self.gateway_port}/health"
            try:
                response = requests.get(url, timeout=1)
                status = response.json().get("status", "unknown")
                table.add_row("å¥åº·æ£€æŸ¥", status)
            except requests.RequestException:
                table.add_row("å¥åº·æ£€æŸ¥", "[red]ä¸å¯è¾¾[/red]")
            table.add_row("ç«¯å£", str(self.gateway_port))
            table.add_row("æ—¥å¿—", str(self.gateway_log_file))
        else:
            table.add_row("çŠ¶æ€", "[red]æœªè¿è¡Œ[/red]")
            table.add_row("ç«¯å£", str(self.gateway_port))
            table.add_row("æ—¥å¿—", str(self.gateway_log_file))

        console.print(table)

    def logs(self, follow: bool = False, gateway: bool = False, backend: bool = False):
        """Display logs from Studio services.

        Args:
            follow: Follow log output (like tail -f)
            gateway: Show Gateway logs
            backend: Show Studio backend logs
        """
        if gateway:
            log_file = self.gateway_log_file
            name = "gateway"
        elif backend:
            log_file = self.backend_log_file
            name = "Studio Backend"
        else:
            log_file = self.log_file
            name = "Studio Frontend"

        if not log_file.exists():
            console.print(f"[yellow]{name} æ—¥å¿—ä¸å­˜åœ¨: {log_file}[/yellow]")
            return

        if follow:
            console.print(f"[blue]è·Ÿè¸ª {name} æ—¥å¿— (Ctrl+C é€€å‡º)...[/blue]")
            try:
                subprocess.run(["tail", "-f", str(log_file)])
            except KeyboardInterrupt:
                console.print("\n[blue]åœæ­¢æ—¥å¿—è·Ÿè¸ª[/blue]")
        else:
            console.print(f"[blue]æ˜¾ç¤º {name} æ—¥å¿—: {log_file}[/blue]")
            try:
                with open(log_file) as handle:
                    for line in handle.readlines()[-50:]:
                        print(line.rstrip())
            except OSError as exc:
                console.print(f"[red]è¯»å–æ—¥å¿—å¤±è´¥: {exc}")
