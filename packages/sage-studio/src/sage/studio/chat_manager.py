"""Chat Mode Manager - Studio Manager with integrated LLM support"""

from __future__ import annotations

import os
import signal
import subprocess
import sys
import time
from pathlib import Path

import psutil
import requests
from rich.console import Console
from rich.table import Table

from sage.common.config.ports import SagePorts

from .studio_manager import StudioManager
from .utils.gpu_check import is_gpu_available

console = Console()


class ChatModeManager(StudioManager):
    """Studio Manager with integrated local LLM support.

    Extends StudioManager to add sageLLM integration for local LLM services.
    This is now the default manager - no need for backward compatibility.
    """

    def __init__(self):
        super().__init__()

        # Local LLM service management (via sageLLM)
        self.llm_service = None  # Will be VLLMService or other sageLLM service
        # Default to enabling LLM with a small model
        self.llm_enabled = os.getenv("SAGE_STUDIO_LLM", "true").lower() in ("true", "1", "yes")
        # Use Qwen2.5-0.5B as default - very small and fast
        self.llm_model = os.getenv("SAGE_STUDIO_LLM_MODEL", "Qwen/Qwen2.5-0.5B-Instruct")
        self.llm_port = SagePorts.BENCHMARK_LLM  # Unified default port (8901)

    # ------------------------------------------------------------------
    # Fine-tuned Model Discovery
    # ------------------------------------------------------------------
    def list_finetuned_models(self) -> list[dict]:
        """List available fine-tuned models from Studio's finetune manager.

        Returns:
            List of fine-tuned model info dictionaries
        """
        try:
            from sage.studio.services.finetune_manager import finetune_manager

            models = []
            for task in finetune_manager.tasks.values():
                if task.status.value == "completed":
                    # Check for merged model (preferred) or LoRA checkpoint
                    output_path = Path(task.output_dir)
                    merged_path = output_path / "merged_model"
                    lora_path = output_path / "lora"

                    model_path = None
                    model_type = None

                    if merged_path.exists():
                        model_path = str(merged_path)
                        model_type = "merged"
                    elif lora_path.exists():
                        model_path = str(lora_path)
                        model_type = "lora"

                    if model_path:
                        models.append(
                            {
                                "path": model_path,
                                "name": task.task_id,
                                "base_model": task.model_name,
                                "type": model_type,
                                "completed_at": task.completed_at,
                            }
                        )

            return models
        except ImportError:
            console.print("[yellow]âš ï¸  FinetuneManager not available[/yellow]")
            return []

    def get_finetuned_model_path(self, model_name: str) -> str | None:
        """Get path of a fine-tuned model by name.

        Args:
            model_name: Task ID or model name

        Returns:
            Path to the fine-tuned model, or None if not found
        """
        models = self.list_finetuned_models()
        for model in models:
            if model["name"] == model_name or model_name in model["path"]:
                return model["path"]
        return None

    # ------------------------------------------------------------------
    # Local LLM Service helpers (via sageLLM LLMLauncher)
    # ------------------------------------------------------------------
    def _start_llm_service(self, model: str | None = None, use_finetuned: bool = False) -> bool:
        """Start local LLM service via sageLLM.

        Uses sageLLM's unified LLMLauncher to start a local LLM HTTP server.
        The server provides OpenAI-compatible API at http://localhost:{port}/v1

        Args:
            model: Model name/path to load (can be HF model or local path)
            use_finetuned: If True, try to use a fine-tuned model

        Returns:
            True if started successfully, False otherwise
        """
        try:
            from sage.common.components.sage_llm import LLMLauncher
        except ImportError:
            console.print(
                "[yellow]âš ï¸  sageLLM LLMLauncher ä¸å¯ç”¨ï¼Œè·³è¿‡æœ¬åœ° LLM å¯åŠ¨[/yellow]\n"
                "æç¤ºï¼šç¡®ä¿å·²å®‰è£… sage-common åŒ…"
            )
            return False

        # Determine which model to use
        model_name = model or self.llm_model

        # Get finetuned models list if needed
        finetuned_models = None
        if use_finetuned and not model:
            finetuned_models = self.list_finetuned_models()
            if not finetuned_models:
                console.print("[yellow]âš ï¸  æœªæ‰¾åˆ°å¯ç”¨çš„å¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹[/yellow]")

        # Use unified launcher
        result = LLMLauncher.launch(
            model=model_name,
            port=self.llm_port,
            gpu_memory=float(os.getenv("SAGE_STUDIO_LLM_GPU_MEMORY", "0.7")),
            tensor_parallel=int(os.getenv("SAGE_STUDIO_LLM_TENSOR_PARALLEL", "1")),
            background=True,
            use_finetuned=use_finetuned,
            finetuned_models=finetuned_models,
            verbose=True,
            check_existing=False,  # We handle existing check at Studio level
        )

        if result.success:
            self.llm_service = result.server
            return True
        else:
            console.print("[yellow]ğŸ’¡ æç¤ºï¼šå®‰è£…æ¨ç†å¼•æ“åå¯ä½¿ç”¨æœ¬åœ°æœåŠ¡[/yellow]")
            console.print("   ç¤ºä¾‹ï¼špip install vllm  # å®‰è£… vLLM å¼•æ“")
            return False

    def _stop_llm_service(self) -> bool:
        """Stop local LLM service."""
        try:
            from sage.common.components.sage_llm import LLMLauncher
        except ImportError:
            return True

        # First, try to stop via self.llm_service if it exists
        if self.llm_service is not None:
            console.print("[blue]ğŸ›‘ åœæ­¢æœ¬åœ° LLM æœåŠ¡...[/blue]")
            try:
                self.llm_service.stop()
                self.llm_service = None
                LLMLauncher.clear_service_info()
                console.print("[green]âœ… æœ¬åœ° LLM æœåŠ¡å·²åœæ­¢[/green]")
                return True
            except Exception as exc:
                console.print(f"[red]âŒ åœæ­¢ LLM æœåŠ¡å¤±è´¥: {exc}[/red]")
                return False

        # Use LLMLauncher to stop any running service
        return LLMLauncher.stop(verbose=True)

    # ------------------------------------------------------------------
    # Gateway helpers
    # ------------------------------------------------------------------
    def _is_gateway_running(self) -> int | None:
        if not self.gateway_pid_file.exists():
            return None

        try:
            pid = int(self.gateway_pid_file.read_text().strip())
        except Exception:
            return None

        if psutil.pid_exists(pid):
            return pid

        # æ¸…ç†è„ PID æ–‡ä»¶
        try:
            self.gateway_pid_file.unlink()
        except OSError:
            pass
        return None

    def _start_gateway(self, port: int | None = None) -> bool:
        if self._is_gateway_running():
            console.print("[green]âœ… sage-gateway å·²è¿è¡Œ[/green]")
            return True

        # Skip slow import check - just try to start directly
        # If gateway is not installed, subprocess will fail anyway
        gateway_port = port or self.gateway_port
        env = os.environ.copy()
        env.setdefault("SAGE_GATEWAY_PORT", str(gateway_port))

        console.print(f"[blue]ğŸš€ å¯åŠ¨ sage-gateway (ç«¯å£: {gateway_port})...[/blue]")
        try:
            log_handle = open(self.gateway_log_file, "w")
            process = subprocess.Popen(
                [sys.executable, "-m", "sage.gateway.server"],
                stdout=log_handle,
                stderr=subprocess.STDOUT,
                preexec_fn=os.setsid if os.name != "nt" else None,
                env=env,
            )
            self.gateway_pid_file.write_text(str(process.pid))
        except Exception as exc:
            console.print(f"[red]âŒ å¯åŠ¨ gateway å¤±è´¥: {exc}")
            console.print(
                "[yellow]æç¤º: è¯·ç¡®ä¿å·²å®‰è£… sage-gateway: "
                "pip install -e packages/sage-gateway[/yellow]"
            )
            return False

        # ç­‰å¾…æœåŠ¡å°±ç»ª
        url = f"http://localhost:{gateway_port}/health"
        for _ in range(20):
            try:
                response = requests.get(url, timeout=1)
                if response.status_code == 200:
                    console.print("[green]âœ… gateway å·²å°±ç»ª[/green]")
                    return True
            except requests.RequestException:
                time.sleep(0.5)
        console.print("[yellow]âš ï¸ gateway ä»åœ¨å¯åŠ¨ï¼Œè¯·ç¨åæ£€æŸ¥[/yellow]")
        return True

    def _stop_gateway(self) -> bool:
        pid = self._is_gateway_running()
        if not pid:
            console.print("[yellow]gateway æœªè¿è¡Œ[/yellow]")
            return True

        console.print("[blue]ğŸ›‘ åœæ­¢ sage-gateway...[/blue]")
        try:
            if os.name == "nt":
                subprocess.run(["taskkill", "/F", "/PID", str(pid)], check=True)
            else:
                os.killpg(os.getpgid(pid), signal.SIGTERM)
                time.sleep(1)
                if psutil.pid_exists(pid):
                    os.killpg(os.getpgid(pid), signal.SIGKILL)

            self.gateway_pid_file.unlink(missing_ok=True)
            console.print("[green]âœ… gateway å·²åœæ­¢[/green]")
            return True
        except Exception as exc:
            console.print(f"[red]âŒ åœæ­¢ gateway å¤±è´¥: {exc}")
            return False

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    def start(
        self,
        frontend_port: int | None = None,
        backend_port: int | None = None,
        gateway_port: int | None = None,
        host: str = "localhost",
        dev: bool = True,
        llm: bool | None = None,
        llm_model: str | None = None,
        use_finetuned: bool = False,
    ) -> bool:
        """Start Studio Chat Mode services.

        Args:
            frontend_port: Studio frontend port
            backend_port: Studio backend port
            gateway_port: Gateway API port (default: 8000)
            host: Host to bind to
            dev: Run in development mode
            llm: Enable local LLM service via sageLLM (default: from SAGE_STUDIO_LLM env)
            llm_model: Model to load (default: from SAGE_STUDIO_LLM_MODEL env)
            use_finetuned: Use latest fine-tuned model (overrides llm_model if True)

        Returns:
            True if all services started successfully
        """
        if gateway_port:
            self.gateway_port = gateway_port

        # Determine if local LLM should be started
        start_llm = llm if llm is not None else self.llm_enabled

        # DEBUG
        console.print(
            f"[dim]DEBUG: llm arg={llm}, llm_enabled={self.llm_enabled}, start_llm={start_llm}[/dim]"
        )

        # Force disable LLM if no GPU is detected (vLLM requires GPU)
        if start_llm and not is_gpu_available():
            console.print("[yellow]âš ï¸  æœªæ£€æµ‹åˆ° NVIDIA GPUï¼Œè‡ªåŠ¨ç¦ç”¨æœ¬åœ° LLM æœåŠ¡[/yellow]")
            console.print("[dim]   æç¤ºï¼švLLM éœ€è¦ NVIDIA GPU æ”¯æŒ[/dim]")
            start_llm = False

        # Start local LLM service first (if enabled)
        if start_llm:
            model = llm_model or self.llm_model if not use_finetuned else None
            llm_started = self._start_llm_service(model=model, use_finetuned=use_finetuned)
            if llm_started:
                console.print(
                    "[green]ğŸ’¡ Gateway å°†è‡ªåŠ¨ä½¿ç”¨æœ¬åœ° LLM æœåŠ¡ï¼ˆé€šè¿‡ IntelligentLLMClient è‡ªåŠ¨æ£€æµ‹ï¼‰[/green]"
                )
            else:
                console.print(
                    "[yellow]âš ï¸  æœ¬åœ° LLM æœªå¯åŠ¨ï¼ŒGateway å°†ä½¿ç”¨äº‘ç«¯ APIï¼ˆå¦‚å·²é…ç½®ï¼‰[/yellow]"
                )

        # Start Gateway
        if not self._start_gateway(port=self.gateway_port):
            return False

        # Start Studio UI (use parent class method)
        console.print("[blue]âš™ï¸ å¯åŠ¨ Studio æœåŠ¡...[/blue]")
        success = super().start(
            port=frontend_port,
            host=host,
            dev=dev,
            backend_port=backend_port,
            auto_gateway=False,  # We manage gateway ourselves
        )

        if success:
            console.print("\n" + "=" * 70)
            console.print("[green]ğŸ‰ Chat æ¨¡å¼å°±ç»ªï¼[/green]")
            if start_llm and self.llm_service:
                console.print("[green]ğŸ¤– æœ¬åœ° LLM: ç”± sageLLM ç®¡ç†[/green]")
            console.print(f"[green]ğŸŒ Gateway API: http://localhost:{self.gateway_port}[/green]")
            console.print("[green]ğŸ’¬ æ‰“å¼€é¡¶éƒ¨ Chat æ ‡ç­¾å³å¯ä½“éªŒ[/green]")
            console.print("=" * 70)

        return success

    def stop(self) -> bool:
        """Stop all Studio Chat Mode services."""
        frontend_backend = super().stop(stop_gateway=False)  # Don't stop gateway via parent
        gateway = self._stop_gateway()
        llm = self._stop_llm_service()
        return frontend_backend and gateway and llm

    def status(self):
        """Display status of all Studio Chat Mode services."""
        super().status()  # Show Studio status first

        # Local LLM Service status (via sageLLM)
        llm_table = Table(title="æœ¬åœ° LLM æœåŠ¡çŠ¶æ€ï¼ˆsageLLMï¼‰")
        llm_table.add_column("å±æ€§", style="cyan", width=14)
        llm_table.add_column("å€¼", style="white")

        if self.llm_service:
            llm_table.add_row("çŠ¶æ€", "[green]è¿è¡Œä¸­[/green]")
            llm_table.add_row("å¼•æ“", "sageLLM (å¯é…ç½®ä¸åŒ vendor)")
            llm_table.add_row("æ¨¡å‹", self.llm_model)
            llm_table.add_row("è¯´æ˜", "ç”± IntelligentLLMClient è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨")
        else:
            llm_table.add_row("çŠ¶æ€", "[red]æœªè¿è¡Œ[/red]")
            llm_table.add_row("æç¤º", "ä½¿ç”¨ --llm å¯åŠ¨æœ¬åœ°æœåŠ¡")
            llm_table.add_row("è¯´æ˜", "æ”¯æŒé€šè¿‡ sageLLM é…ç½®ä¸åŒæ¨ç†å¼•æ“")

        console.print(llm_table)

        # Gateway status
        table = Table(title="sage-gateway çŠ¶æ€")
        table.add_column("å±æ€§", style="cyan", width=14)
        table.add_column("å€¼", style="white")

        pid = self._is_gateway_running()
        if pid:
            table.add_row("çŠ¶æ€", "[green]è¿è¡Œä¸­[/green]")
            table.add_row("PID", str(pid))
            url = f"http://localhost:{self.gateway_port}/health"
            try:
                response = requests.get(url, timeout=1)
                status = response.json().get("status", "unknown")
                table.add_row("å¥åº·æ£€æŸ¥", status)
            except requests.RequestException:
                table.add_row("å¥åº·æ£€æŸ¥", "[red]ä¸å¯è¾¾[/red]")
            table.add_row("ç«¯å£", str(self.gateway_port))
            table.add_row("æ—¥å¿—", str(self.gateway_log_file))
        else:
            table.add_row("çŠ¶æ€", "[red]æœªè¿è¡Œ[/red]")
            table.add_row("ç«¯å£", str(self.gateway_port))
            table.add_row("æ—¥å¿—", str(self.gateway_log_file))

        console.print(table)

    def logs(self, follow: bool = False, gateway: bool = False, backend: bool = False):
        """Display logs from Studio services.

        Args:
            follow: Follow log output (like tail -f)
            gateway: Show Gateway logs
            backend: Show Studio backend logs
        """
        if gateway:
            log_file = self.gateway_log_file
            name = "gateway"
        elif backend:
            log_file = self.backend_log_file
            name = "Studio Backend"
        else:
            log_file = self.log_file
            name = "Studio Frontend"

        if not log_file.exists():
            console.print(f"[yellow]{name} æ—¥å¿—ä¸å­˜åœ¨: {log_file}[/yellow]")
            return

        if follow:
            console.print(f"[blue]è·Ÿè¸ª {name} æ—¥å¿— (Ctrl+C é€€å‡º)...[/blue]")
            try:
                subprocess.run(["tail", "-f", str(log_file)])
            except KeyboardInterrupt:
                console.print("\n[blue]åœæ­¢æ—¥å¿—è·Ÿè¸ª[/blue]")
        else:
            console.print(f"[blue]æ˜¾ç¤º {name} æ—¥å¿—: {log_file}[/blue]")
            try:
                with open(log_file) as handle:
                    for line in handle.readlines()[-50:]:
                        print(line.rstrip())
            except OSError as exc:
                console.print(f"[red]è¯»å–æ—¥å¿—å¤±è´¥: {exc}")
