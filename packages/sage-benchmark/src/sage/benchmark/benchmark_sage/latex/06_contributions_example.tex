% Example contributions list for the SAGE systems paper (content, not prompt)
%
% This snippet can be used as a stand-alone "Contributions" section
% or inlined at the end of the Introduction.

\section*{Contributions}

We summarize the main contributions of SAGE as follows:

\begin{enumerate}
  \item \textbf{A layered architecture for declarative LLM/AI pipelines.}

  We introduce SAGE, a framework that organizes LLM/AI data processing pipelines into a strict six-layer architecture, from foundational utilities (\texttt{sage-common}) and platform services (\texttt{sage-platform}), through kernel and middleware components (\texttt{sage-kernel}, \texttt{sage-libs}, \texttt{sage-middleware}), up to applications and user-facing tools (\texttt{sage-apps}, \texttt{sage-benchmark}, \texttt{sage-cli}, \texttt{sage-studio}, \texttt{sage-tools}, \texttt{sage-gateway}). By enforcing no upward dependencies, SAGE cleanly separates concerns between configuration, scheduling, execution, and user interfaces, enabling independent evolution of layers, easier testing, and simplified large-scale system maintenance.

  \item \textbf{A unified control plane for LLM and embedding workloads.}

  We design and implement a \emph{sageLLM} control plane that jointly manages LLM and embedding workloads across a shared pool of engines. The control plane classifies requests (chat/generation vs. embeddings), applies hybrid scheduling and batching policies (for example, \texttt{HybridSchedulingPolicy}), and exposes an OpenAI-compatible API via \texttt{sage-gateway} on standardized ports derived from \texttt{SagePorts}. This unified design improves resource utilization and reduces tail latency for mixed LLM+embedding traffic compared to siloed vLLM plus separate embedding deployments, while preserving a familiar client-facing interface.

  \item \textbf{Systems support for heterogeneous CPU/GPU deployments with reproducible tooling.}

  SAGE provides kernel-level mechanisms for CPU-only and GPU nodes, including job management in \texttt{sage-kernel} and node selection policies that are aware of hardware capabilities and load. Platform services in \texttt{sage-platform} cover storage, queuing, and service management, while C++ operators in \texttt{sage-middleware} accelerate performance-critical paths. Together with reproducible installation and quality pipelines (\texttt{quickstart.sh}, \texttt{manage.sh}, \texttt{sage-dev}, and pre-commit tooling), the system lowers the barrier to deploying complex LLM pipelines on heterogeneous clusters and makes end-to-end experiments repeatable for both developers and researchers.

  \item \textbf{A comprehensive benchmark suite and reusable testbed for LLM-centric systems.}

  To evaluate the system, we provide \texttt{sage-benchmark}, which instantiates workloads for agent behavior (tool selection, multi-step planning, timing decisions) and control-plane scheduling under diverse traffic patterns, as well as additional suites targeting retrieval, memory, data systems, and scheduler behavior in LLM-centric pipelines. The suite reports not only task- or model-level accuracy but also systems metrics such as throughput, latency distributions, SLO satisfaction, and resource utilization, and it exposes standard interfaces so that alternative agents, scheduling algorithms, or middleware components can be plugged in and compared on a common testbed built on top of SAGE's layered architecture and unified control plane.
\end{enumerate}

% ---------------------------------------------------------------------------
% Author notes: quantitative claims and mapping to experiments
% ---------------------------------------------------------------------------
% Each contribution should eventually be backed by at least one quantitative
% claim, using experiments from Section~\ref{sec:experiments}:
%   - Architecture (1): developer study or configuration/LOC comparison to
%     support statements like "reduces configuration complexity by [Y]%".
%   - Control plane (2): mixed LLM+embedding benchmarks showing p99 latency
%     reduction by [X]% and throughput improvements by [Y]\texttimes{} over
%     vLLM + separate embedding baselines (cf. end-to-end and control-plane
%     experiments).
%   - Heterogeneous support (3): CPU vs. GPU embedding or pipeline benchmarks
%     demonstrating that CPU-only nodes achieve [X]% of GPU performance for
%     embedding-heavy workloads, and hybrid deployments improve GPU efficiency
%     by [H\textsubscript{eff}]% while maintaining latency targets.
%   - Benchmark suite (4): comparative evaluations revealing scheduling or
%     policy insights (for example, FIFO degrading p99 latency by [C]\texttimes{}
%     relative to hybrid policies) using the \texttt{sage-benchmark} workloads.
%
% If space is tight, items (3) and (4) can be merged into a single contribution
% on end-to-end deployment and evaluation support.
