% Related Work section for the SAGE systems paper (content, not prompt)

\section{Related Work}

\subsection{LLM Serving Engines}

LLM serving engines such as vLLM~[REF: VLLM], TensorRT-LLM~[REF: TRT_LLM], SGLang~[REF: SGLANG], and Orca~[REF: ORCA] focus on maximizing single-model throughput and GPU utilization. They introduce optimizations such as paged attention, continuous batching, and kernel fusion to reduce per-token latency and improve memory efficiency. These systems are highly effective at treating a single LLM as a service, exposing low-level APIs for prompt submission and token streaming, and are now widely adopted as backends for higher-level applications.
SAGE is complementary to these serving engines rather than a replacement. It assumes the presence of engines like vLLM as backend components and operates at a higher abstraction level, orchestrating multiple LLM and embedding instances under a unified control plane. Instead of re-implementing intra-engine scheduling, SAGE focuses on cross-engine and cross-workload decisions: classifying requests, co-scheduling embeddings with generation, and routing traffic across heterogeneous CPU/GPU nodes. Furthermore, while common benchmarks for serving engines emphasize tokens-per-second and per-request latency, SAGE's evaluation framework measures end-to-end pipeline behavior, including interference between workloads and resource utilization under mixed traffic.

\subsection{ML Serving Frameworks and Workflow Platforms}

ML serving frameworks and workflow platforms such as Ray Serve~[REF: RAY_SERVE], KServe~[REF: KSERVE], Triton Inference Server~[REF: TRITON], MLflow~[REF: MLFLOW], Kubeflow~[REF: KUBEFLOW], and Airflow~[REF: AIRFLOW] provide general-purpose abstractions for deploying models and orchestrating data-processing DAGs. They address concerns such as autoscaling, model versioning, A/B testing, and scheduling of batch and streaming jobs across clusters. However, their abstractions are largely model-agnostic: they treat LLMs, embeddings, and other components as black-box services without specialized support for joint scheduling, prompt-level SLOs, or LLM-specific observability.
SAGE differs by building LLM awareness into both its declarative dataflow model and its control plane. Its execution kernel and control-plane components understand the distinction between chat/generation and embedding requests and exploit this structure when batching and routing work. While SAGE can be deployed on top of general frameworks---for example, using Ray or Kubernetes as the underlying resource manager---it exposes a unified LLM+embedding interface via \texttt{sage-gateway} and provides policy hooks tailored to LLM-centric workloads. In addition, SAGE ships with system-level benchmarks that evaluate scheduling policies and deployment configurations for LLM pipelines, something that is typically outside the scope of generic serving or workflow systems.

\subsection{LLM Application Frameworks and Agents}

LLM application frameworks and agent toolkits such as LangChain~[REF: LANGCHAIN], LlamaIndex~[REF: LLAMAINDEX], DSPy~[REF: DSPY], and various agent frameworks~[REF: AGENT_FRAMEWORKS] aim to simplify the construction of LLM applications. They provide abstractions for prompt templates, retrieval-augmented generation, tool use, and iterative reasoning, enabling developers to prototype complex behaviors with relatively little code. Some of these frameworks include limited execution-time tracing and evaluation, but they typically delegate all systems concerns---latency, resource allocation, isolation, and deployment---to external infrastructure.
SAGE targets a different layer of the stack: it provides the systems substrate on which such application frameworks can run. Its declarative dataflow abstraction can express the same retrieval, tool, and multi-step patterns that application-level libraries capture, but with an explicit mapping to jobs, nodes, and scheduling policies. In this sense, SAGE can use frameworks like LangChain or DSPy as sources of logical pipeline descriptions, while still treating vLLM and embedding servers as underlying engines. Moreover, SAGE's benchmark suite complements task-level evaluations from application frameworks by focusing on how agents behave under resource contention, heterogeneous hardware, and different control-plane policies.

\subsection{LLM Benchmarks and Evaluation Frameworks}

Benchmark suites such as AgentBench~[REF: AGENTBENCH], ToolBench~[REF: TOOLBENCH], HELM~[REF: HELM], and vLLM's own performance benchmarks~[REF: VLLM_BENCH] provide critical visibility into LLM accuracy, robustness, and single-engine performance. AgentBench and ToolBench emphasize agent capabilities such as tool selection, planning, and function calling, primarily measuring task success and reasoning quality. HELM offers a broad evaluation of models across tasks and risk dimensions, while vLLM benchmarks quantify throughput and latency for individual engines under synthetic workloads.
SAGE builds on the insights from these works but extends the evaluation scope to full systems. Its \texttt{sage-benchmark} package includes experiments that stress the control plane and dataflow runtime, measuring throughput, latency distributions, SLO satisfaction, interference between tenants, and scaling across multiple backends. Rather than competing with task-centric benchmarks, SAGE can ingest workloads or task definitions inspired by AgentBench or ToolBench and run them through its own infrastructure to study system-level behavior. This dual focus allows practitioners to reason about both task quality and the underlying system's ability to deliver predictable performance at scale.

\subsection{Data and Storage Systems for AI Pipelines}

Data and storage systems used in AI pipelines, including vector databases such as FAISS, Milvus, or commercial offerings~[REF: VECTOR_DB], time-series databases for monitoring~[REF: TSDB], and general-purpose dataflow engines like Flink or Spark~[REF: DATAFLOW_SYS], provide essential building blocks for retrieval, logging, and streaming analytics. These systems excel at indexing, querying, and transporting data with strong consistency and availability guarantees, and many integrate with LLM applications as external components. However, they do not directly address how LLM inference and embedding workloads share compute resources or how end-to-end pipelines should be scheduled across heterogeneous clusters.
SAGE is designed to interoperate with such data and storage systems rather than replace them. Its declarative dataflow model can treat vector databases, time-series stores, and streaming engines as operators or external services within a larger LLM/AI pipeline. The system's main contribution is to manage the compute side of the pipeline---LLM and embedding engines, control-plane scheduling, and node selection---while exposing hooks for integrating storage components through standardized interfaces. In doing so, SAGE fills the gap between data-centric infrastructure and application-level frameworks by providing a dedicated, LLM-aware execution and evaluation layer.

\subsection{Summary}

Across these categories, SAGE occupies a distinct position in the LLM systems landscape. It is neither a single-model serving engine nor a generic workflow tool, but a unified, layered platform that combines declarative dataflow, an LLM+embedding control plane, heterogeneous deployment support, and comprehensive system-level benchmarks. SAGE can leverage existing engines such as vLLM as backends, run on top of general-purpose serving and workflow frameworks, and host application-level libraries and benchmarks as workloads. By providing an integrated stack that spans architecture, runtime, control plane, and evaluation, SAGE addresses a gap between low-level serving systems and high-level application frameworks for LLM/AI pipelines.

% ---------------------------------------------------------------------------
% Citation placeholders (to be mapped to concrete references later)
% ---------------------------------------------------------------------------
% LLM Serving Engines:
%   [REF: VLLM], [REF: TRT_LLM], [REF: SGLANG], [REF: ORCA], [REF: VLLM_BENCH]
% ML Serving Frameworks and Workflow Platforms:
%   [REF: RAY_SERVE], [REF: KSERVE], [REF: TRITON], [REF: MLFLOW],
%   [REF: KUBEFLOW], [REF: AIRFLOW]
% LLM Application Frameworks and Agents:
%   [REF: LANGCHAIN], [REF: LLAMAINDEX], [REF: DSPY], [REF: AGENT_FRAMEWORKS]
% LLM Benchmarks and Evaluation Frameworks:
%   [REF: AGENTBENCH], [REF: TOOLBENCH], [REF: HELM]
% Data and Storage Systems:
%   [REF: VECTOR_DB], [REF: TSDB], [REF: DATAFLOW_SYS]
