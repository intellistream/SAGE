% Introduction section for the SAGE systems paper (content, not prompt)

\section{Introduction}

Large language models (LLMs) are increasingly deployed as part of rich application pipelines that combine retrieval, tools, structured data access, and multi-stage post-processing across heterogeneous CPU/GPU clusters. While modern serving engines deliver impressive single-model throughput and latency, practitioners still struggle to manage end-to-end LLM/AI pipelines: orchestrating multiple models and embedding services, sharing resources across workloads with different latency requirements, and reproducing complex deployments reliably. Existing MLOps and workflow platforms provide generic abstractions for training and batch jobs, but they lack LLM-aware scheduling and dataflow support tailored to online inference pipelines.
We argue that building and operating LLM-centric pipelines requires a full-stack system that unifies dataflow, control, and evaluation rather than another isolated serving component. Low-level LLM engines such as vLLM, TensorRT-LLM, and SGLang optimize GPU utilization for a single model, but they do not address cross-model coordination or embedding co-scheduling. ML serving frameworks and workflow platforms such as Ray Serve, KServe, Triton, MLflow, Kubeflow, and Airflow focus on deployment and generic DAGs, but offer limited support for fine-grained resource sharing and observability across mixed LLM and embedding workloads. Application frameworks such as LangChain, LlamaIndex, and DSPy simplify prompt engineering and tool wiring, yet they treat the underlying system as a black box and cannot reason about hardware heterogeneity, SLOs, or interference.
We present SAGE, a Python 3.10+ framework that treats LLM/AI applications as declarative dataflow pipelines mapped onto a strict six-layer architecture. At the bottom, \texttt{sage-common} and \texttt{sage-platform} provide shared configuration, XDG-compliant user paths, port management, and platform services such as storage and cluster configuration. The \texttt{sage-kernel} and \texttt{sage-libs} layers implement a job management and node selection runtime that is aware of CPU-only and GPU nodes, while \texttt{sage-middleware} contributes C++ operators for performance-critical components. On top, \texttt{sage-apps} and \texttt{sage-benchmark} host applications and evaluation suites, and user-facing tools---including \texttt{sage-cli}, \texttt{sage-studio}, \texttt{sage-tools}, and the \texttt{sage-gateway} OpenAI-compatible endpoint---expose the system to developers.
A central component of SAGE is a unified LLM and embedding control plane (``sageLLM'') that fronts a pool of vLLM and embedding backends behind the \texttt{sage-gateway}. The control plane classifies requests, co-schedules chat/generation and embedding workloads using policies such as \texttt{HybridSchedulingPolicy}, and batches work across engines to improve throughput and tail latency while honoring per-tenant SLOs. Unlike single-engine benchmarks such as AgentBench, ToolBench, or HELM, SAGE's benchmarks focus on system-level behavior: they stress the control plane, dataflow runtime, and heterogeneous deployments, measuring throughput, latency distributions, SLO satisfaction, and interference under realistic RAG and agent-style workloads.
This paper makes the following contributions. \emph{First}, we design a six-layer architecture and declarative dataflow model for LLM-centric pipelines that cleanly separates concerns between common utilities, platform services, execution kernel, middleware operators, applications, and user interfaces, with strict no-upward dependencies. \emph{Second}, we introduce a unified LLM and embedding control plane that shares resources across multiple engines and supports policy-driven scheduling for mixed interactive and batch traffic. \emph{Third}, we provide systems support for heterogeneous CPU/GPU deployments, including job management, node selection, and reproducible tooling for deployment, testing, and CI. \emph{Fourth}, we develop a comprehensive benchmark suite that evaluates both agent capabilities and system-level behavior, enabling rigorous comparison of scheduling policies, deployment topologies, and baselines. Finally, we show in Section~\ref{sec:experiments} that SAGE reduces end-to-end p99 latency by [X]\% and improves throughput by [Y]\texttimes{} over separately managed LLM and embedding services, while maintaining strict latency SLOs under mixed interactive and batch workloads.

% ---------------------------------------------------------------------------
% Internal notes for authors (not part of the camera-ready content)
% ---------------------------------------------------------------------------
% Possible future refinements to the Introduction:
% - Once experiments are finalized, add one sentence summarizing the main
%   quantitative result (e.g., p99 reduction and throughput gain) near the end
%   of the last paragraph, aligned with the abstract.
% - Replace generic references to ``RAG and agent-style workloads" with the
%   concrete benchmark names and datasets used in Section 5.
% - If space is tight, consider shortening the list of external systems in the
%   second paragraph and move detailed positioning to Related Work.
% - Optionally add a final paragraph that previews the paper structure.
