\begin{abstract} 
Modern LLM applications are inference pipelines that interleave retrieval, embedding and vector search, semantic state and memory, context refinement, and agentic tool use. These pipelines must meet tail-latency and SLO targets while spanning heterogeneous CPU/GPU resources and auxiliary services, yet the current ecosystem remains split: serving engines optimize a single model backend, while orchestration and application frameworks provide limited control over end-to-end execution, resource sharing, and reproducibility. We present \textbf{SAGE} (Streaming-Augmented Generative Execution), a full-stack system for building LLM inference pipelines. SAGE treats the pipeline---not the model---as the primary unit of abstraction, providing a declarative dataflow interface that compiles into distributed execution plans over heterogeneous CPU/GPU resources. The system integrates a suite of specialized inference components: a high-performance vector database (SageVDB), a vector-native stream engine for incremental semantic state (SageFlow), structured memory backends (NeuroMem), time-series operators, and context refinement modules---all unified under a strict five-layer architecture with downward-only dependencies. For LLM inference, SAGE leverages vLLM as the backend engine. A companion benchmark suite jointly evaluates system metrics (throughput, TTFT/TBT, tail latency) and agent behaviors (tool selection, planning). On a 16-node cluster, SAGE processes 17.6 RAG tasks per second with sub-millisecond scheduling overhead, achieves 10--11$\times$ speedup (68\% parallel efficiency), and reduces multi-tenant tail latency by 57\% through staggered admission.
\end{abstract}