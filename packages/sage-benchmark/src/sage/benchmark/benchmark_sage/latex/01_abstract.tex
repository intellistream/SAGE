% Abstract for the SAGE systems paper (content, not prompt)

\begin{abstract}
Modern LLM applications increasingly rely on complex pipelines that combine retrieval, tool calls, and multiple foundation models deployed across heterogeneous CPU/GPU clusters. Existing serving engines and workflow platforms either optimize single-model inference or provide generic orchestration, but they lack unified dataflow and control-plane support for mixed LLM and embedding workloads. This gap makes it difficult to reason about performance, resource usage, and reproducibility for end-to-end LLM-centric systems.
We present SAGE, a full-stack framework that organizes LLM/AI pipelines into a strict six-layer architecture with declarative dataflow and no upward dependencies. Users specify pipelines at a high level, while platform, kernel, and middleware layers compile them into execution plans that span CPU-only and GPU nodes. SAGE integrates a unified LLM and embedding control plane, exposed through an OpenAI-compatible gateway, that performs request classification, hybrid scheduling, and batching across multiple vLLM and embedding backends. The framework further provides reproducible tooling and benchmark suites that evaluate both agent capabilities and system-level behavior.
Across representative RAG and agent-style workloads, SAGE reduces p99 end-to-end latency by [X]\% and improves throughput by [Y]\texttimes{} over a baseline of separately managed LLM and embedding services, while maintaining p95 latency below [Z] ms. Under mixed interactive and batch traffic, it achieves [A]\% SLO satisfaction compared to [B]\% for the baseline and scales to [G] GPU nodes and [H] concurrent clients with near-linear throughput.
\end{abstract}

% ---------------------------------------------------------------------------
% Internal notes for authors (not part of the camera-ready content)
% ---------------------------------------------------------------------------
% Experiments needed to instantiate placeholders:
% - [X], [Y], [Z]: Mixed LLM+embedding RAG/agent workload comparing SAGE vs.
%   baseline "vLLM + separate embedding service"; measure end-to-end latency CDF
%   and throughput under increasing load.
% - [A], [B]: Isolation / SLO study with interactive vs. batch tenants, with and
%   without SAGE's unified control-plane scheduling policies.
% - [G], [H]: Scalability experiment varying number of GPU nodes and concurrent
%   clients; report near-linear scaling region and saturation point.

% Suggestions for refining the abstract once numbers and baselines are fixed:
% - Replace generic "representative RAG and agent-style workloads" with the
%   specific benchmark names and dataset characteristics.
% - Tighten the quantitative sentence to highlight one primary claim (e.g.,
%   tail latency reduction) and move secondary numbers to the introduction.
% - Align wording of baselines with the exact experimental setup section titles
%   (e.g., "Separate Services" vs. "Decoupled LLM/Embedding Deployment").
% - If space is tight, consider shortening the description of reproducible
%   tooling and benchmarks and instead reference the Experiments section.
