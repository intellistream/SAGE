\section{Design Details}
\label{sec:design_details}

\paragraph{Design goals.}
SAGE targets hybrid RAG/agent workloads in which each query triggers a multi-stage pipeline---retrieval over vector/text indices, memory read/write, optional refinement (e.g., compression or reranking), and final generation/tool use---and the same workload must scale from local runs to heterogeneous clusters. 
First, we prioritize \emph{composability} by making the pipeline (a dataflow DAG of operators) the primary abstraction, so retrieval, memory, refinement, and generation are expressed as interchangeable stages rather than entangled control flow. 
Second, we emphasize \emph{resource-aware execution} by supporting explicit placement and scheduling over CPU-only and GPU nodes, enabling mixed deployments where embedding/retrieval-heavy stages can run on CPUs while generation-heavy stages are assigned to GPUs. 
Third, we enforce \emph{isolation and stability} through streaming-style flow control (bounded queues/backpressure) and policy-driven routing decisions, so slow or bursty stages do not destabilize the rest of the pipeline. 
This architecture also supports rigorous, repeatable system evaluation by representing each workflow as a declarative DAG with interchangeable operators, which enables controlled ablations and fair comparisons; we describe the evaluation methodology in \S\ref{sec:experiments}.

\subsection{Architecture Overview and Layering}
\label{subsec:dd_arch_overview}

SAGE is organized as a layered modular monolith with \textbf{one-way dependencies} to keep the system extensible while avoiding architectural erosion. Concretely, code in higher layers may depend only on lower layers (L5$\rightarrow$L4$\rightarrow$L3$\rightarrow$L2$\rightarrow$L1), and \emph{reverse} (``upward'') dependencies are disallowed. This separation makes it possible to evolve execution/runtime mechanisms independently from domain operators, and to enforce clean boundaries between reusable primitives (types, configs, algorithms) and runtime-bound components (operators, backends, C++ extensions). When cross-layer instantiation is required (e.g., a lower layer needs to trigger creation of a higher-layer implementation), SAGE uses explicit registration/factory hooks rather than direct imports, preserving the one-way dependency invariant.

\paragraph{Core packages by layer.}
\begin{itemize}
  \item \textbf{L1 (Foundation): \texttt{sage-common}.} Shared types, configuration, and utilities (e.g., standardized ports and environment keys), intended to be dependency-minimal and reusable across the stack.
  \item \textbf{L2 (Platform services): \texttt{sage-platform}.} Abstract services such as queue/storage/service lifecycles that underpin distributed execution without tying the design to a single backend.
  \item \textbf{L3 (Core): \texttt{sage-kernel}, \texttt{sage-libs}.} The dataflow runtime and execution engine live in \texttt{sage-kernel}, while \texttt{sage-libs} provides algorithmic building blocks (e.g., RAG utilities and agentic logic) that remain runnable without heavyweight external services.
  \item \textbf{L4 (Domain/middleware): \texttt{sage-middleware}.} Domain operators and runtime-bound components for end-to-end pipelines (retrieval/memory/refinement/generation connectors), including performance-critical C++ extensions (e.g., streaming/vector and memory backends) exposed via Python bindings.
  \item \textbf{L5 (Interfaces and tooling): \texttt{sage-cli}, \texttt{sage-tools}.} User-facing command-line entry points and developer tooling (testing, quality checks, maintenance workflows).
\end{itemize}
In addition, SAGE ships a lightweight meta-package \texttt{sage} (PyPI: \texttt{isage}) that aggregates the core packages for installation convenience, without changing the layer boundaries above.

\paragraph{Independent repositories (outside the core).}
Several ecosystem components are intentionally maintained outside the SAGE core repository and are treated as external dependencies: \textbf{\texttt{isagellm}} (LLM control plane/gateway and unified inference client), \textbf{\texttt{isage-benchmark}} (evaluation framework), \textbf{\texttt{sage-studio}} (visual workflow builder), and \textbf{\texttt{sage-examples}} (tutorials and applications). This separation keeps the core layering stable and lightweight while allowing fast iteration on product-facing services and benchmarking infrastructure.

\subsection{Compilation and Execution Graph}
\label{subsec:dd_compilation}
SAGE compiles a user-authored \emph{logical} pipeline DAG (a sequence of transformations registered in the environment) into a \emph{physical} execution graph that is explicit about parallelism, communication, and routing. Concretely, the runtime component \texttt{ExecutionGraph} in \texttt{sage-kernel} lowers each logical operator (a \texttt{BaseTransformation}) into a set of \emph{task replicas} according to its declared parallelism; each replica is represented as a \texttt{TaskNode} and is associated with a \texttt{TaskFactory} that instantiates the runnable task. For every logical edge between operators, the compiler then materializes a complete bipartite set of physical edges between upstream replicas and downstream replicas (i.e., an $m{\times}n$ expansion when the upstream and downstream parallelisms are $m$ and $n$), producing an execution graph in which the degree of fan-out and fan-in is no longer implicit.

Edges are \emph{materialized as bounded channels} by allocating queue descriptors for communication at the node level. In particular, each task replica owns a bounded input channel (implemented as a platform-specific queue descriptor, e.g., Python queues locally or Ray queues remotely), and all upstream replicas targeting that replica write into this queue; this implements backpressure because the router performs blocking queue writes when the channel is full. Multi-input operators are supported by tagging each physical edge with an explicit input index, so that downstream tasks can demultiplex incoming packets by logical input port even though the physical channel is shared.

Fan-out and fan-in become first-class objects in the execution graph and routing layer. Fan-out is represented by grouping the outgoing physical edges of a replica into explicit output groups (one group per downstream logical connection), which are compiled into \texttt{downstream\_groups} inside \texttt{TaskContext}. At runtime, the \texttt{BaseRouter} uses these groups to implement policy-driven delivery, including round-robin routing across downstream replicas, broadcast, and partitioned (hash) routing based on packet metadata. Symmetrically, fan-in is realized by multiple upstream replicas writing into the same bounded input queue of each downstream replica, making contention and flow control explicit and observable in the physical plan.

\subsection{Scheduling, Placement, and Backpressure}
\label{subsec:dd_scheduling}

At runtime, each task replica executes a simple \emph{worker loop} in \texttt{sage-kernel}: (i) dequeue one item from its input channel, (ii) execute the operator logic on that item, and (iii) route any emitted outputs to downstream replicas using partition hints carried in the data packet. Concretely, non-source tasks block on \texttt{get()} over a queue descriptor, then invoke \texttt{operator.receive\_packet(packet)}; downstream delivery is handled by the router (\texttt{BaseRouter}), which inspects \texttt{Packet} metadata (e.g., \texttt{partition\_key} and \texttt{partition\_strategy}) to implement round-robin, broadcast, or hash-partitioned routing across downstream replicas.

SAGE enforces stability via \emph{bounded queues} and \emph{backpressure propagation}. Each replica is provisioned with a bounded input channel (queue descriptors in \texttt{sage-platform} such as \texttt{PythonQueueDescriptor} locally or \texttt{RayQueueDescriptor} in distributed mode), and all upstream replicas write into these bounded channels. The router performs blocking \texttt{put()} into downstream queues; when a downstream stage is saturated (e.g., a GPU-heavy generation operator under bursty arrivals), queue occupancy naturally throttles upstream producers, preventing unbounded buffering and limiting interference across pipeline stages. This mechanism is particularly important for mixed workloads that combine retrieval/memory stages (often CPU-bound and parallel) with generation or refinement stages (often GPU-bound and latency-sensitive), since it provides isolation without requiring global synchronization.

Placement and scheduling are coordinated by \texttt{Dispatcher} in \texttt{sage-kernel}, which separates \emph{decision} from \emph{execution}. A scheduler (\texttt{BaseScheduler} and its implementations) produces a \texttt{PlacementDecision} that can encode (a) resource requirements (CPU/GPU/memory/custom resources) and (b) target-node affinity in distributed settings. The \texttt{PlacementExecutor} then translates this decision into physical placement—e.g., mapping resource requests to Ray actor options (\texttt{num\_cpus}, \texttt{num\_gpus}, memory) and optionally enforcing node affinity—so that GPU-heavy stages can be steered toward GPU nodes while retrieval or preprocessing can be placed on CPU-only nodes. Together, the scheduler/placement split and the queue-based backpressure provide a resource-aware execution model that scales across heterogeneous CPU/GPU clusters while maintaining predictable behavior under load.

\subsection{State Management and Fault Tolerance}
\label{subsec:dd_ft}

SAGE distinguishes between \emph{transient} execution state (in-flight packets buffered in bounded queues) and \emph{durable} operator/task state (e.g., counters, caches, aggregation tables) that must be preserved to avoid recomputation. To make state explicit and composable, SAGE exposes a uniform state interface at the runtime boundary: each task can materialize a self-contained snapshot via \texttt{get\_state()} and rehydrate from a snapshot via \texttt{restore\_state(state)}. Concretely, the task-level snapshot aggregates (i) task metadata and progress counters and (ii) operator-provided state when available; operator state can in turn include user-function state when the operator wraps a stateful function. This layered snapshot structure lets SAGE checkpoint at a stable runtime boundary without requiring application code to understand queue internals or replica placement decisions (implemented in \texttt{sage-kernel}, e.g., \texttt{runtime/task/base\_task.py}).

Fault tolerance is configured declaratively through the environment configuration (\texttt{env.config["fault\_tolerance"]}) and instantiated by the kernel at submission time (see \texttt{sage-kernel/fault\_tolerance/factory.py}). SAGE currently provides two built-in recovery policies. (1) \textbf{Checkpoint-based recovery} periodically persists task snapshots at a user-configurable interval (\texttt{checkpoint\_interval}) and retries recovery up to \texttt{max\_recovery\_attempts}. In the steady state, the worker loop opportunistically triggers checkpoint writes while processing (and also forces a checkpoint on exceptions to maximize recovery fidelity). Checkpoints are managed by a checkpoint manager (\texttt{CheckpointManagerImpl}) that versions snapshots by task identifier and timestamp and stores them under a configurable directory (default \texttt{.sage/checkpoints}) (see \texttt{sage-kernel/fault\_tolerance/impl/checkpoint\_recovery.py} and \texttt{.../checkpoint\_impl.py}). Upon failure, the recovery handler loads the most recent checkpoint and requests a task restart with state restoration through the dispatcher. The dispatcher stops and cleans up the failed task, re-creates a fresh replica using the normal scheduling/placement path, restores state, and resumes execution (see \texttt{runtime/dispatcher.py}, \texttt{restart\_task\_with\_state}).

(2) \textbf{Restart-based recovery} targets stateless (or externally stateful) stages, where correctness does not require restoring in-memory state and the simplest recovery is to restart computation. SAGE parameterizes restart policies via explicit restart strategies (e.g., fixed delay, exponential backoff, and failure-rate based control), allowing users to trade off recovery aggressiveness versus stability under repeated failures (see \texttt{sage-kernel/fault\_tolerance/impl/restart\_strategy.py} and \texttt{.../restart\_recovery.py}). In the current implementation, checkpoint-based recovery provides end-to-end task restart \emph{with} state restoration via the dispatcher, while restart-based recovery provides the restart policy surface (and job-level restart helper) and is being integrated with dispatcher-driven task restarts for fully stateless pipelines.

For distributed (remote) execution, SAGE complements exception-driven recovery with active liveness monitoring. A heartbeat monitor periodically polls each remote task for heartbeat statistics and treats repeated timeouts/stale heartbeats as failures; such failures are forwarded into the same fault-handler interface, ensuring a single recovery entry point regardless of whether failures manifest as explicit exceptions or as silent task death (implemented in \texttt{sage-kernel/runtime/heartbeat\_monitor.py}). Resource cleanup is centralized via a lifecycle manager that performs best-effort task/actor cleanup and termination with bounded waiting, preventing leaked resources from compounding failures during recovery (see \texttt{sage-kernel/fault\_tolerance/impl/lifecycle\_impl.py}).

Finally, durable state requires a storage boundary that is stable across process restarts and placement changes. SAGE factors storage access into an L2 abstraction layer: \texttt{sage-platform} defines backend-agnostic key-value storage interfaces (\texttt{BaseKVBackend}) and provides lightweight implementations (e.g., in-memory dict with explicit load/store) with optional filesystem support (e.g., HDFS integration) (\texttt{sage-platform/storage/*}). While the current checkpoint manager persists snapshots to the local filesystem by default, this layering makes it straightforward to route checkpoints or auxiliary operator state to alternative backends by implementing the same storage interface, without entangling fault-tolerance logic with any particular storage system.

\subsection{Middleware Operators for RAG Pipelines}
\label{subsec:dd_middleware}

A practical RAG system is not a single “retrieval call” followed by a single “generation call”: it is a multi-stage subsystem that must (i) access external indices and memory, (ii) transform and filter retrieved evidence, (iii) construct prompts with provenance, and (iv) invoke LLM inference and downstream tools. In SAGE, we therefore promote RAG subsystems to \emph{first-class operators} in the dataflow graph, rather than hiding them behind black-box library calls. This design gives the runtime visibility into stage boundaries and costs—enabling pipeline-wide scheduling, parallelism scaling, backpressure propagation, and monitoring at the same granularity as other streaming transformations—while also making cross-stage optimizations (e.g., batching, caching, and placement decisions for retrieval-heavy vs.\ LLM-heavy stages) explicit and composable. Concretely, these domain operators live in the L4 middleware layer (not L3 libraries) because they depend on “upward” runtime-bound capabilities such as vector stores, memory backends, and refinement services.

SAGE’s middleware layer provides a set of reusable operator namespaces for RAG and agentic applications (\texttt{packages/sage-middleware/src/sage/middleware/operators/}):
\textbf{(i) \texttt{operators/rag}} encapsulates canonical RAG stages—retrieval, reranking, refinement, prompting, generation, evaluation, and lightweight orchestration. For example, \texttt{RAGPipeline} composes a retriever/reranker/refiner/generator sequence (\texttt{operators/rag/pipeline.py}); retrievers such as \texttt{ChromaRetriever} and \texttt{Milvus*Retriever} expose retrieval as a map-style operator (\texttt{operators/rag/retriever.py}); prompt construction is implemented as a prompt operator (e.g., \texttt{QAPromptor}, \texttt{operators/rag/promptor.py}); and generation is implemented via OpenAI-compatible endpoints (including local OpenAI-compatible services) or via the internal SageLLM generator (\texttt{operators/rag/generator.py}). Importantly, context compression is also modeled as an operator: \texttt{RefinerOperator} wraps external refinement compressors (LongRefiner/REFORM/Provence) from the independent \texttt{isage-refiner} package (\texttt{operators/rag/refiner.py}), so compression becomes a schedulable, monitorable stage rather than a hidden post-processing step.

\textbf{(ii) \texttt{operators/llm}} provides LLM inference as an operator with a backend-decoupled engine factory interface. The primary entry point is \texttt{SageLLMGenerator} (\texttt{operators/llm/sagellm\_generator.py}), which normalizes inputs (context/prompt/options), instantiates an engine via a factory (no hard-coded backend classes), and exposes synchronous or streaming generation. This supports treating LLM inference uniformly as a dataflow transformation—subject to the same runtime controls (parallelism, placement, backpressure) as non-LLM stages.

\textbf{(iii) \texttt{operators/tools}} exposes tool invocation as operators for agentic/RAG workflows (e.g., web search, paper search, URL text extraction, and optional heavy tools such as image captioning). This makes tool calls explicit nodes in the graph, enabling consistent logging, throttling, and policy routing across heterogeneous external services (\texttt{operators/tools/\_\_init\_\_.py}).

Under the hood, these operators are backed by core middleware components (\texttt{packages/sage-middleware/src/sage/middleware/components/}) that provide the runtime-facing “systems” a RAG pipeline needs:
\textbf{(a) \texttt{sage\_db}} integrates a vector database backend via a compatibility layer to the independent C++-backed SageVDB package (\texttt{isage-vdb}, Python import \texttt{sagevdb}) (\texttt{components/sage\_db/\_\_init\_\_.py}).
\textbf{(b) \texttt{sage\_mem}} exposes memory backends as a namespace package, with NeuroMem-backed collections/managers provided by the independent \texttt{isage-neuromem} package (\texttt{components/sage\_mem/\_\_init\_\_.py}).
\textbf{(c) \texttt{sage\_refiner}} is integrated at the operator level (via \texttt{RefinerOperator}) through the independent \texttt{isage-refiner} package (import \texttt{sage\_refiner}); this keeps compression/refinement as an explicit pipeline stage while allowing the underlying algorithms to evolve independently.
\textbf{(d) \texttt{sage\_flow}} provides vector-native streaming building blocks and micro-service wrappers, integrating the independent \texttt{isage-flow} package and its C++ extension for high-throughput streaming vector processing (\texttt{components/sage\_flow/\_\_init\_\_.py}).
\textbf{(e) \texttt{sage\_tsdb}} provides time-series storage and streaming analytics hooks via the independent C++-backed \texttt{isage-tsdb} package (import \texttt{sage\_tsdb}), plus SAGE-specific algorithms/services for windowing and out-of-order stream joins (\texttt{components/sage\_tsdb/\_\_init\_\_.py}).

Finally, placing these subsystems in L4 is also a performance choice. Vector search, vector-native streaming operators, and time-series window/join primitives benefit from native implementations (C++ cores with Python bindings) to reduce per-record overhead and to enable batched execution paths. By integrating these high-performance backends as middleware components and exposing their use through explicit operators, SAGE can combine domain-level performance (native backends) with system-level optimization (graph-visible stages under the unified scheduler/backpressure runtime).

\subsection{Algorithmic Libraries and Agentic Tooling}
\label{subsec:dd_libs}

SAGE separates \emph{algorithmic building blocks} from \emph{runtime-bound execution}, enabling rapid iteration without entangling methods with placement, backends, or orchestration details. The L3 package \texttt{sage-libs} is explicitly designed as an interface/registry layer: it defines abstract base classes, typed data structures, and factory/registration APIs (\texttt{register\_*}/\texttt{create\_*}) for core domains such as agentic reasoning, RAG, fine-tuning, evaluation, privacy, safety, and algorithmic primitives (e.g., ANN/AMM) (see \texttt{packages/sage-libs/README.md} and \texttt{sage/libs/\_\_init\_\_.py}). In this design, \texttt{sage-libs} modules remain independent of runtime placement and external services: a planner, reranker, or safety policy can be swapped without changing how the pipeline is scheduled, replicated, or backpressured.

Concretely, \texttt{sage-libs} provides (i) \textbf{agentic interfaces} (\texttt{sage.libs.agentic.interface}) that standardize agents, planners, tool selectors, and orchestration components, with concrete implementations intended to live in external packages (e.g., \texttt{isage-agentic}); (ii) \textbf{RAG utilities and interfaces} (\texttt{sage.libs.rag}), including typed RAG artifacts for pipeline interoperability (\texttt{sage.libs.rag.types}) and lightweight built-ins such as document loaders and chunkers (\texttt{sage.libs.rag.document\_loaders}, \texttt{sage.libs.rag.chunk}), while retrievers/rerankers/pipelines are instantiated via registries (\texttt{sage.libs.rag.interface}); (iii) \textbf{integrations} that are local and algorithmic (e.g., a HuggingFace client wrapper for local inference, \texttt{sage.libs.integrations.huggingface}), and (iv) \textbf{built-in utilities} for data operations and basic processing (\texttt{foundation/}, \texttt{dataops/}) that support experimental pipelines with minimal external dependencies. In addition, \texttt{sage-libs} provides interface layers for evaluation, fine-tuning, privacy/unlearning, and safety guardrails, again emphasizing pluggability through registries and externalized heavyweight implementations.

The boundary with L4 middleware is deliberate: \textbf{libs provide reusable algorithms; middleware wraps them as operators}. Middleware (\texttt{sage-middleware}) is allowed to depend on runtime-bound backends and services (vector stores, memory systems, refinement engines, external APIs) and exposes these capabilities as schedulable dataflow operators (e.g., RAG retrievers, promptors, generators, tool operators). By keeping \texttt{sage-libs} focused on interfaces, policies, and lightweight utilities, SAGE makes it easy to compose and ablate algorithmic choices—e.g., swapping a retriever, reranker, refiner policy, planner strategy, or safety filter—without rewriting pipeline orchestration. This separation supports systematic evaluation: experiments can vary a single algorithmic component while holding the execution graph and runtime controls fixed, yielding cleaner ablations and more repeatable comparisons across agentic and RAG workloads.

\subsection{Inference Engine Integration}
\label{subsec:dd_inference}
SAGE treats LLM/embedding inference as an \emph{external service interface} rather than a library-internal dependency, decoupling the dataflow runtime from model serving. Concretely, SAGE integrates with the independent \texttt{isagellm} Gateway/Control Plane, which exposes OpenAI-compatible endpoints (e.g., \texttt{/v1/chat/completions} and \texttt{/v1/embeddings}) and manages underlying inference engines (e.g., vLLM-based backends) as separately scalable services. This separation allows SAGE pipelines to remain portable across deployment environments: the same compiled execution graph can target different model servers by changing only endpoint configuration (e.g., \texttt{base\_url}, model identifier), without modifying operator logic or the kernel scheduler.

At the operator level, generation and embedding are implemented as request/response calls to OpenAI-compatible APIs. For example, RAG generation operators instantiate a standard OpenAI client with a configurable \texttt{base\_url} and issue chat-completion requests with structured \texttt{messages} payloads, while embedding components similarly invoke the OpenAI embeddings API and return dense vectors for downstream retrieval and indexing. Because these operators depend only on a stable HTTP contract, SAGE can route requests through the Gateway/Control Plane (for unified engine management and admission control) or directly to a compatible backend, keeping the pipeline runtime agnostic to the concrete inference implementation.

This service-oriented interface also enables \emph{independent scaling} of inference backends versus pipeline stages. In practice, preprocessing, retrieval, and postprocessing stages can be replicated and scheduled on CPU-rich nodes, while LLM/embedding throughput is scaled by adding or resizing dedicated inference engines behind the Gateway/Control Plane. The two dimensions of parallelism—(i) dataflow operator replication in the SAGE execution graph and (ii) inference-engine provisioning in the serving layer—can therefore be tuned separately to meet latency/throughput targets and to reduce resource contention between GPU-bound model execution and CPU-bound orchestration.

Finally, the same pattern generalizes beyond LLMs: any request/response service (LLM, embedding, reranking, moderation, or domain tools) can be wrapped as an operator with explicit resource annotations (e.g., CPU/GPU requirements, concurrency limits, and batching policies). These annotations allow SAGE’s runtime to incorporate service calls into its placement and backpressure mechanisms, preserving end-to-end stability while supporting heterogeneous backends and evolving model-serving stacks.