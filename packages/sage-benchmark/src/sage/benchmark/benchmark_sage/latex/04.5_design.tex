\section{Design Details}
\label{sec:design_details}
This section details the key mechanisms that enable SAGE to execute these pipelines efficiently on
heterogeneous resources. We discuss the programming model, the runtime execution engine, the integrated
middleware components, and the control plane for hybrid workloads.

\subsection{Programming Model: Pipelines as Declarative Dataflows}
\label{subsec:dd_dataflow_model}

SAGE expresses an application as a directed acyclic graph (DAG) where nodes are operators and edges are
typed record streams.
% ... (Dataflow details) ...
At runtime, each stream element is represented as a \,\emph{Packet}---a self-contained record carrying
payload and routing metadata. Concretely, a packet contains the user payload (\texttt{data}), an event
timestamp in milliseconds (\texttt{timestamp}), an optional key for stateful processing (\texttt{key}), and
partitioning annotations (\texttt{partition\_strategy}, \texttt{partition\_key}) that guide downstream routing.
Additional fields (e.g., \texttt{source} and a free-form \texttt{metadata} map) support provenance and
application-specific tags. Partition metadata can be propagated along the graph (inheritance) and updated
at boundaries when an operator re-keys the stream.

Operators follow a minimal contract: they consume packets, optionally maintain operator-local state, and
emit zero or more output packets. Stateless transformations are expressed as map-style operators, while
stateful operators use the packet \texttt{key} to define per-key state access. In the base operator
implementation, the runtime sets the ``current key'' context before invoking the user-defined execution
logic and clears it afterward, ensuring keyed state is consistently scoped to the triggering record.
For fault tolerance, operators expose \texttt{get\_state}/\texttt{restore\_state} to serialize and restore
checkpointable state (with explicit include/exclude lists). Finally, batching is represented at the record
level by treating \texttt{data} as an opaque payload: a packet may carry a single item or a collection (e.g.,
a list of texts for embeddings), allowing batch-aware operators to exploit vectorized execution without
changing the streaming interface. Exceptions raised inside operator logic propagate to the task boundary,
where the configured recovery strategy (restart- or checkpoint-based) can be applied.

\subsection{Compilation and Runtime Execution}
\label{subsec:dd_runtime}

\paragraph{Compilation.}
The system compiles logical dataflows into physical execution plans, resolving operator placement (CPU vs
GPU) and parallelism strategies.
Compilation performs two core rewrites that bridge the logical DAG and the physical execution plan.
First, each logical operator is expanded into \emph{parallel task replicas} according to its configured
parallelism. Second, each logical edge between an upstream operator (parallelism $m$) and a downstream
operator (parallelism $n$) is materialized as an all-to-all set of physical channels ($m\times n$), each
backed by a serializable queue descriptor. This expansion yields a concrete \emph{execution graph} whose
nodes are task contexts (operator replica + resources) and whose edges are concrete queue endpoints.

Placement constraints are expressed as part of the per-task context and translated by the placement
executor into the cluster scheduler's native hints. In the Ray-backed deployment, placement decisions are
compiled into \texttt{NodeAffinitySchedulingStrategy} (hard affinity) and resource requirements (e.g.,
			\texttt{num\_cpus}, \texttt{num\_gpus}, \texttt{memory}, and custom resources), enabling the compiler/scheduler
to co-locate GPU-heavy stages (e.g., decoding) while pinning CPU-heavy stages (e.g., retrieval/refinement)
to CPU nodes. Service-style components (e.g., middleware microservices) are compiled similarly but may
add explicit request/response queue mappings, enabling RPC-like interactions over the same queue substrate.

\paragraph{Scheduling and Queueing.}
Operators execute over bounded queues with backpressure. The runtime scheduler manages concurrency to
prevent slow stages (e.g., refinement) from starving latency-sensitive stages (e.g., decoding).
Scheduling is performed at the granularity of \emph{task replicas}: each physical operator instance is
executed as an independent worker that repeatedly dequeues input packets, applies operator logic, and
routes outputs to downstream queues. Routing supports common dataflow patterns, including hash partitioning
and broadcast; the routing policy is chosen via the packet's partition annotations and determines how
packets are mapped to the $n$ downstream replicas.

Backpressure is enforced directly in the router via blocking enqueues on bounded queues: when a downstream
queue reaches capacity, the upstream \texttt{put} call blocks (with a finite timeout) instead of dropping
records, naturally propagating congestion backward through the pipeline. This mechanism is uniform across
local and distributed execution because both are expressed in terms of queue descriptors. The queue
descriptor abstraction also supports lazy initialization and serialization, which is crucial for deploying
graphs across processes/nodes; the runtime pre-initializes descriptors before launching workers to avoid
deadlocks caused by concurrent, cross-actor queue initialization.

\paragraph{Distributed Execution.}
The runtime supports local and distributed deployment via a unified queue abstraction, allowing seamless
scaling from a single node to a cluster.
In local mode, the pipeline is executed within a single process using the same execution graph abstraction
as in the distributed setting. In distributed mode, task replicas are deployed as Ray actors/workers, and
queue descriptors are serialized and shipped to the appropriate workers, providing a uniform channel API
regardless of placement. The dispatcher orchestrates the deployment by (i) constructing the physical graph,
(ii) applying the placement executor's decisions, and (iii) coordinating lifecycle signals such as
termination and end-of-stream propagation (including join-specific stop-signal handling).

Failure recovery is configured through a fault-tolerance strategy factory that can instantiate restart-based
policies (e.g., fixed-delay, exponential backoff, or failure-rate aware restarts) or checkpoint-based
policies. Checkpointing leverages the operator-level \texttt{get\_state}/\texttt{restore\_state} contract to
materialize recoverable state snapshots, while restart policies provide best-effort recovery for stateless
or externally checkpointed stages. Together, these mechanisms allow the same application definition to scale
from single-node debugging to cluster execution with explicit recovery semantics.

\subsection{Inference Components as First-Class Operators}
\label{subsec:dd_components}

SAGE integrates critical inference subsystems directly into the middleware layer (L4) as composable
operators, avoiding the overhead and opacity of external black-box services.

\paragraph{Vector Storage and Search (SageDB).}
A C++ vector database core providing pluggable ANNS algorithms and metadata filtering.
SageDB stores float32 vectors and associates each vector with a 64-bit identifier and a string-valued
metadata map. Its configuration exposes both algorithm selection and index hyperparameters. The index type
is chosen from \texttt{FLAT} (exact), \texttt{IVF\_FLAT}, \texttt{IVF\_PQ}, \texttt{HNSW}, or \texttt{AUTO},
and the distance metric can be \texttt{L2}, \texttt{INNER\_PRODUCT}, or \texttt{COSINE}. For IVF-based indices,
the configuration includes \texttt{nlist} (cluster count), \texttt{m} (sub-quantizers), and \texttt{nbits}; for
HNSW, it includes \texttt{M} and \texttt{efConstruction}. Query-time parameters are specified via
			\texttt{SearchParams} (e.g., \texttt{k}, \texttt{nprobe}, optional \texttt{radius}, and whether to include
metadata), and SageDB additionally supports metadata-aware filtering through a predicate-based
			\texttt{filtered\_search} interface.

The API provides single and batch insertion (\texttt{add}/\texttt{add\_batch}), updates and deletions
(\texttt{update}/\texttt{remove}), and batch querying. Persistence is supported through explicit
			\texttt{save}/\texttt{load} calls that serialize the vector store, metadata store, and index state to a
user-provided path, enabling offline index construction and warm-started deployment.

\paragraph{Vector-Native Stream Processing (sageFlow).}
An engine for incrementally maintaining semantic state (e.g., document streams) using windowed vector
operations.
sageFlow provides streaming operators over vector-native records with explicit window semantics. Windows
are parameterized by a \texttt{WindowType} (\texttt{Tumbling} or \texttt{Sliding}), a \texttt{window\_size},
and (for sliding windows) a \texttt{slide\_size}. The corresponding \texttt{WindowOperator} implementations
maintain per-window buffers and guard window state with mutexes to support concurrent ingestion.

To handle out-of-order arrivals, sageFlow includes a Flink-style watermarking component
(\texttt{LateArrivalHandler}). The handler tracks the maximum observed event timestamp and advances the
watermark as \texttt{max\_seen\_timestamp - watermark\_delay}. Records are classified as on-time, late (within
an allowed lateness bound), or too-late (beyond allowed lateness), and late records can be buffered with
summary statistics for monitoring and tuning.

Beyond window aggregation, sageFlow exposes join operators whose behavior is configured via a rich
			\texttt{JoinStrategyConfig}. The configuration captures the join algorithm family (including brute force and
multiple ANN-accelerated strategies), partitioning strategy, and window state management. Join methods are
registered through a join-method registry, enabling pluggable algorithm implementations while preserving a
stable operator-level interface.

\paragraph{Structured Memory (NeuromMem).}
Backends for short-term session history and persistent semantic memory, supporting VDB, KV, and Graph
storage.
NeuromMem exposes a uniform store/recall interface across multiple memory backends. At the service level,
memory backends implement \texttt{insert(entry, vector, metadata, ...)} and
			\texttt{retrieve(query, vector, metadata, top\_k, ...)}; deletion is supported via \texttt{delete(entry\_id)}.
For example, the short-term memory service maintains a bounded FIFO queue of recent dialogue entries and
stores them in a VDB collection; it can retrieve either (i) semantically similar entries via a vector index
(\texttt{global\_index}) or (ii) the most recent entries in temporal order when no vector query is provided.
The VDB service generalizes this model to multiple pre-existing collections and supports both ``passive''
insertion (default collection) and ``active'' insertion (explicit target collection and priority hints).

The gateway binds memory to sessions and selects a backend based on access patterns: \emph{short-term}
storage provides low-latency recency windows; \emph{VDB} enables semantic recall through similarity search;
\emph{KV} supports fast exact-match retrieval (e.g., by keys/attributes); and \emph{Graph} captures relational
structure for reasoning over entities and links. This design allows the same conversational interface to
mix recency, semantic similarity, and symbolic lookup within a single pipeline.

\paragraph{Context Refinement (sageRefiner).}
Operators for compressing and distilling retrieved context to optimize token budgets.
sageRefiner is implemented as a middleware service plus a pipeline operator adapter. The operator consumes
the user query and raw retrieval results and produces a refined document set (\texttt{refining\_results})
that is subsequently fed into the generator/decoder stage. This placement makes refinement an explicit
pipeline stage that can be profiled and scheduled independently.

Algorithm selection is handled dynamically by \texttt{RefinerService}. The current implementation supports
three modes: \texttt{LONG\_REFINER} (a long-context refinement algorithm), \texttt{SIMPLE} (lightweight
compression), and \texttt{NONE} (pass-through). To reduce repeated computation, the service optionally
enables an LRU cache with TTL, keyed by the tuple \{query, documents, budget\}; it also records per-request
metrics such as refine time and token counts, which can be surfaced in benchmark traces.

\paragraph{Time-Series Operators (sageTSDB).}
Primitives for windowing and joining out-of-order streams, enabling temporal analytics alongside LLM
reasoning.
sageTSDB adopts an event-time model in which each data point is a \texttt{TimeSeriesData} record carrying a
millisecond timestamp, a scalar or vector value, and optional \texttt{tags}/\texttt{fields} maps. Queries are
specified by an explicit \texttt{TimeRange} (with automatic conversion from \texttt{datetime}) and optional
tag predicates; results can be further processed via window-based aggregations.

For out-of-order ingestion, the default stream-join algorithm buffers each input stream and maintains a
watermark defined as \texttt{latest\_timestamp - max\_delay}. Records with timestamps not exceeding the
watermark are considered ready for processing, while later records remain buffered until the watermark
advances. Join semantics are window-based: two records are eligible to join when
$|t_\ell - t_r| \leq \texttt{window\_size}$, with optional equi-join on a tag key (hash join) or a fallback
nested-loop join.

Window aggregation supports tumbling, sliding, and session windows. Tumbling windows align timestamps to
fixed boundaries; sliding windows emit overlapping windows controlled by \texttt{slide\_interval}; and
session windows merge events separated by gaps no larger than \texttt{session\_gap}. Aggregations include
			\texttt{sum}, \texttt{avg}, \texttt{min}, \texttt{max}, \texttt{count}, and related functions.

\subsection{Gateway and Unified Control Plane}
\label{subsec:dd_control_plane}

\paragraph{Gateway Data Plane.}
The SAGE Gateway exposes OpenAI-compatible endpoints while managing stateful sessions. It implements a
\emph{Pipeline-as-a-Service} architecture for RAG requests, keeping the pipeline resident in memory to avoid
re-initialization overhead.
The gateway's data plane implements a queue-based \emph{bridge} between HTTP request handlers and a
persistent in-process SAGE pipeline. Each incoming chat request is packaged into a payload and submitted to
a \texttt{PipelineBridge}; submission returns a per-request response queue. A long-lived pipeline job
(``Source $\rightarrow$ Map $\rightarrow$ Sink'') continuously polls the bridge for new requests, performs
retrieval and generation, and pushes results back to the corresponding response queue. The HTTP handler then
blocks on \texttt{response\_queue.get(timeout)} to deliver the response (or a timeout error) to the caller.
This design isolates the pipeline's execution thread(s) from the web server event loop while preserving a
simple request/response abstraction.

Session state is managed by a \texttt{SessionManager} with pluggable persistence backends. By default,
sessions are stored as JSON under \texttt{\$HOME/.sage/gateway/sessions.json}. An alternative backend uses
NeuromMem storage engines (\texttt{TextStorage} + \texttt{MetadataStorage}) and persists under
			\texttt{\$HOME/.sage/gateway/neuromem\_sessions/}, enabling session recovery across gateway restarts without
external services. In addition, the session layer can bind a per-session memory backend
(short-term/VDB/KV/Graph) to support retrieval-augmented and memory-augmented conversations.

\paragraph{Unified Control Plane (sageLLM).}
For hybrid workloads, the control plane coordinates multiple LLM and embedding backends. It handles service
discovery, health monitoring, and policy-driven routing (e.g., separating prefill/decode or prioritizing
latency-critical requests).
The control plane represents each backend as an execution instance with an explicit instance type
(e.g., LLM-only, embedding-only, or mixed). Incoming requests are first classified by a request classifier
using a content-driven priority rule: explicit request type overrides inference; otherwise, non-empty
			\texttt{embedding\_texts} implies an embedding request, and the default falls back to chat-style LLM requests
for backward compatibility. The classifier then filters candidate instances by compatibility, ensuring that
embedding requests are routed only to embedding-capable instances while excluding embedding-only instances
from LLM scheduling.

Hybrid workload handling is implemented by \texttt{HybridSchedulingPolicy}. The policy groups pending
requests by type, batches embedding requests by model (target \texttt{embedding\_batch\_size} with a bounded
wait time), and schedules embeddings to specialized instances when available (optionally falling back to
mixed instances). LLM requests are scheduled via a configurable fallback policy (FIFO, priority, SLO-aware,
or adaptive), and the overall ordering between embeddings and LLMs is controlled by an explicit embedding
priority setting.

Engine lifecycle is managed by the control plane manager through explicit engine registration
(\texttt{register\_engine} with \{engine\_id, model\_id, host, port, engine\_kind\}) and background health
checks. When enabled, the manager performs auto-restart with bounded retry counts and exponential backoff
once consecutive failures exceed a threshold. The same manager optionally enables prefill/decode separation
via a dedicated routing strategy, allowing policy-controlled isolation between latency-sensitive decoding
and throughput-oriented prefilling.

\subsection{Benchmark Harness}
\label{subsec:dd_benchmark}

SAGE includes a dual-view benchmark suite:
(1) \textbf{System Metrics}: Throughput, TTFT/TBT, tail latency, and SLO compliance.
(2) \textbf{Agent Metrics}: Tool selection accuracy, planning quality, and timing.
This allows users to measure the cost of algorithmic improvements (e.g., better planning) on system load.
System-level benchmarks generate mixed OpenAI-compatible traffic against the gateway endpoints
			\texttt{/v1/chat/completions} (LLM) and \texttt{/v1/embeddings} (embedding). Workloads are parameterized by a
total request count, a request rate, an LLM/embedding mix ratio, and an arrival process
(\texttt{constant}, \texttt{poisson}, or \texttt{bursty}); the generator produces per-request payloads from a
fixed prompt/text pool to provide deterministic, reproducible load. Experiments report throughput and
latency percentiles (p50/p95/p99) computed over successful requests.

SLO compliance is defined in terms of per-type p99 latency thresholds. The benchmark configuration exposes
			\texttt{slo\_chat\_p99\_ms} and \texttt{slo\_embedding\_p99\_ms} (defaulting to 500ms and 100ms,
respectively), and the experiment computes an overall satisfaction rate as the fraction of successful
requests whose end-to-end latency meets the threshold of their request type.

For agent evaluation, SAGE includes three JSONL split files for tool selection, task planning, and timing
judgment under \texttt{packages/sage-benchmark/src/sage/data/sources/agent\_benchmark/splits/}. Tool
selection uses a tool catalog stored as \texttt{tool\_catalog.jsonl} (loaded by \texttt{SageToolsLoader})
under \texttt{packages/sage-benchmark/src/sage/data/sources/agent\_tools/data/}. Runtime-generated benchmark
artifacts (prepared splits, candidate pools, and results) are written to the repository-local
			\texttt{.sage/benchmark/\{data,results\}/} directories, which are treated as ephemeral and are not committed.