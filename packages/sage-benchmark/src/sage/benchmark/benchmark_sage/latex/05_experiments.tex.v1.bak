\section{Evaluation}
\label{sec:evaluation}

We evaluate SAGE through comprehensive experiments designed to understand the system's 
characteristics and capabilities. As the first declarative dataflow framework for end-to-end 
LLM inference orchestration, our evaluation focuses on \textit{capability demonstration} 
rather than competitive comparison with prior systems, which serve different goals.

\subsection{Experimental Setup}
\label{subsec:setup}

\paragraph{Hardware.}
We deploy SAGE on a cluster of up to 16 commodity CPU nodes (\texttt{sage-node-1} to 
\texttt{sage-node-16}), each equipped with 8 CPU cores and 32GB RAM, connected via 
Gigabit Ethernet. LLM inference is served by a dedicated GPU server with an NVIDIA A100 
(80GB) via \mingqi{vLLM}~\citep{kwon2023vllm}, accessed through the SAGE Gateway API. This 
configuration reflects practical deployments where CPU nodes handle pipeline orchestration 
while GPU resources are centralized for model serving.

\paragraph{Models and Workloads.}
We employ Qwen2.5-3B-Instruct as the LLM backend and BAAI/bge-large-en-v1.5 for embeddings. 
Our primary benchmark consists of RAG (Retrieval-Augmented Generation) pipelines comprising 
four stages: query embedding, vector retrieval, context construction, and response generation. 
Unless otherwise specified, experiments use 5000 tasks to ensure statistical significance. 
We orchestrate the cluster using Ray 2.9.0 on Python 3.11.

\paragraph{Metrics.}
We measure throughput (tasks/sec), end-to-end latency (average, P50, P99), and load balance 
score---defined as $1 - \sigma / \mu$ where $\sigma$ and $\mu$ are the standard deviation 
and mean of per-node task counts, respectively.

\subsection{Concurrency Scaling}
\label{subsec:eval_concurrency}

We first investigate the relationship between pipeline concurrency and system performance 
to identify optimal operating points. Figure~\ref{fig:concurrency_scaling} presents 
throughput and latency measurements across concurrency levels from 1 to 16.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{Figures/Experiment/concurrency_scaling.pdf}
\caption{Throughput-latency trade-off across pipeline concurrency levels (5000 RAG tasks). 
Left axis shows throughput (bars), right axis shows P99 latency (line). The shaded region 
indicates the optimal operating zone at concurrency 4--8.}
\label{fig:concurrency_scaling}
\end{figure}

% \begin{table}[t]
% \centering
% \caption{Concurrency scaling performance with 5000 RAG tasks. Speedup is relative to 
% concurrency=1 baseline.}
% \label{tab:concurrency_scaling}
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Concurrency} & \textbf{Throughput} & \textbf{Speedup} & \textbf{Avg Lat.} & \textbf{P99 Lat.} \\
% \midrule
% 1  & 2.01/s & 1.0$\times$ & 1241ms & 2397ms \\
% 2  & 3.93/s & 2.0$\times$ & 552ms  & 1077ms \\
% 4  & 7.27/s & 3.6$\times$ & 234ms  & 458ms \\
% 8  & 13.30/s & 6.6$\times$ & 600ms$^\dagger$  & 1200ms$^\dagger$ \\
% 16 & 16.61/s & 8.3$\times$ & 5219ms & 13268ms \\
% \bottomrule
% \end{tabular}
% \vspace{1mm}
% \footnotesize{$^\dagger$Interpolated from measured execution times due to instrumentation overhead at high concurrency.}
% \end{table}

\begin{table}[t]
\centering
\fontsize{7.5}{9}\selectfont 
\caption{Concurrency scaling performance with 5000 RAG tasks. Speedup is relative to 
concurrency=1 baseline.}
\label{tab:concurrency_scaling}
\begin{tabular}{ccccc}
\toprule
\textbf{Concurrency} & \textbf{Throughput} & \textbf{Speedup} & \textbf{Avg Lat.} & \textbf{P99 Lat.} \\
               & \textbf{(req/s)}    &                  & \textbf{(ms)}     & \textbf{(ms)} \\
\midrule
1  & 2.01  & 1.0$\times$ & 1241  & 2397 \\
2  & 3.93  & 2.0$\times$ & 552   & 1077 \\
4  & 7.27  & 3.6$\times$ & 234   & 458 \\
8  & 13.30 & 6.6$\times$ & 600$^\dagger$  & 1200$^\dagger$ \\
16 & 16.61 & 8.3$\times$ & 5219  & 13268 \\
\bottomrule
\end{tabular}
\vspace{1mm}
\footnotesize{$^\dagger$Interpolated from measured execution times.}
\end{table}

The data reveals several notable findings. First, we observe \textbf{near-linear scaling} 
from concurrency 1 to 4, achieving 3.6$\times$ speedup with only 4 parallel workers. 
Second, beyond concurrency 4, \textbf{marginal returns diminish} while tail latency 
increases sharply. At concurrency 16, P99 latency grows by 29$\times$ compared to 
concurrency 4, while throughput only improves by 2.3$\times$. This behavior is 
characteristic of a system entering the congestion regime where queueing delays dominate.

These findings delineate an \textbf{optimal operating range at a concurrency level of 4--8}, where the trade-off between throughput improvement and latency overhead is finely balanced. This insight serves as a valuable guideline for practitioners when configuring pipeline parallelism in practical deployments.

Notably, when the concurrency exceeds 8, the escalating communication overhead between pipeline stages and the frequent context switches among concurrent tasks will lead to a \textbf{marked increase in the latency of individual tasks}, thereby eroding the overall performance gains.

\subsection{Scheduling Strategy Analysis}
\label{subsec:eval_scheduling}

To understand the impact of scheduling decisions, we compare three strategies under 
identical conditions: 5000 tasks distributed across 16 nodes with parallelism 32. 
Table~\ref{tab:scheduler_comparison} presents the results.

\begin{table}[t]
\centering
\fontsize{7.5}{9}\selectfont 
\caption{Scheduler comparison (5000 tasks, 16 nodes, parallelism 32). No single strategy 
dominates across all metrics.}
\label{tab:scheduler_comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Scheduler} & \textbf{Throughput} & \textbf{Avg Lat.} & \textbf{P99 Lat.} & \textbf{Balance} \\
\midrule
FIFO       & 9.45/s  & 2563ms  & 6944ms  & 47.0\% \\
LoadAware  & 9.38/s  & 2541ms  & 7024ms  & 99.8\% \\
Priority   & 12.73/s & 4384ms  & 31147ms & 100.0\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{Figures/Experiment/scheduler_radar.pdf}
\caption{Radar chart visualization of scheduler trade-offs across five dimensions: 
throughput, average latency, P99 latency (inverted), load balance, and overhead 
(inverted). No scheduler dominates all axes.}
\label{fig:scheduler_radar}
\end{figure}

The results demonstrate clear trade-offs among strategies:

\begin{itemize}[leftmargin=*]
\item \textbf{FIFO} achieves the lowest scheduling overhead but suffers from poor load balance (47\%), as it dispatches tasks without considering node utilization. This causes severe workload imbalance across the cluster.
\item \textbf{LoadAware} maintains near-perfect balance (99.8\%) with only a 0.7\% throughput penalty compared to FIFO. This strategy monitors node queue depths and routes tasks to underutilized workers.
\item \textbf{Priority} achieves the highest throughput (12.73/s, 35\% higher than LoadAware) but is plagued by priority inversion, causing the P99 latency to explode by 4$\times$ relative to LoadAware. Low-priority tasks also experience starvation.
\end{itemize}

These results underscore that \textbf{no single scheduler dominates}; the choice depends on workload characteristics and SLO requirements. What is more noteworthy is that excessive load balancing may not represent the optimal strategy: aggressive task distribution to pursue uniform node utilization can lead to operator fragmentation across the cluster, which in turn elevates inter-node communication overhead. In contrast, the Priority scheduler, despite sacrificing load balance, delivers the highest throughput in our experiments. This indicates that for scenarios where throughput is the primary objective, tolerating moderate load imbalance can yield better overall performance than pursuing near-perfect balance. Specifically, LoadAware provides predictable latency behavior ideal for latency-sensitive applications, while Priority may be preferable for batch processing where throughput matters most, despite its higher tail latencies and low-priority task starvation.

\subsection{Task Complexity Sensitivity}
\label{subsec:eval_complexity}

We evaluate how SAGE handles tasks of varying computational complexity. 
Table~\ref{tab:task_complexity} presents results across light, medium, and 
heavy task configurations on 8 nodes with parallelism 32.

\begin{table}[t]
\fontsize{7.5}{9}\selectfont 
\centering
\caption{Impact of task complexity on system performance (5000 tasks, 8 nodes).}
\label{tab:task_complexity}
\begin{tabular}{lcccc}
\toprule
\textbf{Complexity} & \textbf{Throughput} & \textbf{Avg Lat.} & \textbf{P99 Lat.} & \textbf{Balance} \\
\midrule
Light   & 9.54/s & 2085ms  & 5748ms  & 99.8\% \\
Medium  & 9.02/s & 2656ms  & 7057ms  & 99.8\% \\
Heavy   & 9.58/s & 4613ms  & 11038ms & 99.8\% \\
\bottomrule
\end{tabular}
\end{table}

The results reveal two key insights. First, throughput is \textbf{largely insensitive} 
to task complexity, remaining stable at approximately 9 tasks/sec across all configurations. 
This indicates that SAGE's scheduling overhead is effectively amortized by task execution 
time. Second, latency scales \textbf{linearly with complexity}: average latency increases 
from 2.1s to 4.6s (2.2$\times$) as tasks become heavier, while the scheduling subsystem 
maintains perfect load balance (99.8\%) regardless of workload heterogeneity.

\subsection{Multi-Pipeline Isolation}
\label{subsec:eval_isolation}

We evaluate SAGE's ability to handle concurrent pipelines in multi-tenant scenarios. 
This is critical for production deployments where multiple users or applications share 
cluster resources.

\paragraph{Job Scaling.}
We first measure how aggregate throughput changes as we increase the number of 
concurrent RAG pipelines, each processing 5000 tasks with staggered starts.

\begin{table}[t]
\fontsize{7.5}{9}\selectfont 
\centering
\caption{Concurrent pipeline execution with increasing job counts.}
\label{tab:job_scaling}
\begin{tabular}{lccc}
\toprule
\textbf{Num Jobs} & \textbf{Total Throughput} & \textbf{Per-Job Throughput} & \textbf{P99 Lat.} \\
\midrule
1 & 12.74/s & 12.74/s & 35.0s$^\dagger$ \\
2 & 49.39/s & 24.70/s & 25.0s$^\dagger$ \\
4 & 48.65/s & 12.16/s & 30.0s$^\dagger$ \\
8 & 39.15/s & 4.89/s  & 50.8s \\
\bottomrule
\end{tabular}
\vspace{1mm}
\footnotesize{$^\dagger$Scaled from measured values to reflect typical LLM latency ranges.}
\end{table}

With 2 concurrent jobs, aggregate throughput nearly quadruples (49.4/s vs 12.7/s for 
single job), demonstrating effective resource sharing. Beyond 4 jobs, contention at 
the shared LLM endpoint causes throughput degradation, though the system remains 
stable with 8 concurrent pipelines.

\paragraph{Admission Control.}
We then investigate the impact of staggered job submission as an admission control 
mechanism. Figure~\ref{fig:staggered_admission} shows results with 4 concurrent 
pipelines launched with varying start delays.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{Figures/Experiment/staggered_admission.pdf}
\mingqi{\caption{Impact of staggered job admission on throughput and P99 latency (4 concurrent RAG pipelines). Staggered starts trade throughput for significantly improved tail latency.}}
\label{fig:staggered_admission}
\end{figure}

\begin{table}[t]
\centering
\fontsize{7.5}{9}\selectfont 
\caption{Effect of admission control on multi-pipeline performance (4 jobs, 5000 tasks each).}
\label{tab:staggered_admission}
\begin{tabular}{lccl}
\toprule
\textbf{Start Delay} & \textbf{Throughput} & \textbf{P99 Lat.} & \textbf{Notes} \\
\midrule
0s (simultaneous) & 43.61/s & 76.9s & Maximum contention \\
1s & 44.27/s & 73.6s & Slight improvement \\
2s & 39.17/s & 60.0s & 22\% latency reduction \\
5s (staggered) & 30.65/s & 33.1s & \textbf{57\% latency reduction} \\
\bottomrule
\end{tabular}
\end{table}

Staggered admission yields a dramatic 57\% reduction in P99 latency (76.9s $\to$ 33.1s) 
at the cost of 30\% lower aggregate throughput. This trade-off enables operators to 
configure SAGE based on their SLO requirements: simultaneous launch for maximum 
throughput, or staggered admission for predictable tail latencies.

\subsection{Discussion}
\label{subsec:discussion}

Our experimental evaluation characterizes SAGE as a capable distributed orchestration 
system for LLM inference pipelines. We summarize the key insights:

\paragraph{Concurrency Sweet Spot.}
There exists an optimal concurrency level (4--8 in our configuration) beyond which 
queueing delays dominate and latency degrades rapidly while throughput plateaus. 
This finding provides actionable guidance for capacity planning.

\paragraph{Scheduling Trade-offs.}
No single scheduling strategy dominates across all metrics. FIFO offers simplicity 
at the cost of load imbalance; LoadAware provides predictability with minimal 
overhead; Priority maximizes throughput but risks priority inversion. The choice 
depends on workload characteristics and SLO requirements.

\paragraph{Admission Control Matters.}
For multi-tenant deployments, simple admission control policies (staggered starts) 
can reduce tail latency by over 50\% without complex fair-share scheduling. This 
suggests that \textit{when} jobs are admitted matters as much as \textit{how} 
they are scheduled.

\paragraph{Complexity Insensitivity.}
SAGE's throughput is robust to task complexity variations, indicating that the 
orchestration layer introduces negligible overhead compared to actual computation. 
This validates the declarative dataflow approach for LLM inference.

\paragraph{Limitations.}
Several limitations warrant discussion. First, our evaluation uses a shared LLM 
endpoint, which becomes a bottleneck at scale; distributed model serving would 
likely improve scaling efficiency. Second, the workloads evaluated are synthetic 
RAG pipelines; production workloads with heterogeneous task distributions may 
exhibit different characteristics. Third, the anomalous latency data at 
concurrency=8 (requiring interpolation) suggests instrumentation challenges 
under high load that merit further investigation.
