"""
Pipeline B: Long Context Refiner (é•¿æ–‡æœ¬ç²¾ç‚¼)
============================================

æ‹“æ‰‘: Source â†’ FlatMap(Chunking) â†’ Filter(Relevance) â†’ Map(Embedding) â†’ Future(Summarize) â†’ Aggregate â†’ Sink

ç®—å­:
- Source: åŠ è½½é•¿æ–‡æ¡£æ•°æ®é›† (LoCoMo)
- FlatMap (Chunking): å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªå—
- Filter (Relevance): è¿‡æ»¤ä½ç›¸å…³æ€§çš„å—
- Map (Embedding): å¯¹æ¯ä¸ªå—è¿›è¡Œå‘é‡åŒ–
- Map (Summarize): è°ƒç”¨ LLM å¯¹æ¯ä¸ªå—ç”Ÿæˆæ‘˜è¦ (Future è¯­ä¹‰)
- Sink (Aggregate): åˆå¹¶æ‰€æœ‰å—çš„æ‘˜è¦å¹¶è¾“å‡º

æ•°æ®é›†: LoCoMo
"""

from __future__ import annotations

import os
import time
from dataclasses import dataclass, field
from typing import Any, Optional

# ç¦ç”¨ä»£ç†ï¼Œç¡®ä¿å†…ç½‘æœåŠ¡å¯è®¿é—®
os.environ.pop("http_proxy", None)
os.environ.pop("HTTP_PROXY", None)
os.environ.pop("https_proxy", None)
os.environ.pop("HTTPS_PROXY", None)

import httpx

from sage.common.core import (
    FilterFunction,
    FlatMapFunction,
    MapFunction,
    SinkFunction,
    SourceFunction,
)
from sage.kernel.api import RemoteEnvironment

from .scheduler import HeadNodeScheduler


@dataclass
class RefinerConfig:
    """Refiner Pipeline é…ç½®"""
    # æ•°æ®é›†
    num_samples: int = 50
    min_context_length: int = 8000

    # Chunking
    chunk_size: int = 2000
    chunk_overlap: int = 200

    # Filter
    relevance_threshold: float = 0.3

    # æ¨¡å‹
    embedding_model: str = "BAAI/bge-m3"
    llm_model: str = "Qwen/Qwen2.5-7B-Instruct"

    # æœåŠ¡ç«¯ç‚¹
    embedding_base_url: str = "http://localhost:8090/v1"
    llm_base_url: str = "http://localhost:8001/v1"

    # è¿è¡Œæ—¶
    job_manager_host: str = "localhost"
    job_manager_port: int = 19001
    request_timeout: float = 120.0


@dataclass
class Chunk:
    """æ–‡æ¡£å—"""
    doc_id: int
    chunk_index: int
    text: str
    query: str
    embedding: list[float] = field(default_factory=list)
    relevance_score: float = 0.0
    summary: str = ""


# ============================================================================
# Source: æ•°æ®é›†åŠ è½½
# ============================================================================


class RefinerSourceFunction(SourceFunction):
    """Refiner Source: ä» LoCoMo æ•°æ®é›†åŠ è½½é•¿æ–‡æ¡£"""

    def __init__(
        self,
        num_samples: int = 50,
        min_context_length: int = 8000,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.num_samples = num_samples
        self.min_context_length = min_context_length
        self._data: list[dict] = []
        self._index = 0
        self._loaded = False

    def _load_data(self) -> None:
        """åŠ è½½æ•°æ®é›†"""
        if self._loaded:
            return

        from sage.data.sources.locomo.dataloader import LocomoDataLoader
        loader = LocomoDataLoader()
        raw_data = loader.load()

        # è¿‡æ»¤å‡ºé•¿æ–‡æ¡£
        self._data = [
            d for d in raw_data
            if len(d.get("context", d.get("conversation", ""))) >= self.min_context_length
        ][: self.num_samples]

        self._loaded = True
        print(f"ğŸ“‚ Loaded {len(self._data)} long documents from LoCoMo")

    def execute(self, data: Any = None) -> Optional[dict]:
        """è¿”å›ä¸‹ä¸€ä¸ªé•¿æ–‡æ¡£"""
        self._load_data()

        if self._index >= len(self._data):
            return None

        sample = self._data[self._index]
        self._index += 1

        return {
            "doc_id": self._index,
            "query": sample.get("question", sample.get("query", "")),
            "context": sample.get("context", sample.get("conversation", "")),
            "ground_truth": sample.get("answer", ""),
        }


# ============================================================================
# FlatMap (Chunking): æ–‡æ¡£åˆ†å—
# ============================================================================


class ChunkingFlatMapFunction(FlatMapFunction):
    """FlatMap (Chunking): å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªå—"""

    def __init__(
        self,
        chunk_size: int = 2000,
        chunk_overlap: int = 200,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def execute(self, data: dict) -> list[Chunk]:
        """æ‰§è¡Œåˆ†å—"""
        doc_id = data["doc_id"]
        query = data["query"]
        context = data["context"]

        chunks = []
        step = self.chunk_size - self.chunk_overlap

        for i, start in enumerate(range(0, len(context), step)):
            end = min(start + self.chunk_size, len(context))
            chunk_text = context[start:end]

            chunks.append(Chunk(
                doc_id=doc_id,
                chunk_index=i,
                text=chunk_text,
                query=query,
            ))

        print(f"ğŸ“„ Doc {doc_id}: Split into {len(chunks)} chunks")
        return chunks


# ============================================================================
# Filter (Relevance): ç›¸å…³æ€§è¿‡æ»¤
# ============================================================================


class RelevanceScoreMapFunction(MapFunction):
    """Map (RelevanceScore): è®¡ç®—å—çš„ç›¸å…³æ€§åˆ†æ•°"""

    def execute(self, chunk: Chunk) -> Chunk:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        query_terms = set(chunk.query.lower().split())
        chunk_terms = set(chunk.text.lower().split())

        overlap = len(query_terms & chunk_terms)
        chunk.relevance_score = overlap / (len(query_terms) + 1)
        return chunk


class RelevanceFilterFunction(FilterFunction):
    """Filter (Relevance): è¿‡æ»¤ä½ç›¸å…³æ€§çš„å—

    æ³¨æ„: FilterFunction.execute() åº”è¯¥è¿”å› boolï¼Œè¡¨ç¤ºæ•°æ®æ˜¯å¦é€šè¿‡è¿‡æ»¤ã€‚
    æ•°æ®æœ¬èº«ä¸ä¼šè¢«ä¿®æ”¹ã€‚å¦‚æœéœ€è¦åœ¨è¿‡æ»¤å‰è®¡ç®—åˆ†æ•°ï¼Œåº”è¯¥å…ˆä½¿ç”¨ MapFunctionã€‚
    """

    def __init__(self, relevance_threshold: float = 0.3, **kwargs):
        super().__init__(**kwargs)
        self.relevance_threshold = relevance_threshold

    def execute(self, chunk: Chunk) -> bool:
        """æ‰§è¡Œç›¸å…³æ€§è¿‡æ»¤: è¿”å› True è¡¨ç¤ºé€šè¿‡, False è¡¨ç¤ºè¿‡æ»¤æ‰"""
        return chunk.relevance_score >= self.relevance_threshold


# ============================================================================
# Map (Embedding): å—å‘é‡åŒ–
# ============================================================================


class ChunkEmbeddingMapFunction(MapFunction):
    """Map (Embedding): å¯¹æ¯ä¸ªå—è¿›è¡Œå‘é‡åŒ–"""

    def __init__(
        self,
        embedding_base_url: str = "http://localhost:8090/v1",
        embedding_model: str = "BAAI/bge-m3",
        timeout: float = 60.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.embedding_base_url = embedding_base_url
        self.embedding_model = embedding_model
        self.timeout = timeout

    def execute(self, chunk: Chunk) -> Chunk:
        """æ‰§è¡Œ embedding"""
        with httpx.Client(timeout=self.timeout) as client:
            response = client.post(
                f"{self.embedding_base_url}/embeddings",
                json={"input": chunk.text[:1000], "model": self.embedding_model},
            )
            response.raise_for_status()
            result = response.json()

        chunk.embedding = result["data"][0]["embedding"]
        return chunk


# ============================================================================
# Map (Summarize): æ‘˜è¦ç”Ÿæˆ
# ============================================================================


class SummarizeMapFunction(MapFunction):
    """Map (Summarize): è°ƒç”¨ LLM å¯¹æ¯ä¸ªå—ç”Ÿæˆæ‘˜è¦"""

    def __init__(
        self,
        llm_base_url: str = "http://localhost:8001/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        timeout: float = 120.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.timeout = timeout

    def execute(self, chunk: Chunk) -> Chunk:
        """æ‰§è¡Œæ‘˜è¦ç”Ÿæˆ"""
        prompt = f"""Summarize the following text in 2-3 sentences:

{chunk.text[:2000]}

Summary:"""

        with httpx.Client(timeout=self.timeout) as client:
            response = client.post(
                f"{self.llm_base_url}/chat/completions",
                json={
                    "model": self.llm_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 150,
                    "temperature": 0.5,
                },
            )
            response.raise_for_status()
            result = response.json()

        chunk.summary = result["choices"][0]["message"]["content"]
        return chunk


# ============================================================================
# Sink (Aggregate): åˆå¹¶æ‘˜è¦å¹¶è¾“å‡º
# ============================================================================


class RefinerSinkFunction(SinkFunction):
    """Refiner Sink: åˆå¹¶æ‘˜è¦å¹¶è¾“å‡ºç»“æœ

    æ³¨: è¿™é‡Œç”¨ Sink å®ç° Aggregate è¯­ä¹‰ï¼Œæ”¶é›†åŒä¸€æ–‡æ¡£çš„æ‰€æœ‰å—æ‘˜è¦ååˆå¹¶
    """

    def __init__(self, output_path: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.output_path = output_path
        self.doc_chunks: dict[int, list[Chunk]] = {}
        self.results: list[dict] = []

    def execute(self, chunk: Chunk) -> None:
        """æ”¶é›†å—å¹¶åˆå¹¶æ‘˜è¦"""
        doc_id = chunk.doc_id

        if doc_id not in self.doc_chunks:
            self.doc_chunks[doc_id] = []

        self.doc_chunks[doc_id].append(chunk)

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å—éƒ½å·²æ”¶é›†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼šæ¯æ¬¡æ”¶åˆ°ä¸€ä¸ªå—å°±è¾“å‡ºè¯¥å—çš„æ‘˜è¦ï¼‰
        print(f"âœ… Doc {doc_id} Chunk {chunk.chunk_index}: {chunk.summary[:50]}...")

        if self.output_path:
            import json
            result = {
                "doc_id": doc_id,
                "chunk_index": chunk.chunk_index,
                "query": chunk.query,
                "summary": chunk.summary,
            }
            with open(self.output_path, "a") as f:
                f.write(json.dumps(result, ensure_ascii=False) + "\n")


# ============================================================================
# Refiner Pipeline å°è£…
# ============================================================================


class RefinerPipeline:
    """Refiner Pipeline å°è£…ç±»"""

    def __init__(self, config: RefinerConfig):
        self.config = config
        self.env: Optional[RemoteEnvironment] = None

    def build(self) -> RemoteEnvironment:
        """æ„å»º Refiner Pipeline"""
        scheduler = HeadNodeScheduler()

        self.env = RemoteEnvironment(
            "refiner_pipeline",
            host=self.config.job_manager_host,
            port=self.config.job_manager_port,
            scheduler=scheduler,
        )

        # æ„å»º Pipeline: Source â†’ FlatMap â†’ Map(Score) â†’ Filter â†’ Map â†’ Map â†’ Sink
        (
            self.env.from_source(
                RefinerSourceFunction,
                num_samples=self.config.num_samples,
                min_context_length=self.config.min_context_length,
            )
            .flatmap(
                ChunkingFlatMapFunction,
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap,
            )
            .map(RelevanceScoreMapFunction)
            .filter(
                RelevanceFilterFunction,
                relevance_threshold=self.config.relevance_threshold,
            )
            .map(
                ChunkEmbeddingMapFunction,
                embedding_base_url=self.config.embedding_base_url,
                embedding_model=self.config.embedding_model,
                timeout=self.config.request_timeout,
            )
            .map(
                SummarizeMapFunction,
                llm_base_url=self.config.llm_base_url,
                llm_model=self.config.llm_model,
                timeout=self.config.request_timeout,
            )
            .sink(RefinerSinkFunction)
        )

        return self.env

    def run(self) -> dict:
        """è¿è¡Œ Pipeline"""
        if self.env is None:
            self.build()

        start_time = time.time()
        try:
            self.env.submit()
            time.sleep(10)  # é•¿æ–‡æ¡£å¤„ç†éœ€è¦æ›´å¤šæ—¶é—´
        finally:
            self.env.close()

        duration = time.time() - start_time
        return {
            "pipeline": "B (Refiner)",
            "duration_seconds": duration,
            "config": {
                "num_samples": self.config.num_samples,
                "chunk_size": self.config.chunk_size,
            },
        }
