"""
Workload 4 Pipeline Factory
============================

æ•´åˆæ‰€æœ‰ç®—å­ï¼Œæ„å»ºå®Œæ•´çš„ Workload 4 åˆ†å¸ƒå¼æ•°æ®æµã€‚

Pipeline ç»“æ„:
1. åŒæµæºï¼ˆQuery + Documentï¼‰
2. Embedding é¢„è®¡ç®—
3. Semantic Join (60s å¤§çª—å£)
4. åŒè·¯ VDB æ£€ç´¢ï¼ˆ4-stage eachï¼‰
5. å›¾éå†å†…å­˜æ£€ç´¢
6. ç»“æœæ±‡èš
7. DBSCAN èšç±»å»é‡
8. 5ç»´è¯„åˆ†é‡æ’åº
9. MMR å¤šæ ·æ€§è¿‡æ»¤
10. åŒå±‚ Batch èšåˆ
11. æ‰¹é‡ LLM ç”Ÿæˆ
12. Metrics Sink
"""

from __future__ import annotations

import time
from pathlib import Path
from typing import TYPE_CHECKING, Any

from sage.kernel.api.service import BaseService

if TYPE_CHECKING:
    from sage.kernel.api.local_environment import LocalEnvironment
    from sage.kernel.api.remote_environment import RemoteEnvironment

try:
    from .config import Workload4Config
    from .models import (
        QueryEvent,
        DocumentEvent,
        JoinedEvent,
        VDBRetrievalResult,
        GraphMemoryResult,
        ClusteringResult,
        RerankingResult,
        BatchContext,
        Workload4Metrics,
    )
    from .sources import (
        Workload4QuerySource,
        Workload4DocumentSource,
        EmbeddingPrecompute,
    )
    from .semantic_join import SemanticJoinOperator
    from .vdb_retrieval import (
        VDBRetriever,
        VDBResultFilter,
        LocalReranker,
        StageAggregator,
    )
    from .graph_memory import GraphMemoryRetriever
    from .clustering import DBSCANClusteringOperator
    from .reranking import MultiDimensionalReranker, MMRDiversityFilter
    from .batching import CategoryBatchAggregator, GlobalBatchAggregator
    from .generation import BatchLLMGenerator, Workload4MetricsSink
    from .union_operator import UnionCoMap
    # ğŸ”§ ä¸´æ—¶æ·»åŠ ï¼šå•æºæµ‹è¯•ç”¨çš„è½¬æ¢å™¨
    from .mappers import QueryToJoinedMapper
except ImportError:
    from config import Workload4Config
    from models import (
        QueryEvent,
        DocumentEvent,
        JoinedEvent,
        VDBRetrievalResult,
        GraphMemoryResult,
        ClusteringResult,
        RerankingResult,
        BatchContext,
        Workload4Metrics,
    )
    from sources import (
        Workload4QuerySource,
        Workload4DocumentSource,
        EmbeddingPrecompute,
    )
    from semantic_join import SemanticJoinOperator
    from vdb_retrieval import (
        VDBRetriever,
        VDBResultFilter,
        LocalReranker,
        StageAggregator,
    )
    from graph_memory import GraphMemoryRetriever
    from clustering import DBSCANClusteringOperator
    from reranking import MultiDimensionalReranker, MMRDiversityFilter
    from batching import CategoryBatchAggregator, GlobalBatchAggregator
    from generation import BatchLLMGenerator, Workload4MetricsSink
    from union_operator import UnionCoMap
    # ğŸ”§ ä¸´æ—¶æ·»åŠ ï¼šå•æºæµ‹è¯•ç”¨çš„è½¬æ¢å™¨
    from mappers import QueryToJoinedMapper


# =============================================================================
# Service Registration
# =============================================================================

def register_embedding_service(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> bool:
    """
    æ³¨å†Œ Embedding æœåŠ¡ã€‚
    
    ä½¿ç”¨è¿œç«¯ Embedding APIï¼ˆOpenAI å…¼å®¹ï¼‰ã€‚
    """
    try:
        from .services import EmbeddingService
        
        env.register_service(
            "embedding",
            EmbeddingService,
            base_url=config.embedding_base_url,
            model=config.embedding_model,
        )
        
        print(f"âœ“ Registered embedding_service: {config.embedding_base_url}")
        return True
        
    except Exception as e:
        print(f"âœ— Failed to register embedding_service: {e}")
        return False


def register_vdb_services(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> dict[str, bool]:
    """
    æ³¨å†ŒåŒè·¯ VDB æœåŠ¡ï¼ˆvdb1 å’Œ vdb2ï¼‰ã€‚
    
    ä½¿ç”¨çœŸå®çš„ FiQA æ•°æ®é›†ï¼ˆ57,638 æ–‡æ¡£ï¼Œ1024 ç»´ï¼‰ã€‚
    vdb1 å’Œ vdb2 å…±äº«ç›¸åŒçš„ FAISS ç´¢å¼•ï¼ˆfiqa_faiss.indexï¼‰ã€‚
    ç´¢å¼•å’Œæ–‡æ¡£å­˜å‚¨åœ¨ config.vdb_index_dirã€‚
    
    **æ•°æ®æº**ï¼š/home/sage/data/fiqa_faiss.index + fiqa_documents.jsonl
    """
    results = {}
    
    # Import Service class from module
    from .services import FAISSVDBService
    
    for vdb_name in ["vdb1", "vdb2"]:
        try:
            env.register_service(
                vdb_name,
                FAISSVDBService,
                vdb_name=vdb_name,
                dimension=config.embedding_dimension,
                index_dir=config.vdb_index_dir,
                dataset_name=config.vdb_dataset_name,
            )
            
            results[vdb_name] = True
            print(f"âœ“ Registered {vdb_name} (FiQA dataset, shared index)")
            
        except Exception as e:
            results[vdb_name] = False
            print(f"âœ— Failed to register {vdb_name}: {e}")
            import traceback
            traceback.print_exc()
    
    return results


def register_graph_memory_service(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> bool:
    """
    æ³¨å†Œå›¾å†…å­˜æœåŠ¡ã€‚
    
    ä½¿ç”¨ Mock å›¾ç»“æ„æˆ– NeuroMem Graph backendã€‚
    """
    # Import Service class from module
    from .services import GraphMemoryService
    
    try:
        env.register_service(
            "graph_memory",
            GraphMemoryService,
            max_depth=config.graph_max_depth,
            max_nodes=config.graph_max_nodes,
        )
        
        print(f"âœ“ Registered graph_memory_service")
        return True
        
    except Exception as e:
        print(f"âœ— Failed to register graph_memory_service: {e}")
        return False


def register_llm_service(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> bool:
    """
    æ³¨å†Œ LLM æœåŠ¡ã€‚
    
    ä½¿ç”¨è¿œç«¯ LLM APIï¼ˆOpenAI å…¼å®¹ï¼‰ã€‚
    """
    # Import Service class from module
    from .services import LLMService
    
    try:
        env.register_service(
            "llm",
            LLMService,
            base_url=config.llm_base_url,
            model=config.llm_model,
            max_tokens=config.llm_max_tokens,
        )
        
        print(f"âœ“ Registered llm_service: {config.llm_base_url}")
        return True
        
    except Exception as e:
        print(f"âœ— Failed to register llm_service: {e}")
        return False


def register_all_services(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> dict[str, bool]:
    """
    æ³¨å†Œæ‰€æœ‰å¿…è¦çš„ servicesã€‚
    
    Returns:
        æœåŠ¡æ³¨å†Œç»“æœå­—å…¸
    """
    results = {}
    
    print("\n" + "=" * 80)
    print("Registering Workload 4 Services")
    print("=" * 80)
    
    # 1. Embedding Service
    results["embedding"] = register_embedding_service(env, config)
    
    # 2. VDB Services (vdb1, vdb2)
    vdb_results = register_vdb_services(env, config)
    results.update(vdb_results)
    
    # 3. Graph Memory Service
    results["graph_memory"] = register_graph_memory_service(env, config)
    
    # 4. LLM Service
    results["llm"] = register_llm_service(env, config)
    
    print("=" * 80)
    print(f"Service Registration Summary: {sum(results.values())}/{len(results)} successful")
    print("=" * 80)
    
    return results


# =============================================================================
# Pipeline Factory
# =============================================================================


class Workload4Pipeline:
    """
    Workload 4 Pipeline å·¥å‚ã€‚
    
    æ•´åˆæ‰€æœ‰ç®—å­ï¼Œæ„å»ºå®Œæ•´çš„åˆ†å¸ƒå¼æ•°æ®æµã€‚
    """
    
    def __init__(self, config: Workload4Config):
        """
        åˆå§‹åŒ– Pipelineã€‚
        
        Args:
            config: Workload 4 é…ç½®
        """
        self.config = config
        self.env = None
        self.metrics = None
    
    def _create_environment(self, name: str):
        """åˆ›å»ºæ‰§è¡Œç¯å¢ƒï¼ˆæœ¬åœ°æˆ–è¿œç¨‹ï¼‰"""
        if self.config.use_remote:
            from pathlib import Path
            from sage.kernel.api.remote_environment import RemoteEnvironment
            
            # workload4 æ‰€åœ¨ç›®å½•ï¼ˆå½“å‰æ–‡ä»¶çš„çˆ¶ç›®å½•çš„çˆ¶ç›®å½•ï¼‰
            workload_dir = str(Path(__file__).parent.parent)
            
            # RemoteEnvironment å‚æ•°ï¼šname, config, host, port, scheduler, extra_python_paths
            env = RemoteEnvironment(
                name=name,
                scheduler=self.config.scheduler_type,  # "fifo" æˆ– "load_aware"
                extra_python_paths=[workload_dir],  # è®©è¿œç¨‹èŠ‚ç‚¹èƒ½æ‰¾åˆ° workload4 æ¨¡å—
            )
            return env
        else:
            from sage.kernel.api.local_environment import LocalEnvironment
            return LocalEnvironment(name=name)
    
    def build(self, name: str = "workload4_benchmark") -> Workload4Pipeline:
        """
        æ„å»ºå®Œæ•´ pipelineã€‚
        
        Pipeline ç»“æ„:
        1. åŒæµæºï¼ˆQuery + Documentï¼‰
        2. Embedding é¢„è®¡ç®—
        3. Semantic Join (60s å¤§çª—å£, parallelism=16)
        4. å›¾éå†å†…å­˜æ£€ç´¢
        5. åŒè·¯ VDB æ£€ç´¢ï¼ˆ4-stage eachï¼‰
        6. æ±‡èšæ‰€æœ‰æ£€ç´¢ç»“æœ
        7. DBSCAN èšç±»å»é‡
        8. 5ç»´è¯„åˆ†é‡æ’åº
        9. MMR å¤šæ ·æ€§è¿‡æ»¤
        10. åŒå±‚ Batch èšåˆ
        11. æ‰¹é‡ LLM ç”Ÿæˆ
        12. Metrics Sink
        
        Returns:
            self (æ”¯æŒé“¾å¼è°ƒç”¨)
        """
        print("\n" + "=" * 80)
        print("Building Workload 4 Pipeline")
        print("=" * 80)
        print(f"Pipeline Name: {name}")
        print(f"Use Remote: {self.config.use_remote}")
        print(f"Num Nodes: {self.config.num_nodes}")
        print(f"Num Tasks: {self.config.num_tasks}")
        print(f"Duration: {self.config.duration}s")
        print("=" * 80)
        
        # === 1. åˆ›å»ºç¯å¢ƒ ===
        # CRITICAL: Create environment in a local scope and build pipeline immediately
        # This avoids storing RemoteEnvironment in self, which causes serialization issues
        env = self._create_environment(name)
        print(f"âœ“ Created {'Remote' if self.config.use_remote else 'Local'}Environment")
        if self.config.use_remote:
            print(f"  Scheduler: {self.config.scheduler_type}")
        
        # === 2. æ³¨å†Œæ‰€æœ‰ services ===
        service_results = register_all_services(env, self.config)
        
        if not all(service_results.values()):
            failed = [k for k, v in service_results.items() if not v]
            print(f"âš ï¸  Some services failed to register: {failed}")
            print("   Pipeline may not work correctly.")
        
        # === 3. æ„å»ºåŒæµæº ===
        print("\n" + "=" * 80)
        print("Building Data Streams")
        print("=" * 80)
        
        # Query æµï¼ˆä¼ å…¥ç±»è€Œéå®ä¾‹ï¼‰
        query_stream = env.from_source(
            Workload4QuerySource,
            num_tasks=self.config.num_tasks,
            qps=self.config.query_qps,
            query_types=list(self.config.query_type_distribution.keys()),
            categories=list(self.config.category_distribution.keys()),
            use_fiqa=False,  # å¯é…ç½®
        )
        print(f"âœ“ Created Query Stream (QPS={self.config.query_qps})")
        
        # Document æµï¼ˆä¼ å…¥ç±»è€Œéå®ä¾‹ï¼‰
        doc_stream = env.from_source(
            Workload4DocumentSource,
            num_docs=self.config.num_tasks * 20,  # æ¯ä¸ªqueryå¯¹åº”20ä¸ªdoc
            qps=self.config.doc_qps,
            categories=list(self.config.category_distribution.keys()),
        )
        print(f"âœ“ Created Document Stream (QPS={self.config.doc_qps})")
        
        # === 4. Embedding é¢„è®¡ç®— ===
        query_stream = query_stream.map(
            EmbeddingPrecompute,
            embedding_base_url=self.config.embedding_base_url,
            embedding_model=self.config.embedding_model,
            batch_size=32,
            field_name="query_text",
        )
        print("âœ“ Added EmbeddingPrecompute for Query Stream")
        
        doc_stream = doc_stream.map(
            EmbeddingPrecompute,
            embedding_base_url=self.config.embedding_base_url,
            embedding_model=self.config.embedding_model,
            batch_size=32,
            field_name="doc_text",
        )
        print("âœ“ Added EmbeddingPrecompute for Document Stream")
        
        # === 5. Semantic Join ===
        print("\n" + "=" * 80)
        print("Building Semantic Join")
        print("=" * 80)
        
        # SemanticJoinOperator æ˜¯ BaseCoMapFunctionï¼Œä½¿ç”¨ comap è€Œä¸æ˜¯ join
        joined_stream = query_stream.connect(doc_stream).comap(
            SemanticJoinOperator,
            window_seconds=self.config.join_window_seconds,
            threshold=self.config.join_threshold,
            max_matches=self.config.join_max_matches,
            batch_compute=True,
        ).keyby(lambda x: hash(x.joined_id) % self.config.join_parallelism)
        
        print(f"âœ“ Added Semantic Join (window={self.config.join_window_seconds}s, "
              f"threshold={self.config.join_threshold}, "
              f"parallelism={self.config.join_parallelism})")
        
        # === 6. å›¾éå†å†…å­˜æ£€ç´¢ ===
        print("\n" + "=" * 80)
        print("Building Graph Memory Retrieval")
        print("=" * 80)
        
        graph_stream = joined_stream.map(
            GraphMemoryRetriever,
            max_depth=self.config.graph_max_depth,
            max_nodes=self.config.graph_max_nodes,
            beam_width=self.config.graph_bfs_beam_width,
        )
        print(f"âœ“ Added Graph Memory Retrieval (max_depth={self.config.graph_max_depth}, "
              f"max_nodes={self.config.graph_max_nodes})")
        
        # === 7. åŒè·¯ VDB æ£€ç´¢ï¼ˆ4-stage eachï¼‰===
        print("\n" + "=" * 80)
        print("Building VDB Retrieval Branches")
        print("=" * 80)
        
        # VDB1 åˆ†æ”¯ (4-stage cascade)
        vdb1_stream = joined_stream
        for stage in range(1, 5):
            top_k = self.config.vdb1_top_k // stage  # é€’å‡ top-k
            filter_threshold = self.config.vdb_filter_threshold + (stage - 1) * 0.05
            
            vdb1_stream = vdb1_stream.map(
                VDBRetriever,
                vdb_name="vdb1",
                top_k=top_k,
                stage=stage,
            )
            
            if stage < 4:  # æœ€åä¸€ stage ä¸è¿‡æ»¤
                vdb1_stream = vdb1_stream.filter(
                    VDBResultFilter,
                    threshold=filter_threshold,
                )
                
                vdb1_stream = vdb1_stream.map(
                    LocalReranker,
                    top_k=max(5, top_k // 2),
                )
        
        print(f"âœ“ Added VDB1 Branch (4-stage cascade)")
        
        # VDB2 åˆ†æ”¯ (4-stage cascade)
        vdb2_stream = joined_stream
        for stage in range(1, 5):
            top_k = self.config.vdb2_top_k // stage
            filter_threshold = self.config.vdb_filter_threshold + (stage - 1) * 0.05
            
            vdb2_stream = vdb2_stream.map(
                VDBRetriever,
                vdb_name="vdb2",
                top_k=top_k,
                stage=stage,
            )
            
            if stage < 4:
                vdb2_stream = vdb2_stream.filter(
                    VDBResultFilter,
                    threshold=filter_threshold,
                )
                
                vdb2_stream = vdb2_stream.map(
                    LocalReranker,
                    top_k=max(5, top_k // 2),
                )
        
        print(f"âœ“ Added VDB2 Branch (4-stage cascade)")
        
        # === 8. æ±‡èšæ‰€æœ‰æ£€ç´¢ç»“æœ ===
        print("\n" + "=" * 80)
        print("Building Result Aggregation")
        print("=" * 80)
        
        # ä½¿ç”¨ connect + comap æ¥åˆå¹¶æ‰€æœ‰æµï¼ˆUnionCoMap åœ¨æ–‡ä»¶é¡¶éƒ¨å®šä¹‰ï¼‰
        all_results = vdb1_stream.connect(vdb2_stream).connect(graph_stream).comap(UnionCoMap)
        print("âœ“ Added Union of all retrieval results (VDB1 + VDB2 + Graph)")
        
        # === 9. DBSCAN èšç±»å»é‡ ===
        print("\n" + "=" * 80)
        print("Building Clustering & Deduplication")
        print("=" * 80)
        
        deduplicated_stream = all_results.map(
            DBSCANClusteringOperator,
            eps=self.config.dbscan_eps,
            min_samples=self.config.dbscan_min_samples,
            metric="cosine",
        )
        print(f"âœ“ Added DBSCAN Clustering (eps={self.config.dbscan_eps}, "
              f"min_samples={self.config.dbscan_min_samples})")
        
        # === 10. 5ç»´è¯„åˆ†é‡æ’åº ===
        print("\n" + "=" * 80)
        print("Building Reranking")
        print("=" * 80)
        
        reranked_stream = deduplicated_stream.map(
            MultiDimensionalReranker,
            score_weights=self.config.rerank_score_weights,
            top_k=self.config.rerank_top_k,
        )
        print(f"âœ“ Added MultiDimensional Reranking (5 dimensions, top_k={self.config.rerank_top_k})")
        
        # === 11. MMR å¤šæ ·æ€§è¿‡æ»¤ ===
        reranked_stream = reranked_stream.map(
            MMRDiversityFilter,
            lambda_param=self.config.mmr_lambda,
            top_k=self.config.rerank_top_k,
        )
        print(f"âœ“ Added MMR Diversity Filter (lambda={self.config.mmr_lambda})")
        
        # === 12. åŒå±‚ Batch èšåˆ ===
        print("\n" + "=" * 80)
        print("Building Batch Aggregation")
        print("=" * 80)
        
        # ç¬¬ä¸€å±‚: Category Batch
        category_batched = reranked_stream.keyby(
            lambda x: x.query.category  # æŒ‰ category åˆ†ç»„
        ).map(
            CategoryBatchAggregator,
            batch_size=self.config.category_batch_size,
            timeout_ms=self.config.category_batch_timeout_ms,
        )
        print(f"âœ“ Added Category Batch (size={self.config.category_batch_size}, "
              f"timeout={self.config.category_batch_timeout_ms}ms)")
        
        # ç¬¬äºŒå±‚: Global Batch
        global_batched = category_batched.map(
            GlobalBatchAggregator,
            batch_size=self.config.global_batch_size,
            timeout_ms=self.config.global_batch_timeout_ms,
        )
        print(f"âœ“ Added Global Batch (size={self.config.global_batch_size}, "
              f"timeout={self.config.global_batch_timeout_ms}ms)")
        
        # === 13. æ‰¹é‡ LLM ç”Ÿæˆ ===
        print("\n" + "=" * 80)
        print("Building LLM Generation")
        print("=" * 80)
        
        generated_stream = global_batched.map(
            BatchLLMGenerator,
            llm_base_url=self.config.llm_base_url,
            llm_model=self.config.llm_model,
            max_tokens=self.config.llm_max_tokens,
        )
        print(f"âœ“ Added Batch LLM Generator (model={self.config.llm_model})")
        
        # === 14. Metrics Sink ===
        print("\n" + "=" * 80)
        print("Building Metrics Sink")
        print("=" * 80)
        
        generated_stream.sink(
            Workload4MetricsSink,
            metrics_output_dir=self.config.metrics_output_dir,
            verbose=True,
        )
        print(f"âœ“ Added Metrics Sink (output_dir={self.config.metrics_output_dir})")
        
        print("\n" + "=" * 80)
        print("Pipeline Build Complete")
        print("=" * 80)
        
        # Store environment for run()
        self.env = env
        
        return self
    
    def run(self) -> Workload4Metrics:
        """
        æ‰§è¡Œ pipelineã€‚
        
        Returns:
            Workload4Metrics: æ±‡æ€»æŒ‡æ ‡
        """
        if self.env is None:
            raise RuntimeError("Pipeline not built. Call build() first.")
        
        print("\n" + "=" * 80)
        print("Starting Workload 4 Execution")
        print("=" * 80)
        print(f"Duration: {self.config.duration}s")
        print(f"Expected Tasks: {self.config.num_tasks}")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œ pipeline
            self.env.submit(autostop=True)
            
            end_time = time.time()
            elapsed = end_time - start_time
            
            print("\n" + "=" * 80)
            print("Workload 4 Execution Complete")
            print("=" * 80)
            print(f"Elapsed Time: {elapsed:.2f}s")
            print("=" * 80)
            
            # æ”¶é›†æ±‡æ€»æŒ‡æ ‡
            # TODO: ä» Sink æ”¶é›†è¯¦ç»†æŒ‡æ ‡
            self.metrics = Workload4Metrics(
                task_id="summary",
                query_id="summary",
                query_arrival_time=start_time,
                doc_arrival_time=start_time,
                join_time=0.0,
                vdb1_start_time=0.0,
                vdb1_end_time=0.0,
                vdb2_start_time=0.0,
                vdb2_end_time=0.0,
                graph_start_time=0.0,
                graph_end_time=0.0,
                clustering_time=0.0,
                reranking_time=0.0,
                batch_time=0.0,
                generation_time=0.0,
                end_to_end_time=elapsed,
                join_matched_docs=0,
                vdb1_results=0,
                vdb2_results=0,
                graph_nodes_visited=0,
                clusters_found=0,
                duplicates_removed=0,
                final_top_k=0,
                cpu_time=0.0,
                memory_peak_mb=0.0,
            )
            
            return self.metrics
            
        except Exception as e:
            print(f"\nâœ— Pipeline execution failed: {e}")
            import traceback
            traceback.print_exc()
            raise


# =============================================================================
# Convenience Functions
# =============================================================================


def create_workload4_pipeline(
    config: Workload4Config | None = None,
    **config_overrides
) -> Workload4Pipeline:
    """
    åˆ›å»º Workload 4 Pipelineï¼ˆä¾¿æ·å‡½æ•°ï¼‰ã€‚
    
    Args:
        config: Workload4Config å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        **config_overrides: è¦†ç›–é…ç½®é¡¹
    
    Returns:
        Workload4Pipeline å®ä¾‹
    
    Example:
        >>> pipeline = create_workload4_pipeline(num_tasks=50, duration=600)
        >>> pipeline.build().run()
    """
    if config is None:
        config = Workload4Config()
    
    # åº”ç”¨è¦†ç›–
    for key, value in config_overrides.items():
        if hasattr(config, key):
            setattr(config, key, value)
        else:
            print(f"âš ï¸  Unknown config key: {key}")
    
    return Workload4Pipeline(config)


def run_workload4(
    config: Workload4Config | None = None,
    **config_overrides
) -> Workload4Metrics:
    """
    ä¸€é”®è¿è¡Œ Workload 4ï¼ˆä¾¿æ·å‡½æ•°ï¼‰ã€‚
    
    Args:
        config: Workload4Config å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        **config_overrides: è¦†ç›–é…ç½®é¡¹
    
    Returns:
        Workload4Metrics: æ±‡æ€»æŒ‡æ ‡
    
    Example:
        >>> metrics = run_workload4(num_tasks=100, use_remote=True)
    """
    pipeline = create_workload4_pipeline(config, **config_overrides)
    pipeline.build()
    return pipeline.run()
