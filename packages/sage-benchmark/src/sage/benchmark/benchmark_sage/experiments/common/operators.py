"""
Distributed Scheduling Benchmark - Pipeline Operators
======================================================

Pipeline ç®—å­:
- TaskSource: ä»»åŠ¡ç”Ÿæˆæº
- ComputeOperator: CPU è®¡ç®—ä»»åŠ¡ (ç”¨äºè°ƒåº¦æµ‹è¯•)
- LLMOperator: LLM æ¨ç†ä»»åŠ¡
- RAGOperator: RAG æ£€ç´¢+ç”Ÿæˆä»»åŠ¡
- MetricsSink: æŒ‡æ ‡æ”¶é›†
"""

from __future__ import annotations

import hashlib
import os
import socket
import time
from pathlib import Path
from typing import TYPE_CHECKING, Any

from sage.common.core.functions.filter_function import FilterFunction
from sage.common.core.functions.map_function import MapFunction
from sage.common.core.functions.sink_function import SinkFunction
from sage.common.core.functions.source_function import SourceFunction
from sage.kernel.runtime.communication.packet import StopSignal

if TYPE_CHECKING:
    from .models import TaskState

try:
    from .models import TaskState
except ImportError:
    from models import TaskState


# ç¤ºä¾‹æŸ¥è¯¢æ±  - åŒ…å« ZERO/SINGLE/MULTI ä¸‰ç§å¤æ‚åº¦
SAMPLE_QUERIES = [
    # äº¤æ›¿æ’åˆ—ä»¥ç¡®ä¿æ¯ç§ç±»å‹éƒ½èƒ½è¦†ç›–
    # --- Group 1 ---
    "SAGE version",  # ZERO
    "What is SAGE framework and what are its main features?",  # SINGLE
    "Compare LocalEnvironment and RemoteEnvironment in terms of performance and use cases",  # MULTI
    # --- Group 2 ---
    "Python requirements",  # ZERO
    "How do I install SAGE on Ubuntu?",  # SINGLE
    "Analyze the pros and cons of different scheduler strategies in SAGE",  # MULTI
    # --- Group 3 ---
    "License type",  # ZERO
    "What are the different scheduler strategies available?",  # SINGLE
    "What is the relationship between sage-kernel and sage-middleware components?",  # MULTI
    # --- Group 4 ---
    "Default port",  # ZERO
    "How does the memory service work in SAGE?",  # SINGLE
    "Compare FIFO and LoadAware schedulers and their impact on throughput",  # MULTI
    # --- Group 5 ---
    "Ray cluster",  # ZERO
    "What is the role of middleware components?",  # SINGLE
    "Analyze the effects of parallelism settings on pipeline performance",  # MULTI
    # --- Extra SINGLE queries ---
    "How to configure LLM services in SAGE?",
    "What embedding models are supported?",
    "What is the purpose of sage-kernel package?",
    "What vector databases are supported?",
    "What are the CPU node requirements?",
]

# çŸ¥è¯†åº“
SAMPLE_KNOWLEDGE_BASE = [
    {
        "id": "1",
        "title": "SAGE Framework Overview",
        "content": "SAGE is a Python 3.10+ framework for building AI/LLM data processing pipelines.",
    },
    {
        "id": "2",
        "title": "SAGE Installation Guide",
        "content": "To install SAGE, run ./quickstart.sh --dev --yes for development.",
    },
    {
        "id": "3",
        "title": "Pipeline Architecture",
        "content": "SAGE pipelines use SourceFunction, MapFunction, and SinkFunction operators.",
    },
    {
        "id": "4",
        "title": "Scheduler Strategies",
        "content": "SAGE supports FIFO, LoadAware, Random, RoundRobin, and Priority schedulers.",
    },
    {
        "id": "5",
        "title": "Memory Services",
        "content": "sage-mem provides HierarchicalMemoryService with STM/MTM/LTM tiers.",
    },
]


class TaskSource(SourceFunction):
    """
    ä»»åŠ¡ç”Ÿæˆæºã€‚

    ä»æŸ¥è¯¢æ± ç”Ÿæˆæµ‹è¯•ä»»åŠ¡ã€‚
    """

    def __init__(
        self,
        num_tasks: int = 100,
        query_pool: list[str] | None = None,
        task_complexity: str = "medium",
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.query_pool = query_pool or SAMPLE_QUERIES
        self.num_tasks = num_tasks
        self.task_complexity = task_complexity
        self.current_index = 0

    def execute(self, data=None) -> TaskState | StopSignal:
        """ç”Ÿæˆä¸‹ä¸€ä¸ªä»»åŠ¡"""
        if self.current_index >= self.num_tasks:
            time.sleep(60.0)  # 60 ç§’ç­‰å¾…æ‰€æœ‰ actor å¯åŠ¨å’Œ LLM å“åº”
            return StopSignal("All tasks generated")

        query = self.query_pool[self.current_index % len(self.query_pool)]
        self.current_index += 1

        state = TaskState(
            task_id=f"task_{self.current_index:05d}",
            query=query,
            created_time=time.time(),
            metadata={"complexity": self.task_complexity},
        )

        return state


class ComputeOperator(MapFunction):
    """
    CPU è®¡ç®—ä»»åŠ¡ç®—å­ã€‚

    ç”¨äºæµ‹è¯•çº¯è°ƒåº¦æ€§èƒ½ï¼Œä¸ä¾èµ–å¤–éƒ¨æœåŠ¡ã€‚
    å¯é…ç½®è®¡ç®—å¤æ‚åº¦ (light/medium/heavy)ã€‚
    """

    def __init__(
        self,
        complexity: str = "medium",
        stage: int = 1,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.complexity = complexity
        self.stage = stage
        self._hostname = socket.gethostname()

        # å¤æ‚åº¦å¯¹åº”çš„è¿­ä»£æ¬¡æ•°
        self.iterations = {
            "light": 1000,
            "medium": 10000,
            "heavy": 100000,
        }.get(complexity, 10000)

    def _do_compute(self, data: str) -> str:
        """æ‰§è¡Œ CPU å¯†é›†è®¡ç®—"""
        result = data
        for i in range(self.iterations):
            result = hashlib.md5(f"{result}{i}".encode()).hexdigest()
        return result

    def execute(self, data: TaskState) -> TaskState:
        """æ‰§è¡Œè®¡ç®—ä»»åŠ¡"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"ComputeOperator_{self.stage}"
        state.mark_started()

        try:
            # æ‰§è¡Œè®¡ç®—
            result = self._do_compute(state.query)
            state.metadata[f"compute_result_{self.stage}"] = result[:16]
            state.success = True
        except Exception as e:
            state.success = False
            state.error = str(e)

        state.mark_completed()
        return state


class LLMOperator(MapFunction):
    """
    LLM æ¨ç†ä»»åŠ¡ç®—å­ã€‚

    è°ƒç”¨çœŸå® LLM æœåŠ¡è¿›è¡Œæ¨ç†ã€‚
    """

    def __init__(
        self,
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        max_tokens: int = 256,
        stage: int = 1,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.max_tokens = max_tokens
        self.stage = stage
        self._hostname = socket.gethostname()
        self._llm_client = None

    def _get_client(self):
        """å»¶è¿Ÿåˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if self._llm_client is None:
            try:
                from sage.common.components.sage_llm import UnifiedInferenceClient

                self._llm_client = UnifiedInferenceClient.create(
                    control_plane_url=self.llm_base_url,
                    default_llm_model=self.llm_model,
                )
            except Exception as e:
                print(f"[LLMOperator] Client init error: {e}")
        return self._llm_client

    def execute(self, data: TaskState) -> TaskState:
        """æ‰§è¡Œ LLM æ¨ç†"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"LLMOperator_{self.stage}"
        state.mark_started()

        try:
            client = self._get_client()
            if client:
                messages = [
                    {"role": "system", "content": "You are a helpful assistant. Be concise."},
                    {"role": "user", "content": state.query},
                ]
                response = client.chat(messages, max_tokens=self.max_tokens)
                state.response = str(response) if not isinstance(response, str) else response
            else:
                # Fallback: æ¨¡æ‹Ÿå“åº”
                state.response = f"[Simulated] Response to: {state.query[:50]}..."
            state.success = True
        except Exception as e:
            state.success = False
            state.error = str(e)
            state.response = f"[Error] {str(e)}"

        state.mark_completed()
        return state


class RAGOperator(MapFunction):
    """
    RAG æ£€ç´¢+ç”Ÿæˆä»»åŠ¡ç®—å­ã€‚

    å…ˆä½¿ç”¨ Embedding æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œå†è°ƒç”¨ LLM ç”Ÿæˆå“åº”ã€‚
    """

    def __init__(
        self,
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        embedding_base_url: str = "http://11.11.11.7:8090/v1",
        embedding_model: str = "BAAI/bge-m3",
        max_tokens: int = 256,
        top_k: int = 3,
        knowledge_base: list[dict] | None = None,
        stage: int = 1,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.embedding_base_url = embedding_base_url
        self.embedding_model = embedding_model
        self.max_tokens = max_tokens
        self.top_k = top_k
        self.knowledge_base = knowledge_base or SAMPLE_KNOWLEDGE_BASE
        self.stage = stage
        self._hostname = socket.gethostname()
        self._client = None
        self._initialized = False

    def _initialize(self):
        """å»¶è¿Ÿåˆå§‹åŒ–å®¢æˆ·ç«¯"""
        if self._initialized:
            return
        try:
            from sage.common.components.sage_llm import UnifiedInferenceClient

            self._client = UnifiedInferenceClient.create(
                control_plane_url=self.llm_base_url,
                default_llm_model=self.llm_model,
                default_embedding_model=self.embedding_model,
            )
            self._initialized = True
        except Exception as e:
            print(f"[RAGOperator] Init error: {e}")
            self._initialized = True

    def _retrieve(self, query: str) -> list[dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # ç®€å•å…³é”®è¯åŒ¹é…ä½œä¸º fallback
        query_lower = query.lower()
        results = []
        for doc in self.knowledge_base:
            content_lower = doc.get("content", "").lower()
            title_lower = doc.get("title", "").lower()
            query_words = set(query_lower.split())
            content_words = set(content_lower.split())
            title_words = set(title_lower.split())
            overlap = len(query_words & (content_words | title_words))
            if overlap > 0:
                results.append(
                    {
                        "score": overlap,
                        "title": doc.get("title", ""),
                        "content": doc.get("content", ""),
                    }
                )
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[: self.top_k]

    def execute(self, data: TaskState) -> TaskState:
        """æ‰§è¡Œ RAG ä»»åŠ¡"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"RAGOperator_{self.stage}"
        state.mark_started()

        self._initialize()

        try:
            # æ£€ç´¢
            retrieval_start = time.time()
            state.retrieved_docs = self._retrieve(state.query)
            retrieval_time = time.time() - retrieval_start
            state.metadata["retrieval_time_ms"] = retrieval_time * 1000

            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = [f"{doc['title']}: {doc['content']}" for doc in state.retrieved_docs]
            state.context = "\n".join(context_parts)

            # ç”Ÿæˆ
            if self._client:
                messages = [
                    {"role": "system", "content": "Answer based on the context. Be concise."},
                    {
                        "role": "user",
                        "content": f"Context:\n{state.context}\n\nQuestion: {state.query}",
                    },
                ]
                response = self._client.chat(messages, max_tokens=self.max_tokens)
                state.response = str(response) if not isinstance(response, str) else response
            else:
                state.response = f"[Simulated RAG] Based on {len(state.retrieved_docs)} docs."

            state.success = True
        except Exception as e:
            state.success = False
            state.error = str(e)

        state.mark_completed()
        return state


class MetricsSink(SinkFunction):
    """
    æŒ‡æ ‡æ”¶é›† Sinkã€‚

    æ”¶é›†ä»»åŠ¡æŒ‡æ ‡å¹¶èšåˆç»Ÿè®¡ã€‚
    å°†ç»“æœå†™å…¥æ–‡ä»¶ä»¥æ”¯æŒ Remote æ¨¡å¼ã€‚
    """

    # Metrics è¾“å‡ºç›®å½•
    METRICS_OUTPUT_DIR = "/tmp/sage_metrics"

    # Drain é…ç½®ï¼šç­‰å¾…è¿œç¨‹èŠ‚ç‚¹ä¸Š Generator å®Œæˆå¤„ç†
    # é—®é¢˜ï¼šStopSignal å¯èƒ½æ¯”æ•°æ®å…ˆåˆ°è¾¾ï¼Œè€Œ Generator è¿˜åœ¨ç­‰å¾… LLM å“åº”
    # Adaptive-RAG ç­‰å¤æ‚åœºæ™¯å¯èƒ½éœ€è¦å¤šè½® LLM è°ƒç”¨ï¼ŒP99 å¯è¾¾ 150+ ç§’
    # è®¾ç½® drain_timeout=300sï¼ˆæ€»ç­‰å¾…æ—¶é—´ï¼‰å’Œ quiet_period=90sï¼ˆæ— æ•°æ®é™é»˜æœŸï¼‰
    drain_timeout: float = 300.0
    drain_quiet_period: float = 90.0

    def __init__(
        self,
        metrics_collector: Any = None,
        verbose: bool = False,
        drain_timeout: float | None = None,
        drain_quiet_period: float | None = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.metrics_collector = metrics_collector
        self.verbose = verbose
        self.test_mode = os.getenv("SAGE_TEST_MODE") == "true"

        # å…è®¸é€šè¿‡å‚æ•°è¦†ç›–é»˜è®¤ drain é…ç½®
        if drain_timeout is not None:
            self.drain_timeout = drain_timeout
        if drain_quiet_period is not None:
            self.drain_quiet_period = drain_quiet_period

        # æœ¬åœ°ç»Ÿè®¡
        self.count = 0
        self.success_count = 0
        self.fail_count = 0
        self.latencies: list[float] = []
        self.node_stats: dict[str, int] = {}

        # åˆ›å»ºå”¯ä¸€çš„è¾“å‡ºæ–‡ä»¶
        self._start_time = time.time()
        self.instance_id = f"{socket.gethostname()}_{os.getpid()}_{int(time.time() * 1000)}"
        os.makedirs(self.METRICS_OUTPUT_DIR, exist_ok=True)
        self.metrics_output_file = f"{self.METRICS_OUTPUT_DIR}/metrics_{self.instance_id}.jsonl"

        # å†™å…¥ header
        self._write_header()

    def _write_header(self) -> None:
        """å†™å…¥ metrics æ–‡ä»¶ header"""
        import json
        import sys

        try:
            header = {
                "type": "header",
                "instance_id": self.instance_id,
                "start_time": self._start_time,
                "hostname": socket.gethostname(),
                "pid": os.getpid(),
            }
            with open(self.metrics_output_file, "w") as f:
                f.write(json.dumps(header) + "\n")
            print(
                f"    [MetricsSink] Initialized: {self.metrics_output_file}",
                file=sys.stderr,
                flush=True,
            )
        except Exception as e:
            print(f"    [MetricsSink] Init error: {e}", file=sys.stderr, flush=True)

    def _write_task_to_file(self, task: TaskState) -> None:
        """å°†ä»»åŠ¡ç»“æœå†™å…¥æ–‡ä»¶"""
        import json

        try:
            record = {
                "type": "task",
                "task_id": task.task_id,
                "success": task.success,
                "node_id": task.node_id,
                "total_latency_ms": getattr(
                    task,
                    "total_latency_ms",
                    task.total_latency * 1000 if hasattr(task, "total_latency") else 0,
                ),
                "timestamp": time.time(),
            }
            with open(self.metrics_output_file, "a") as f:
                f.write(json.dumps(record) + "\n")
        except Exception as e:
            import sys

            print(f"    [MetricsSink] Write error: {e}", file=sys.stderr, flush=True)

    def _write_summary(self) -> None:
        """å†™å…¥æœ€ç»ˆæ‘˜è¦"""
        import json
        import sys

        try:
            elapsed = time.time() - self._start_time
            avg_latency = sum(self.latencies) / len(self.latencies) if self.latencies else 0
            summary = {
                "type": "summary",
                "total_tasks": self.count,
                "success_count": self.success_count,
                "fail_count": self.fail_count,
                "elapsed_seconds": elapsed,
                "throughput": self.count / elapsed if elapsed > 0 else 0,
                "avg_latency_ms": avg_latency,
                "node_distribution": self.node_stats,
            }
            with open(self.metrics_output_file, "a") as f:
                f.write(json.dumps(summary) + "\n")
            print(
                f"    [MetricsSink] Summary: {self.count} tasks, {self.success_count} success -> {self.metrics_output_file}",
                file=sys.stderr,
                flush=True,
            )
        except Exception as e:
            print(f"    [MetricsSink] Summary error: {e}", file=sys.stderr, flush=True)

    def execute(self, data: TaskState) -> None:
        """æ”¶é›†ä»»åŠ¡ metrics"""
        if not isinstance(data, TaskState):
            return

        state = data
        self.count += 1

        # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥
        if state.success:
            self.success_count += 1
        else:
            self.fail_count += 1

        # è®°å½•å»¶è¿Ÿ
        latency_ms = getattr(
            state,
            "total_latency_ms",
            state.total_latency * 1000 if hasattr(state, "total_latency") else 0,
        )
        if latency_ms > 0:
            self.latencies.append(latency_ms)

        # æ›´æ–°èŠ‚ç‚¹ç»Ÿè®¡
        if state.node_id:
            self.node_stats[state.node_id] = self.node_stats.get(state.node_id, 0) + 1

        # å†™å…¥æ–‡ä»¶ (Remote æ¨¡å¼å¯ç”¨)
        self._write_task_to_file(state)

        # è®°å½•åˆ°å…±äº«æ”¶é›†å™¨ (ä»… Local æ¨¡å¼æœ‰æ•ˆ)
        if self.metrics_collector:
            self.metrics_collector.record_task(state)

        # è¯¦ç»†è¾“å‡º
        if self.verbose and (not self.test_mode or self.count <= 5):
            print(f"[{self.count}] Task: {state.task_id}, Node: {state.node_id}")
            print(f"    Latency: {latency_ms:.1f}ms, Success: {state.success}")
            if hasattr(state, "error") and state.error:
                print(f"    Error: {state.error}")
        elif self.verbose and self.count == 6:
            print("    ... (remaining output suppressed)")

        # Periodic progress report
        if self.count % 100 == 0:
            print(f"[Progress] {self.count} tasks completed")
            if self.node_stats:
                print("  Node distribution:", dict(sorted(self.node_stats.items())))

    def close(self) -> None:
        """å…³é—­æ—¶å†™å…¥æ‘˜è¦"""
        self._write_summary()


# =============================================================================
# Simple RAG Operators - Using Remote Embedding Service
# =============================================================================
# è¿™äº›ç®—å­ä½¿ç”¨è¿œç¨‹ embedding æœåŠ¡ï¼Œä¸éœ€è¦æœ¬åœ°ä¸‹è½½æ¨¡å‹
# Embedding æœåŠ¡: http://{LLM_HOST}:8090/v1
# Embedding æ¨¡: BAAI/bge-large-zh-v1.5

# é»˜è®¤æœåŠ¡é…ç½®
LLM_HOST = os.getenv("LLM_HOST", "11.11.11.7")
EMBEDDING_BASE_URL = f"http://{LLM_HOST}:8090/v1"
EMBEDDING_MODEL = "BAAI/bge-large-zh-v1.5"
LLM_BASE_URL = f"http://{LLM_HOST}:8903/v1"
LLM_MODEL = "Qwen/Qwen2.5-7B-Instruct"


def get_remote_embeddings(
    texts: list[str],
    base_url: str = EMBEDDING_BASE_URL,
    model: str = EMBEDDING_MODEL,
) -> list[list[float]] | None:
    """
    ä½¿ç”¨è¿œç¨‹ embedding æœåŠ¡è·å–å‘é‡ã€‚

    Args:
        texts: è¦ç¼–ç çš„æ–‡æœ¬åˆ—è¡¨
        base_url: Embedding æœåŠ¡åœ°å€
        model: Embedding æ¨¡å‹å

    Returns:
        å‘é‡åˆ—è¡¨ï¼Œæˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    try:
        import requests

        response = requests.post(
            f"{base_url}/embeddings",
            json={
                "input": texts,
                "model": model,
            },
            timeout=30,
        )
        response.raise_for_status()
        result = response.json()

        # æå– embeddings
        embeddings = [item["embedding"] for item in result["data"]]
        return embeddings
    except Exception as e:
        print(f"[Embedding] Error: {e}")
        return None


def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    import math

    dot = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = math.sqrt(sum(a * a for a in vec1))
    norm2 = math.sqrt(sum(b * b for b in vec2))

    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot / (norm1 * norm2)


class SimpleRetriever(MapFunction):
    """
    ç®€å•æ£€ç´¢å™¨ - ä½¿ç”¨è¿œç¨‹ Embedding æœåŠ¡ã€‚

    åŸºäºä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚
    ä¸ä¾èµ– ChromaDB æˆ–æœ¬åœ°æ¨¡å‹ã€‚
    """

    def __init__(
        self,
        embedding_base_url: str = EMBEDDING_BASE_URL,
        embedding_model: str = EMBEDDING_MODEL,
        top_k: int = 5,
        knowledge_base: list[dict] | None = None,
        stage: int = 1,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.embedding_base_url = embedding_base_url
        self.embedding_model = embedding_model
        self.top_k = top_k
        self.knowledge_base = knowledge_base or SAMPLE_KNOWLEDGE_BASE
        self.stage = stage
        self._hostname = socket.gethostname()
        self._kb_embeddings: list[list[float]] | None = None
        self._initialized = False

    def _initialize(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“å‘é‡"""
        if self._initialized:
            return

        # è·å–çŸ¥è¯†åº“æ–‡æ¡£çš„ embeddings
        texts = [doc.get("content", doc.get("text", "")) for doc in self.knowledge_base]
        self._kb_embeddings = get_remote_embeddings(
            texts,
            base_url=self.embedding_base_url,
            model=self.embedding_model,
        )

        if self._kb_embeddings:
            print(
                f"[SimpleRetriever] Initialized with {len(self._kb_embeddings)} document embeddings"
            )
        else:
            print("[SimpleRetriever] Warning: Failed to get KB embeddings, using keyword fallback")

        self._initialized = True

    def _retrieve_by_embedding(self, query: str) -> list[dict]:
        """ä½¿ç”¨ embedding æ£€ç´¢"""
        # è·å–æŸ¥è¯¢å‘é‡
        query_embeddings = get_remote_embeddings(
            [query],
            base_url=self.embedding_base_url,
            model=self.embedding_model,
        )

        if not query_embeddings or not self._kb_embeddings:
            return self._retrieve_by_keyword(query)

        query_vec = query_embeddings[0]

        # è®¡ç®—ç›¸ä¼¼åº¦
        scored_docs = []
        for i, (doc, doc_vec) in enumerate(zip(self.knowledge_base, self._kb_embeddings)):
            score = cosine_similarity(query_vec, doc_vec)
            scored_docs.append(
                {
                    "id": doc.get("id", str(i)),
                    "title": doc.get("title", ""),
                    "content": doc.get("content", doc.get("text", "")),
                    "score": score,
                }
            )

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        scored_docs.sort(key=lambda x: x["score"], reverse=True)
        return scored_docs[: self.top_k]

    def _retrieve_by_keyword(self, query: str) -> list[dict]:
        """å…³é”®è¯æ£€ç´¢ fallback"""
        query_lower = query.lower()
        query_words = set(query_lower.split())

        scored_docs = []
        for i, doc in enumerate(self.knowledge_base):
            content = doc.get("content", doc.get("text", "")).lower()
            title = doc.get("title", "").lower()

            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            score = 0
            for word in query_words:
                if len(word) > 2:
                    if word in content:
                        score += 2
                    if word in title:
                        score += 3

            if score > 0:
                scored_docs.append(
                    {
                        "id": doc.get("id", str(i)),
                        "title": doc.get("title", ""),
                        "content": doc.get("content", doc.get("text", "")),
                        "score": score / 10.0,  # å½’ä¸€åŒ–
                    }
                )

        scored_docs.sort(key=lambda x: x["score"], reverse=True)
        return scored_docs[: self.top_k]

    def execute(self, data: TaskState) -> TaskState:
        """æ‰§è¡Œæ£€ç´¢"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"SimpleRetriever_{self.stage}"
        state.mark_started()

        self._initialize()

        try:
            retrieval_start = time.time()
            state.retrieved_docs = self._retrieve_by_embedding(state.query)
            retrieval_time = time.time() - retrieval_start
            state.metadata["retrieval_time_ms"] = retrieval_time * 1000
            state.metadata["num_retrieved"] = len(state.retrieved_docs)
            state.success = True
        except Exception as e:
            state.success = False
            state.error = str(e)
            state.retrieved_docs = []

        state.mark_completed()
        return state


class SimpleReranker(MapFunction):
    """
        ç®€å•é‡æ’>> - ä½¿ç”¨è¿œç¨‹ Embedding æœåŠ¡ã€‚

    #    Supports multiple pipeline types:

        ä½¿ç”¨ embedding æ¨¡å‹è®¡ç®—æ›´ç²¾ç»†çš„ç›¸å…³æ€§åˆ†æ•°ã€‚
    """

    def __init__(
        self,
        embedding_base_url: str = EMBEDDING_BASE_URL,
        embedding_model: str = EMBEDDING_MODEL,
        top_k: int = 3,
        stage: int = 2,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.embedding_base_url = embedding_base_url
        self.embedding_model = embedding_model
        self.top_k = top_k
        self.stage = stage
        self._hostname = socket.gethostname()

    def _rerank(self, query: str, docs: list[dict]) -> list[dict]:
        """
        é‡æ’æ–‡æ¡£ã€‚

        ä½¿ç”¨ [query + document] ç»„åˆçš„ embedding è¿›è¡Œæ›´ç²¾ç»†çš„ç›¸å…³æ€§è®¡ç®—ã€‚
        """
        if not docs:
            return []

        # æ„å»º query-doc å¯¹è¿›è¡Œè¯„åˆ†
        # ä½¿ç”¨æ ¼å¼: "Query: {query} Document: {content}"
        pairs = []
        for doc in docs:
            content = doc.get("content", "")[:500]  # æˆªæ–­
            pair_text = f"Query: {query} Document: {content}"
            pairs.append(pair_text)

        # è·å– query embedding
        query_embedding = get_remote_embeddings(
            [query],
            base_url=self.embedding_base_url,
            model=self.embedding_model,
        )

        # è·å–æ¯ä¸ª doc çš„ embedding
        doc_texts = [doc.get("content", "")[:500] for doc in docs]
        doc_embeddings = get_remote_embeddings(
            doc_texts,
            base_url=self.embedding_base_url,
            model=self.embedding_model,
        )

        if not query_embedding or not doc_embeddings:
            # Fallback: ä¿æŒåŸæœ‰æ’åº
            return docs[: self.top_k]

        query_vec = query_embedding[0]

        # è®¡ç®—æ–°çš„ç›¸å…³æ€§åˆ†æ•°
        reranked = []
        for i, (doc, doc_vec) in enumerate(zip(docs, doc_embeddings)):
            score = cosine_similarity(query_vec, doc_vec)
            reranked.append(
                {
                    **doc,
                    "rerank_score": score,
                }
            )

        # æŒ‰æ–°åˆ†æ•°æ’åº
        reranked.sort(key=lambda x: x.get("rerank_score", 0), reverse=True)
        return reranked[: self.top_k]

    def execute(self, data: TaskState) -> TaskState:
        """æ‰§è¡Œé‡æ’"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"SimpleReranker_{self.stage}"
        state.mark_started()

        try:
            rerank_start = time.time()
            state.retrieved_docs = self._rerank(state.query, state.retrieved_docs)
            rerank_time = time.time() - rerank_start
            state.metadata["rerank_time_ms"] = rerank_time * 1000
            state.metadata["num_reranked"] = len(state.retrieved_docs)
            state.success = True
        except Exception as e:
            state.success = False
            state.error = str(e)

        state.mark_completed()
        return state


class SimplePromptor(MapFunction):
    """
    ç®€å•æç¤ºæ„å»ºå™¨ã€‚

    å°†æ£€ç´¢çš„æ–‡æ¡£å’ŒæŸ¥è¯¢ç»„åˆæˆ LLM æç¤ºã€‚
    """

    def __init__(
        self,
        stage: int = 3,
        max_context_length: int = 2000,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.stage = stage
        self.max_context_length = max_context_length
        self._hostname = socket.gethostname()

    def execute(self, data: TaskState) -> TaskState:
        """æ„å»ºæç¤º"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"SimplePromptor_{self.stage}"
        state.mark_started()

        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            total_length = 0

            for i, doc in enumerate(state.retrieved_docs):
                title = doc.get("title", f"Document {i + 1}")
                content = doc.get("content", "")

                doc_text = f"[{title}]\n{content}"
                if total_length + len(doc_text) > self.max_context_length:
                    break

                context_parts.append(doc_text)
                total_length += len(doc_text)

            state.context = "\n\n".join(context_parts)
            state.metadata["context_length"] = len(state.context)
            state.success = True
        except Exception as e:
            state.success = False
            state.error = str(e)
            state.context = ""

        state.mark_completed()
        return state


class SimpleGenerator(MapFunction):
    """
    ç®€å•ç”Ÿæˆå™¨ - ä½¿ç”¨è¿œç¨‹ LLM æœåŠ¡ã€‚

    åŸºäºä¸Šä¸‹æ–‡å’ŒæŸ¥è¯¢ç”Ÿæˆå›å¤ã€‚
    """

    def __init__(
        self,
        llm_base_url: str = LLM_BASE_URL,
        llm_model: str = LLM_MODEL,
        max_tokens: int = 256,
        stage: int = 4,
        output_file: str | None = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.max_tokens = max_tokens
        self.stage = stage
        self.output_file = output_file
        self._hostname = socket.gethostname()

        #!/usr/bin/env python3
        if self.output_file:
            output_path = Path(self.output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)

    def _generate(self, query: str, context: str) -> str:
        """è°ƒç”¨ LLM ç”Ÿæˆå›å¤"""
        try:
            import requests

            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªhelpful--------æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´æ˜ã€‚",
                },
                {
                    "role": "user",
                    "content": f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {query}",
                },
            ]

            response = requests.post(
                f"{self.llm_base_url}/chat/completions",
                json={
                    "model": self.llm_model,
                    "messages": messages,
                    "max_tokens": self.max_tokens,
                    "temperature": 0.7,
                },
                timeout=60,
            )
            response.raise_for_status()
            result = response.json()

            return result["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[Generation Error] {str(e)}"

    def _save_response_to_file(self, state: TaskState, gen_time: float) -> None:
        """ä¿å­˜ LLM å›å¤åˆ°æŒ‡å®šæ–‡ä»¶"""
        if self.output_file is None:
            return

        try:
            import json
            from datetime import datetime

            record = {
                "timestamp": datetime.now().isoformat(),
                "task_id": state.task_id,
                "node_id": state.node_id,
                "query": state.query,
                "context": state.context,
                "response": state.response,
                "generation_time_ms": gen_time * 1000,
                "model": self.llm_model,
            }

            # è¿½åŠ << JSONL æ ¼å¼
            with open(self.output_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")

        except Exception as e:
            print(f"[Warning] Failed to save response to file: {e}")

    def execute(self, data: TaskState) -> TaskState:
        """æ‰§è¡Œç”Ÿæˆ"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"SimpleGenerator_{self.stage}"
        state.mark_started()

        try:
            gen_start = time.time()
            state.response = self._generate(state.query, state.context)
            gen_time = time.time() - gen_start
            state.metadata["generation_time_ms"] = gen_time * 1000
            state.success = True
            # è¾“å‡ºåˆ°æŒ‡å®šæ–‡ä»¶
            if self.output_file:
                self._save_response_to_file(state, gen_time)

        except Exception as e:
            state.success = False
            state.error = str(e)
            state.response = f"[Error] {str(e)}"

        state.mark_completed()
        return state


# ============================================================================
# Adaptive-RAG Operators
# ============================================================================

try:
    from .models import (
        AdaptiveRAGQueryData,
        AdaptiveRAGResultData,
        ClassificationResult,
        IterativeState,
        QueryComplexityLevel,
    )
except ImportError:
    from models import (
        AdaptiveRAGQueryData,
        AdaptiveRAGResultData,
        ClassificationResult,
        IterativeState,
        QueryComplexityLevel,
    )


class AdaptiveRAGQuerySource(SourceFunction):
    """Adaptive-RAG æŸ¥è¯¢æ•°æ®æº"""

    def __init__(self, queries: list[str], delay: float = 0.0, **kwargs):
        super().__init__(**kwargs)
        self.queries = queries
        self.delay = delay
        self.counter = 0

    def execute(self, data=None) -> AdaptiveRAGQueryData | StopSignal:
        if self.counter >= len(self.queries):
            # ç­‰å¾…æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡å®Œæˆ
            time.sleep(30.0)
            return StopSignal("All queries generated")
        query = self.queries[self.counter]
        self.counter += 1
        if self.delay > 0:
            time.sleep(self.delay)
        import sys
        print(f"[Source] [{self.counter}/{len(self.queries)}]: {query}", file=sys.stderr, flush=True)
        return AdaptiveRAGQueryData(query=query, metadata={"index": self.counter - 1})


class QueryClassifier(MapFunction):
    """
    æŸ¥è¯¢å¤æ‚åº¦åˆ†ç±»å™¨

    æ”¯æŒä¸‰ç§åˆ†ç±»æ–¹å¼:
    - rule: åŸºäºå…³é”®è¯çš„è§„åˆ™åˆ†ç±»
    - llm: ä½¿ç”¨ LLM è¿›è¡Œåˆ†ç±»
    - hybrid: å…ˆè§„åˆ™ï¼Œä¸ç¡®å®šæ—¶ç”¨ LLM

    å¤æ‚åº¦å®šä¹‰ (å‚è€ƒ Adaptive-RAG è®ºæ–‡):
    - ZERO (A): ç®€å•äº‹å®æŸ¥è¯¢ï¼ŒLLM å¯ç›´æ¥å›ç­”ï¼Œæ— éœ€æ£€ç´¢
    - SINGLE (B): éœ€è¦å•æ¬¡æ£€ç´¢çš„æŸ¥è¯¢
    - MULTI (C): éœ€è¦å¤šè·³æ¨ç†æˆ–è¿­ä»£æ£€ç´¢çš„å¤æ‚æŸ¥è¯¢
    """

    # MULTI: å¤šè·³æ¨ç†ã€æ¯”è¾ƒåˆ†æã€å› æœå…³ç³»
    MULTI_KEYWORDS = [
        "compare", "contrast", "analyze", "relationship", "between",
        "pros and cons", "advantages and disadvantages", "impact", "effects",
        "differences", "similarities", "how does .* affect", "why does",
        "what factors", "explain the relationship", "connection between",
    ]

    # SINGLE: éœ€è¦æ£€ç´¢ä½†å•æ­¥å¯å®Œæˆ
    SINGLE_KEYWORDS = [
        "what is", "who is", "when was", "where is", "how to",
        "define", "describe", "explain", "how does .* work",
        "what are the", "list the", "name the",
    ]

    # ZERO: å¸¸è¯†æ€§é—®é¢˜ï¼ŒLLM å¯ç›´æ¥å›ç­”
    ZERO_INDICATORS = [
        # çŸ­æŸ¥è¯¢ (â‰¤3 words)
        # å¸¸è§çŸ¥è¯†é—®é¢˜
        "capital of", "meaning of", "synonym", "antonym",
        "what year", "how many", "true or false",
    ]

    def __init__(
        self,
        classifier_type: str = "rule",
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        **kwargs
    ):
        super().__init__(**kwargs)
        self.classifier_type = classifier_type
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model

    def _classify_by_rule(self, query: str) -> ClassificationResult:
        import re
        query_lower = query.lower()
        word_count = len(query.split())

        # 1. æ£€æŸ¥ ZERO æŒ‡ç¤ºè¯æˆ–çŸ­æŸ¥è¯¢
        if word_count <= 3:
            return ClassificationResult(
                complexity=QueryComplexityLevel.ZERO,
                confidence=0.8,
                reasoning=f"Very short query ({word_count} words)"
            )

        for indicator in self.ZERO_INDICATORS:
            if indicator in query_lower:
                return ClassificationResult(
                    complexity=QueryComplexityLevel.ZERO,
                    confidence=0.7,
                    reasoning=f"ZERO indicator: '{indicator}'"
                )

        # 2. æ£€æŸ¥ MULTI å…³é”®è¯ (ä¼˜å…ˆçº§é«˜äº SINGLE)
        for keyword in self.MULTI_KEYWORDS:
            if re.search(keyword, query_lower):
                return ClassificationResult(
                    complexity=QueryComplexityLevel.MULTI,
                    confidence=0.8,
                    reasoning=f"MULTI keyword: '{keyword}'"
                )

        # 3. æ£€æŸ¥ SINGLE å…³é”®è¯
        for keyword in self.SINGLE_KEYWORDS:
            if re.search(keyword, query_lower):
                return ClassificationResult(
                    complexity=QueryComplexityLevel.SINGLE,
                    confidence=0.8,
                    reasoning=f"SINGLE keyword: '{keyword}'"
                )

        # 4. åŸºäºé•¿åº¦çš„é»˜è®¤åˆ†ç±»
        if word_count <= 8:
            return ClassificationResult(
                complexity=QueryComplexityLevel.ZERO,
                confidence=0.5,
                reasoning=f"Short query without special keywords ({word_count} words)"
            )
        elif word_count <= 20:
            return ClassificationResult(
                complexity=QueryComplexityLevel.SINGLE,
                confidence=0.5,
                reasoning=f"Medium query ({word_count} words)"
            )
        else:
            return ClassificationResult(
                complexity=QueryComplexityLevel.MULTI,
                confidence=0.5,
                reasoning=f"Long query ({word_count} words)"
            )

    def _classify_by_llm(self, query: str) -> ClassificationResult:
        """ä½¿ç”¨ LLM è¿›è¡Œå¤æ‚åº¦åˆ†ç±»"""
        import requests

        prompt = f'''Classify the following query into one of three complexity levels:

A (ZERO): Simple factual questions that can be answered directly from common knowledge, no retrieval needed.
B (SINGLE): Questions requiring a single retrieval step to find relevant information.
C (MULTI): Complex questions requiring multi-hop reasoning, comparison, or iterative retrieval.

Query: "{query}"

Respond with only the letter (A, B, or C) and a brief reason.
Format: [LETTER]: [reason]'''

        try:
            response = requests.post(
                f"{self.llm_base_url}/chat/completions",
                headers={"Content-Type": "application/json"},
                json={
                    "model": self.llm_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 50,
                    "temperature": 0.1,
                },
                timeout=30,
            )
            if response.status_code == 200:
                content = response.json()["choices"][0]["message"]["content"].strip()
                # Parse response: "A: reason" or "B: reason" or "C: reason"
                if content.startswith("A"):
                    return ClassificationResult(
                        complexity=QueryComplexityLevel.ZERO,
                        confidence=0.9,
                        reasoning=f"LLM: {content}"
                    )
                elif content.startswith("B"):
                    return ClassificationResult(
                        complexity=QueryComplexityLevel.SINGLE,
                        confidence=0.9,
                        reasoning=f"LLM: {content}"
                    )
                elif content.startswith("C"):
                    return ClassificationResult(
                        complexity=QueryComplexityLevel.MULTI,
                        confidence=0.9,
                        reasoning=f"LLM: {content}"
                    )
        except Exception as e:
            print(f"[Classifier] LLM error: {e}, falling back to rule-based")

        # Fallback to rule-based
        return self._classify_by_rule(query)

    def execute(self, data: AdaptiveRAGQueryData) -> AdaptiveRAGQueryData:
        if self.classifier_type == "llm":
            classification = self._classify_by_llm(data.query)
        elif self.classifier_type == "hybrid":
            classification = self._classify_by_rule(data.query)
            if classification.confidence < 0.7:
                classification = self._classify_by_llm(data.query)
        else:
            classification = self._classify_by_rule(data.query)

        data.classification = classification
        import sys
        print(f"[Classify] {data.query[:50]}... -> {classification.complexity.name} ({classification.reasoning})", file=sys.stderr, flush=True)
        return data


class ZeroComplexityFilter(FilterFunction):
    """è¿‡æ»¤: åªä¿ç•™ ZERO å¤æ‚åº¦çš„æŸ¥è¯¢"""
    def execute(self, data: AdaptiveRAGQueryData) -> bool:
        if not isinstance(data, AdaptiveRAGQueryData) or data.classification is None:
            return False
        is_match = data.classification.complexity == QueryComplexityLevel.ZERO
        if is_match:
            print(f"  ZERO branch: {data.query[:50]}...")
        return is_match


class SingleComplexityFilter(FilterFunction):
    """è¿‡æ»¤: åªä¿ç•™ SINGLE å¤æ‚åº¦çš„æŸ¥è¯¢"""
    def execute(self, data: AdaptiveRAGQueryData) -> bool:
        if not isinstance(data, AdaptiveRAGQueryData) or data.classification is None:
            return False
        is_match = data.classification.complexity == QueryComplexityLevel.SINGLE
        if is_match:
            print(f"  SINGLE branch: {data.query[:50]}...")
        return is_match


class MultiComplexityFilter(FilterFunction):
    """è¿‡æ»¤: åªä¿ç•™ MULTI å¤æ‚åº¦çš„æŸ¥è¯¢"""
    def execute(self, data: AdaptiveRAGQueryData) -> bool:
        if not isinstance(data, AdaptiveRAGQueryData) or data.classification is None:
            return False
        is_match = data.classification.complexity == QueryComplexityLevel.MULTI
        if is_match:
            print(f"  MULTI branch: {data.query[:50]}...")
        return is_match


class NoRetrievalStrategy(MapFunction):
    """ç­–ç•¥ A: æ— æ£€ç´¢ - ç›´æ¥ LLM ç”Ÿæˆ"""

    def __init__(self, llm_base_url: str = "http://11.11.11.7:8903/v1", llm_model: str = "Qwen/Qwen2.5-7B-Instruct", max_tokens: int = 512, **kwargs):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.max_tokens = max_tokens

    def _generate(self, query: str) -> str:
        import requests
        try:
            response = requests.post(f"{self.llm_base_url}/chat/completions", json={"model": self.llm_model, "messages": [{"role": "user", "content": query}], "max_tokens": self.max_tokens, "temperature": 0.7}, timeout=60)
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[Generation Error] {str(e)}"

    def execute(self, data: AdaptiveRAGQueryData) -> AdaptiveRAGResultData:
        start_time = time.time()
        print(f"  ğŸ”µ NoRetrieval: {data.query[:50]}...")
        answer = self._generate(data.query)
        return AdaptiveRAGResultData(query=data.query, answer=answer, strategy_used="no_retrieval", complexity="ZERO", retrieval_steps=0, processing_time_ms=(time.time() - start_time) * 1000)


class SingleRetrievalStrategy(MapFunction):
    """ç­–ç•¥ B: å•æ¬¡æ£€ç´¢ + ç”Ÿæˆï¼ˆæœåŠ¡åŒ–å‘é‡æ£€ç´¢ç‰ˆæœ¬ï¼‰

    ä½¿ç”¨ self.call_service("embedding") å’Œ self.call_service("vector_db") è¿›è¡ŒçœŸæ­£çš„å‘é‡æ£€ç´¢ã€‚
    ä½¿ç”¨ self.call_service("llm") è¿›è¡Œç”Ÿæˆã€‚
    """

    def __init__(self, llm_base_url: str = "http://11.11.11.7:8903/v1", llm_model: str = "Qwen/Qwen2.5-7B-Instruct", max_tokens: int = 512, top_k: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.max_tokens = max_tokens
        self.top_k = top_k
        self._hostname = socket.gethostname()

    def _retrieve_via_service(self, query: str) -> list[dict]:
        """ä½¿ç”¨æœåŠ¡è¿›è¡Œå‘é‡æ£€ç´¢"""
        import numpy as np
        try:
            # RPC: call_service(name, *args, **kwargs) calls service.process(*args, **kwargs)
            query_embeddings = self.call_service("embedding", texts=[query])
            if not query_embeddings:
                print(f"    âš ï¸ Failed to get query embedding")
                return []

            query_vec = np.array(query_embeddings[0], dtype=np.float32)

            # RPC: call vector_db.process(query_vec, k=self.top_k)
            results = self.call_service("vector_db", query_vec=query_vec, k=self.top_k)

            # Convert to document format
            docs = []
            for score, metadata in results:
                docs.append({
                    "id": metadata.get("id", ""),
                    "content": metadata.get("content", metadata.get("text", "")),
                    "score": float(score),
                })
            return docs

        except Exception as e:
            print(f"    âš ï¸ Service retrieval error: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _generate_via_service(self, query: str, context: str) -> str:
        """ä½¿ç”¨ LLM æœåŠ¡è¿›è¡Œç”Ÿæˆ"""
        try:
            messages = [
                {"role": "system", "content": "Answer based on the provided context."},
                {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"},
            ]
            # RPC: call llm.process(messages, max_tokens, temperature)
            # timeout=120 to avoid service call timeout for LLM inference
            return self.call_service("llm", messages=messages, max_tokens=self.max_tokens, temperature=0.7, timeout=120)
        except Exception as e:
            # Fallback to direct request
            import requests
            try:
                response = requests.post(f"{self.llm_base_url}/chat/completions", json={"model": self.llm_model, "messages": [{"role": "system", "content": "Answer based on the provided context."}, {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}], "max_tokens": self.max_tokens, "temperature": 0.7}, timeout=60)
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e2:
                return f"[Generation Error] {str(e2)}"

    def execute(self, data: AdaptiveRAGQueryData) -> AdaptiveRAGResultData:
        start_time = time.time()
        print(f"  ğŸŸ¡ SingleRetrieval[{self._hostname}]: {data.query[:50]}...")
        docs = self._retrieve_via_service(data.query)
        context = "\n".join([f"[Doc {i+1}]: {d['content']}" for i, d in enumerate(docs)]) or "No relevant documents found."
        answer = self._generate_via_service(data.query, context)
        return AdaptiveRAGResultData(query=data.query, answer=answer, strategy_used="single_retrieval", complexity="SINGLE", retrieval_steps=len(docs), processing_time_ms=(time.time() - start_time) * 1000)


class IterativeRetrievalInit(MapFunction):
    """ç­–ç•¥ C: è¿­ä»£æ£€ç´¢åˆå§‹åŒ–"""
    def execute(self, data: AdaptiveRAGQueryData) -> IterativeState:
        print(f"  ğŸ”´ IterativeRetrieval Init: {data.query[:50]}...")
        return IterativeState(original_query=data.query, current_query=data.query, accumulated_docs=[], reasoning_chain=[], iteration=0, is_complete=False, start_time=time.time(), classification=data.classification)


class IterativeRetriever(MapFunction):
    """è¿­ä»£æ£€ç´¢ç®—å­ï¼ˆæœåŠ¡åŒ–å‘é‡æ£€ç´¢ç‰ˆæœ¬ï¼‰

    ä½¿ç”¨ self.call_service("embedding") å’Œ self.call_service("vector_db") è¿›è¡ŒçœŸæ­£çš„å‘é‡æ£€ç´¢ã€‚
    """

    def __init__(self, top_k: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.top_k = top_k
        self._hostname = socket.gethostname()

    def _retrieve_via_service(self, query: str) -> list[dict]:
        """ä½¿ç”¨æœåŠ¡è¿›è¡Œå‘é‡æ£€ç´¢"""
        import numpy as np
        try:
            # RPC: call embedding.process(texts=[query])
            query_embeddings = self.call_service("embedding", texts=[query])
            if not query_embeddings:
                print(f"      âš ï¸ Failed to get query embedding")
                return []

            query_vec = np.array(query_embeddings[0], dtype=np.float32)

            # RPC: call vector_db.process(query_vec, k=self.top_k)
            results = self.call_service("vector_db", query_vec=query_vec, k=self.top_k)

            # Convert to document format
            docs = []
            for score, metadata in results:
                docs.append({
                    "id": metadata.get("id", ""),
                    "content": metadata.get("content", metadata.get("text", "")),
                    "score": float(score),
                })
            return docs

        except Exception as e:
            print(f"      âš ï¸ Service retrieval error: {e}")
            import traceback
            traceback.print_exc()
            return []

    def execute(self, state: IterativeState) -> IterativeState:
        if state.is_complete:
            return state

        new_docs = self._retrieve_via_service(state.current_query)
        state.accumulated_docs.extend(new_docs)
        state.reasoning_chain.append(f"[Retrieve] Query: '{state.current_query[:30]}' -> {len(new_docs)} docs")
        print(f"    ğŸ“š Retrieve[{self._hostname}][{state.iteration}]: {len(new_docs)} docs")
        return state


class IterativeReasoner(MapFunction):
    """è¿­ä»£æ¨ç†ç®—å­ï¼ˆæœåŠ¡åŒ– LLM ç‰ˆæœ¬ï¼‰"""

    def __init__(self, llm_base_url: str = "http://11.11.11.7:8903/v1", llm_model: str = "Qwen/Qwen2.5-7B-Instruct", max_iterations: int = 3, min_docs: int = 5, **kwargs):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.max_iterations = max_iterations
        self.min_docs = min_docs
        self._hostname = socket.gethostname()

    def _llm_call_via_service(self, messages: list[dict]) -> str:
        """ä½¿ç”¨ LLM æœåŠ¡è¿›è¡Œè°ƒç”¨"""
        try:
            # RPC: call llm.process(messages, max_tokens, temperature)
            # timeout=120 to avoid service call timeout for LLM inference
            return self.call_service("llm", messages=messages, max_tokens=256, temperature=0.7, timeout=120)
        except Exception:
            # Fallback to direct request
            import requests
            try:
                response = requests.post(f"{self.llm_base_url}/chat/completions", json={"model": self.llm_model, "messages": messages, "max_tokens": 256, "temperature": 0.7}, timeout=60)
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                return f"[LLM Error] {str(e)}"

    def execute(self, state: IterativeState) -> IterativeState:
        if state.is_complete:
            return state
        state.iteration += 1
        if state.iteration >= self.max_iterations or len(state.accumulated_docs) >= self.min_docs:
            state.is_complete = True
            state.reasoning_chain.append(f"[Reason] Complete (docs={len(state.accumulated_docs)})")
            print(f"    ğŸ§  Reason[{self._hostname}][{state.iteration}]: COMPLETE")
            return state
        context_so_far = "\n".join([f"- {d['content']}" for d in state.accumulated_docs[-3:]])
        messages = [{"role": "system", "content": "Generate a follow-up search query. Reply with ONLY the query."}, {"role": "user", "content": f"Original: {state.original_query}\n\nContext:\n{context_so_far}\n\nFollow-up query:"}]
        new_query = self._llm_call_via_service(messages).strip()
        state.current_query = new_query
        state.reasoning_chain.append(f"[Reason] Next query = '{new_query[:40]}'")
        print(f"    ğŸ§  Reason[{self._hostname}][{state.iteration}]: Next -> '{new_query[:30]}...'")
        return state


class FinalSynthesizer(MapFunction):
    """ç»¼åˆç”Ÿæˆç®—å­ï¼ˆæœåŠ¡åŒ– LLM ç‰ˆæœ¬ï¼‰"""

    def __init__(self, llm_base_url: str = "http://11.11.11.7:8903/v1", llm_model: str = "Qwen/Qwen2.5-7B-Instruct", **kwargs):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self._hostname = socket.gethostname()

    def _llm_call_via_service(self, messages: list[dict]) -> str:
        """ä½¿ç”¨ LLM æœåŠ¡è¿›è¡Œè°ƒç”¨"""
        try:
            # RPC: call llm.process(messages, max_tokens, temperature)
            # timeout=120 to avoid service call timeout for LLM inference
            return self.call_service("llm", messages=messages, max_tokens=512, temperature=0.7, timeout=120)
        except Exception:
            # Fallback to direct request
            import requests
            try:
                response = requests.post(f"{self.llm_base_url}/chat/completions", json={"model": self.llm_model, "messages": messages, "max_tokens": 512, "temperature": 0.7}, timeout=60)
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                return f"[LLM Error] {str(e)}"

    def execute(self, state: IterativeState) -> AdaptiveRAGResultData:
        context = "\n".join([f"[Doc {i+1}]: {d['content']}" for i, d in enumerate(state.accumulated_docs)])
        chain_text = "\n".join(state.reasoning_chain)
        messages = [{"role": "system", "content": "Synthesize all information to answer comprehensively."}, {"role": "user", "content": f"Question: {state.original_query}\n\nReasoning:\n{chain_text}\n\nContext:\n{context}\n\nAnswer:"}]
        answer = self._llm_call_via_service(messages)
        print(f"    âœ¨ Synthesize[{self._hostname}]: Generated answer ({len(answer)} chars)")
        return AdaptiveRAGResultData(query=state.original_query, answer=answer, strategy_used="iterative_retrieval", complexity="MULTI", retrieval_steps=state.iteration, processing_time_ms=(time.time() - state.start_time) * 1000)


class AdaptiveRAGResultSink(SinkFunction):
    """Adaptive-RAG ç»“æœæ”¶é›†å™¨ (å…¼å®¹ MetricsSink æ ¼å¼)"""

    METRICS_OUTPUT_DIR = "/tmp/sage_metrics"  # ä¸ MetricsSink ä½¿ç”¨ç›¸åŒç›®å½•
    _all_results: list[AdaptiveRAGResultData] = []

    # Drain é…ç½®ï¼šç­‰å¾…è¿œç¨‹èŠ‚ç‚¹ä¸Š Generator å®Œæˆå¤„ç†
    # é—®é¢˜ï¼šStopSignal å¯èƒ½æ¯”æ•°æ®å…ˆåˆ°è¾¾ï¼Œè€Œ Generator è¿˜åœ¨ç­‰å¾… LLM å“åº”
    # Adaptive-RAG ç­‰å¤æ‚åœºæ™¯å¯èƒ½éœ€è¦å¤šè½® LLM è°ƒç”¨ï¼ŒP99 å¯è¾¾ 150+ ç§’
    # è®¾ç½® drain_timeout=300sï¼ˆæ€»ç­‰å¾…æ—¶é—´ï¼‰å’Œ quiet_period=90sï¼ˆæ— æ•°æ®é™é»˜æœŸï¼‰
    drain_timeout: float = 300.0
    drain_quiet_period: float = 90.0

    def __init__(self, branch_name: str = "", **kwargs):
        super().__init__(**kwargs)
        self.branch_name = branch_name
        self.count = 0
        self.instance_id = f"{socket.gethostname()}_{os.getpid()}_{int(time.time() * 1000)}"
        os.makedirs(self.METRICS_OUTPUT_DIR, exist_ok=True)
        # ä½¿ç”¨ metrics_ å‰ç¼€ä»¥ä¾¿ run() çš„ _collect_metrics_from_files() èƒ½æ‰¾åˆ°
        self.metrics_output_file = f"{self.METRICS_OUTPUT_DIR}/metrics_{self.instance_id}.jsonl"

    def _write_to_file(self, data: AdaptiveRAGResultData) -> None:
        import json
        try:
            # å†™å…¥ MetricsSink å…¼å®¹æ ¼å¼ï¼ˆåŒ…å« type, success, total_latency_ms ç­‰å­—æ®µï¼‰
            record = {
                "type": "task",
                "success": True,
                "total_latency_ms": data.processing_time_ms,
                "node_id": socket.gethostname(),
                # Adaptive-RAG ç‰¹æœ‰å­—æ®µ
                "query": data.query,
                "answer": data.answer,
                "strategy_used": data.strategy_used,
                "complexity": data.complexity,
                "retrieval_steps": data.retrieval_steps,
                "branch_name": self.branch_name,
                "timestamp": time.time(),
            }
            with open(self.metrics_output_file, "a") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        except Exception as e:
            import sys
            print(f"[ResultSink] Write error: {e}", file=sys.stderr, flush=True)

    def execute(self, data: AdaptiveRAGResultData):
        self.count += 1
        AdaptiveRAGResultSink._all_results.append(data)
        self._write_to_file(data)
        import sys
        print(f"\n  [{self.branch_name}] Result #{self.count}: {data.query[:40]}... -> {data.strategy_used}", file=sys.stderr, flush=True)
        return data

    @classmethod
    def get_all_results(cls) -> list[AdaptiveRAGResultData]:
        return cls._all_results.copy()

    @classmethod
    def clear_results(cls):
        cls._all_results.clear()


# =============================================================================
# ä¸º Adaptive-RAG ç±»è®¾ç½®å›ºå®šçš„ __module__ï¼Œç¡®ä¿ Ray åºåˆ—åŒ–/ååºåˆ—åŒ–ä¸€è‡´æ€§
# Worker èŠ‚ç‚¹é€šè¿‡ common.operators å¯¼å…¥ï¼Œæ‰€ä»¥éœ€è¦è®¾ç½® __module__ ä¸º common.operators
# =============================================================================
_ADAPTIVE_RAG_CLASSES = [
    AdaptiveRAGQuerySource,
    QueryClassifier,
    ZeroComplexityFilter,
    SingleComplexityFilter,
    MultiComplexityFilter,
    NoRetrievalStrategy,
    SingleRetrievalStrategy,
    IterativeRetrievalInit,
    IterativeRetriever,
    IterativeReasoner,
    FinalSynthesizer,
    AdaptiveRAGResultSink,
]

for _cls in _ADAPTIVE_RAG_CLASSES:
    _cls.__module__ = "common.operators"


# =============================================================================
# Service-Based RAG Operators
# =============================================================================
# These operators use registered services via self.call_service()
# to avoid distributed access issues (each worker loading its own model/index).
#
# Services are registered in pipeline.py:
#   - embedding: EmbeddingService for vectorization
#   - vector_db: SageDBService for knowledge base retrieval
#   - llm: LLMService for generation
#
# Usage:
#   from .pipeline import register_all_services
#   register_all_services(env, config)
#   env.from_source(...).map(ServiceRetriever, ...).map(ServiceGenerator, ...)


class ServiceRetriever(MapFunction):
    """
    Service-based retriever using registered vector_db service.

    Uses self.call_service("vector_db") for retrieval.
    Avoids each worker loading its own embedding model and knowledge base.
    """

    def __init__(
        self,
        top_k: int = 5,
        stage: int = 1,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.top_k = top_k
        self.stage = stage
        self._hostname = socket.gethostname()

    def execute(self, data: TaskState) -> TaskState:
        """Execute retrieval using vector_db service"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"ServiceRetriever_{self.stage}"
        state.mark_started()

        try:
            retrieval_start = time.time()

            # Get embedding service and vector_db service
            embedding_service = self.call_service("embedding")
            vector_db = self.call_service("vector_db")

            # Embed query
            query_embeddings = embedding_service.embed([state.query])
            if not query_embeddings:
                raise ValueError("Failed to get query embedding")

            import numpy as np
            query_vec = np.array(query_embeddings[0], dtype=np.float32)

            # Search vector_db
            results = vector_db.search(query_vec, top_k=self.top_k)

            # Convert results to document format
            state.retrieved_docs = []
            for score, metadata in results:
                state.retrieved_docs.append({
                    "id": metadata.get("id", ""),
                    "title": metadata.get("title", ""),
                    "content": metadata.get("content", metadata.get("text", "")),
                    "score": float(score),
                })

            retrieval_time = time.time() - retrieval_start
            state.metadata["retrieval_time_ms"] = retrieval_time * 1000
            state.metadata["num_retrieved"] = len(state.retrieved_docs)
            state.success = True

        except Exception as e:
            state.success = False
            state.error = str(e)
            state.retrieved_docs = []
            import traceback
            traceback.print_exc()

        state.mark_completed()
        return state


class ServiceReranker(MapFunction):
    """
    Service-based reranker using registered embedding service.

    Uses self.call_service("embedding") for reranking.
    Computes more refined relevance scores using query-document similarity.
    """

    def __init__(
        self,
        top_k: int = 3,
        stage: int = 2,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.top_k = top_k
        self.stage = stage
        self._hostname = socket.gethostname()

    def execute(self, data: TaskState) -> TaskState:
        """Execute reranking using embedding service"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"ServiceReranker_{self.stage}"
        state.mark_started()

        try:
            rerank_start = time.time()

            if not state.retrieved_docs:
                state.success = True
                state.mark_completed()
                return state

            # Get embedding service
            embedding_service = self.call_service("embedding")

            # Get query embedding
            query_embeddings = embedding_service.embed([state.query])
            if not query_embeddings:
                # Fallback: keep original order
                state.retrieved_docs = state.retrieved_docs[:self.top_k]
                state.success = True
                state.mark_completed()
                return state

            # Get document embeddings
            doc_texts = [
                doc.get("content", "")[:500]
                for doc in state.retrieved_docs
            ]
            doc_embeddings = embedding_service.embed(doc_texts)

            if not doc_embeddings:
                state.retrieved_docs = state.retrieved_docs[:self.top_k]
                state.success = True
                state.mark_completed()
                return state

            # Compute cosine similarity and rerank
            import math
            query_vec = query_embeddings[0]

            reranked = []
            for doc, doc_vec in zip(state.retrieved_docs, doc_embeddings):
                # Cosine similarity
                dot = sum(a * b for a, b in zip(query_vec, doc_vec))
                norm1 = math.sqrt(sum(a * a for a in query_vec))
                norm2 = math.sqrt(sum(b * b for b in doc_vec))
                score = dot / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0
                reranked.append({**doc, "rerank_score": score})

            reranked.sort(key=lambda x: x.get("rerank_score", 0), reverse=True)
            state.retrieved_docs = reranked[:self.top_k]

            rerank_time = time.time() - rerank_start
            state.metadata["rerank_time_ms"] = rerank_time * 1000
            state.metadata["num_reranked"] = len(state.retrieved_docs)
            state.success = True

        except Exception as e:
            state.success = False
            state.error = str(e)

        state.mark_completed()
        return state


class ServicePromptor(MapFunction):
    """
    Promptor that builds prompts from retrieved documents.

    Combines query and context into LLM-ready format.
    No service needed - just formatting.
    """

    def __init__(
        self,
        stage: int = 3,
        max_context_length: int = 2000,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.stage = stage
        self.max_context_length = max_context_length
        self._hostname = socket.gethostname()

    def execute(self, data: TaskState) -> TaskState:
        """Build prompt from retrieved docs"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"ServicePromptor_{self.stage}"
        state.mark_started()

        try:
            context_parts = []
            total_length = 0

            for i, doc in enumerate(state.retrieved_docs):
                title = doc.get("title", f"Document {i + 1}")
                content = doc.get("content", "")
                doc_text = f"[{title}]\n{content}"

                if total_length + len(doc_text) > self.max_context_length:
                    break

                context_parts.append(doc_text)
                total_length += len(doc_text)

            state.context = "\n\n".join(context_parts)
            state.metadata["context_length"] = len(state.context)
            state.success = True

        except Exception as e:
            state.success = False
            state.error = str(e)
            state.context = ""

        state.mark_completed()
        return state


class ServiceGenerator(MapFunction):
    """
    Service-based generator using registered LLM service.

    Uses self.call_service("llm") for generation.
    Avoids each worker initializing its own LLM client.
    """

    def __init__(
        self,
        stage: int = 4,
        output_file: str | None = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.stage = stage
        self.output_file = output_file
        self._hostname = socket.gethostname()

        if self.output_file:
            output_path = Path(self.output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)

    def execute(self, data: TaskState) -> TaskState:
        """Execute generation using LLM service"""
        if not isinstance(data, TaskState):
            return data

        state = data
        state.node_id = self._hostname
        state.stage = self.stage
        state.operator_name = f"ServiceGenerator_{self.stage}"
        state.mark_started()

        try:
            gen_start = time.time()

            # Build messages
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful assistant. Answer based on the provided context. If no relevant information, say so.",
                },
                {
                    "role": "user",
                    "content": f"Context:\n{state.context}\n\nQuestion: {state.query}",
                },
            ]

            # Generate response via RPC (timeout=120 for LLM inference)
            state.response = self.call_service("llm", messages=messages, max_tokens=512, temperature=0.7, timeout=120)

            gen_time = time.time() - gen_start
            state.metadata["generation_time_ms"] = gen_time * 1000
            state.success = True

            # Save to file if configured
            if self.output_file:
                self._save_response(state, gen_time)

        except Exception as e:
            state.success = False
            state.error = str(e)
            state.response = f"[Error] {str(e)}"

        state.mark_completed()
        return state

    def _save_response(self, state: TaskState, gen_time: float) -> None:
        """Save response to file"""
        if not self.output_file:
            return
        try:
            import json
            from datetime import datetime

            record = {
                "timestamp": datetime.now().isoformat(),
                "task_id": state.task_id,
                "node_id": state.node_id,
                "query": state.query,
                "context": state.context,
                "response": state.response,
                "generation_time_ms": gen_time * 1000,
            }
            with open(self.output_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        except Exception as e:
            print(f"[Warning] Failed to save response: {e}")


# Set __module__ for Service-based operators for Ray serialization
_SERVICE_OPERATOR_CLASSES = [
    ServiceRetriever,
    ServiceReranker,
    ServicePromptor,
    ServiceGenerator,
]

for _cls in _SERVICE_OPERATOR_CLASSES:
    _cls.__module__ = "common.operators"