dataset:
  flashrag:
    data_dir: null
    dataset_name: nq
    split: train
  max_context_tokens: 4096
  num_samples: 100
  seed: 42
  top_k: 8
  type: wiki18_local
  wiki18:
    documents_path: /home/cyb/wiki18_fulldoc.jsonl
    embedding_dim: 1024
    embedding_model: BAAI/bge-large-en-v1.5
    index_path: /home/cyb/wiki18_maxp.index
    mapping_path: /home/cyb/wiki18_maxp_maxp_mapping.json
evaluation:
  compute_mrr: true
  compute_recall: true
  context_pooling: mean
  metric: mean_normalized_rank
  output_top_k: 10
  query_pooling: max
  recall_k:
  - 1
  - 3
  - 5
  - 10
  window_size: 128
metadata:
  author: SAGE Benchmark
  date: null
  description: Attention head selection for retrieval performance evaluation
  paper: 'REFORM: Compress, Gather, and Recompute (Appendix B.5)'
model:
  cache_dir: null
  dtype: bfloat16
  gpu_memory_utilization: 0.8
  layer_range:
  - 0
  - 32
  max_length: 8192
  name: /home/cyb/Llama-3.1-8B-Instruct
  use_flash_attention: false
name: head_selection_experiment
output:
  log_interval: 10
  plot_heatmap: true
  plot_layer_stats: true
  plot_top_heads: true
  save_dir: ./results
  save_per_head_results: true
  save_plots: true
output_dir: ./results
