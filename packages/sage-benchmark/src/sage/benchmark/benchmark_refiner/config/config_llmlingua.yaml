pipeline:
  name: "sage-benchmark-llmlingua-rag"
  description: "LLMLingua RAG Pipeline with Perplexity-based Compression"
  version: "1.0.0"

source:
  # 数据源类型：'local'（本地 JSONL） 或 'hf'（HuggingFace Dataset）
  type: "hf"
  # HuggingFace Dataset 参数
  hf_dataset_name: "RUC-NLPIR/FlashRAG_datasets"
  hf_dataset_config: "nq"  # Natural Questions dataset
  hf_split: "test"
  max_samples: 20

retriever:
  # 检索器类型: wiki18_faiss
  type: "wiki18_faiss"

  # 通用配置
  dimension: 1024    # BGE-Large-EN-v1.5模型的维度
  top_k: 100

  # Wiki18 FAISS 专用配置
  faiss:
    index_path: "${HOME}/wiki18_maxp.index"
    documents_path: "${HOME}/wiki18_fulldoc.jsonl"
    mapping_path: "${HOME}/wiki18_maxp_maxp_mapping.json"  # 段落到文档的映射

  # 嵌入模型配置
  embedding:
    method: "hf"
    model: "BAAI/bge-large-en-v1.5"  # BGE-Large-EN-v1.5模型
    gpu_device: 0

generator:
  vllm:
    api_key: "token-abc123"
    method: "openai"
    model_name: "${HOME}/Llama-3.1-8B-Instruct"
    base_url: "http://sage2:8000/v1"
    seed: 42

promptor:
  platform: "local"

llmlingua:
  # LLMLingua 压缩配置
  enabled: true  # 设为false即为baseline模式

  # 模型配置
  # LLMLingua-2 (推荐，快速):
  model_name: "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank"
  use_llmlingua2: true

  # LLMLingua-1 (LLM-based，较慢但质量更好):
  # model_name: "meta-llama/Llama-2-7b-hf"
  # use_llmlingua2: false

  # 设备配置
  device: "cuda:0"  # 或 "cpu", "cuda", "cuda:1" 等

  # 压缩参数
  budget: 2048              # 压缩后最大token数
  compression_ratio:        # 压缩比例 (0-1)，可选，与 budget 二选一

  # LLMLingua 特有参数
  target_token: -1          # 目标 token 数 (-1 表示使用 rate/budget)
  context_budget: "+100"    # 上下文预算调整

  # 过滤层级
  use_sentence_level_filter: false  # 句子级过滤（较粗粒度）
  use_context_level_filter: true    # 上下文级过滤
  use_token_level_filter: true      # token 级过滤（最细粒度）

  # 特殊 token 处理
  force_tokens:             # 强制保留的 token
    - "\n"
    - "."
    - "?"
    - "!"
    - ","
  drop_consecutive: false   # 是否合并连续空格

sink:
  platform: "local"

evaluate:
  platform: "local"
