pipeline:
  name: "sage-api-operator-operator_test"
  description: "Test pipeline for Sage API Operator"
  version: "1.0.0"

source:
  # 数据源类型：'local'（本地 JSONL） 或 'hf'（HuggingFace Dataset）
  type: "hf"
  # 本地 JSONL 文件路径（type=local 时生效）
  data_path: "packages/sage-benchmark/src/sage/benchmark/benchmark_rag/data/sample/evaluate.json"

  # HuggingFace Dataset 参数（type=hf 时生效）
  hf_dataset_name:   "RUC-NLPIR/FlashRAG_datasets"
  hf_dataset_config: "nq"
  hf_split:          "test"

retriever:
  # 检索器类型选择: 'chroma' 或 'wiki18_faiss'
  type: "wiki18_faiss"  # 使用Wiki18 FAISS检索器（MaxP模式）
  
  # 通用配置
  dimension: 1024    # BGE-large-en-v1.5模型的维度
  top_k: 8           # LongRAG默认Top-8
  
  # Wiki18 FAISS MaxP 配置（LongRAG标准）
  faiss:
    # MaxP索引路径（passage级别）
    index_path: ""
    documents_path: ""
    
    # MaxP参数（Dai and Callan, 2019）
    passage_length: 100    # words，每个passage的长度
    passage_stride: 50     # words，滑动窗口步长
    max_length: 512        # tokens，编码时的最大序列长度
    normalize: true        # 向量归一化（LongRAG配置）
  
  # 嵌入模型配置（LongRAG使用bge-large-en-v1.5）
  embedding:
    method: "hf"
    model: "BAAI/bge-large-en-v1.5"  # LongRAG官方使用的模型
    gpu_device: 1

generator:
  local:
    method: "hf"
    model_name: "meta-llama/Llama-2-13b-chat-hf"
    seed: 42

  vllm:
    api_key: "token-abc123"
    method: "openai"
    model_name: "meta-llama/Llama-3.1-8B-Instruct"
    base_url: "http://sage2:8000/v1"
    seed: 42

  remote:
    api_key: ""
    method: "openai"
    model_name: "qwen-turbo-0919"
    base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
    seed: 42

promptor:
  platform: "local"

sink:
  platform: "local"

refiner:
  base_model_path: "Qwen/Qwen2.5-3B-Instruct"
  query_analysis_module_lora_path: "jinjiajie/Query-Analysis-Qwen2.5-3B-Instruct"
  doc_structuring_module_lora_path: "jinjiajie/Doc-Structuring-Qwen2.5-3B-Instruct"
  global_selection_module_lora_path: "jinjiajie/Global-Selection-Qwen2.5-3B-Instruct"
  score_model_name: "bge-reranker-v2-m3"
  score_model_path: "BAAI/bge-reranker-v2-m3"
  max_model_len: 25000
  budget: 2048
  gpu_device: 1  # Refiner使用GPU 1（避免与retriever的GPU 0冲突）
  gpu_memory_utilization: 0.7

evaluate:
  platform: "local"