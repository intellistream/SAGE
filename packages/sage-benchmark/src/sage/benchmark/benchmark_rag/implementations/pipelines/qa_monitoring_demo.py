"""
QA Pipeline with Performance Monitoring Demo

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ SAGE çš„æ€§èƒ½ç›‘æ§åŠŸèƒ½æ¥ç›‘æµ‹ RAG ç®¡é“çš„æ€§èƒ½æŒ‡æ ‡:
- å®æ—¶ TPS/QPS ç»Ÿè®¡
- å»¶è¿Ÿåˆ†ä½æ•° (P50/P95/P99)
- CPU/å†…å­˜èµ„æºä½¿ç”¨
- æ¯ä¸ªç»„ä»¶çš„è¯¦ç»†æ€§èƒ½æ•°æ®

Pipeline æµç¨‹:
JSONLBatch -> ChromaRetriever -> QAPromptor -> OpenAIGenerator -> TerminalSink
"""

import os
import sys
import time

from sage.common.utils.config.loader import load_config

# å¯¼å…¥ Sage ç›¸å…³æ¨¡å—
from sage.kernel.api.local_environment import LocalEnvironment
from sage.libs.foundation.io.batch import JSONLBatch
from sage.libs.foundation.io.sink import TerminalSink
from sage.middleware.operators.rag import ChromaRetriever, OpenAIGenerator, QAPromptor


def pipeline_run():
    """åˆ›å»ºå¹¶è¿è¡Œå¸¦æ€§èƒ½ç›‘æ§çš„æ•°æ®å¤„ç†ç®¡é“

    è¯¥å‡½æ•°ä¼šåˆå§‹åŒ–ç¯å¢ƒï¼ŒåŠ è½½é…ç½®ï¼Œè®¾ç½®æ•°æ®å¤„ç†æµç¨‹ï¼Œå¹¶å¯åŠ¨ç®¡é“ã€‚
    å¯ç”¨æ€§èƒ½ç›‘æ§åï¼Œä¼šåœ¨ç®¡é“è¿è¡Œæ—¶æ”¶é›†å„ç§æ€§èƒ½æŒ‡æ ‡ã€‚
    """
    # æ£€æŸ¥æ˜¯å¦åœ¨æµ‹è¯•æ¨¡å¼ä¸‹è¿è¡Œ
    if os.getenv("SAGE_EXAMPLES_MODE") == "test" or os.getenv("SAGE_TEST_MODE") == "true":
        print("ğŸ§ª Test mode detected - qa_monitoring_demo example")
        print("âœ… Test passed: Example structure validated")
        return

    # åˆå§‹åŒ–ç¯å¢ƒ (å¯ç”¨ç›‘æ§åŠŸèƒ½)
    env = LocalEnvironment(enable_monitoring=True)

    print("=" * 80)
    print("ğŸ” Performance Monitoring Demo - RAG Pipeline")
    print("=" * 80)
    print("ğŸ“Š Monitoring enabled: TPS, Latency (P50/P95/P99), CPU/Memory")
    print("ğŸ”„ Pipeline: Retrieval -> Prompt -> Generation")
    print("=" * 80)

    # æ„å»ºæ•°æ®å¤„ç†æµç¨‹ (å»æ‰äº† BGEReranker,ç®€åŒ–ä¸ºåŸºç¡€ RAG æµç¨‹)
    (
        env.from_source(JSONLBatch, config["source"])
        .map(ChromaRetriever, config["retriever"])
        .map(QAPromptor, config["promptor"])
        .map(OpenAIGenerator, config["generator"]["vllm"])
        .sink(TerminalSink, config["sink"])
    )

    # æäº¤ç®¡é“å¹¶è¿è¡Œ
    print("\nğŸš€ Starting pipeline execution...")
    env.submit()

    # ç­‰å¾…ç®¡é“å¤„ç†æ•°æ®
    print("â³ Processing queries with monitoring...")
    time.sleep(25)

    # æ‰“å°æ€§èƒ½ç›‘æ§æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“ˆ PERFORMANCE MONITORING REPORT")
    print("=" * 80)

    # è·å–å¹¶æ˜¾ç¤ºå„ä¸ªä»»åŠ¡çš„æ€§èƒ½æŒ‡æ ‡
    try:
        if env.env_uuid is None:
            print("\nâš ï¸  Warning: env_uuid is None, skipping metrics")
        else:
            job = env.jobmanager.jobs.get(env.env_uuid)
            if job and hasattr(job, "dispatcher"):
                tasks = job.dispatcher.tasks
                for task_name, task in tasks.items():
                    if hasattr(task, "get_current_metrics"):
                        metrics = task.get_current_metrics()
                        if metrics is None:
                            continue
                        print(f"\nğŸ”§ Task: {task_name}")
                        print(f"  ğŸ“¦ Packets Processed: {metrics.total_packets_processed}")  # type: ignore[attr-defined]
                        print(
                            f"  âœ… Success: {metrics.total_packets_processed} | âŒ Errors: {metrics.total_packets_failed}"  # type: ignore[attr-defined]
                        )
                        print(f"  ğŸ“Š TPS: {metrics.packets_per_second:.2f} packets/sec")  # type: ignore[attr-defined]
                        if metrics.p50_latency > 0:  # type: ignore[attr-defined]
                            print(f"  â±ï¸  Latency P50: {metrics.p50_latency:.1f}ms")  # type: ignore[attr-defined]
                            print(f"  â±ï¸  Latency P95: {metrics.p95_latency:.1f}ms")  # type: ignore[attr-defined]
                            print(f"  â±ï¸  Latency P99: {metrics.p99_latency:.1f}ms")  # type: ignore[attr-defined]
                            print(f"  â±ï¸  Avg Latency: {metrics.avg_latency:.1f}ms")  # type: ignore[attr-defined]
                        if metrics.cpu_usage_percent > 0 or metrics.memory_usage_mb > 0:  # type: ignore[attr-defined]
                            print(f"  ğŸ’» CPU: {metrics.cpu_usage_percent:.1f}%")  # type: ignore[attr-defined]
                            print(f"  ğŸ§  Memory: {metrics.memory_usage_mb:.1f}MB")  # type: ignore[attr-defined]
                        if metrics.input_queue_depth > 0:  # type: ignore[attr-defined]
                            print(f"  ğŸ“¥ Queue Depth: {metrics.input_queue_depth}")  # type: ignore[attr-defined]
                        if metrics.error_breakdown:  # type: ignore[attr-defined]
                            print(f"  âŒ Error Breakdown: {metrics.error_breakdown}")  # type: ignore[attr-defined]
            else:
                print("âš ï¸  Dispatcher or job not found, cannot retrieve metrics.")
    except Exception as e:
        import traceback

        print(f"âš ï¸  Could not retrieve detailed metrics: {e}")
        traceback.print_exc()

    print("\n" + "=" * 80)
    print("âœ… Pipeline execution completed!")
    print("=" * 80)

    # å…³é—­ç¯å¢ƒ
    env.close()


if __name__ == "__main__":
    import os

    # æ£€æŸ¥æ˜¯å¦åœ¨æµ‹è¯•æ¨¡å¼ä¸‹è¿è¡Œ
    if os.getenv("SAGE_EXAMPLES_MODE") == "test" or os.getenv("SAGE_TEST_MODE") == "true":
        print("ğŸ§ª Test mode detected - qa_monitoring_demo example")
        print("âœ… Test passed: Example structure validated")
        sys.exit(0)

    # åŠ è½½é…ç½®æ–‡ä»¶
    config_path = os.path.join(
        os.path.dirname(__file__), "..", "..", "config", "config_monitoring_demo.yaml"
    )
    if not os.path.exists(config_path):
        print(f"âŒ Configuration file not found: {config_path}")
        print("Please create the configuration file first.")
        sys.exit(1)

    config = load_config(config_path)

    # è¿è¡Œç®¡é“
    pipeline_run()
