import argparse
import json
import re
import string
from collections import Counter
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
from sage.common.config.output_paths import get_output_file

# ============================================================================
# æ–‡æœ¬æ ‡å‡†åŒ–æ¨¡å—
# ============================================================================

# # è‹±æ–‡å¸¸è§åœé¡¿è¯/åœç”¨è¯åˆ—è¡¨
STOP_WORDS = {}
#     'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
#     'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
#     'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'shall',
#     'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
#     'my', 'your', 'his', 'her', 'its', 'our', 'their', 'mine', 'yours', 'ours', 'theirs',
#     'this', 'that', 'these', 'those', 'here', 'there', 'where', 'when', 'why', 'how',
#     'what', 'which', 'who', 'whom', 'whose', 'if', 'then', 'else', 'so', 'as', 'than',
#     'not', 'no', 'yes', 'all', 'any', 'some', 'each', 'every', 'other', 'another',
#     'more', 'most', 'less', 'least', 'much', 'many', 'few', 'little', 'very', 'quite',
#     'just', 'only', 'also', 'too', 'even', 'still', 'yet', 'already', 'again',
#     'up', 'down', 'out', 'off', 'over', 'under', 'above', 'below', 'through', 'between',
#     'into', 'onto', 'from', 'within', 'without', 'during', 'before', 'after', 'since', 'until'
# }


def normalize_text_basic(text: str) -> str:
    """
    åŸºç¡€æ–‡æœ¬æ ‡å‡†åŒ–ï¼ˆç”¨äºç®€å•åŒ¹é…ï¼‰
    Args:
        text: åŸå§‹æ–‡æœ¬
    Returns:
        æ ‡å‡†åŒ–åçš„æ–‡æœ¬
    """
    # ç§»é™¤æ•°å­—æ ‡è®° (1., 2., 3., etc.)
    text = re.sub(r"\d+\.\s*", "", text)
    # ç§»é™¤æ¢è¡Œç¬¦
    text = text.replace("\n", " ")
    # ç§»é™¤å¤šä½™ç©ºæ ¼å¹¶è½¬ä¸ºå°å†™
    text = " ".join(text.split()).lower().strip()
    return text


def normalize_text_advanced(text: str) -> str:
    """
    é«˜çº§æ–‡æœ¬æ ‡å‡†åŒ–ï¼ˆç”¨äºç²¾ç¡®åŒ¹é…ï¼Œç§»é™¤åœç”¨è¯å’Œæ ‡ç‚¹ï¼‰
    Args:
        text: åŸå§‹æ–‡æœ¬
    Returns:
        æ ‡å‡†åŒ–åçš„æ–‡æœ¬
    """
    # è½¬ä¸ºå°å†™
    # text = text.lower()

    # ç§»é™¤æ ‡ç‚¹ç¬¦å·
    text = "".join(ch for ch in text if ch not in string.punctuation)

    # ç§»é™¤articles (a, an, the)
    text = re.sub(r"\b(a|an|the)\b", " ", text)

    # ç§»é™¤åœç”¨è¯
    words = text.split()
    words = [word for word in words if word not in STOP_WORDS]

    # æ ‡å‡†åŒ–ç©ºæ ¼
    text = " ".join(words)

    return text


# ============================================================================
# è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
# ============================================================================


def compute_f1(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•°"""
    pred_tokens = normalize_text_advanced(prediction).split()
    gt_tokens = normalize_text_advanced(ground_truth).split()

    if not pred_tokens or not gt_tokens:
        return 0.0

    common = Counter(pred_tokens) & Counter(gt_tokens)
    num_same = sum(common.values())

    if num_same == 0:
        return 0.0

    precision = num_same / len(pred_tokens)
    recall = num_same / len(gt_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1


def compute_exact_match(prediction: str, ground_truth: str) -> int:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…åˆ†æ•°"""
    return int(
        normalize_text_advanced(prediction) == normalize_text_advanced(ground_truth)
    )


def compute_accuracy_single(prediction: str, ground_truths: List[str]) -> float:
    """
    è®¡ç®—å•ä¸ªé¢„æµ‹çš„accuracyåˆ†æ•°ï¼Œä½¿ç”¨å¤šç§åŒ¹é…ç­–ç•¥
    Args:
        prediction: æ¨¡å‹é¢„æµ‹ç»“æœ
        ground_truths: æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨
    Returns:
        accuracyåˆ†æ•° (0.0 æˆ– 1.0)
    """
    # åŸºç¡€æ ‡å‡†åŒ–
    norm_pred = normalize_text_advanced(prediction)

    for gt in ground_truths:
        norm_gt = normalize_text_advanced(gt)

        if norm_gt in norm_pred:
            return 1.0

        # pred_words = set(normalize_text_advanced(prediction).split())
        # gt_words = set(normalize_text_advanced(gt).split())

        # if not pred_words or not gt_words:
        #     continue

        # # å¦‚æœground truthçš„æ‰€æœ‰å…³é”®è¯éƒ½åœ¨predictionä¸­
        # if gt_words.issubset(pred_words):
        #     return 1.0

    return 0.0


def evaluate_predictions(
    predictions: List[str], ground_truths: List[List[str]], metric: str = "accuracy"
) -> Dict[str, float]:
    """
    è¯„ä¼°é¢„æµ‹ç»“æœ
    Args:
        predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
        ground_truths: æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨çš„åˆ—è¡¨
        metric: è¯„ä¼°æŒ‡æ ‡ ("accuracy", "f1", "exact_match", "all")
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    results = {}

    if metric in ["accuracy", "all"]:
        accuracy_scores = [
            compute_accuracy_single(pred, truths)
            for pred, truths in zip(predictions, ground_truths)
        ]
        results["accuracy"] = 100 * np.mean(accuracy_scores)

    if metric in ["f1", "all"]:
        f1_scores = []
        for pred, truths in zip(predictions, ground_truths):
            # å¯¹æ¯ä¸ªground truthè®¡ç®—F1ï¼Œå–æœ€å¤§å€¼
            f1_max = max([compute_f1(pred, gt) for gt in truths]) if truths else 0.0
            f1_scores.append(f1_max)
        results["f1"] = 100 * np.mean(f1_scores)

    if metric in ["exact_match", "all"]:
        em_scores = []
        for pred, truths in zip(predictions, ground_truths):
            # å¯¹æ¯ä¸ªground truthè®¡ç®—EMï¼Œå–æœ€å¤§å€¼
            em_max = (
                max([compute_exact_match(pred, gt) for gt in truths]) if truths else 0
            )
            em_scores.append(em_max)
        results["exact_match"] = 100 * np.mean(em_scores)

    return results


def load_results(file_path: str) -> Dict[str, Any]:
    """åŠ è½½æ¨ç†ç»“æœæ–‡ä»¶"""
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)


def calculate_overall_scores(
    results_data: Dict[str, Any], metric: str = "all"
) -> Dict[str, Any]:
    """
    è®¡ç®—æ•´ä½“è¯„ä¼°åˆ†æ•°ï¼ˆä¸è¾“å‡ºæ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†åˆ†æ•°ï¼‰

    Args:
        results_data: æ¨ç†ç»“æœæ•°æ®
        metric: è¯„ä¼°æŒ‡æ ‡
    Returns:
        åŒ…å«æ•´ä½“åˆ†æ•°çš„æ•°æ®
    """
    results = results_data["results"]

    # æå–é¢„æµ‹å’ŒçœŸå®ç­”æ¡ˆ
    # å…¼å®¹ä¸åŒçš„å­—æ®µåç§°
    predictions = []
    ground_truths = []

    for item in results:
        # é¢„æµ‹ç»“æœå­—æ®µ
        pred = item.get("prediction") or item.get("model_output", "")
        predictions.append(pred)

        # çœŸå®ç­”æ¡ˆå­—æ®µ
        gt = item.get("ground_truth", [])
        if isinstance(gt, str):
            gt = [gt]
        ground_truths.append(gt)

    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    overall_scores = evaluate_predictions(predictions, ground_truths, metric)

    # æ„å»ºè¯„ä¼°ç»“æœï¼ˆåªåŒ…å«æ•´ä½“æŒ‡æ ‡ï¼‰
    evaluation_result = {
        "metadata": results_data.get("metadata", {}),
        "overall_scores": overall_scores,
        "summary": {
            "total_samples": len(results),
            "evaluation_metric": metric,
        },
    }

    return evaluation_result


def analyze_retrieval_quality(
    evaluation_result: Dict[str, Any], results_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    åˆ†ææ£€ç´¢è´¨é‡

    Args:
        evaluation_result: è¯„ä¼°ç»“æœæ•°æ®
        results_data: åŸå§‹ç»“æœæ•°æ®
    Returns:
        æ£€ç´¢è´¨é‡åˆ†æç»“æœ
    """
    detailed_results = results_data["results"]

    # ç»Ÿè®¡æ£€ç´¢ç›¸å…³ä¿¡æ¯
    total_samples = len(detailed_results)
    samples_with_context = 0
    context_lengths = []

    # åˆ†ææ£€ç´¢ä¸Šä¸‹æ–‡ä¸ç­”æ¡ˆçš„ç›¸å…³æ€§
    context_relevance_scores = []

    for item in detailed_results:
        # å…¼å®¹ä¸åŒçš„å­—æ®µåç§°
        contexts = item.get("retrieved_docs") or item.get("retrieved_context", [])

        if contexts:
            samples_with_context += 1
            context_lengths.append(len(contexts))

            # ç®€å•çš„ç›¸å…³æ€§åˆ†æï¼šæ£€æŸ¥çœŸå®ç­”æ¡ˆæ˜¯å¦å‡ºç°åœ¨æ£€ç´¢çš„ä¸Šä¸‹æ–‡ä¸­
            ground_truth = item.get("ground_truth", [])
            if isinstance(ground_truth, str):
                ground_truth = [ground_truth]

            # å¯¹æ¯ä¸ªçœŸå®ç­”æ¡ˆæ£€æŸ¥æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
            found_in_context = False
            for gt in ground_truth:
                gt_normalized = normalize_text_basic(gt)
                for context in contexts:
                    # å¤„ç†ä¸åŒçš„ä¸Šä¸‹æ–‡æ ¼å¼
                    if isinstance(context, dict):
                        context_text = context.get("text", "")
                    else:
                        context_text = str(context)

                    context_normalized = normalize_text_basic(context_text)
                    if gt_normalized in context_normalized:
                        found_in_context = True
                        break
                if found_in_context:
                    break

            context_relevance_scores.append(1.0 if found_in_context else 0.0)

    retrieval_analysis = {
        "total_samples": total_samples,
        "samples_with_context": samples_with_context,
        "context_coverage": (
            samples_with_context / total_samples if total_samples > 0 else 0.0
        ),
        "avg_context_count": np.mean(context_lengths) if context_lengths else 0.0,
        "context_relevance_rate": (
            np.mean(context_relevance_scores) if context_relevance_scores else 0.0
        ),
    }

    return retrieval_analysis


def print_evaluation_summary(evaluation_result: Dict[str, Any]):
    """æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦"""
    metadata = evaluation_result.get("metadata", {})
    scores = evaluation_result["overall_scores"]
    summary = evaluation_result["summary"]

    print("\n" + "=" * 60)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
    print("=" * 60)

    # æ‰“å°é…ç½®ä¿¡æ¯
    if metadata:
        print("ğŸ”§ é…ç½®ä¿¡æ¯:")
        if "pipeline_name" in metadata:
            print(f"   Pipeline: {metadata['pipeline_name']}")
        if "timestamp" in metadata:
            print(f"   æ—¶é—´: {metadata['timestamp']}")
        if "config" in metadata:
            config = metadata["config"]
            if "pipeline" in config and "pipeline_config" in config["pipeline"]:
                pipeline_config = config["pipeline"]["pipeline_config"]
                if "model_name" in pipeline_config:
                    print(f"   æ¨¡å‹: {pipeline_config['model_name']}")
                if "top_k" in pipeline_config:
                    print(f"   Top-K: {pipeline_config['top_k']}")
        if "total_samples" in metadata:
            print(f"   æ ·æœ¬æ•°: {metadata['total_samples']}")
        elif "summary" in metadata and "total_samples" in summary:
            print(f"   æ ·æœ¬æ•°: {summary['total_samples']}")

    print(f"\nğŸ“Š æ€»æ ·æœ¬æ•°: {summary['total_samples']}")
    print(f"ğŸ“ è¯„ä¼°æŒ‡æ ‡: {summary['evaluation_metric']}")

    print("\nğŸ“ˆ æ•´ä½“æ€§èƒ½æŒ‡æ ‡:")
    for metric, score in scores.items():
        print(f"   {metric.upper()}: {score:.2f}%")

    # æ·»åŠ æ£€ç´¢è´¨é‡åˆ†æ
    if "retrieval_analysis" in evaluation_result:
        retrieval_stats = evaluation_result["retrieval_analysis"]
        print("\nğŸ” æ£€ç´¢è´¨é‡åˆ†æ:")
        print(f"   ä¸Šä¸‹æ–‡è¦†ç›–ç‡: {100 * retrieval_stats['context_coverage']:.2f}%")
        print(f"   å¹³å‡æ£€ç´¢æ•°é‡: {retrieval_stats['avg_context_count']:.2f}")
        print(
            f"   ä¸Šä¸‹æ–‡ç›¸å…³æ€§: {100 * retrieval_stats['context_relevance_rate']:.2f}%"
        )

    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°RAGæ¨ç†ç»“æœ")
    parser.add_argument(
        "--results", "-r", type=str, required=True, help="æ¨ç†ç»“æœæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--metric",
        choices=["accuracy", "f1", "exact_match", "all"],
        default="all",
        help="è¯„ä¼°æŒ‡æ ‡",
    )
    parser.add_argument("--output", "-o", type=str, help="è¾“å‡ºè¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    # åŠ è½½æ¨ç†ç»“æœ
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ¨ç†ç»“æœ: {args.results}")
    results_data = load_results(args.results)

    # è®¡ç®—æ•´ä½“è¯„ä¼°åˆ†æ•°
    print(f"ğŸ”„ æ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡: {args.metric}")
    evaluation_result = calculate_overall_scores(results_data, args.metric)

    # åˆ†ææ£€ç´¢è´¨é‡ï¼ˆå¦‚æœæœ‰æ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
    print("ğŸ” æ­£åœ¨åˆ†ææ£€ç´¢è´¨é‡...")
    retrieval_analysis = analyze_retrieval_quality(evaluation_result, results_data)
    evaluation_result["retrieval_analysis"] = retrieval_analysis

    # æ‰“å°æ‘˜è¦
    print_evaluation_summary(evaluation_result)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    if args.output:
        output_path = args.output
    else:
        # ç”Ÿæˆé»˜è®¤è¾“å‡ºæ–‡ä»¶å
        input_path = Path(args.results)
        output_filename = f"evaluation_{input_path.stem}.json"
        output_path = get_output_file(output_filename, "benchmarks")

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(evaluation_result, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


if __name__ == "__main__":
    main()
