# SAGE Agent Bench å®éªŒè„šæœ¬# sage-benchmark å®éªŒè„šæœ¬# SAGE-Bench è¯„æµ‹æ¡†æ¶# SAGE-Bench è¯„æµ‹æ¡†æ¶

æœ¬ç›®å½•åŒ…å« Paper 1 (Agent èƒ½åŠ›è¯„æµ‹æ¡†æ¶) çš„å®Œæ•´å®éªŒè„šæœ¬ï¼ŒæŒ‰è®ºæ–‡ç« èŠ‚ç»„ç»‡ã€‚

## ç›®å½•ç»“æ„æœ¬ç›®å½•åŒ…å« Paper 1 (Agent èƒ½åŠ›è¯„æµ‹æ¡†æ¶) çš„å®Œæ•´å®éªŒè„šæœ¬ï¼ŒæŒ‰è®ºæ–‡ç« èŠ‚ç»„ç»‡ã€‚> æ”¯æŒ **15+ ç§æ–¹æ³•** å’Œ **8+ æ•°æ®é›†** çš„ Agent èƒ½åŠ›è¯„æµ‹æ¡†æ¶> æ”¯æŒ **15+ ç§æ–¹æ³•** å’Œ **6+ å¤–éƒ¨æ•°æ®é›†** çš„ Agent èƒ½åŠ›è¯„æµ‹æ¡†æ¶

````

scripts/

â”œâ”€â”€ sage-agent-bench              # CLI å…¥å£è„šæœ¬ (å¯æ‰§è¡Œ)## ç›®å½•ç»“æ„æœ¬æ¡†æ¶æœåŠ¡äºä¸¤ç¯‡è®ºæ–‡ï¼šæœ¬æ¡†æ¶æœåŠ¡äºä¸¤ç¯‡è®ºæ–‡ï¼š

â”œâ”€â”€ README.md                     # æœ¬æ–‡ä»¶

â””â”€â”€ experiments/                  # å®éªŒæ¨¡å—åŒ…

    â”œâ”€â”€ __init__.py

    â”‚```1. **Paper 1 (Benchmark)**: SAGE-Bench - ç»Ÿä¸€è¯„æµ‹æ¡†æ¶ï¼Œå¯¹æ¯”ç°æœ‰ SOTA æ–¹æ³•1. **Paper 1 (Benchmark)**: SAGE-Bench -

    â”‚  === æ ¸å¿ƒå·¥å…· ===

    â”œâ”€â”€ exp_utils.py              # å…±äº«å·¥å…· (ç¯å¢ƒã€æ•°æ®ã€ä¿å­˜ã€LLMå®¢æˆ·ç«¯)scripts/   ç»Ÿä¸€è¯„æµ‹æ¡†æ¶ï¼Œå¯¹æ¯”ç°æœ‰ SOTA æ–¹æ³•

    â”œâ”€â”€ figure_generator.py       # å­¦æœ¯å›¾è¡¨ç”Ÿæˆå™¨ (PDF/PNG)

    â”œâ”€â”€ table_generator.py        # LaTeX è¡¨æ ¼ç”Ÿæˆå™¨â”œâ”€â”€ sage_bench                    # CLI å…¥å£è„šæœ¬ (å¯æ‰§è¡Œ)

    â”œâ”€â”€ llm_service.py            # LLM æœåŠ¡ç®¡ç† (vLLM)

    â”œâ”€â”€ sage_bench_cli.py         # CLI å®ç°â”œâ”€â”€ README.md                     # æœ¬æ–‡ä»¶1. **Paper 2 (Method)**: SAGE-Agent - Streaming Adaptive Learning æ¡†æ¶1. **Paper 2 (Method)**:

    â”‚

    â”‚  === Section 5.2: ä¸»è¦è¯„æµ‹ ===â””â”€â”€ experiments/                  # å®éªŒæ¨¡å—åŒ…   SAGE-Agent - Streaming Adaptive Learning æ¡†æ¶

    â”œâ”€â”€ exp_main_timing.py        # å·¥å…·è°ƒç”¨æ—¶æœºè¯„æµ‹

    â”œâ”€â”€ exp_main_planning.py      # ä»»åŠ¡è§„åˆ’èƒ½åŠ›è¯„æµ‹    â”œâ”€â”€ __init__.py

    â”œâ”€â”€ exp_main_selection.py     # å·¥å…·é€‰æ‹©å‡†ç¡®ç‡è¯„æµ‹

    â”‚    â”‚---\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    â”‚  === Section 5.3: æ·±åº¦åˆ†æ ===

    â”œâ”€â”€ exp_analysis_error.py     # é”™è¯¯ç±»å‹åˆ†å¸ƒåˆ†æ    â”‚  === æ ¸å¿ƒå·¥å…· ===

    â”œâ”€â”€ exp_analysis_scaling.py   # å·¥å…·æ•°é‡æ‰©å±•æ€§åˆ†æ

    â”œâ”€â”€ exp_analysis_robustness.py # é²æ£’æ€§åˆ†æ    â”œâ”€â”€ exp_utils.py              # å…±äº«å·¥å…· (ç¯å¢ƒã€æ•°æ®ã€ä¿å­˜ã€LLMå®¢æˆ·ç«¯)## ğŸš€ å¿«é€Ÿå¼€å§‹## ğŸ“ è„šæœ¬æ¶æ„

    â”œâ”€â”€ exp_analysis_ablation.py  # æ¶ˆèå®éªŒ

    â”‚    â”œâ”€â”€ figure_generator.py       # å­¦æœ¯å›¾è¡¨ç”Ÿæˆå™¨ (PDF/PNG)

    â”‚  === Section 5.4: è·¨æ•°æ®é›† ===

    â”œâ”€â”€ exp_cross_dataset.py      # è·¨æ•°æ®é›†æ³›åŒ–è¯„æµ‹    â”œâ”€â”€ table_generator.py        # LaTeX è¡¨æ ¼ç”Ÿæˆå™¨### ç»Ÿä¸€ CLI å…¥å£ (æ¨è)### ç»Ÿä¸€å…¥å£ (æ¨è)

    â”‚

    â”‚  === Section 5.5: è®­ç»ƒæ–¹æ³•å¯¹æ¯” ===    â”œâ”€â”€ llm_service.py            # LLM æœåŠ¡ç®¡ç† (vLLM)

    â”œâ”€â”€ exp_training_comparison.py # è®­ç»ƒæ–¹æ³•å¯¹æ¯” (A-D)

    â”‚    â”œâ”€â”€ sage_bench_cli.py         # CLI å®ç°æ‰€æœ‰åŠŸèƒ½é€šè¿‡ `sage-bench` CLI è®¿é—®ï¼š\`\`\`bash

    â”‚  === ä¸»è¿è¡Œå™¨ ===

    â””â”€â”€ run_paper1_experiments.py  # Paper 1 å…¨æµç¨‹è¿è¡Œå™¨    â”‚

````

```
â”‚  === Section 5.2: ä¸»è¦è¯„æµ‹ ===# äº¤äº’å¼è¿è¡Œ
```

## å¿«é€Ÿå¼€å§‹

```
â”œâ”€â”€ exp_main_timing.py        # å·¥å…·è°ƒç”¨æ—¶æœºè¯„æµ‹
```

### 1. ç¯å¢ƒå‡†å¤‡

`````
â”œâ”€â”€ exp_main_planning.py      # ä»»åŠ¡è§„åˆ’èƒ½åŠ›è¯„æµ‹````bashpython sage_benchmark_cli.py
`````

````bash

# å®‰è£… sage-benchmark    â”œâ”€â”€ exp_main_selection.py     # å·¥å…·é€‰æ‹©å‡†ç¡®ç‡è¯„æµ‹

cd /path/to/SAGE

./quickstart.sh --dev --yes    â”‚# åˆ—å‡ºå¯ç”¨æ•°æ®é›†



# è®¾ç½®ç¯å¢ƒå˜é‡    â”‚  === Section 5.3: æ·±åº¦åˆ†æ ===

export SAGE_TEST_MODE=true  # å¯é€‰ï¼šå¯ç”¨æµ‹è¯•æ¨¡å¼

```    â”œâ”€â”€ exp_analysis_error.py     # é”™è¯¯ç±»å‹åˆ†å¸ƒåˆ†æsage-bench list datasets# æˆ–ç›´æ¥æŒ‡å®šå®éªŒ



### 2. ä½¿ç”¨ CLI    â”œâ”€â”€ exp_analysis_scaling.py   # å·¥å…·æ•°é‡æ‰©å±•æ€§åˆ†æ



```bash    â”œâ”€â”€ exp_analysis_robustness.py # é²æ£’æ€§åˆ†æpython sage_benchmark_cli.py --paper 1 --experiment tool_selection

# æŸ¥çœ‹å¸®åŠ©

./sage-agent-bench --help    â”œâ”€â”€ exp_analysis_ablation.py  # æ¶ˆèå®éªŒ



# === LLM æœåŠ¡ç®¡ç† ===    â”‚# åˆ—å‡ºå¯ç”¨æ–¹æ³•python sage_benchmark_cli.py --paper 2 --experiment sage_agent_full

./sage-agent-bench llm start                    # å¯åŠ¨ vLLM æœåŠ¡

./sage-agent-bench llm status                   # æ£€æŸ¥æœåŠ¡çŠ¶æ€    â”‚  === Section 5.4: è·¨æ•°æ®é›† ===

./sage-agent-bench llm stop                     # åœæ­¢æœåŠ¡

    â”œâ”€â”€ exp_cross_dataset.py      # è·¨æ•°æ®é›†æ³›åŒ–è¯„æµ‹sage-bench list methods```

# === è¿è¡Œå®éªŒ ===

# è¿è¡Œå•ä¸ªç« èŠ‚    â”‚

./sage-agent-bench run --section 5.2            # ä¸»è¦è¯„æµ‹

./sage-agent-bench run --section 5.3            # æ·±åº¦åˆ†æ    â”‚  === Section 5.5: è®­ç»ƒæ–¹æ³•å¯¹æ¯” ===

./sage-agent-bench run --section 5.4            # è·¨æ•°æ®é›†

./sage-agent-bench run --section 5.5            # è®­ç»ƒæ–¹æ³•å¯¹æ¯”    â”œâ”€â”€ exp_training_comparison.py # è®­ç»ƒæ–¹æ³•å¯¹æ¯” (A-D)



# è¿è¡Œå…¨éƒ¨å®éªŒ    â”‚# å·¥å…·é€‰æ‹©è¯„æµ‹### è„šæœ¬å¯¹ç…§è¡¨

./sage-agent-bench run --all

    â”‚  === ä¸»è¿è¡Œå™¨ ===

# å¿«é€Ÿæµ‹è¯•

./sage-agent-bench run --quick    â””â”€â”€ run_paper1_experiments.py  # Paper 1 å…¨æµç¨‹è¿è¡Œå™¨sage-bench eval --dataset sage --samples 100



# === åˆ—å‡ºèµ„æº ===```

./sage-agent-bench list datasets                # åˆ—å‡ºæ•°æ®é›†

./sage-agent-bench list methods                 # åˆ—å‡ºæ–¹æ³•sage-bench eval --dataset acebench --methods keyword,embedding,gorilla| è„šæœ¬                              | Paper | ç”¨é€”                                    |

./sage-agent-bench list experiments             # åˆ—å‡ºå®éªŒ

```## å¿«é€Ÿå¼€å§‹



### 3. ä½¿ç”¨ Python APIsage-bench eval --dataset all        # è·¨æ•°æ®é›†å¯¹æ¯”| --------------------------------- | ----- | --------------------------------------- |



```python### 1. ç¯å¢ƒå‡†å¤‡

# è¿è¡Œå•ä¸ªå®éªŒ

from sage.benchmark.benchmark_agent.scripts.experiments import exp_main_timing| `sage_benchmark_cli.py`           | 1 & 2 | **ç»Ÿä¸€äº¤äº’å¼å…¥å£**                      |

exp_main_timing.main()

```bash

# è¿è¡Œå…¨éƒ¨å®éªŒ

from sage.benchmark.benchmark_agent.scripts.experiments import run_paper1_experiments# å®‰è£… sage-benchmark# è¿è¡Œå®Œæ•´ Benchmark (ä¸‰ä¸ª Challenge)| `run_all_experiments.py`          | 1     | Benchmark: ä¸‰ä¸ª Challenge å…¨é‡è¯„æµ‹      |

run_paper1_experiments.main(sections=["5.2", "5.3", "5.4", "5.5"])

cd /path/to/SAGE

# ç”Ÿæˆè¡¨æ ¼

from sage.benchmark.benchmark_agent.scripts.experiments.table_generator import (./quickstart.sh --dev --yessage-bench run --quick               # å¿«é€Ÿæ¨¡å¼| `run_unified_eval.py`             | 1     | Benchmark: è·¨æ•°æ®é›† Tool Selection å¯¹æ¯” |

    generate_main_results_table,

    generate_training_comparison_table,

)

latex = generate_main_results_table(results_data)# è®¾ç½®ç¯å¢ƒå˜é‡sage-bench run --challenge timing    # å•ä¸ª Challenge| `run_full_training_comparison.py` | 2     | Method: SAGE-Agent æ–¹æ³•å¯¹æ¯”             |



# ç®¡ç† LLM æœåŠ¡export SAGE_TEST_MODE=true  # å¯é€‰ï¼šå¯ç”¨æµ‹è¯•æ¨¡å¼

from sage.benchmark.benchmark_agent.scripts.experiments.llm_service import (

    start_llm_service, stop_llm_service, check_llm_status```| `run_acebench_comparison.py`      | 1     | Benchmark: å¤–éƒ¨æ•°æ®é›†éªŒè¯               |

)

start_llm_service(model="Qwen/Qwen2.5-7B-Instruct")

````

### 2. ä½¿ç”¨ CLI# è®­ç»ƒæ–¹æ³•å¯¹æ¯” (Paper 2)

## æ§åˆ¶å¸¸é‡

æ‰€æœ‰å®éªŒä½¿ç”¨ç»Ÿä¸€çš„æ§åˆ¶å¸¸é‡ï¼Œå®šä¹‰åœ¨ `exp_utils.py`ï¼š

````bashsage-bench train --quick______________________________________________________________________

```python

RANDOM_SEED = 42                                    # éšæœºç§å­# æŸ¥çœ‹å¸®åŠ©

BENCHMARK_EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"  # Embedding æ¨¡å‹

BENCHMARK_LLM_TEMPERATURE = 0.1                     # LLM æ¸©åº¦./sage_bench --helpsage-bench train --methods A_baseline,D_combined

````

## è¾“å‡ºç›®å½•

# === LLM æœåŠ¡ç®¡ç† ===## ğŸ¯ æ–¹æ³•åˆ†ç±»

æ‰€æœ‰è¾“å‡ºä¿å­˜åœ¨ `.sage/benchmark/paper1/`ï¼š

./sage_bench llm start # å¯åŠ¨ vLLM æœåŠ¡

`````

.sage/benchmark/paper1/./sage_bench llm status                   # æ£€æŸ¥æœåŠ¡çŠ¶æ€# LLM æœåŠ¡ç®¡ç†

â”œâ”€â”€ section_5_2_main/           # ä¸»è¦è¯„æµ‹ç»“æœ

â”‚   â”œâ”€â”€ timing_results.json./sage_bench llm stop                     # åœæ­¢æœåŠ¡

â”‚   â”œâ”€â”€ planning_results.json

â”‚   â””â”€â”€ selection_results.jsonsage-bench llm status### ğŸ“˜ Paper 1: Benchmark (ç°æœ‰ SOTA æ–¹æ³•å¯¹æ¯”)

â”œâ”€â”€ section_5_3_analysis/       # æ·±åº¦åˆ†æç»“æœ

â”‚   â”œâ”€â”€ error_analysis.json# === è¿è¡Œå®éªŒ ===

â”‚   â”œâ”€â”€ scaling_analysis.json

â”‚   â”œâ”€â”€ robustness_analysis.json# è¿è¡Œå•ä¸ªç« èŠ‚sage-bench llm start --model Qwen/Qwen2.5-7B-Instruct

â”‚   â””â”€â”€ ablation_results.json

â”œâ”€â”€ section_5_4_generalization/ # è·¨æ•°æ®é›†ç»“æœ./sage_bench run --section 5.2            # ä¸»è¦è¯„æµ‹

â”‚   â””â”€â”€ cross_dataset_results.json

â”œâ”€â”€ section_5_5_training/       # è®­ç»ƒå¯¹æ¯”ç»“æœ./sage_bench run --section 5.3            # æ·±åº¦åˆ†æsage-bench llm stopè¿™äº›æ˜¯ **æ–‡çŒ®ä¸­å·²æœ‰çš„æ–¹æ³•**ï¼Œç”¨äºå»ºç«‹ baseline å¯¹æ¯”ã€‚ **Benchmark è®ºæ–‡ä¸æå‡ºæ–°æ–¹æ³•ï¼Œåªåšç³»ç»Ÿæ€§è¯„æµ‹ã€‚**

â”‚   â””â”€â”€ training_comparison.json

â”œâ”€â”€ figures/                    # ç”Ÿæˆçš„å›¾è¡¨ (PDF/PNG)./sage_bench run --section 5.4            # è·¨æ•°æ®é›†

â””â”€â”€ tables/                     # ç”Ÿæˆçš„ LaTeX è¡¨æ ¼

```./sage_bench run --section 5.5            # è®­ç»ƒæ–¹æ³•å¯¹æ¯”



## è®ºæ–‡ç« èŠ‚å¯¹åº”



| ç« èŠ‚ | å®éªŒè„šæœ¬ | æè¿° |# è¿è¡Œå…¨éƒ¨å®éªŒ# äº¤äº’å¼æ¨¡å¼#### Challenge 1: Timing Judgment

|------|----------|------|

| 5.2.1 | `exp_main_timing.py` | å·¥å…·è°ƒç”¨æ—¶æœºè¯„æµ‹ |./sage_bench run --all

| 5.2.2 | `exp_main_planning.py` | ä»»åŠ¡è§„åˆ’èƒ½åŠ›è¯„æµ‹ |

| 5.2.3 | `exp_main_selection.py` | å·¥å…·é€‰æ‹©å‡†ç¡®ç‡è¯„æµ‹ |sage-bench interactive

| 5.3.1 | `exp_analysis_error.py` | é”™è¯¯ç±»å‹åˆ†å¸ƒåˆ†æ |

| 5.3.2 | `exp_analysis_scaling.py` | å·¥å…·æ•°é‡æ‰©å±•æ€§åˆ†æ |# å¿«é€Ÿæµ‹è¯•

| 5.3.3 | `exp_analysis_robustness.py` | é²æ£’æ€§åˆ†æ |

| 5.3.4 | `exp_analysis_ablation.py` | æ¶ˆèå®éªŒ |./sage_bench run --quick```| æ–¹æ³• ID             | åç§°       | æ¥æº   | æè¿°                  |

| 5.4 | `exp_cross_dataset.py` | è·¨æ•°æ®é›†æ³›åŒ–è¯„æµ‹ |

| 5.5 | `exp_training_comparison.py` | è®­ç»ƒæ–¹æ³•å¯¹æ¯” |



## è®­ç»ƒæ–¹æ³•è¯´æ˜ (Section 5.5)# === ç”Ÿæˆè¾“å‡º ===| ------------------- | ---------- | ------ | --------------------- |



| æ–¹æ³• | åç§° | æè¿° |./sage_bench tables                       # ç”Ÿæˆ LaTeX è¡¨æ ¼

|------|------|------|

| A | Baseline SFT | åŸºç¡€ç›‘ç£å¾®è°ƒ |./sage_bench figures                      # ç”Ÿæˆå›¾è¡¨---| `timing.rule_based` | Rule-based | Common | å…³é”®è¯åŒ¹é… + æ­£åˆ™æ¨¡å¼ |

| B1 | Random Coreset | éšæœºé‡‡æ ·æ ¸å¿ƒé›† |

| B2 | Stratified Coreset | åˆ†å±‚é‡‡æ ·æ ¸å¿ƒé›† |

| B3 | Embedding Coreset | åµŒå…¥èšç±»æ ¸å¿ƒé›† |

| B4 | Difficulty Coreset | éš¾åº¦å¹³è¡¡æ ¸å¿ƒé›† |# === æŠ¥å‘Š ===| `timing.llm_based`  | LLM-based  | Common | ç›´æ¥ç”¨ LLM åˆ¤æ–­       |

| C | Continual Learning | æŒç»­å­¦ä¹  |

| D | Combined | ç»„åˆæ–¹æ³• (B3 + C) |./sage_bench report                       # æŸ¥çœ‹å®éªŒçŠ¶æ€



## LLM æœåŠ¡é…ç½®```## ğŸ“ è„šæœ¬æ¶æ„| `timing.hybrid`     | Hybrid     | Common | Rule åˆç­› + LLM ç²¾åˆ¤  |



å®éªŒé»˜è®¤ä½¿ç”¨ vLLM åœ¨ç«¯å£ 8901 (SagePorts.BENCHMARK_LLM)ï¼š



```bash### 3. ä½¿ç”¨ Python API| `timing.embedding`  | Embedding  | Common | è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­        |

# é»˜è®¤é…ç½®

Model: Qwen/Qwen2.5-7B-Instruct

Port: 8901

GPU Memory: 90%```python````



# è‡ªå®šä¹‰æ¨¡å‹# è¿è¡Œå•ä¸ªå®éªŒ

./sage-agent-bench llm start --model "meta-llama/Llama-3.1-8B-Instruct"

```from sage.benchmark.benchmark_agent.scripts.experiments import exp_main_timingscripts/#### Challenge 2: Task Planning



## ä¾èµ–å…³ç³»exp_main_timing.main()



```â”œâ”€â”€ sage_bench # ğŸŒŸ ç»Ÿä¸€ CLI å…¥å£ (æ¨èä½¿ç”¨)

exp_utils.py â† æ‰€æœ‰å®éªŒè„šæœ¬ä¾èµ–

    â†‘# è¿è¡Œå…¨éƒ¨å®éªŒ

figure_generator.py, table_generator.py â† å¯è§†åŒ–å·¥å…·

    â†‘from sage.benchmark.benchmark_agent.scripts.experiments import run_paper1_experimentsâ”œâ”€â”€ \_internal/ # å†…éƒ¨æ¨¡å— (ä¸è¦ç›´æ¥è°ƒç”¨)| æ–¹æ³• ID | åç§° | æ¥æº | å‚è€ƒæ–‡çŒ® |

llm_service.py â† LLM ç®¡ç†

    â†‘run_paper1_experiments.main(sections=["5.2", "5.3", "5.4", "5.5"])

run_paper1_experiments.py â† ä¸»è¿è¡Œå™¨

    â†‘â”‚ â”œâ”€â”€ unified_eval.py # å·¥å…·é€‰æ‹©è¯„æµ‹| ---------------------- | ---------------- | ------ |

sage_bench_cli.py â† CLI å®ç°

```# ç”Ÿæˆè¡¨æ ¼\---------------- |



## æ•…éšœæ’é™¤from sage.benchmark.benchmark_agent.scripts.experiments.table_generator import (



### LLM æœåŠ¡æ— æ³•å¯åŠ¨    generate_main_results_table,â”‚ â”œâ”€â”€ all_experiments.py # å®Œæ•´ Benchmark| `planner.simple` | Simple (Greedy) | Common | - |



```bash    generate_training_comparison_table,

# æ£€æŸ¥ç«¯å£å ç”¨

lsof -i :8901)â”‚ â”œâ”€â”€ training_comparison.py # è®­ç»ƒå¯¹æ¯”| `planner.hierarchical` | Hierarchical | Common | - |



# æ£€æŸ¥ GPU çŠ¶æ€latex = generate_main_results_table(results_data)

nvidia-smi

â”‚ â””â”€â”€ interactive.py # äº¤äº’æ¨¡å¼| `planner.llm_based` | LLM-based | Common | - |

# æ‰‹åŠ¨å¯åŠ¨ vLLM

vllm serve Qwen/Qwen2.5-7B-Instruct --port 8901 --gpu-memory-utilization 0.9# ç®¡ç† LLM æœåŠ¡

`````

from sage.benchmark.benchmark_agent.scripts.experiments.llm_service import (â”œâ”€â”€ run_unified_eval.py
\# åŠŸèƒ½æ¨¡å— (æ”¯æŒç›´æ¥è°ƒç”¨ï¼Œå»ºè®®ç”¨ CLI)| `planner.react` | ReAct | SOTA | Yao et al., 2023 |

### å®éªŒç»“æœä¸ä¸€è‡´

```
start_llm_service, stop_llm_service, check_llm_status
```

ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­å’Œæ§åˆ¶å¸¸é‡ï¼š

)â”œâ”€â”€ run_all_experiments.py # åŠŸèƒ½æ¨¡å— (æ”¯æŒç›´æ¥è°ƒç”¨ï¼Œå»ºè®®ç”¨ CLI)| `planner.tot` | Tree-of-Thoughts | SOTA | Yao
et

````python

from sage.benchmark.benchmark_agent.scripts.experiments.exp_utils import RANDOM_SEEDstart_llm_service(model="Qwen/Qwen2.5-7B-Instruct")al., 2023 |

import random

random.seed(RANDOM_SEED)```

````

â””â”€â”€ ...

### å†…å­˜ä¸è¶³

## æ§åˆ¶å¸¸é‡

`````bash

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹````#### Challenge 3: Tool Selection

./sage-agent-bench llm start --model "Qwen/Qwen2.5-1.5B-Instruct"

æ‰€æœ‰å®éªŒä½¿ç”¨ç»Ÿä¸€çš„æ§åˆ¶å¸¸é‡ï¼Œå®šä¹‰åœ¨ `exp_utils.py`ï¼š

# æˆ–é™ä½ GPU å†…å­˜ä½¿ç”¨ç‡

# ç¼–è¾‘ llm_service.py ä¸­çš„ gpu_memory_utilization å‚æ•°

`````

```python

## ç›¸å…³æ–‡æ¡£

RANDOM_SEED = 42                                    # éšæœºç§å­### CLI å­å‘½ä»¤| æ–¹æ³• ID              | åç§°            | æ¥æº    | å‚è€ƒæ–‡çŒ®           |

- [SAGE å¼€å‘æŒ‡å—](../../../../../../DEVELOPER.md)

- [Benchmark æ¶æ„](../../../../docs/benchmark_architecture.md)BENCHMARK_EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"  # Embedding æ¨¡å‹

- [è¯„æµ‹æŒ‡æ ‡å®šä¹‰](../evaluation/metrics.py)

BENCHMARK_LLM_TEMPERATURE = 0.1                     # LLM æ¸©åº¦| -------------------- | --------------- | ------- | ------------------ |

```

| å‘½ä»¤ | åŠŸèƒ½ | ç¤ºä¾‹ || `selector.keyword` | Keyword (BM25) | Classic | Robertson et al. |

## è¾“å‡ºç›®å½•

|------|------|------|| `selector.embedding` | Embedding | Common | BGE-M3 (BAAI) |

æ‰€æœ‰è¾“å‡ºä¿å­˜åœ¨ `.sage/benchmark/paper1/`ï¼š

| `eval` | å·¥å…·é€‰æ‹©è¯„æµ‹ | `sage-bench eval --dataset all` || `selector.hybrid` | Hybrid (RRF) | Common | -
|

````

.sage/benchmark/paper1/| `run` | å®Œæ•´ Benchmark | `sage-bench run --quick` || `selector.gorilla`   | Gorilla         | SOTA    | Patil et al., 2023 |

â”œâ”€â”€ section_5_2_main/           # ä¸»è¦è¯„æµ‹ç»“æœ

â”‚   â”œâ”€â”€ timing_results.json| `train` | è®­ç»ƒæ–¹æ³•å¯¹æ¯” | `sage-bench train --dry-run` || `selector.dfsdt`     | DFSDT (ToolLLM) | SOTA    | Qin et al., 2023   |

â”‚   â”œâ”€â”€ planning_results.json

â”‚   â””â”€â”€ selection_results.json| `llm` | LLM æœåŠ¡ç®¡ç† | `sage-bench llm status` || `llm_direct`         | LLM Direct      | Common  | -                  |

â”œâ”€â”€ section_5_3_analysis/       # æ·±åº¦åˆ†æç»“æœ

â”‚   â”œâ”€â”€ error_analysis.json| `list` | åˆ—å‡ºå¯ç”¨èµ„æº | `sage-bench list datasets` |

â”‚   â”œâ”€â”€ scaling_analysis.json

â”‚   â”œâ”€â”€ robustness_analysis.json| `interactive` | äº¤äº’å¼æ¨¡å¼ | `sage-bench interactive` |______________________________________________________________________

â”‚   â””â”€â”€ ablation_results.json

â”œâ”€â”€ section_5_4_generalization/ # è·¨æ•°æ®é›†ç»“æœ

â”‚   â””â”€â”€ cross_dataset_results.json

â”œâ”€â”€ section_5_5_training/       # è®­ç»ƒå¯¹æ¯”ç»“æœ---### ğŸ“™ Paper 2: SAGE-Agent (åŸåˆ›æ–¹æ³•)

â”‚   â””â”€â”€ training_comparison.json

â”œâ”€â”€ figures/                    # ç”Ÿæˆçš„å›¾è¡¨ (PDF/PNG)

â””â”€â”€ tables/                     # ç”Ÿæˆçš„ LaTeX è¡¨æ ¼

```## ğŸ“Š æ”¯æŒçš„æ•°æ®é›†**æ ¸å¿ƒåˆ›æ–°**: å°† Agent å­¦ä¹ é‡æ–°å®šä¹‰ä¸º **åœ¨çº¿æµå­¦ä¹ é—®é¢˜**ï¼Œæå‡º Streaming Adaptive Learning æ¡†æ¶ã€‚



## è®ºæ–‡ç« èŠ‚å¯¹åº”



| ç« èŠ‚ | å®éªŒè„šæœ¬ | æè¿° || æ•°æ®é›† | æ¥æº | æè¿° |#### æ¶æ„æ¦‚è§ˆ

|------|----------|------|

| 5.2.1 | `exp_main_timing.py` | å·¥å…·è°ƒç”¨æ—¶æœºè¯„æµ‹ ||--------|------|------|

| 5.2.2 | `exp_main_planning.py` | ä»»åŠ¡è§„åˆ’èƒ½åŠ›è¯„æµ‹ |

| 5.2.3 | `exp_main_selection.py` | å·¥å…·é€‰æ‹©å‡†ç¡®ç‡è¯„æµ‹ || `sage` | Built-in | SAGE-Bench (1200 synthetic tools) |```

| 5.3.1 | `exp_analysis_error.py` | é”™è¯¯ç±»å‹åˆ†å¸ƒåˆ†æ |

| 5.3.2 | `exp_analysis_scaling.py` | å·¥å…·æ•°é‡æ‰©å±•æ€§åˆ†æ || `acebench` | HuggingFace | ToolACE from Team-ACE |â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

| 5.3.3 | `exp_analysis_robustness.py` | é²æ£’æ€§åˆ†æ |

| 5.3.4 | `exp_analysis_ablation.py` | æ¶ˆèå®éªŒ || `apibank` | External | API-Bank (Microsoft/Alibaba) |â”‚                    SAGE-Agent Framework                      â”‚

| 5.4 | `exp_cross_dataset.py` | è·¨æ•°æ®é›†æ³›åŒ–è¯„æµ‹ |

| 5.5 | `exp_training_comparison.py` | è®­ç»ƒæ–¹æ³•å¯¹æ¯” || `toolalpaca` | External | ToolAlpaca (Microsoft) |â”‚                                                             â”‚



## è®­ç»ƒæ–¹æ³•è¯´æ˜ (Section 5.5)| `bfcl` | External | Berkeley Function Calling Leaderboard |â”‚  Query Stream â”€â”€â†’ [SSIS] â”€â”€â†’ [Priority Buffer] â”€â”€â†’ [Train]  â”‚



| æ–¹æ³• | åç§° | æè¿° || `toolbench` | External | ToolBench (Tsinghua/OpenBMB) |â”‚       â”‚              â”‚              â”‚                â”‚      â”‚

|------|------|------|

| A | Baseline SFT | åŸºç¡€ç›‘ç£å¾®è°ƒ || `taskbench` | External | TaskBench (PKU) |â”‚       â”‚     Importance Score   Experience      Online Update â”‚

| B1 | Random Coreset | éšæœºé‡‡æ ·æ ¸å¿ƒé›† |

| B2 | Stratified Coreset | åˆ†å±‚é‡‡æ ·æ ¸å¿ƒé›† || `metatool` | External | MetaTool (Tsinghua) |â”‚       â”‚     (U + D + F)        Replay               â”‚      â”‚

| B3 | Embedding Coreset | åµŒå…¥èšç±»æ ¸å¿ƒé›† |

| B4 | Difficulty Coreset | éš¾åº¦å¹³è¡¡æ ¸å¿ƒé›† |â”‚       â”‚                                              â”‚      â”‚

| C | Continual Learning | æŒç»­å­¦ä¹  |

| D | Combined | ç»„åˆæ–¹æ³• (B3 + C) |æŸ¥çœ‹æ‰€æœ‰æ•°æ®é›†ï¼šâ”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚



## LLM æœåŠ¡é…ç½®```bashâ”‚                                                             â”‚



å®éªŒé»˜è®¤ä½¿ç”¨ vLLM åœ¨ç«¯å£ 8901 (SagePorts.BENCHMARK_LLM)ï¼šsage-bench list datasetsâ”‚  [Unified Multi-Task Network]                               â”‚



```bash```â”‚  â”œâ”€â”€ Timing Head    â†â”€â”€â”                                    â”‚

# é»˜è®¤é…ç½®

Model: Qwen/Qwen2.5-7B-Instructâ”‚  â”œâ”€â”€ Selection Head â†â”€â”€â”¼â”€â”€ Cross-Task Attention             â”‚

Port: 8901

GPU Memory: 90%---â”‚  â””â”€â”€ Planning Head  â†â”€â”€â”˜                                    â”‚



# è‡ªå®šä¹‰æ¨¡å‹â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

./sage_bench llm start --model "meta-llama/Llama-3.1-8B-Instruct"

```## ğŸ¯ æ”¯æŒçš„æ–¹æ³•```



## ä¾èµ–å…³ç³»



```### Challenge 3: Tool Selection#### ä¸‰å¤§æ ¸å¿ƒç»„ä»¶

exp_utils.py â† æ‰€æœ‰å®éªŒè„šæœ¬ä¾èµ–

    â†‘

figure_generator.py, table_generator.py â† å¯è§†åŒ–å·¥å…·

    â†‘| æ–¹æ³• | æ¥æº | æè¿° || ç»„ä»¶                     | å…¨ç§°                               | åŠŸèƒ½                 | åˆ›æ–°ç‚¹                                            |

llm_service.py â† LLM ç®¡ç†

    â†‘|------|------|------|| ------------------------ | ---------------------------------- | -------------------- | ------------------------------------------------- |

run_paper1_experiments.py â† ä¸»è¿è¡Œå™¨

    â†‘| `keyword` | Classic | BM25 keyword matching || **SSIS**                 | Streaming Sample Importance Scorer | å®æ—¶è¯„ä¼°æ ·æœ¬è®­ç»ƒä»·å€¼ | ä¸‰ç»´åº¦è¯„åˆ† (Uncertainty + Diversity + Forgetting) |

sage_bench_cli.py â† CLI å®ç°

```| `embedding` | Common | Semantic embedding similarity || **Priority Replay**      | Importance-Weighted Replay Buffer  | ä¼˜å…ˆçº§ç»éªŒå›æ”¾       | Sum-tree O(log n) é‡‡æ · + IS æƒé‡æ ¡æ­£              |



## æ•…éšœæ’é™¤| `hybrid` | Common | Keyword + Embedding fusion (RRF) || **Cross-Task Attention** | Unified Multi-Task Network         | è·¨ä»»åŠ¡ä¿¡æ¯å…±äº«       | Timing â†” Selection â†” Planning ååŒ                |



### LLM æœåŠ¡æ— æ³•å¯åŠ¨| `gorilla` | Berkeley | Retrieval + LLM reranking |



```bash| `dfsdt` | Tsinghua | Tree search (ToolLLM) |#### æ¶ˆèå®éªŒé…ç½®

# æ£€æŸ¥ç«¯å£å ç”¨

lsof -i :8901| `llm_direct` | Baseline | Direct LLM prompting |



# æ£€æŸ¥ GPU çŠ¶æ€| æ–¹æ³• ID             | åç§°                | SSIS | Replay | CrossTask | è¯´æ˜               |

nvidia-smi

æŸ¥çœ‹æ‰€æœ‰æ–¹æ³•ï¼š| ------------------- | ------------------- | :--: | :----: | :-------: | ------------------ |

# æ‰‹åŠ¨å¯åŠ¨ vLLM

vllm serve Qwen/Qwen2.5-7B-Instruct --port 8901 --gpu-memory-utilization 0.9```bash| `SAGE_sft_baseline` | Baseline SFT        |  âŒ  |   âŒ   |    âŒ     | æ¶ˆèåŸºå‡†           |

````

sage-bench list methods| `SAGE_ssis_only` | +SSIS | âœ… | âŒ | âŒ | åŠ å…¥æ ·æœ¬é‡è¦æ€§è¯„ä¼° |

### å®éªŒç»“æœä¸ä¸€è‡´

\`\`\`| `SAGE_ssis_replay` | +SSIS +Replay | âœ… | âœ… | âŒ | åŠ å…¥ä¼˜å…ˆçº§å›æ”¾ |

ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­å’Œæ§åˆ¶å¸¸é‡ï¼š

| `SAGE_full` | **Full SAGE-Agent** | âœ… | âœ… | âœ… | å®Œæ•´æ–¹æ³• |

```python

from sage.benchmark.benchmark_agent.scripts.experiments.exp_utils import RANDOM_SEED---

import random

random.seed(RANDOM_SEED)#### é¢„æœŸæ€§èƒ½æå‡

```

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### å†…å­˜ä¸è¶³

| Challenge | Baseline Best | SAGE-Agent | æå‡ |

````bash

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹### Paper 1: Benchmark å®éªŒ| ----------------------- | ------------- | ---------- | ---- |

./sage_bench llm start --model "Qwen/Qwen2.5-1.5B-Instruct"

| Tool Selection (Top-5)  | 82%           | **94%**    | +12% |

# æˆ–é™ä½ GPU å†…å­˜ä½¿ç”¨ç‡

# ç¼–è¾‘ llm_service.py ä¸­çš„ gpu_memory_utilization å‚æ•°```bash| Task Planning (Success) | 27%           | **85%**    | +58% |

````

# 1. å¿«é€Ÿè¯„æµ‹ (è·³è¿‡ LLM æ–¹æ³•)| Timing Judgment (Acc) | 76% | **95%** | +19% |

## ç›¸å…³æ–‡æ¡£

sage-bench run --quick --skip-llm

- [SAGE å¼€å‘æŒ‡å—](../../../../../../DEVELOPER.md)

- [Benchmark æ¶æ„](../../../../docs/benchmark_architecture.md)#### æ•ˆç‡æå‡

- [è¯„æµ‹æŒ‡æ ‡å®šä¹‰](../evaluation/metrics.py)

# 2. è·¨æ•°æ®é›†å·¥å…·é€‰æ‹©å¯¹æ¯”

sage-bench eval --dataset all --methods keyword,embedding,hybrid,gorilla --samples 100| æŒ‡æ ‡ | ä¼ ç»Ÿ SFT
| SAGE-Agent | æå‡ |

| \---------------------- | -------- | ---------- | ---------- |

# 3. å•ä¸ª Challenge è¯„æµ‹| è®­ç»ƒæ—¶é—´ | 1.0x | **0.4x** | 2.5x æ›´å¿« |

sage-bench run --challenge tool_selection| æ•°æ®åˆ©ç”¨ | 100% | **~35%** | æ›´é«˜æ•ˆ |

sage-bench run --challenge timing| åœ¨çº¿é€‚åº” (æ€§èƒ½ä¸‹é™/è½®) | -8% | **-0.5%** | 16x æ›´ç¨³å®š |

sage-bench run --challenge planning

````______________________________________________________________________



### Paper 2: SAGE-Agent å®éªŒ## ğŸ“š æ•°æ®é›†



```bash### SAGE-Bench åŸç”Ÿæ•°æ®

# 1. å¿«é€Ÿè®­ç»ƒå¯¹æ¯”

sage-bench train --quick| ä»»åŠ¡            | æ ·æœ¬æ•°    | Train | Dev | Test |

| --------------- | --------- | ----- | --- | ---- |

# 2. å®Œæ•´æ¶ˆèå®éªŒ| Tool Selection  | 600       | 420   | 90  | 90   |

sage-bench train --methods A_baseline,B_coreset,C_continual,D_combined| Task Planning   | 300       | 210   | 45  | 45   |

| Timing Judgment | 300       | 210   | 45  | 45   |

# 3. æ¨¡æ‹Ÿè¿è¡Œ (ä¸å®é™…è®­ç»ƒ)| **Total**       | **1,200** | 840   | 180 | 180  |

sage-bench train --dry-run

```### å¤–éƒ¨æ•°æ®é›†é›†æˆ



### LLM æœåŠ¡ç®¡ç†| æ•°æ®é›†     | æ¥æº                   | æ ·æœ¬æ•°  | ç”¨é€”           |

| ---------- | ---------------------- | ------- | -------------- |

```bash| ACEBench   | Team-ACE (HuggingFace) | 10,000+ | è·¨æ•°æ®é›†éªŒè¯   |

# æ£€æŸ¥æœåŠ¡çŠ¶æ€| API-Bank   | Microsoft/Alibaba      | 2,138   | å¤šè½® API å¯¹è¯  |

sage-bench llm status| ToolAlpaca | Microsoft              | 3,928   | å·¥å…·å­¦ä¹ å¯¹è¯   |

| ToolBench  | Tsinghua/OpenBMB       | 16,000+ | å¤§è§„æ¨¡å·¥å…·æ£€ç´¢ |

# å¯åŠ¨ vLLM æœåŠ¡

sage-bench llm start --model Qwen/Qwen2.5-0.5B-Instruct --port 8901______________________________________________________________________



# åœæ­¢æœåŠ¡## ğŸš€ å¿«é€Ÿå¼€å§‹

sage-bench llm stop

```### Paper 1: Benchmark å®éªŒ



---```bash

# 1. è¿è¡Œå®Œæ•´ Benchmark (ä¸‰ä¸ª Challenge)

## ğŸ“ è¾“å‡ºç»“æ„python run_all_experiments.py --quick  # å¿«é€Ÿæµ‹è¯•

python run_all_experiments.py          # å®Œæ•´è¯„æµ‹

æ‰€æœ‰ç»“æœä¿å­˜åœ¨ `~/.sage/benchmark/results/`:

# 2. è·¨æ•°æ®é›†éªŒè¯

```python run_unified_eval.py --datasets sage acebench --samples 100

~/.sage/benchmark/results/

â”œâ”€â”€ unified_eval_results.json      # å·¥å…·é€‰æ‹©è¯„æµ‹ç»“æœ# 3. å•ç‹¬è¯„æµ‹ Tool Selection

â”œâ”€â”€ all_results.json               # å®Œæ•´ Benchmark ç»“æœpython sage_benchmark_cli.py --paper 1 --experiment tool_selection

â”œâ”€â”€ figures/                       # ç”Ÿæˆçš„å›¾è¡¨```

â”‚   â”œâ”€â”€ fig4_overall_comparison.pdf

â”‚   â””â”€â”€ fig5_planning_by_complexity.pdf### Paper 2: SAGE-Agent å®éªŒ

â””â”€â”€ tables/                        # LaTeX è¡¨æ ¼

    â”œâ”€â”€ table1_projected_performance.tex```bash

    â””â”€â”€ table2_observed_benchmark.tex# 1. å®Œæ•´æ¶ˆèå®éªŒ

```python run_full_training_comparison.py --quick  # å¿«é€Ÿæµ‹è¯•

python run_full_training_comparison.py          # A100 å®Œæ•´è®­ç»ƒ

---

# 2. å•ç‹¬æµ‹è¯• SAGE-Agent Full

## ğŸ”§ å¼€å‘è€…æŒ‡å—python sage_benchmark_cli.py --paper 2 --experiment sage_agent_full



### æ·»åŠ æ–°æ•°æ®é›†# 3. åœ¨çº¿é€‚åº”å®éªŒ (åŠ¨æ€å·¥å…·åº“)

python sage_benchmark_cli.py --paper 2 --experiment online_adaptation

1. åœ¨ `external_benchmarks/` ä¸­æ·»åŠ ä¸‹è½½è„šæœ¬```

2. åœ¨ `EXTERNAL_BENCHMARKS` å­—å…¸ä¸­æ³¨å†Œ

3. æ›´æ–° `sage-bench list datasets` è¾“å‡º______________________________________________________________________



### æ·»åŠ æ–°æ–¹æ³•## ğŸ“Š ç»“æœè¾“å‡º



1. å®ç° `BaseSelectorAdapter` æ¥å£æ‰€æœ‰å®éªŒç»“æœä¿å­˜åœ¨ `outputs/` ç›®å½•ï¼š

2. åœ¨ `create_evaluator()` ä¸­æ³¨å†Œ

3. æ›´æ–° `sage-bench list methods` è¾“å‡º```

outputs/

---â”œâ”€â”€ paper1_benchmark/

â”‚   â”œâ”€â”€ timing_results.json

## ğŸ“ å¼•ç”¨â”‚   â”œâ”€â”€ planning_results.json

â”‚   â”œâ”€â”€ tool_selection_results.json

```bibtexâ”‚   â””â”€â”€ cross_dataset_validation.json

@inproceedings{sage-bench-2026,â”‚

  title={SAGE-Bench: A Unified Benchmark for Evaluating Agent Capabilities},â””â”€â”€ paper2_method/

  author={...},    â”œâ”€â”€ ablation_study/

  booktitle={ICML},    â”‚   â”œâ”€â”€ SAGE_sft_baseline.json

  year={2026}    â”‚   â”œâ”€â”€ SAGE_ssis_only.json

}    â”‚   â”œâ”€â”€ SAGE_ssis_replay.json

```    â”‚   â””â”€â”€ SAGE_full.json

    â”œâ”€â”€ efficiency_analysis.json
    â””â”€â”€ online_adaptation.json
````

______________________________________________________________________

## ğŸ“– ç›¸å…³è®ºæ–‡

### Paper 1: SAGE-Bench (Benchmark)

- **å®šä½**: Dataset & Benchmark Track
- **è´¡çŒ®**: ç»Ÿä¸€è¯„æµ‹æ¡†æ¶ + æ•°æ®é›† + ç°æœ‰æ–¹æ³•ç³»ç»Ÿå¯¹æ¯”
- **ä¸åŒ…å«**: æ–°æ–¹æ³•æå‡º

### Paper 2: SAGE-Agent (Method)

- **å®šä½**: Method Paper
- **è´¡çŒ®**: Streaming Adaptive Learning æ¡†æ¶
- **æ ¸å¿ƒç»„ä»¶**: SSIS + Priority Replay + Cross-Task Attention
- **èƒŒæ™¯**: åŸºäºæµè®¡ç®—å’Œåœ¨çº¿æŒç»­å­¦ä¹ çš„ç ”ç©¶ç»éªŒ

______________________________________________________________________

## ğŸ”§ å¼€å‘è€…æŒ‡å—

### æ·»åŠ æ–°æ–¹æ³•

1. åœ¨ `adapter_registry.py` ä¸­æ³¨å†Œå·¥å‚å‡½æ•°
1. å®ç°ç¬¦åˆ `SelectorAdapter`/`PlannerAdapter`/`TimingAdapter` æ¥å£
1. åœ¨ README ä¸­æ·»åŠ æ–¹æ³•æè¿°
1. è¿è¡Œæµ‹è¯•éªŒè¯

### æ·»åŠ æ–°æ•°æ®é›†

1. å®ç° `DataLoader` æ¥å£
1. åœ¨ `DataManager` ä¸­æ³¨å†Œ
1. æ·»åŠ æ•°æ®é›†æè¿°åˆ°æ–‡æ¡£

______________________________________________________________________

## ğŸ“ å¼•ç”¨

```bibtex
@inproceedings{sage-bench-2026,
  title={SAGE-Bench: A Unified Benchmark for Evaluating Agent Capabilities in Tool-Augmented LLMs},
  author={...},
  booktitle={ICML},
  year={2026}
}

@inproceedings{sage-agent-2026,
  title={SAGE-Agent: Streaming Adaptive Learning for Tool-Augmented LLM Agents},
  author={...},
  booktitle={ICML},
  year={2026}
}
```
