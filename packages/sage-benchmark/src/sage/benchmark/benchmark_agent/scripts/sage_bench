#!/usr/bin/env python3
"""
SAGE-Bench CLI - ç»Ÿä¸€å‘½ä»¤è¡Œå…¥å£

è¿™æ˜¯ SAGE Benchmark çš„å”¯ä¸€å®˜æ–¹å…¥å£ï¼Œæ‰€æœ‰åŠŸèƒ½é€šè¿‡å­å‘½ä»¤è®¿é—®ã€‚

Usage:
    sage-bench <command> [options]

Commands:
    eval        å·¥å…·é€‰æ‹©è¯„æµ‹ (è·¨æ•°æ®é›†)
    run         è¿è¡Œå®Œæ•´ Benchmark (ä¸‰ä¸ª Challenge)
    train       è®­ç»ƒæ–¹æ³•å¯¹æ¯” (Paper 2)
    llm         LLM æœåŠ¡ç®¡ç†
    list        åˆ—å‡ºå¯ç”¨æ•°æ®é›†/æ–¹æ³•/å®éªŒ
    interactive äº¤äº’å¼æ¨¡å¼

Examples:
    # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    sage-bench list datasets

    # åœ¨ ACEBench ä¸Šè¯„æµ‹
    sage-bench eval --dataset acebench --samples 100

    # è·¨æ•°æ®é›†å¯¹æ¯”
    sage-bench eval --dataset all --methods keyword,embedding,gorilla

    # è¿è¡Œå®Œæ•´ Benchmark
    sage-bench run --quick

    # LLM æœåŠ¡ç®¡ç†
    sage-bench llm start
    sage-bench llm status
    sage-bench llm stop

    # äº¤äº’å¼æ¨¡å¼
    sage-bench interactive
"""

from __future__ import annotations

import argparse
import os
import signal
import sys
import time
from pathlib import Path

# è·å–è„šæœ¬ç›®å½•
SCRIPT_DIR = Path(__file__).resolve().parent

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(SCRIPT_DIR.parent.parent.parent.parent))


# =============================================================================
# LLM æœåŠ¡ç®¡ç†
# =============================================================================

try:
    from sage.common.config.ports import SagePorts

    DEFAULT_LLM_PORT = SagePorts.BENCHMARK_LLM
except ImportError:
    DEFAULT_LLM_PORT = 8901

DEFAULT_LLM_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
LLM_PID_FILE = Path.home() / ".sage" / "benchmark_llm.pid"


def check_llm_service(port: int = DEFAULT_LLM_PORT) -> dict:
    """æ£€æŸ¥ LLM æœåŠ¡çŠ¶æ€"""
    import httpx

    result = {"running": False, "port": port, "model": None, "error": None}
    try:
        response = httpx.get(f"http://localhost:{port}/v1/models", timeout=5.0)
        if response.status_code == 200:
            data = response.json()
            models = data.get("data", [])
            if models:
                result["running"] = True
                result["model"] = models[0].get("id", "unknown")
        else:
            result["error"] = f"HTTP {response.status_code}"
    except httpx.ConnectError:
        result["error"] = "Connection refused"
    except httpx.TimeoutException:
        result["error"] = "Timeout"
    except Exception as e:
        result["error"] = str(e)
    return result


def cmd_llm(args):
    """LLM æœåŠ¡ç®¡ç†å­å‘½ä»¤"""
    import subprocess

    if args.llm_action == "status":
        print("\nğŸ“¡ LLM æœåŠ¡çŠ¶æ€")
        print("=" * 50)
        try:
            from sage.common.config.ports import SagePorts

            ports = [SagePorts.BENCHMARK_LLM] + SagePorts.get_llm_ports()
        except ImportError:
            ports = [DEFAULT_LLM_PORT, 8001, 8000]

        seen = set()
        for port in ports:
            if port in seen:
                continue
            seen.add(port)
            status = check_llm_service(port)
            if status["running"]:
                print(f"  âœ… Port {port}: è¿è¡Œä¸­")
                print(f"     æ¨¡å‹: {status['model']}")
            else:
                print(f"  âŒ Port {port}: {status['error'] or 'æœªè¿è¡Œ'}")
        return 0

    elif args.llm_action == "start":
        status = check_llm_service(args.port)
        if status["running"]:
            print(f"âœ… LLM æœåŠ¡å·²åœ¨è¿è¡Œ (port={args.port}, model={status['model']})")
            return 0

        print("ğŸš€ å¯åŠ¨ LLM æœåŠ¡...")
        print(f"   æ¨¡å‹: {args.model}")
        print(f"   ç«¯å£: {args.port}")

        LLM_PID_FILE.parent.mkdir(parents=True, exist_ok=True)
        cmd = [
            sys.executable,
            "-m",
            "vllm.entrypoints.openai.api_server",
            "--model",
            args.model,
            "--port",
            str(args.port),
            "--gpu-memory-utilization",
            str(args.gpu_memory),
            "--trust-remote-code",
        ]

        try:
            process = subprocess.Popen(
                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, start_new_session=True
            )
            with open(LLM_PID_FILE, "w") as f:
                f.write(str(process.pid))
            print(f"   PID: {process.pid}")
            print("   ç­‰å¾…æœåŠ¡å¯åŠ¨...")

            for i in range(120):
                time.sleep(1)
                if check_llm_service(args.port)["running"]:
                    print(f"\nâœ… LLM æœåŠ¡å·²å¯åŠ¨ (è€—æ—¶ {i + 1}s)")
                    return 0
                if i % 10 == 9:
                    print(f"   å·²ç­‰å¾… {i + 1}s...")

            print("\nâŒ æœåŠ¡å¯åŠ¨è¶…æ—¶")
            return 1
        except Exception as e:
            print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
            return 1

    elif args.llm_action == "stop":
        if not LLM_PID_FILE.exists():
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°è¿è¡Œä¸­çš„ LLM æœåŠ¡")
            return 0
        try:
            with open(LLM_PID_FILE) as f:
                pid = int(f.read().strip())
            print(f"ğŸ›‘ åœæ­¢ LLM æœåŠ¡ (PID={pid})...")
            os.kill(pid, signal.SIGTERM)
            LLM_PID_FILE.unlink(missing_ok=True)
            print("âœ… LLM æœåŠ¡å·²åœæ­¢")
            return 0
        except Exception as e:
            print(f"âŒ åœæ­¢å¤±è´¥: {e}")
            return 1

    return 0


# =============================================================================
# è¯„æµ‹å‘½ä»¤
# =============================================================================


def cmd_eval(args):
    """å·¥å…·é€‰æ‹©è¯„æµ‹å­å‘½ä»¤"""
    from sage.benchmark.benchmark_agent.scripts._internal.unified_eval import run_evaluation

    return run_evaluation(
        dataset=args.dataset,
        methods=args.methods.split(",") if args.methods else ["keyword", "embedding", "hybrid"],
        max_samples=args.samples,
        top_k=args.top_k,
        use_embedded=args.use_embedded,
        output_dir=args.output,
    )


def cmd_run(args):
    """è¿è¡Œå®Œæ•´ Benchmark"""
    from sage.benchmark.benchmark_agent.scripts._internal.all_experiments import run_all_experiments

    return run_all_experiments(
        challenge=args.challenge,
        quick=args.quick,
        skip_llm=args.skip_llm,
        max_samples=args.samples,
        dataset=args.dataset,
        output_dir=args.output,
    )


def cmd_train(args):
    """è®­ç»ƒæ–¹æ³•å¯¹æ¯” (Paper 2)"""
    from sage.benchmark.benchmark_agent.scripts._internal.training_comparison import (
        run_training_comparison,
    )

    return run_training_comparison(
        methods=args.methods.split(",") if args.methods else ["A_baseline", "D_combined"],
        base_model=args.model,
        quick=args.quick,
        dry_run=args.dry_run,
        output_dir=args.output,
    )


# =============================================================================
# åˆ—è¡¨å‘½ä»¤
# =============================================================================


def cmd_list(args):
    """åˆ—å‡ºå¯ç”¨èµ„æº"""
    if args.resource == "datasets":
        print("\n" + "=" * 70)
        print("Available Datasets for Tool Selection Evaluation")
        print("=" * 70)
        print()
        print(f"{'Dataset':<15} {'Description':<50} {'Status'}")
        print("-" * 70)

        datasets = [
            ("sage", "SAGE-Bench (1200 synthetic tools)", "Built-in"),
            ("acebench", "ToolACE from HuggingFace", "HuggingFace"),
            ("apibank", "API-Bank (Microsoft/Alibaba)", "External"),
            ("toolalpaca", "ToolAlpaca (Microsoft)", "External"),
            ("bfcl", "BFCL (Berkeley Function Calling)", "External"),
            ("toolbench", "ToolBench (Tsinghua/OpenBMB)", "External"),
            ("taskbench", "TaskBench (PKU)", "External"),
            ("metatool", "MetaTool (Tsinghua)", "External"),
            ("all", "Evaluate on ALL datasets", "-"),
        ]
        for name, desc, status in datasets:
            print(f"{name:<15} {desc:<50} {status}")
        print()
        return 0

    elif args.resource == "methods":
        print("\n" + "=" * 70)
        print("Available Tool Selection Methods")
        print("=" * 70)
        print()
        print(f"{'Method':<15} {'Description':<40} {'Source'}")
        print("-" * 70)

        methods = [
            ("keyword", "BM25 keyword matching", "Classic"),
            ("embedding", "Semantic embedding similarity", "Common"),
            ("hybrid", "Keyword + Embedding fusion", "Common"),
            ("gorilla", "Retrieval + LLM reranking", "Berkeley"),
            ("dfsdt", "Tree search (ToolLLM)", "Tsinghua"),
            ("llm_direct", "Direct LLM prompting", "Baseline"),
        ]
        for name, desc, source in methods:
            print(f"{name:<15} {desc:<40} {source}")
        print()
        return 0

    elif args.resource == "experiments":
        print("\n" + "=" * 70)
        print("Available Experiments")
        print("=" * 70)

        print("\nğŸ“˜ Paper 1: Benchmark (ç°æœ‰ SOTA æ–¹æ³•å¯¹æ¯”)")
        print("-" * 70)
        experiments_p1 = [
            ("timing", "Challenge 1: Timing Judgment", "~10 min"),
            ("planning", "Challenge 2: Task Planning", "~15 min"),
            ("tool_selection", "Challenge 3: Tool Selection", "~20 min"),
            ("cross_dataset", "Cross-Dataset Comparison", "~30 min"),
        ]
        for exp_id, name, time_est in experiments_p1:
            print(f"  {exp_id:<20} {name:<35} {time_est}")

        print("\nğŸ“™ Paper 2: Method (SAGE åŸåˆ›æ–¹æ³•)")
        print("-" * 70)
        experiments_p2 = [
            ("baseline", "Baseline SFT training", "~2 hours"),
            ("coreset", "Coreset hybrid strategy", "~1.5 hours"),
            ("continual", "Continual learning + replay", "~2.5 hours"),
            ("combined", "Full SAGE method", "~2 hours"),
        ]
        for exp_id, name, time_est in experiments_p2:
            print(f"  {exp_id:<20} {name:<35} {time_est}")
        print()
        return 0

    return 1


# =============================================================================
# äº¤äº’å¼æ¨¡å¼
# =============================================================================


def cmd_interactive(args):
    """äº¤äº’å¼æ¨¡å¼"""
    # å¯¼å…¥åŸæœ‰çš„äº¤äº’å¼é€»è¾‘
    from sage.benchmark.benchmark_agent.scripts._internal.interactive import run_interactive_mode

    return run_interactive_mode()


# =============================================================================
# ä¸»å…¥å£
# =============================================================================


def print_banner():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                       SAGE-Bench CLI v2.0                                 â•‘
â•‘                                                                           â•‘
â•‘  Unified benchmark for evaluating Agent capabilities                      â•‘
â•‘  Paper 1: SAGE-Bench | Paper 2: SAGE Methods                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


def main():
    parser = argparse.ArgumentParser(
        prog="sage-bench",
        description="SAGE-Bench CLI - ç»Ÿä¸€å‘½ä»¤è¡Œå…¥å£",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    sage-bench list datasets          # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    sage-bench eval --dataset all     # è·¨æ•°æ®é›†è¯„æµ‹
    sage-bench run --quick            # å¿«é€Ÿè¿è¡Œå®Œæ•´ Benchmark
    sage-bench llm start              # å¯åŠ¨ LLM æœåŠ¡
    sage-bench interactive            # äº¤äº’å¼æ¨¡å¼
        """,
    )

    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")

    # =========================================================================
    # eval å­å‘½ä»¤
    # =========================================================================
    eval_parser = subparsers.add_parser("eval", help="å·¥å…·é€‰æ‹©è¯„æµ‹ (è·¨æ•°æ®é›†)")
    eval_parser.add_argument(
        "--dataset",
        "-d",
        default="sage",
        choices=["sage", "acebench", "apibank", "toolalpaca", "bfcl", "toolbench", "all"],
        help="è¯„æµ‹æ•°æ®é›† (default: sage)",
    )
    eval_parser.add_argument(
        "--methods",
        "-m",
        default="keyword,embedding,hybrid",
        help="è¯„æµ‹æ–¹æ³•ï¼Œé€—å·åˆ†éš” (default: keyword,embedding,hybrid)",
    )
    eval_parser.add_argument(
        "--samples", "-n", type=int, default=100, help="æœ€å¤§æ ·æœ¬æ•° (default: 100)"
    )
    eval_parser.add_argument("--top-k", "-k", type=int, default=5, help="Top-K è¯„æµ‹ (default: 5)")
    eval_parser.add_argument(
        "--use-embedded", action="store_true", help="ä½¿ç”¨å†…åµŒ LLM (for llm_direct method)"
    )
    eval_parser.add_argument("--output", "-o", type=str, default=None, help="è¾“å‡ºç›®å½•")
    eval_parser.set_defaults(func=cmd_eval)

    # =========================================================================
    # run å­å‘½ä»¤
    # =========================================================================
    run_parser = subparsers.add_parser("run", help="è¿è¡Œå®Œæ•´ Benchmark (ä¸‰ä¸ª Challenge)")
    run_parser.add_argument(
        "--challenge",
        "-c",
        choices=["timing", "planning", "tool_selection"],
        help="åªè¿è¡ŒæŒ‡å®š Challenge",
    )
    run_parser.add_argument("--quick", "-q", action="store_true", help="å¿«é€Ÿæ¨¡å¼ (fewer samples)")
    run_parser.add_argument("--skip-llm", action="store_true", help="è·³è¿‡ LLM æ–¹æ³•")
    run_parser.add_argument("--samples", "-n", type=int, default=None, help="æœ€å¤§æ ·æœ¬æ•°")
    run_parser.add_argument("--dataset", "-d", default="sage", help="æ•°æ®é›† (default: sage)")
    run_parser.add_argument("--output", "-o", type=str, default=None, help="è¾“å‡ºç›®å½•")
    run_parser.set_defaults(func=cmd_run)

    # =========================================================================
    # train å­å‘½ä»¤
    # =========================================================================
    train_parser = subparsers.add_parser("train", help="è®­ç»ƒæ–¹æ³•å¯¹æ¯” (Paper 2)")
    train_parser.add_argument(
        "--methods", "-m", default="A_baseline,D_combined", help="è®­ç»ƒæ–¹æ³•ï¼Œé€—å·åˆ†éš”"
    )
    train_parser.add_argument("--model", default="Qwen/Qwen2.5-1.5B-Instruct", help="åŸºç¡€æ¨¡å‹")
    train_parser.add_argument("--quick", "-q", action="store_true", help="å¿«é€Ÿæ¨¡å¼")
    train_parser.add_argument("--dry-run", action="store_true", help="æ¨¡æ‹Ÿè¿è¡Œ (ä¸å®é™…è®­ç»ƒ)")
    train_parser.add_argument("--output", "-o", type=str, default=None, help="è¾“å‡ºç›®å½•")
    train_parser.set_defaults(func=cmd_train)

    # =========================================================================
    # llm å­å‘½ä»¤
    # =========================================================================
    llm_parser = subparsers.add_parser("llm", help="LLM æœåŠ¡ç®¡ç†")
    llm_parser.add_argument(
        "llm_action", choices=["start", "stop", "status"], help="æ“ä½œ: start/stop/status"
    )
    llm_parser.add_argument(
        "--model", default=DEFAULT_LLM_MODEL, help=f"LLM æ¨¡å‹ (default: {DEFAULT_LLM_MODEL})"
    )
    llm_parser.add_argument(
        "--port", type=int, default=DEFAULT_LLM_PORT, help=f"ç«¯å£ (default: {DEFAULT_LLM_PORT})"
    )
    llm_parser.add_argument(
        "--gpu-memory", type=float, default=0.5, help="GPU æ˜¾å­˜æ¯”ä¾‹ (default: 0.5)"
    )
    llm_parser.set_defaults(func=cmd_llm)

    # =========================================================================
    # list å­å‘½ä»¤
    # =========================================================================
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºå¯ç”¨èµ„æº")
    list_parser.add_argument(
        "resource",
        choices=["datasets", "methods", "experiments"],
        help="èµ„æºç±»å‹: datasets/methods/experiments",
    )
    list_parser.set_defaults(func=cmd_list)

    # =========================================================================
    # interactive å­å‘½ä»¤
    # =========================================================================
    interactive_parser = subparsers.add_parser("interactive", help="äº¤äº’å¼æ¨¡å¼")
    interactive_parser.set_defaults(func=cmd_interactive)

    # =========================================================================
    # è§£æå¹¶æ‰§è¡Œ
    # =========================================================================
    args = parser.parse_args()

    if args.command is None:
        print_banner()
        parser.print_help()
        return 0

    return args.func(args)


if __name__ == "__main__":
    sys.exit(main())
