#!/usr/bin/env python3
"""
SAGE Benchmark CLI - ç»Ÿä¸€äº¤äº’å¼å…¥å£

æ”¯æŒä¸¤ç¯‡è®ºæ–‡çš„å®éªŒï¼š
- Paper 1 (Benchmark): SAGE-Bench è¯„æµ‹æ¡†æ¶ï¼Œå¯¹æ¯”ç°æœ‰ SOTA æ–¹æ³•
- Paper 2 (Method): SAGE åŸåˆ›æ–¹æ³• (Coreset + Continual Learning)

Usage:
    # äº¤äº’å¼è¿è¡Œ
    python sage_benchmark_cli.py

    # ç›´æ¥æŒ‡å®š Paper å’Œå®éªŒ
    python sage_benchmark_cli.py --paper 1 --experiment tool_selection
    python sage_benchmark_cli.py --paper 2 --experiment training

    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨å®éªŒ
    python sage_benchmark_cli.py --list

    # LLM æœåŠ¡ç®¡ç†
    python sage_benchmark_cli.py --start-llm                    # å¯åŠ¨æœ¬åœ° LLM æœåŠ¡
    python sage_benchmark_cli.py --start-llm --llm-model Qwen/Qwen2.5-7B-Instruct
    python sage_benchmark_cli.py --llm-status                   # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    python sage_benchmark_cli.py --stop-llm                     # åœæ­¢æœåŠ¡
"""

from __future__ import annotations

import argparse
import os
import signal
import subprocess
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import httpx

# è·å–è„šæœ¬ç›®å½•
SCRIPT_DIR = Path(__file__).resolve().parent

# LLM æœåŠ¡é…ç½® - ä»ç»Ÿä¸€é…ç½®å¯¼å…¥
try:
    from sage.common.config.ports import SagePorts

    DEFAULT_LLM_PORT = SagePorts.BENCHMARK_LLM
except ImportError:
    DEFAULT_LLM_PORT = 8901  # Fallback

DEFAULT_LLM_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
LLM_PID_FILE = Path.home() / ".sage" / "benchmark_llm.pid"


# =============================================================================
# LLM æœåŠ¡ç®¡ç†
# =============================================================================


def check_llm_service(port: int = DEFAULT_LLM_PORT) -> dict:
    """æ£€æŸ¥ LLM æœåŠ¡çŠ¶æ€"""
    result = {"running": False, "port": port, "model": None, "error": None}

    try:
        response = httpx.get(f"http://localhost:{port}/v1/models", timeout=5.0)
        if response.status_code == 200:
            data = response.json()
            models = data.get("data", [])
            if models:
                result["running"] = True
                result["model"] = models[0].get("id", "unknown")
        else:
            result["error"] = f"HTTP {response.status_code}"
    except httpx.ConnectError:
        result["error"] = "Connection refused"
    except httpx.TimeoutException:
        result["error"] = "Timeout"
    except Exception as e:
        result["error"] = str(e)

    return result


def start_llm_service(
    model: str = DEFAULT_LLM_MODEL,
    port: int = DEFAULT_LLM_PORT,
    gpu_memory: float = 0.5,
) -> bool:
    """å¯åŠ¨æœ¬åœ° vLLM æœåŠ¡"""
    # æ£€æŸ¥æ˜¯å¦å·²è¿è¡Œ
    status = check_llm_service(port)
    if status["running"]:
        print(f"âœ… LLM æœåŠ¡å·²åœ¨è¿è¡Œ (port={port}, model={status['model']})")
        return True

    print("ğŸš€ å¯åŠ¨ LLM æœåŠ¡...")
    print(f"   æ¨¡å‹: {model}")
    print(f"   ç«¯å£: {port}")
    print(f"   GPU æ˜¾å­˜: {gpu_memory * 100:.0f}%")

    # ç¡®ä¿ PID æ–‡ä»¶ç›®å½•å­˜åœ¨
    LLM_PID_FILE.parent.mkdir(parents=True, exist_ok=True)

    # æ„å»º vLLM å‘½ä»¤
    cmd = [
        sys.executable,
        "-m",
        "vllm.entrypoints.openai.api_server",
        "--model",
        model,
        "--port",
        str(port),
        "--gpu-memory-utilization",
        str(gpu_memory),
        "--trust-remote-code",
    ]

    try:
        # åå°å¯åŠ¨
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            start_new_session=True,
        )

        # ä¿å­˜ PID
        with open(LLM_PID_FILE, "w") as f:
            f.write(str(process.pid))

        print(f"   PID: {process.pid}")
        print("   ç­‰å¾…æœåŠ¡å¯åŠ¨...")

        # ç­‰å¾…æœåŠ¡å°±ç»ª (æœ€å¤š 120 ç§’)
        for i in range(120):
            time.sleep(1)
            status = check_llm_service(port)
            if status["running"]:
                print(f"\nâœ… LLM æœåŠ¡å·²å¯åŠ¨ (è€—æ—¶ {i + 1}s)")
                print(f"   ç«¯ç‚¹: http://localhost:{port}/v1")
                print(f"   æ¨¡å‹: {status['model']}")
                return True
            if i % 10 == 9:
                print(f"   å·²ç­‰å¾… {i + 1}s...")

        print("\nâŒ æœåŠ¡å¯åŠ¨è¶…æ—¶")
        return False

    except FileNotFoundError:
        print("âŒ vLLM æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install vllm")
        return False
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        return False


def stop_llm_service() -> bool:
    """åœæ­¢ LLM æœåŠ¡"""
    if not LLM_PID_FILE.exists():
        print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°è¿è¡Œä¸­çš„ LLM æœåŠ¡")
        return True

    try:
        with open(LLM_PID_FILE) as f:
            pid = int(f.read().strip())

        print(f"ğŸ›‘ åœæ­¢ LLM æœåŠ¡ (PID={pid})...")
        os.kill(pid, signal.SIGTERM)

        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        for _ in range(10):
            time.sleep(1)
            try:
                os.kill(pid, 0)  # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
            except OSError:
                break

        LLM_PID_FILE.unlink(missing_ok=True)
        print("âœ… LLM æœåŠ¡å·²åœæ­¢")
        return True

    except ProcessLookupError:
        print("â„¹ï¸  è¿›ç¨‹å·²ä¸å­˜åœ¨")
        LLM_PID_FILE.unlink(missing_ok=True)
        return True
    except Exception as e:
        print(f"âŒ åœæ­¢å¤±è´¥: {e}")
        return False


def print_llm_status():
    """æ‰“å° LLM æœåŠ¡çŠ¶æ€"""
    print("\nğŸ“¡ LLM æœåŠ¡çŠ¶æ€")
    print("=" * 50)

    # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„ LLM ç«¯å£
    try:
        from sage.common.config.ports import SagePorts

        ports_to_check = [SagePorts.BENCHMARK_LLM] + SagePorts.get_llm_ports()
    except ImportError:
        ports_to_check = [DEFAULT_LLM_PORT, 8001, 8000]

    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    unique_ports = []
    for p in ports_to_check:
        if p not in seen:
            seen.add(p)
            unique_ports.append(p)

    for port in unique_ports:
        status = check_llm_service(port)
        if status["running"]:
            print(f"  âœ… Port {port}: è¿è¡Œä¸­")
            print(f"     æ¨¡å‹: {status['model']}")
            print(f"     ç«¯ç‚¹: http://localhost:{port}/v1")
        else:
            print(f"  âŒ Port {port}: {status['error'] or 'æœªè¿è¡Œ'}")

    print()


@dataclass
class Experiment:
    """å®éªŒé…ç½®"""

    id: str
    name: str
    description: str
    paper: int  # 1 or 2
    script: str
    default_args: list[str]
    requires_gpu: bool = False
    estimated_time: str = "?"


# =============================================================================
# Paper 1: Benchmark å®éªŒ (å¯¹æ¯”ç°æœ‰ SOTA æ–¹æ³•)
# =============================================================================

PAPER1_EXPERIMENTS = [
    Experiment(
        id="timing",
        name="Challenge 1: Timing Judgment",
        description="è¯„æµ‹ä½•æ—¶è°ƒç”¨å·¥å…· vs ç›´æ¥å›ç­” (Target: â‰¥95%)",
        paper=1,
        script="run_all_experiments.py",
        default_args=["--challenge", "timing", "--eval-only"],
        estimated_time="~10 min",
    ),
    Experiment(
        id="planning",
        name="Challenge 2: Task Planning",
        description="è¯„æµ‹ä»»åŠ¡åˆ†è§£ä¸å¤šæ­¥è§„åˆ’ (Target: â‰¥90%)",
        paper=1,
        script="run_all_experiments.py",
        default_args=["--challenge", "planning", "--eval-only"],
        estimated_time="~15 min",
    ),
    Experiment(
        id="tool_selection",
        name="Challenge 3: Tool Selection",
        description="è¯„æµ‹å·¥å…·æ£€ç´¢ä¸é€‰æ‹© (Target: â‰¥95%)",
        paper=1,
        script="run_all_experiments.py",
        default_args=["--challenge", "tool_selection", "--eval-only"],
        estimated_time="~20 min",
    ),
    Experiment(
        id="all_challenges",
        name="All Challenges (å®Œæ•´è¯„æµ‹)",
        description="è¿è¡Œæ‰€æœ‰ 3 ä¸ª Challenge çš„å®Œæ•´è¯„æµ‹",
        paper=1,
        script="run_all_experiments.py",
        default_args=["--eval-only"],
        estimated_time="~2 hours",
    ),
    Experiment(
        id="cross_dataset",
        name="Cross-Dataset Comparison",
        description="è·¨æ•°æ®é›†å¯¹æ¯” (SAGE + ACEBench + APIBank + ToolAlpaca)",
        paper=1,
        script="run_unified_eval.py",
        default_args=[
            "--dataset",
            "all",
            "--methods",
            "keyword,embedding,hybrid,gorilla,dfsdt",
            "--samples",
            "100",
        ],
        estimated_time="~30 min",
    ),
    Experiment(
        id="list_datasets",
        name="List Available Datasets",
        description="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è¯„æµ‹æ•°æ®é›†",
        paper=1,
        script="run_unified_eval.py",
        default_args=["--list-datasets"],
        estimated_time="<1 min",
    ),
    Experiment(
        id="quick_benchmark",
        name="Quick Benchmark (å¿«é€Ÿ)",
        description="å¿«é€Ÿè¯„æµ‹ï¼Œè·³è¿‡ LLM æ–¹æ³•",
        paper=1,
        script="run_all_experiments.py",
        default_args=["--quick", "--skip-llm"],
        estimated_time="~30 min",
    ),
]

# =============================================================================
# Paper 2: Method å®éªŒ (SAGE åŸåˆ›æ–¹æ³•)
# =============================================================================

PAPER2_EXPERIMENTS = [
    Experiment(
        id="training_quick",
        name="Training Comparison (Quick)",
        description="å¿«é€Ÿè®­ç»ƒå¯¹æ¯”: SAGE_baseline vs SAGE_coreset vs SAGE_continual",
        paper=2,
        script="run_full_training_comparison.py",
        default_args=["--quick"],
        requires_gpu=True,
        estimated_time="~1 hour",
    ),
    Experiment(
        id="training_full",
        name="Training Comparison (Full)",
        description="å®Œæ•´è®­ç»ƒå¯¹æ¯”: æ‰€æœ‰ SAGE æ–¹æ³•",
        paper=2,
        script="run_full_training_comparison.py",
        default_args=["--full"],
        requires_gpu=True,
        estimated_time="~6 hours",
    ),
    Experiment(
        id="sage_baseline",
        name="SAGE_baseline_sft",
        description="åŸºå‡† SFT è®­ç»ƒ",
        paper=2,
        script="run_full_training_comparison.py",
        default_args=["--method", "SAGE_baseline_sft"],
        requires_gpu=True,
        estimated_time="~2 hours",
    ),
    Experiment(
        id="sage_coreset",
        name="SAGE_coreset_hybrid",
        description="Coreset æ··åˆç­–ç•¥ (60% loss + 40% diversity)",
        paper=2,
        script="run_full_training_comparison.py",
        default_args=["--method", "SAGE_coreset_hybrid"],
        requires_gpu=True,
        estimated_time="~1.5 hours",
    ),
    Experiment(
        id="sage_continual",
        name="SAGE_continual",
        description="æŒç»­å­¦ä¹  + ç»éªŒå›æ”¾",
        paper=2,
        script="run_full_training_comparison.py",
        default_args=["--method", "SAGE_continual"],
        requires_gpu=True,
        estimated_time="~2.5 hours",
    ),
    Experiment(
        id="sage_combined",
        name="SAGE_combined (æ¨è)",
        description="å®Œæ•´æ–¹æ¡ˆ: Coreset + Continual Learning",
        paper=2,
        script="run_full_training_comparison.py",
        default_args=["--method", "SAGE_combined"],
        requires_gpu=True,
        estimated_time="~2 hours",
    ),
    Experiment(
        id="ablation",
        name="Ablation Study",
        description="æ¶ˆèå®éªŒ: å„ç»„ä»¶è´¡çŒ®åˆ†æ",
        paper=2,
        script="run_full_training_comparison.py",
        default_args=["--ablation"],
        requires_gpu=True,
        estimated_time="~4 hours",
    ),
]

ALL_EXPERIMENTS = PAPER1_EXPERIMENTS + PAPER2_EXPERIMENTS


def print_banner():
    """æ‰“å°æ¬¢è¿ banner"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     SAGE Benchmark CLI v1.0                               â•‘
â•‘                                                                           â•‘
â•‘  Paper 1: SAGE-Bench - Unified Benchmark for Agent Capabilities           â•‘
â•‘  Paper 2: SAGE Methods - Coreset Selection + Continual Learning           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


def print_experiments(paper: Optional[int] = None):
    """æ‰“å°å¯ç”¨å®éªŒåˆ—è¡¨"""
    if paper is None or paper == 1:
        print("\nğŸ“˜ Paper 1: Benchmark (ç°æœ‰ SOTA æ–¹æ³•å¯¹æ¯”)")
        print("=" * 70)
        for i, exp in enumerate(PAPER1_EXPERIMENTS, 1):
            gpu_tag = " [GPU]" if exp.requires_gpu else ""
            print(f"  [{i}] {exp.name}{gpu_tag}")
            print(f"      {exp.description}")
            print(f"      é¢„è®¡æ—¶é—´: {exp.estimated_time}")
            print()

    if paper is None or paper == 2:
        print("\nğŸ“™ Paper 2: Method (SAGE åŸåˆ›æ–¹æ³•)")
        print("=" * 70)
        offset = len(PAPER1_EXPERIMENTS) if paper is None else 0
        for i, exp in enumerate(PAPER2_EXPERIMENTS, 1):
            gpu_tag = " [GPU]" if exp.requires_gpu else ""
            print(f"  [{offset + i}] {exp.name}{gpu_tag}")
            print(f"      {exp.description}")
            print(f"      é¢„è®¡æ—¶é—´: {exp.estimated_time}")
            print()


def select_experiment_interactive() -> Optional[Experiment]:
    """äº¤äº’å¼é€‰æ‹©å®éªŒ"""
    print_banner()

    # é€‰æ‹© Paper
    print("è¯·é€‰æ‹©è®ºæ–‡:")
    print("  [1] Paper 1: Benchmark (è¯„æµ‹ç°æœ‰æ–¹æ³•)")
    print("  [2] Paper 2: Method (SAGE åŸåˆ›æ–¹æ³•)")
    print("  [0] é€€å‡º")
    print()

    try:
        paper_choice = input("è¯·è¾“å…¥é€‰é¡¹ (1/2/0): ").strip()
        if paper_choice == "0":
            return None
        paper = int(paper_choice)
        if paper not in [1, 2]:
            print("æ— æ•ˆé€‰é¡¹")
            return None
    except (ValueError, KeyboardInterrupt):
        return None

    # é€‰æ‹©å®éªŒ
    experiments = PAPER1_EXPERIMENTS if paper == 1 else PAPER2_EXPERIMENTS
    print_experiments(paper)

    try:
        exp_choice = input(f"è¯·é€‰æ‹©å®éªŒ (1-{len(experiments)}, 0 è¿”å›): ").strip()
        if exp_choice == "0":
            return select_experiment_interactive()
        idx = int(exp_choice) - 1
        if 0 <= idx < len(experiments):
            return experiments[idx]
        else:
            print("æ— æ•ˆé€‰é¡¹")
            return None
    except (ValueError, KeyboardInterrupt):
        return None


def run_experiment(exp: Experiment, extra_args: list[str] = None, skip_confirm: bool = False):
    """è¿è¡Œå®éªŒ"""
    script_path = SCRIPT_DIR / exp.script
    if not script_path.exists():
        print(f"âŒ è„šæœ¬ä¸å­˜åœ¨: {script_path}")
        return False

    # æ„å»ºå‘½ä»¤
    cmd = [sys.executable, str(script_path)] + exp.default_args
    if extra_args:
        cmd.extend(extra_args)

    print(f"\n{'=' * 70}")
    print(f"ğŸš€ è¿è¡Œå®éªŒ: {exp.name}")
    print(f"   è„šæœ¬: {exp.script}")
    print(f"   å‚æ•°: {' '.join(exp.default_args)}")
    print(f"   é¢„è®¡æ—¶é—´: {exp.estimated_time}")
    if exp.requires_gpu:
        print("   âš ï¸  éœ€è¦ GPU")
    print(f"{'=' * 70}\n")

    # ç¡®è®¤è¿è¡Œ (å¯è·³è¿‡)
    if not skip_confirm:
        try:
            confirm = input("ç¡®è®¤è¿è¡Œ? (y/n): ").strip().lower()
            if confirm != "y":
                print("å·²å–æ¶ˆ")
                return False
        except KeyboardInterrupt:
            print("\nå·²å–æ¶ˆ")
            return False

    # è¿è¡Œ
    print(f"\næ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
    try:
        result = subprocess.run(cmd, cwd=str(SCRIPT_DIR))
        return result.returncode == 0
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        return False


def find_experiment(paper: Optional[int], exp_id: str) -> Optional[Experiment]:
    """æ ¹æ® ID æŸ¥æ‰¾å®éªŒ"""
    experiments = ALL_EXPERIMENTS
    if paper == 1:
        experiments = PAPER1_EXPERIMENTS
    elif paper == 2:
        experiments = PAPER2_EXPERIMENTS

    for exp in experiments:
        if exp.id == exp_id:
            return exp
    return None


def main():
    parser = argparse.ArgumentParser(
        description="SAGE Benchmark CLI - ç»Ÿä¸€äº¤äº’å¼å…¥å£",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # äº¤äº’å¼è¿è¡Œ
    python sage_benchmark_cli.py

    # ç›´æ¥è¿è¡Œ Paper 1 å®éªŒ
    python sage_benchmark_cli.py --paper 1 --experiment tool_selection

    # ç›´æ¥è¿è¡Œ Paper 2 å®éªŒ
    python sage_benchmark_cli.py --paper 2 --experiment sage_combined

    # åˆ—å‡ºæ‰€æœ‰å®éªŒ
    python sage_benchmark_cli.py --list

    # LLM æœåŠ¡ç®¡ç†
    python sage_benchmark_cli.py --start-llm                                    # å¯åŠ¨é»˜è®¤æ¨¡å‹
    python sage_benchmark_cli.py --start-llm --llm-model Qwen/Qwen2.5-7B-Instruct
    python sage_benchmark_cli.py --llm-status                                   # æ£€æŸ¥çŠ¶æ€
    python sage_benchmark_cli.py --stop-llm                                     # åœæ­¢æœåŠ¡
        """,
    )

    parser.add_argument(
        "--paper",
        "-p",
        type=int,
        choices=[1, 2],
        help="é€‰æ‹©è®ºæ–‡: 1=Benchmark, 2=Method",
    )
    parser.add_argument(
        "--experiment",
        "-e",
        type=str,
        help="å®éªŒ ID (ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨å®éªŒ)",
    )
    parser.add_argument(
        "--list",
        "-l",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨å®éªŒ",
    )
    parser.add_argument(
        "--yes",
        "-y",
        action="store_true",
        help="è·³è¿‡ç¡®è®¤æç¤ºï¼Œç›´æ¥è¿è¡Œ",
    )

    # LLM æœåŠ¡ç®¡ç†å‚æ•°
    llm_group = parser.add_argument_group("LLM æœåŠ¡ç®¡ç†")
    llm_group.add_argument(
        "--start-llm",
        action="store_true",
        help="å¯åŠ¨æœ¬åœ° vLLM æœåŠ¡",
    )
    llm_group.add_argument(
        "--stop-llm",
        action="store_true",
        help="åœæ­¢æœ¬åœ° vLLM æœåŠ¡",
    )
    llm_group.add_argument(
        "--llm-status",
        action="store_true",
        help="æ£€æŸ¥ LLM æœåŠ¡çŠ¶æ€",
    )
    llm_group.add_argument(
        "--llm-model",
        type=str,
        default=DEFAULT_LLM_MODEL,
        help=f"LLM æ¨¡å‹ (é»˜è®¤: {DEFAULT_LLM_MODEL})",
    )
    llm_group.add_argument(
        "--llm-port",
        type=int,
        default=DEFAULT_LLM_PORT,
        help=f"LLM æœåŠ¡ç«¯å£ (é»˜è®¤: {DEFAULT_LLM_PORT})",
    )
    llm_group.add_argument(
        "--gpu-memory",
        type=float,
        default=0.5,
        help="GPU æ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹ (é»˜è®¤: 0.5)",
    )

    parser.add_argument(
        "extra_args",
        nargs="*",
        help="ä¼ é€’ç»™å®éªŒè„šæœ¬çš„é¢å¤–å‚æ•°",
    )

    args = parser.parse_args()

    # LLM æœåŠ¡ç®¡ç†å‘½ä»¤
    if args.llm_status:
        print_llm_status()
        return 0

    if args.stop_llm:
        return 0 if stop_llm_service() else 1

    if args.start_llm:
        success = start_llm_service(
            model=args.llm_model,
            port=args.llm_port,
            gpu_memory=args.gpu_memory,
        )
        return 0 if success else 1

    # åˆ—å‡ºå®éªŒ
    if args.list:
        print_banner()
        print_experiments()
        print_llm_status()
        return 0

    # ç›´æ¥æŒ‡å®šå®éªŒ
    if args.experiment:
        exp = find_experiment(args.paper, args.experiment)
        if exp is None:
            print(f"âŒ æœªæ‰¾åˆ°å®éªŒ: {args.experiment}")
            print("ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨å®éªŒ")
            return 1

        # æ£€æŸ¥ LLM æœåŠ¡ï¼ˆå¯¹äºéœ€è¦ LLM çš„å®éªŒï¼‰
        if exp.requires_gpu or "llm" in exp.id.lower():
            status = check_llm_service()
            if not status["running"]:
                print("âš ï¸  LLM æœåŠ¡æœªè¿è¡Œï¼ŒæŸäº›æ–¹æ³•å¯èƒ½æ— æ³•ä½¿ç”¨")
                print("   ä½¿ç”¨ --start-llm å¯åŠ¨æœåŠ¡")
                print()

        success = run_experiment(exp, args.extra_args, skip_confirm=args.yes)
        return 0 if success else 1

    # äº¤äº’å¼æ¨¡å¼
    print_banner()
    print_llm_status()

    while True:
        exp = select_experiment_interactive()
        if exp is None:
            print("\nğŸ‘‹ å†è§!")
            break

        run_experiment(exp, args.extra_args)

        # è¯¢é—®æ˜¯å¦ç»§ç»­
        try:
            print()
            cont = input("ç»§ç»­è¿è¡Œå…¶ä»–å®éªŒ? (y/n): ").strip().lower()
            if cont != "y":
                print("\nğŸ‘‹ å†è§!")
                break
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break

    return 0


if __name__ == "__main__":
    sys.exit(main())
