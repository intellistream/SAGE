#!/usr/bin/env python
"""
Agent Tool Planning å¾®è°ƒè®­ç»ƒè„šæœ¬

ä½¿ç”¨ sage.libs.finetune (LoRATrainer) + sage.tools.agent_training æ•°æ®
åœ¨ A100 GPU ä¸Šè®­ç»ƒ Agent Tool Planning æ¨¡å‹

ç”¨æ³•:
    # å¿«é€Ÿæµ‹è¯• (å°æ•°æ® + å°æ¨¡å‹)
    python run_agent_finetune.py --quick

    # æ ‡å‡†è®­ç»ƒ (7Bæ¨¡å‹ + å…¨éƒ¨æ•°æ®)
    python run_agent_finetune.py --standard

    # å¤§æ¨¡å‹è®­ç»ƒ (14B+, éœ€è¦å¤šå¡)
    python run_agent_finetune.py --large

    # è‡ªå®šä¹‰é…ç½®
    python run_agent_finetune.py --model Qwen/Qwen2.5-7B-Instruct --epochs 5 --max-samples 1000
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Any

# è·å–é¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR = Path(__file__).parent.absolute()
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent


def setup_environment():
    """é…ç½®ç¯å¢ƒ"""
    # ç¦ç”¨ tokenizers å¹¶è¡Œ (é¿å…è­¦å‘Š)
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # è®¾ç½® HF ç¼“å­˜ç›®å½•
    cache_dir = PROJECT_ROOT / ".sage" / "cache" / "huggingface"
    cache_dir.mkdir(parents=True, exist_ok=True)
    os.environ["HF_HOME"] = str(cache_dir)


def check_gpu():
    """æ£€æŸ¥ GPU çŠ¶æ€"""
    try:
        import torch

        if not torch.cuda.is_available():
            print("âŒ CUDA ä¸å¯ç”¨")
            return None, 0

        gpu_count = torch.cuda.device_count()
        gpu_info = []
        total_memory = 0

        for i in range(gpu_count):
            name = torch.cuda.get_device_name(i)
            mem_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            mem_free = (
                torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_reserved(i)
            ) / 1024**3
            gpu_info.append(f"  GPU {i}: {name} ({mem_total:.1f}GB total, ~{mem_free:.1f}GB free)")
            total_memory += mem_total

        print("ğŸ–¥ï¸  GPU çŠ¶æ€:")
        print("\n".join(gpu_info))
        print()

        return gpu_count, total_memory

    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return None, 0


def load_agent_sft_data(max_samples: int | None = None) -> list[dict]:
    """åŠ è½½ Agent SFT è®­ç»ƒæ•°æ®

    Args:
        max_samples: æœ€å¤§æ ·æœ¬æ•° (None = å…¨éƒ¨)

    Returns:
        å¯¹è¯æ ¼å¼æ•°æ®åˆ—è¡¨
    """
    from sage.data import DataManager
    from sage.libs.finetune.agent import AgentDialogProcessor

    print("\nğŸ“Š åŠ è½½ Agent SFT æ•°æ®...")

    # ä½¿ç”¨ DataManager åŠ è½½æ•°æ®
    dm = DataManager.get_instance()
    raw_data = dm.load("agent_sft", split="train")
    if max_samples:
        raw_data = raw_data[:max_samples]

    # å¤„ç†æˆå¯¹è¯æ ¼å¼
    processor = AgentDialogProcessor()
    dialogs = processor.process_batch(raw_data)

    # è½¬æ¢æˆ sage.libs.finetune éœ€è¦çš„ conversation æ ¼å¼
    formatted_data = []
    for dialog in dialogs:
        conversations = []
        for msg in dialog.messages:
            conversations.append({"role": msg.role, "content": msg.content})
        formatted_data.append({"conversations": conversations})

    print(f"âœ… åŠ è½½äº† {len(formatted_data)} æ¡å¯¹è¯æ•°æ®")
    return formatted_data


def save_training_data(data: list[dict], output_path: Path) -> Path:
    """ä¿å­˜è®­ç»ƒæ•°æ®ä¸º JSON æ–‡ä»¶

    Args:
        data: å¯¹è¯æ•°æ®
        output_path: è¾“å‡ºç›®å½•

    Returns:
        æ•°æ®æ–‡ä»¶è·¯å¾„
    """
    data_file = output_path / "train_data.json"
    with open(data_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ è®­ç»ƒæ•°æ®ä¿å­˜åˆ°: {data_file}")
    return data_file


def get_training_config(
    preset: str,
    model_name: str | None = None,
    epochs: int | None = None,
    output_dir: Path | None = None,
    data_path: Path | None = None,
) -> Any:
    """è·å–è®­ç»ƒé…ç½®

    Args:
        preset: é¢„è®¾é…ç½® (quick/standard/large)
        model_name: æ¨¡å‹åç§° (è¦†ç›–é¢„è®¾)
        epochs: è®­ç»ƒè½®æ•° (è¦†ç›–é¢„è®¾)
        output_dir: è¾“å‡ºç›®å½•
        data_path: æ•°æ®è·¯å¾„

    Returns:
        TrainingConfig
    """
    from sage.libs.finetune import LoRAConfig, PresetConfigs

    if preset == "quick":
        # å¿«é€Ÿæµ‹è¯•: å°æ¨¡å‹, å°‘ epoch
        config = PresetConfigs.a100()
        config.model_name = model_name or "Qwen/Qwen2.5-1.5B-Instruct"
        config.num_train_epochs = epochs or 1
        config.max_length = 2048
        config.per_device_train_batch_size = 4

    elif preset == "standard":
        # æ ‡å‡†è®­ç»ƒ: 7B æ¨¡å‹
        config = PresetConfigs.a100()
        config.model_name = model_name or "Qwen/Qwen2.5-7B-Instruct"
        config.num_train_epochs = epochs or 3
        config.max_length = 4096

    elif preset == "large":
        # å¤§æ¨¡å‹è®­ç»ƒ: 14B+
        config = PresetConfigs.a100()
        config.model_name = model_name or "Qwen/Qwen2.5-14B-Instruct"
        config.num_train_epochs = epochs or 3
        config.max_length = 4096
        config.per_device_train_batch_size = 4  # å‡å° batch size
        config.gradient_accumulation_steps = 4
        # ä½¿ç”¨æ›´å¤§çš„ LoRA rank
        config.lora = LoRAConfig(
            r=16,
            lora_alpha=32,
            target_modules=[
                "q_proj",
                "v_proj",
                "k_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj",
            ],
        )

    else:
        # è‡ªå®šä¹‰
        config = PresetConfigs.a100()
        if model_name:
            config.model_name = model_name
        if epochs:
            config.num_train_epochs = epochs

    # è®¾ç½®å…¬å…±å‚æ•°
    if output_dir:
        config.output_dir = output_dir
    if data_path:
        config.data_path = data_path

    # Agent-specific ä¼˜åŒ–
    # å·¥å…·è§„åˆ’ä»»åŠ¡éœ€è¦æ›´é•¿çš„ä¸Šä¸‹æ–‡
    if config.max_length < 2048:
        config.max_length = 2048

    return config


def run_training(config) -> Path:
    """æ‰§è¡Œè®­ç»ƒ

    Args:
        config: TrainingConfig

    Returns:
        è¾“å‡ºç›®å½•
    """
    from sage.libs.finetune import LoRATrainer

    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹ Agent Tool Planning å¾®è°ƒè®­ç»ƒ")
    print("=" * 60)
    print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  â€¢ åŸºç¡€æ¨¡å‹: {config.model_name}")
    print(f"  â€¢ è®­ç»ƒè½®æ•°: {config.num_train_epochs}")
    print(f"  â€¢ åºåˆ—é•¿åº¦: {config.max_length}")
    print(f"  â€¢ Batch Size: {config.per_device_train_batch_size}")
    print(f"  â€¢ æ¢¯åº¦ç´¯ç§¯: {config.gradient_accumulation_steps}")
    print(f"  â€¢ æœ‰æ•ˆ Batch: {config.effective_batch_size}")
    print(f"  â€¢ LoRA Rank: {config.lora.r}")
    print(f"  â€¢ ç²¾åº¦: {'bf16' if config.bf16 else 'fp16' if config.fp16 else 'fp32'}")
    print(f"  â€¢ è¾“å‡ºç›®å½•: {config.output_dir}")
    print()

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LoRATrainer(config)

    # æ‰§è¡Œè®­ç»ƒ
    trainer.train()

    return config.output_dir


def evaluate_model(model_dir: Path):
    """è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹

    Args:
        model_dir: æ¨¡å‹ç›®å½•
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹")
    print("=" * 60)

    # TODO: é›†æˆ benchmark è¯„ä¼°
    # from sage.benchmark import ToolSelectionExperiment

    print("\nğŸ’¡ æ‰‹åŠ¨è¯„ä¼°å‘½ä»¤:")
    print("  # æµ‹è¯•å¯¹è¯")
    print(f"  sage finetune chat {model_dir.name}")
    print("  ")
    print("  # åˆå¹¶æƒé‡")
    print(f"  sage finetune merge {model_dir.name}")
    print("  ")
    print("  # è¿è¡Œ benchmark")
    print(f"  python run_method_comparison.py --model-path {model_dir / 'lora_weights'}")


def main():
    parser = argparse.ArgumentParser(
        description="Agent Tool Planning å¾®è°ƒè®­ç»ƒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¿«é€Ÿæµ‹è¯• (1.5Bæ¨¡å‹, 1 epoch, ~100æ ·æœ¬)
  python run_agent_finetune.py --quick

  # æ ‡å‡†è®­ç»ƒ (7Bæ¨¡å‹, 3 epochs, å…¨éƒ¨æ•°æ®)
  python run_agent_finetune.py --standard

  # å¤§æ¨¡å‹è®­ç»ƒ (14Bæ¨¡å‹)
  python run_agent_finetune.py --large

  # è‡ªå®šä¹‰é…ç½®
  python run_agent_finetune.py --model Qwen/Qwen2.5-7B-Instruct --epochs 5 --max-samples 2000
        """,
    )

    # é¢„è®¾é…ç½®
    preset_group = parser.add_mutually_exclusive_group()
    preset_group.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯• (1.5B, 1 epoch)")
    preset_group.add_argument("--standard", action="store_true", help="æ ‡å‡†è®­ç»ƒ (7B, 3 epochs)")
    preset_group.add_argument("--large", action="store_true", help="å¤§æ¨¡å‹è®­ç»ƒ (14B+)")

    # è‡ªå®šä¹‰å‚æ•°
    parser.add_argument("--model", type=str, help="åŸºç¡€æ¨¡å‹åç§° (è¦†ç›–é¢„è®¾)")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•° (è¦†ç›–é¢„è®¾)")
    parser.add_argument("--max-samples", type=int, help="æœ€å¤§æ ·æœ¬æ•° (None=å…¨éƒ¨)")
    parser.add_argument("--output-dir", type=str, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--skip-eval", action="store_true", help="è·³è¿‡è¯„ä¼°")
    parser.add_argument("--dry-run", action="store_true", help="åªæ˜¾ç¤ºé…ç½®, ä¸æ‰§è¡Œè®­ç»ƒ")

    args = parser.parse_args()

    # è®¾ç½®ç¯å¢ƒ
    setup_environment()

    print("=" * 60)
    print("ğŸ§  Agent Tool Planning Fine-tuning")
    print("   ä½¿ç”¨ sage.libs.finetune + sage.tools.agent_training")
    print("=" * 60)

    # æ£€æŸ¥ GPU
    gpu_count, total_memory = check_gpu()
    if gpu_count is None:
        print("\nâŒ æ— æ³•æ£€æµ‹åˆ° GPU, é€€å‡º")
        sys.exit(1)

    # ç¡®å®šé¢„è®¾
    if args.quick:
        preset = "quick"
        max_samples = args.max_samples or 100
    elif args.standard:
        preset = "standard"
        max_samples = args.max_samples
    elif args.large:
        preset = "large"
        max_samples = args.max_samples
    else:
        preset = "standard"  # é»˜è®¤
        max_samples = args.max_samples

    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = PROJECT_ROOT / ".sage" / "finetune_output" / f"agent_tool_planning_{preset}"
    output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ•°æ®
    train_data = load_agent_sft_data(max_samples=max_samples)
    data_file = save_training_data(train_data, output_dir)

    # è·å–é…ç½®
    config = get_training_config(
        preset=preset,
        model_name=args.model,
        epochs=args.epochs,
        output_dir=output_dir,
        data_path=data_file,
    )

    if args.dry_run:
        print("\nğŸ“‹ [Dry Run] è®­ç»ƒé…ç½®:")
        print(f"  é¢„è®¾: {preset}")
        print(f"  æ¨¡å‹: {config.model_name}")
        print(f"  æ•°æ®: {len(train_data)} æ ·æœ¬")
        print(f"  è½®æ•°: {config.num_train_epochs}")
        print(f"  è¾“å‡º: {config.output_dir}")
        print("\nä½¿ç”¨ --standard/--quick/--large æ‰§è¡Œå®é™…è®­ç»ƒ")
        return

    # æ‰§è¡Œè®­ç»ƒ
    output_dir = run_training(config)

    # è¯„ä¼°
    if not args.skip_eval:
        evaluate_model(output_dir)

    print("\n" + "=" * 60)
    print("âœ… å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
