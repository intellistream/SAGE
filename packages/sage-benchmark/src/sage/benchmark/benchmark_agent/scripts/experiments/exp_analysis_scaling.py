#!/usr/bin/env python3
"""
Section 5.3.2: Scaling Analysis

æµ‹è¯•æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡ä¸‹çš„æ€§èƒ½å˜åŒ–ã€‚

åˆ†æå†…å®¹:
1. Tool Set Size Scaling - å·¥å…·æ•°é‡å¯¹å‡†ç¡®ç‡çš„å½±å“
2. LLM Size Scaling - æ¨¡å‹å¤§å°å¯¹æ€§èƒ½çš„å½±å“

è¾“å‡º:
- figures/fig5_analysis_scaling_tool_count.pdf
- figures/fig6_analysis_scaling_llm_size.pdf
- tables/table_scaling_results.tex

Usage:
    python exp_analysis_scaling.py
    python exp_analysis_scaling.py --tool-scaling
    python exp_analysis_scaling.py --llm-scaling
"""

from __future__ import annotations

import argparse
import time
from typing import Any

from .exp_utils import (
    get_figures_dir,
    load_benchmark_data,
    print_section_header,
    print_subsection_header,
    save_results,
    setup_experiment_env,
)

# =============================================================================
# Tool Set Size Scaling
# =============================================================================

TOOL_SCALE_POINTS = [10, 25, 50, 100, 200, 500]


def run_tool_scaling_experiment(
    max_samples: int = 50,
    strategies: list[str] = None,
    verbose: bool = True,
) -> dict[str, list[tuple[int, float, float]]]:
    """
    è¿è¡Œå·¥å…·æ•°é‡ Scaling å®éªŒã€‚

    Args:
        max_samples: æ¯ä¸ªè§„æ¨¡ç‚¹çš„æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°
        strategies: è¦æµ‹è¯•çš„ç­–ç•¥åˆ—è¡¨
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        {strategy: [(tool_count, accuracy, latency_ms), ...]}
    """
    print_subsection_header("Tool Set Size Scaling")

    if strategies is None:
        strategies = ["selector.keyword", "selector.embedding", "selector.hybrid"]

    # åŠ è½½åŸºç¡€æ•°æ®
    samples = load_benchmark_data("selection", split="test", max_samples=max_samples)
    if not samples:
        print("  âŒ No selection data available")
        return {}

    # åŠ è½½æˆ–ç”Ÿæˆ noise tools
    noise_tools = _generate_noise_tools(1000)

    try:
        from sage.benchmark.benchmark_agent import get_adapter_registry

        registry = get_adapter_registry()
    except ImportError:
        print("  âŒ Failed to import adapter registry")
        return {}

    results = {s: [] for s in strategies}

    for scale in TOOL_SCALE_POINTS:
        print(f"\n    Scale: {scale} tools")

        for strategy_name in strategies:
            try:
                selector = registry.get(strategy_name)
            except Exception as e:
                print(f"      âš ï¸  {strategy_name}: {e}")
                continue

            # è¿è¡Œæµ‹è¯•
            hits = 0
            total_time = 0.0

            for sample in samples:
                query = sample.get("instruction", "")
                base_tools = sample.get("candidate_tools", [])
                ground_truth = sample.get("ground_truth", [])

                # æ‰©å±•å·¥å…·é›†åˆ°ç›®æ ‡è§„æ¨¡
                extended_tools = _extend_tool_set(base_tools, noise_tools, scale)

                start = time.time()
                try:
                    predictions = selector.select(query, candidate_tools=extended_tools, top_k=5)
                    pred_ids = (
                        [p.tool_id if hasattr(p, "tool_id") else str(p) for p in predictions]
                        if predictions
                        else []
                    )

                    ref_set = (
                        set(ground_truth) if isinstance(ground_truth, list) else {ground_truth}
                    )
                    if set(pred_ids[:5]) & ref_set:
                        hits += 1
                except Exception:
                    pass

                total_time += time.time() - start

            accuracy = hits / len(samples) if samples else 0
            avg_latency = total_time * 1000 / len(samples) if samples else 0

            results[strategy_name].append((scale, accuracy, avg_latency))

            if verbose:
                print(
                    f"      {strategy_name.split('.')[-1]:12s}: {accuracy * 100:5.1f}% ({avg_latency:.1f}ms)"
                )

    return results


def _generate_noise_tools(count: int) -> list[str]:
    """ç”Ÿæˆå¹²æ‰°å·¥å…·ã€‚"""
    import random

    categories = ["search", "calendar", "email", "file", "math", "weather", "news", "social"]
    actions = ["get", "set", "create", "delete", "update", "list", "find", "check"]

    tools = []
    for i in range(count):
        cat = random.choice(categories)
        action = random.choice(actions)
        tools.append(f"noise_{cat}_{action}_{i:04d}")

    return tools


def _extend_tool_set(base_tools: list[str], noise_tools: list[str], target_size: int) -> list[str]:
    """æ‰©å±•å·¥å…·é›†åˆ°ç›®æ ‡å¤§å°ã€‚"""
    import random

    result = list(base_tools)
    needed = target_size - len(result)

    if needed > 0:
        available_noise = [t for t in noise_tools if t not in result]
        result.extend(random.sample(available_noise, min(needed, len(available_noise))))

    return result[:target_size]


# =============================================================================
# LLM Size Scaling
# =============================================================================

# æ¨¡å‹åˆ—è¡¨ (æŒ‰å‚æ•°è§„æ¨¡æ’åº)
# 2x A100 80GB å¯æ”¯æŒåˆ° 14B æ¨¡å‹å•å¡è¿è¡Œ
LLM_MODELS = [
    # å°æ¨¡å‹
    ("Qwen/Qwen2.5-0.5B-Instruct", "0.5B", 1),  # ~1GB VRAM
    ("Qwen/Qwen2.5-1.5B-Instruct", "1.5B", 1),  # ~3GB VRAM
    ("Qwen/Qwen2.5-3B-Instruct", "3B", 1),  # ~6GB VRAM
    # ä¸­ç­‰æ¨¡å‹
    ("Qwen/Qwen2.5-7B-Instruct", "7B", 1),  # ~14GB VRAM
    ("Qwen/Qwen2.5-14B-Instruct", "14B", 1),  # ~28GB VRAM
]

# å¤‡é€‰ï¼šå¦‚æœ Qwen ä¸‹è½½æ…¢ï¼Œå¯ç”¨ Llama
LLM_MODELS_LLAMA = [
    ("meta-llama/Llama-3.2-1B-Instruct", "1B", 1),
    ("meta-llama/Llama-3.2-3B-Instruct", "3B", 1),
    ("meta-llama/Llama-3.1-8B-Instruct", "8B", 1),
]


def _start_vllm_server(model_id: str, tensor_parallel: int = 1, port: int = 8901) -> bool:
    """
    å¯åŠ¨ vLLM æœåŠ¡å™¨ã€‚

    Args:
        model_id: HuggingFace æ¨¡å‹ ID
        tensor_parallel: Tensor Parallel æ•°é‡ (GPU æ•°)
        port: æœåŠ¡ç«¯å£

    Returns:
        æ˜¯å¦æˆåŠŸå¯åŠ¨
    """
    import subprocess
    import time

    import requests

    # å…ˆåœæ­¢å·²æœ‰æœåŠ¡
    _stop_vllm_server(port)
    time.sleep(2)

    print(f"      Starting vLLM server for {model_id}...")

    cmd = [
        "vllm",
        "serve",
        model_id,
        "--port",
        str(port),
        "--gpu-memory-utilization",
        "0.85",
        "--max-model-len",
        "4096",
        "--trust-remote-code",
    ]

    if tensor_parallel > 1:
        cmd.extend(["--tensor-parallel-size", str(tensor_parallel)])

    # åå°å¯åŠ¨
    try:
        subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            start_new_session=True,
        )
    except Exception as e:
        print(f"      Failed to start vLLM: {e}")
        return False

    # ç­‰å¾…æœåŠ¡å°±ç»ª (æœ€å¤š 5 åˆ†é’Ÿ)
    max_wait = 300
    start_time = time.time()

    while time.time() - start_time < max_wait:
        try:
            resp = requests.get(f"http://localhost:{port}/v1/models", timeout=5)
            if resp.status_code == 200:
                print(f"      vLLM server ready (took {time.time() - start_time:.0f}s)")
                return True
        except Exception:
            pass
        time.sleep(5)

    print("      Timeout waiting for vLLM server")
    return False


def _stop_vllm_server(port: int = 8901) -> None:
    """åœæ­¢ vLLM æœåŠ¡å™¨ã€‚"""
    import subprocess

    # é€šè¿‡ç«¯å£æ‰¾è¿›ç¨‹å¹¶æ€æ‰
    try:
        result = subprocess.run(
            ["lsof", "-t", f"-i:{port}"],
            capture_output=True,
            text=True,
        )
        if result.stdout.strip():
            pids = result.stdout.strip().split("\n")
            for pid in pids:
                subprocess.run(["kill", "-9", pid], capture_output=True)
    except Exception:
        pass


def _test_llm_on_challenge(
    model_id: str,
    samples: list[dict],
    challenge: str,
    port: int = 8901,
) -> tuple[float, float]:
    """
    åœ¨æŒ‡å®š Challenge ä¸Šæµ‹è¯• LLM æ€§èƒ½ã€‚

    Returns:
        (accuracy, avg_latency_ms)
    """
    import time

    import requests

    base_url = f"http://localhost:{port}/v1"

    correct = 0
    total_latency = 0.0
    tested = 0

    for sample in samples:
        query = sample.get("instruction", sample.get("query", ""))
        ground_truth = sample.get("ground_truth", sample.get("label", ""))

        # æ„å»º prompt
        if challenge == "timing":
            prompt = f"""åˆ¤æ–­ä»¥ä¸‹ç”¨æˆ·è¯·æ±‚æ˜¯å¦éœ€è¦è°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚åªå›ç­” "yes" æˆ– "no"ã€‚

ç”¨æˆ·è¯·æ±‚: {query}

å›ç­”:"""
        elif challenge == "planning":
            tools = sample.get("candidate_tools", [])
            tool_str = ", ".join(tools[:10]) if tools else "search, calculate, weather"
            prompt = f"""æ ¹æ®ç”¨æˆ·è¯·æ±‚ï¼Œç”Ÿæˆå·¥å…·è°ƒç”¨è®¡åˆ’ã€‚å¯ç”¨å·¥å…·: {tool_str}

ç”¨æˆ·è¯·æ±‚: {query}

è®¡åˆ’ (JSON æ ¼å¼):"""
        else:  # selection
            tools = sample.get("candidate_tools", [])
            tool_str = "\n".join([f"- {t}" for t in tools[:20]])
            prompt = f"""ä»ä»¥ä¸‹å·¥å…·ä¸­é€‰æ‹©æœ€é€‚åˆçš„å·¥å…·æ¥å®Œæˆç”¨æˆ·è¯·æ±‚ã€‚

å¯ç”¨å·¥å…·:
{tool_str}

ç”¨æˆ·è¯·æ±‚: {query}

é€‰æ‹©çš„å·¥å…· (åªè¾“å‡ºå·¥å…·å):"""

        try:
            start = time.time()
            resp = requests.post(
                f"{base_url}/completions",
                json={
                    "model": model_id,
                    "prompt": prompt,
                    "max_tokens": 256,
                    "temperature": 0.1,
                },
                timeout=60,
            )
            latency = (time.time() - start) * 1000

            if resp.status_code == 200:
                result = resp.json()
                output = result.get("choices", [{}])[0].get("text", "").strip().lower()

                # ç®€å•è¯„ä¼°
                if challenge == "timing":
                    pred = "yes" in output or "éœ€è¦" in output
                    label = (
                        ground_truth
                        if isinstance(ground_truth, bool)
                        else str(ground_truth).lower() in ["yes", "true", "1"]
                    )
                    if pred == label:
                        correct += 1
                elif challenge == "planning":
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ­£ç¡®å·¥å…·
                    if isinstance(ground_truth, list):
                        if any(t.lower() in output for t in ground_truth):
                            correct += 1
                    elif str(ground_truth).lower() in output:
                        correct += 1
                else:  # selection
                    if isinstance(ground_truth, list):
                        if any(t.lower() in output for t in ground_truth):
                            correct += 1
                    elif str(ground_truth).lower() in output:
                        correct += 1

                total_latency += latency
                tested += 1

        except Exception as e:
            print(f"        Error: {e}")
            continue

    accuracy = correct / tested if tested > 0 else 0.0
    avg_latency = total_latency / tested if tested > 0 else 0.0

    return accuracy, avg_latency


def run_llm_scaling_experiment(
    max_samples: int = 30,
    challenge: str = "planning",
    models: list = None,
    verbose: bool = True,
) -> dict[str, list[tuple[str, float, float]]]:
    """
    è¿è¡Œ LLM å¤§å° Scaling å®éªŒ (çœŸå®æµ‹è¯•)ã€‚

    Args:
        max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°
        challenge: æµ‹è¯•çš„ Challenge (planning æœ€èƒ½ä½“ç°å·®å¼‚)
        models: è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ (None ä½¿ç”¨é»˜è®¤)
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        {metric: [(model_size, value, latency_ms), ...]}
    """
    print_subsection_header("LLM Size Scaling (Real Test)")

    if models is None:
        models = LLM_MODELS

    samples = load_benchmark_data(challenge, split="test", max_samples=max_samples)
    if not samples:
        print(f"  No {challenge} data available")
        return {}

    print(f"    Testing {len(models)} models on {len(samples)} samples")
    print(f"    Challenge: {challenge}")

    results = {"accuracy": [], "latency": []}
    port = 8901

    for model_id, model_size, tensor_parallel in models:
        print(f"\n    [{model_size}] {model_id}")

        # å¯åŠ¨ vLLM æœåŠ¡
        if not _start_vllm_server(model_id, tensor_parallel, port):
            print(f"      Skipping {model_size} (failed to start)")
            continue

        try:
            # è¿è¡Œæµ‹è¯•
            accuracy, avg_latency = _test_llm_on_challenge(model_id, samples, challenge, port)

            results["accuracy"].append((model_size, accuracy, avg_latency))

            if verbose:
                print(f"      Accuracy: {accuracy * 100:.1f}%")
                print(f"      Avg Latency: {avg_latency:.0f}ms")

        finally:
            # åœæ­¢æœåŠ¡ï¼Œé‡Šæ”¾ GPU å†…å­˜
            _stop_vllm_server(port)
            import time

            time.sleep(5)  # ç­‰å¾… GPU å†…å­˜é‡Šæ”¾

    return results


# =============================================================================
# Main Experiment
# =============================================================================


def run_scaling_analysis(
    tool_scaling: bool = True,
    llm_scaling: bool = True,
    max_samples: int = 50,
    verbose: bool = True,
) -> dict[str, Any]:
    """
    è¿è¡Œå®Œæ•´çš„ Scaling åˆ†æã€‚
    """
    setup_experiment_env(verbose=verbose)

    print_section_header("Section 5.3.2: Scaling Analysis")

    all_results = {}

    if tool_scaling:
        tool_results = run_tool_scaling_experiment(max_samples=max_samples, verbose=verbose)
        all_results["tool_scaling"] = tool_results

        # åˆ†æç»“æœ
        if tool_results:
            print("\n    Tool Scaling Summary:")
            for strategy, data in tool_results.items():
                if data:
                    # è®¡ç®—æ€§èƒ½ä¸‹é™ç‡
                    first_acc = data[0][1]
                    last_acc = data[-1][1]
                    drop_rate = (first_acc - last_acc) / first_acc if first_acc > 0 else 0
                    print(
                        f"      {strategy}: {first_acc * 100:.1f}% â†’ {last_acc * 100:.1f}% (drop: {drop_rate * 100:.1f}%)"
                    )

    if llm_scaling:
        llm_results = run_llm_scaling_experiment(max_samples=max_samples, verbose=verbose)
        all_results["llm_scaling"] = llm_results

    # ä¿å­˜ç»“æœ
    output_file = save_results(all_results, "5_3_analysis", "scaling_analysis")
    print(f"\n  Results saved to: {output_file}")

    # ç”Ÿæˆå›¾è¡¨
    _generate_scaling_figures(all_results)

    return all_results


def _generate_scaling_figures(results: dict) -> None:
    """ç”Ÿæˆ Scaling åˆ†æå›¾è¡¨ã€‚"""
    try:
        from figure_generator import plot_scaling_curve

        figures_dir = get_figures_dir()

        # Tool scaling curve
        if "tool_scaling" in results and results["tool_scaling"]:
            tool_data = {}
            for strategy, data in results["tool_scaling"].items():
                tool_data[strategy.split(".")[-1]] = [(d[0], d[1]) for d in data]

            plot_scaling_curve(
                tool_data,
                x_label="Number of Candidate Tools",
                y_label="Top-5 Accuracy (%)",
                title="Tool Set Size Scaling",
                output_path=figures_dir / "fig5_analysis_scaling_tool_count.pdf",
                log_x=True,
            )
            print("  Figure saved: fig5_analysis_scaling_tool_count.pdf")

        # LLM scaling curve
        if "llm_scaling" in results and results["llm_scaling"].get("accuracy"):
            llm_data = {"planning": results["llm_scaling"]["accuracy"]}
            plot_scaling_curve(
                {"planning": [(d[0], d[1]) for d in llm_data["planning"]]},
                x_label="Model Size",
                y_label="Plan Success Rate (%)",
                title="LLM Size Scaling",
                output_path=figures_dir / "fig6_analysis_scaling_llm_size.pdf",
            )
            print("  Figure saved: fig6_analysis_scaling_llm_size.pdf")

    except Exception as e:
        print(f"  Warning: Could not generate figures: {e}")


def main():
    parser = argparse.ArgumentParser(description="Section 5.3.2: Scaling Analysis")
    parser.add_argument("--tool-scaling", action="store_true", help="Run tool scaling only")
    parser.add_argument("--llm-scaling", action="store_true", help="Run LLM scaling only")
    parser.add_argument("--max-samples", type=int, default=50, help="Maximum samples per test")
    parser.add_argument("--verbose", action="store_true", default=True, help="Verbose output")
    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æŒ‡å®šå…·ä½“ç±»å‹ï¼Œè¿è¡Œæ‰€æœ‰
    run_tool = args.tool_scaling or not (args.tool_scaling or args.llm_scaling)
    run_llm = args.llm_scaling or not (args.tool_scaling or args.llm_scaling)

    run_scaling_analysis(
        tool_scaling=run_tool,
        llm_scaling=run_llm,
        max_samples=args.max_samples,
        verbose=args.verbose,
    )

    print("\n" + "=" * 70)
    print("ğŸ“Š Scaling Analysis Complete")
    print("=" * 70)


if __name__ == "__main__":
    main()
