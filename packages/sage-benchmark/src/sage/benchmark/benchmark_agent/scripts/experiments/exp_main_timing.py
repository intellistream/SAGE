#!/usr/bin/env python3
"""
Section 5.2.1: Timing Detection Experiment (RQ1)

ç ”ç©¶é—®é¢˜: ç°æœ‰æ–¹æ³•åœ¨åˆ¤æ–­"æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·"ä¸Šçš„è¡¨ç°å¦‚ä½•?

æµ‹è¯•æ–¹æ³•:
- timing.rule_based   : å…³é”®è¯ + æ­£åˆ™è§„åˆ™
- timing.embedding    : è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­
- timing.llm_based    : ç›´æ¥ LLM æ¨ç†
- timing.hybrid       : Rule åˆç­› + LLM ç²¾åˆ¤

ç›®æ ‡æŒ‡æ ‡:
- Primary: Accuracy â‰¥ 95%
- Secondary: Precision, Recall, F1
- Tertiary: Latency (ms)

Usage:
    python exp_main_timing.py
    python exp_main_timing.py --max-samples 100
    python exp_main_timing.py --skip-llm
"""

from __future__ import annotations

import argparse
import time

from .exp_utils import (
    ExperimentResult,
    ExperimentSummary,
    create_progress_bar,
    load_benchmark_data,
    print_metrics_detail,
    print_result_row,
    print_section_header,
    print_subsection_header,
    save_results,
    setup_experiment_env,
)
from .figure_generator import generate_detailed_table, plot_challenge_comparison


def run_timing_experiment(
    max_samples: int = 150,
    skip_llm: bool = False,
    verbose: bool = True,
) -> ExperimentSummary:
    """
    è¿è¡Œ Timing Detection å®éªŒã€‚

    Args:
        max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°
        skip_llm: æ˜¯å¦è·³è¿‡ LLM-based æ–¹æ³•
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        ExperimentSummary å¯¹è±¡
    """
    setup_experiment_env(verbose=verbose)

    print_section_header("Section 5.2.1: Timing Detection (RQ1)")
    print("   Target: Accuracy â‰¥ 95%")
    print(f"   Max samples: {max_samples}")

    # åŠ è½½æ•°æ®
    samples = load_benchmark_data("timing", split="test", max_samples=max_samples)
    if not samples:
        print("  âŒ No timing data available")
        return ExperimentSummary(section="5_2_main", challenge="timing")

    print(f"   Loaded {len(samples)} samples")

    # è·å–ç­–ç•¥æ³¨å†Œè¡¨
    try:
        from sage.benchmark.benchmark_agent import get_adapter_registry

        registry = get_adapter_registry()
    except ImportError as e:
        print(f"  âŒ Failed to import adapter registry: {e}")
        return ExperimentSummary(section="5_2_main", challenge="timing")

    # å®šä¹‰æµ‹è¯•ç­–ç•¥
    strategies = [
        ("timing.rule_based", "Rule-based"),
        ("timing.embedding", "Embedding"),
        ("timing.llm_based", "LLM-based"),
        ("timing.hybrid", "Hybrid"),
    ]

    # è·³è¿‡ LLM ç­–ç•¥
    LLM_STRATEGIES = {"timing.llm_based", "timing.hybrid"}
    if skip_llm:
        strategies = [(name, display) for name, display in strategies if name not in LLM_STRATEGIES]
        print("  âš ï¸  Skipping LLM-based strategies")

    results = []
    target = 0.95

    for strategy_name, display_name in strategies:
        print_subsection_header(f"Testing: {display_name}")

        try:
            detector = registry.get(strategy_name)
        except Exception as e:
            print(f"    âš ï¸  Failed to create detector: {e}")
            continue

        # è¿è¡Œè¯„æµ‹
        start_time = time.time()
        correct = 0
        true_positives = 0
        false_positives = 0
        false_negatives = 0
        true_negatives = 0

        with create_progress_bar(len(samples), desc=f"  {display_name}") as pbar:
            for sample in samples:
                try:
                    # åˆ›å»ºæ¶ˆæ¯å¯¹è±¡
                    from sage.benchmark.benchmark_agent.experiments.timing_detection_exp import (
                        TimingMessage,
                    )

                    message = TimingMessage(
                        sample_id=sample.get("sample_id", ""),
                        message=sample.get("message", ""),
                        context=sample.get("context", {}),
                    )

                    result = detector.decide(message)
                    predicted = result.should_call_tool
                    expected = sample.get("should_call_tool", False)

                    if predicted == expected:
                        correct += 1

                    # æ··æ·†çŸ©é˜µ
                    if predicted and expected:
                        true_positives += 1
                    elif predicted and not expected:
                        false_positives += 1
                    elif not predicted and expected:
                        false_negatives += 1
                    else:
                        true_negatives += 1

                except Exception as e:
                    if verbose:
                        print(f"    Error: {e}")

                pbar.update(1)

        elapsed = time.time() - start_time

        # è®¡ç®—æŒ‡æ ‡
        n = len(samples)
        accuracy = correct / n if n > 0 else 0
        precision = (
            true_positives / (true_positives + false_positives)
            if (true_positives + false_positives) > 0
            else 0
        )
        recall = (
            true_positives / (true_positives + false_negatives)
            if (true_positives + false_negatives) > 0
            else 0
        )
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        exp_result = ExperimentResult(
            challenge="timing",
            strategy=strategy_name,
            metrics={
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1": f1,
            },
            metadata={
                "total_samples": n,
                "correct": correct,
                "latency_ms": elapsed * 1000 / n if n > 0 else 0,
                "confusion_matrix": {
                    "tp": true_positives,
                    "fp": false_positives,
                    "fn": false_negatives,
                    "tn": true_negatives,
                },
            },
            passed=accuracy >= target,
            target=target,
        )
        results.append(exp_result)

        # æ‰“å°ç»“æœ
        print_result_row(display_name, exp_result.metrics, exp_result.passed, target)
        if verbose:
            print_metrics_detail(exp_result.metrics)

    # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
    best_result = max(results, key=lambda r: r.metrics["accuracy"]) if results else None

    summary = ExperimentSummary(
        section="5_2_main",
        challenge="timing",
        results=results,
        best_strategy=best_result.strategy if best_result else None,
        best_metric=best_result.metrics["accuracy"] if best_result else None,
        target_met=any(r.passed for r in results),
    )

    # ä¿å­˜ç»“æœ
    output_file = save_results(summary.to_dict(), "5_2_main", "timing")
    print(f"\n  Results saved to: {output_file}")

    # ç”Ÿæˆå›¾è¡¨
    if results:
        from figure_generator import get_figures_dir, get_tables_dir

        figures_dir = get_figures_dir()
        tables_dir = get_tables_dir()

        # ç”Ÿæˆå¯¹æ¯”å›¾
        plot_challenge_comparison(
            [{"strategy": r.strategy.split(".")[-1], "metrics": r.metrics} for r in results],
            challenge="timing",
            metrics=["accuracy", "precision", "recall", "f1"],
            target=target,
            output_path=figures_dir / "fig1_main_timing_comparison.pdf",
            title="Timing Detection: Strategy Comparison",
        )

        # ç”Ÿæˆè¡¨æ ¼
        generate_detailed_table(
            [{"strategy": r.strategy.split(".")[-1], "metrics": r.metrics} for r in results],
            challenge="timing",
            metrics=["accuracy", "precision", "recall", "f1"],
            output_path=tables_dir / "table_timing_detailed.tex",
        )

        print(f"  Figure saved to: {figures_dir / 'fig1_main_timing_comparison.pdf'}")

    return summary


def main():
    parser = argparse.ArgumentParser(description="Section 5.2.1: Timing Detection Experiment")
    parser.add_argument("--max-samples", type=int, default=150, help="Maximum samples to test")
    parser.add_argument("--skip-llm", action="store_true", help="Skip LLM-based methods")
    parser.add_argument("--verbose", action="store_true", default=True, help="Verbose output")
    args = parser.parse_args()

    summary = run_timing_experiment(
        max_samples=args.max_samples,
        skip_llm=args.skip_llm,
        verbose=args.verbose,
    )

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“Š Summary")
    print("=" * 70)
    print(f"  Best strategy: {summary.best_strategy}")
    print(f"  Best accuracy: {summary.best_metric * 100:.1f}%" if summary.best_metric else "  N/A")
    print(f"  Target met: {'âœ… YES' if summary.target_met else 'âŒ NO'}")


if __name__ == "__main__":
    main()
