#!/usr/bin/env python3
"""
Section 5.2.3: Tool Selection Experiment (RQ3)

ç ”ç©¶é—®é¢˜: ç°æœ‰æ–¹æ³•ä»å¤§è§„æ¨¡å·¥å…·åº“ä¸­é€‰æ‹©æ­£ç¡®å·¥å…·çš„èƒ½åŠ›å¦‚ä½•?

æµ‹è¯•æ–¹æ³•:
- selector.keyword    : BM25 å…³é”®è¯åŒ¹é…
- selector.embedding  : Dense Retrieval è¯­ä¹‰åŒ¹é…
- selector.hybrid     : 40% BM25 + 60% Dense èåˆ
- selector.gorilla    : Embedding æ£€ç´¢ + LLM é‡æ’åº
- selector.dfsdt      : LLM é€ä¸ªè¯„åˆ† (ToolLLM æ–¹æ³•)

ç›®æ ‡æŒ‡æ ‡:
- Primary: Top-K Accuracy â‰¥ 95% (K=5)
- Secondary: MRR, Recall@K, Precision@K
- Tertiary: Latency (ms)

Usage:
    python exp_main_selection.py
    python exp_main_selection.py --max-samples 100 --top-k 5
    python exp_main_selection.py --skip-llm
"""

from __future__ import annotations

import argparse
import time

from .exp_utils import (
    ExperimentResult,
    ExperimentSummary,
    create_progress_bar,
    load_benchmark_data,
    print_metrics_detail,
    print_result_row,
    print_section_header,
    print_subsection_header,
    save_results,
    setup_experiment_env,
)


def compute_mrr(predictions: list[list[str]], references: list[list[str]]) -> float:
    """è®¡ç®— Mean Reciprocal Rankã€‚"""
    rr_sum = 0.0
    for pred, ref in zip(predictions, references):
        ref_set = set(ref)
        for i, p in enumerate(pred):
            if p in ref_set:
                rr_sum += 1.0 / (i + 1)
                break
    return rr_sum / len(predictions) if predictions else 0.0


def compute_top_k_accuracy(
    predictions: list[list[str]], references: list[list[str]], k: int
) -> float:
    """è®¡ç®— Top-K Accuracyã€‚"""
    hits = 0
    for pred, ref in zip(predictions, references):
        pred_top_k = set(pred[:k])
        ref_set = set(ref)
        if pred_top_k & ref_set:
            hits += 1
    return hits / len(predictions) if predictions else 0.0


def compute_recall_at_k(predictions: list[list[str]], references: list[list[str]], k: int) -> float:
    """è®¡ç®— Recall@Kã€‚"""
    recalls = []
    for pred, ref in zip(predictions, references):
        pred_top_k = set(pred[:k])
        ref_set = set(ref)
        if ref_set:
            recalls.append(len(pred_top_k & ref_set) / len(ref_set))
        else:
            recalls.append(0.0)
    return sum(recalls) / len(recalls) if recalls else 0.0


def compute_precision_at_k(
    predictions: list[list[str]], references: list[list[str]], k: int
) -> float:
    """è®¡ç®— Precision@Kã€‚"""
    precisions = []
    for pred, ref in zip(predictions, references):
        pred_top_k = set(pred[:k])
        ref_set = set(ref)
        if pred_top_k:
            precisions.append(len(pred_top_k & ref_set) / len(pred_top_k))
        else:
            precisions.append(0.0)
    return sum(precisions) / len(precisions) if precisions else 0.0


def normalize_ground_truth(ground_truth: object) -> list[str]:
    """å°†åœ°é¢çœŸå®æ ‡ç­¾ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚"""

    if ground_truth is None:
        return []

    if isinstance(ground_truth, str):
        return [ground_truth]

    if isinstance(ground_truth, list):
        return [str(item) for item in ground_truth]

    if isinstance(ground_truth, dict):
        for key in ("top_k", "tool_ids", "tools", "ids"):
            value = ground_truth.get(key)
            if isinstance(value, list):
                return [str(item) for item in value]
            if isinstance(value, str):
                return [value]
        # fall back to any string-like values
        values = [str(value) for value in ground_truth.values() if value]
        if values:
            return values

    return [str(ground_truth)]


def run_selection_experiment(
    max_samples: int = 100,
    top_k: int = 5,
    skip_llm: bool = False,
    verbose: bool = True,
) -> ExperimentSummary:
    """
    è¿è¡Œ Tool Selection å®éªŒã€‚

    Args:
        max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°
        top_k: Top-K å‚æ•°
        skip_llm: æ˜¯å¦è·³è¿‡ LLM-based æ–¹æ³•
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        ExperimentSummary å¯¹è±¡
    """
    setup_experiment_env(verbose=verbose)

    print_section_header("Section 5.2.3: Tool Selection (RQ3)")
    print(f"   Target: Top-{top_k} Accuracy â‰¥ 95%")
    print(f"   Max samples: {max_samples}")

    # åŠ è½½æ•°æ®
    samples = load_benchmark_data("selection", split="test", max_samples=max_samples)
    if not samples:
        print("  âŒ No selection data available")
        return ExperimentSummary(section="5_2_main", challenge="selection")

    print(f"   Loaded {len(samples)} samples")

    # è·å–ç­–ç•¥æ³¨å†Œè¡¨
    try:
        from sage.benchmark.benchmark_agent import get_adapter_registry

        registry = get_adapter_registry()
    except ImportError as e:
        print(f"  âŒ Failed to import adapter registry: {e}")
        return ExperimentSummary(section="5_2_main", challenge="selection")

    # å®šä¹‰æµ‹è¯•ç­–ç•¥
    strategies = [
        ("selector.keyword", "Keyword (BM25)"),
        ("selector.embedding", "Embedding"),
        ("selector.hybrid", "Hybrid"),
        ("selector.gorilla", "Gorilla"),
        ("selector.dfsdt", "DFSDT"),
    ]

    # è·³è¿‡ LLM ç­–ç•¥
    LLM_STRATEGIES = {"selector.gorilla", "selector.dfsdt"}
    if skip_llm:
        strategies = [(name, display) for name, display in strategies if name not in LLM_STRATEGIES]
        print("  âš ï¸  Skipping LLM-based strategies")

    results = []
    target = 0.95

    for strategy_name, display_name in strategies:
        print_subsection_header(f"Testing: {display_name}")

        try:
            selector = registry.get(strategy_name)
        except Exception as e:
            print(f"    âš ï¸  Failed to create selector: {e}")
            continue

        # è¿è¡Œè¯„æµ‹
        start_time = time.time()
        all_predictions = []
        all_references = []

        with create_progress_bar(len(samples), desc=f"  {display_name}") as pbar:
            for sample in samples:
                try:
                    query = sample.get("instruction", sample.get("query", ""))
                    candidate_tools = sample.get("candidate_tools", [])
                    ground_truth = sample.get("ground_truth", sample.get("expected_tools", []))

                    # è°ƒç”¨é€‰æ‹©å™¨
                    predictions = selector.select(
                        query, candidate_tools=candidate_tools, top_k=top_k
                    )

                    # æå–å·¥å…· ID
                    if predictions and hasattr(predictions[0], "tool_id"):
                        pred_ids = [p.tool_id for p in predictions]
                    elif predictions and isinstance(predictions[0], dict):
                        pred_ids = [p.get("tool_id", p.get("id", str(p))) for p in predictions]
                    else:
                        pred_ids = [str(p) for p in predictions] if predictions else []

                    # æ ‡å‡†åŒ– ground truth
                    ref_ids = normalize_ground_truth(ground_truth)

                    all_predictions.append(pred_ids)
                    all_references.append(ref_ids)

                except Exception as e:
                    if verbose:
                        print(f"    Error: {e}")
                    all_predictions.append([])
                    all_references.append(sample.get("ground_truth", []))

                pbar.update(1)

        elapsed = time.time() - start_time

        # è®¡ç®—æŒ‡æ ‡
        n = len(samples)
        top_k_acc = compute_top_k_accuracy(all_predictions, all_references, top_k)
        mrr = compute_mrr(all_predictions, all_references)
        recall_k = compute_recall_at_k(all_predictions, all_references, top_k)
        precision_k = compute_precision_at_k(all_predictions, all_references, top_k)

        exp_result = ExperimentResult(
            challenge="selection",
            strategy=strategy_name,
            metrics={
                "top_k_accuracy": top_k_acc,
                "mrr": mrr,
                f"recall@{top_k}": recall_k,
                f"precision@{top_k}": precision_k,
            },
            metadata={
                "total_samples": n,
                "top_k": top_k,
                "latency_ms": elapsed * 1000 / n if n > 0 else 0,
            },
            passed=top_k_acc >= target,
            target=target,
        )
        results.append(exp_result)

        # æ‰“å°ç»“æœ
        print_result_row(display_name, {"top_k_accuracy": top_k_acc}, exp_result.passed, target)
        if verbose:
            print_metrics_detail(exp_result.metrics)

    # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
    best_result = max(results, key=lambda r: r.metrics["top_k_accuracy"]) if results else None

    summary = ExperimentSummary(
        section="5_2_main",
        challenge="selection",
        results=results,
        best_strategy=best_result.strategy if best_result else None,
        best_metric=best_result.metrics["top_k_accuracy"] if best_result else None,
        target_met=any(r.passed for r in results),
    )

    # ä¿å­˜ç»“æœ
    output_file = save_results(summary.to_dict(), "5_2_main", "selection")
    print(f"\n  Results saved to: {output_file}")

    # ç”Ÿæˆå›¾è¡¨
    if results:
        from figure_generator import (
            generate_detailed_table,
            get_figures_dir,
            get_tables_dir,
            plot_challenge_comparison,
        )

        figures_dir = get_figures_dir()
        tables_dir = get_tables_dir()

        # ç”Ÿæˆå¯¹æ¯”å›¾
        plot_challenge_comparison(
            [{"strategy": r.strategy.split(".")[-1], "metrics": r.metrics} for r in results],
            challenge="selection",
            metrics=["top_k_accuracy", "mrr"],
            target=target,
            output_path=figures_dir / "fig3_main_selection_comparison.pdf",
            title=f"Tool Selection: Strategy Comparison (Top-{top_k})",
        )

        # ç”Ÿæˆè¡¨æ ¼
        generate_detailed_table(
            [{"strategy": r.strategy.split(".")[-1], "metrics": r.metrics} for r in results],
            challenge="selection",
            metrics=["top_k_accuracy", "mrr", f"recall@{top_k}", f"precision@{top_k}"],
            output_path=tables_dir / "table_selection_detailed.tex",
        )

        print(f"  Figure saved to: {figures_dir / 'fig3_main_selection_comparison.pdf'}")

    return summary


def main():
    parser = argparse.ArgumentParser(description="Section 5.2.3: Tool Selection Experiment")
    parser.add_argument("--max-samples", type=int, default=100, help="Maximum samples to test")
    parser.add_argument("--top-k", type=int, default=5, help="Top-K parameter")
    parser.add_argument("--skip-llm", action="store_true", help="Skip LLM-based methods")
    parser.add_argument("--verbose", action="store_true", default=True, help="Verbose output")
    args = parser.parse_args()

    summary = run_selection_experiment(
        max_samples=args.max_samples,
        top_k=args.top_k,
        skip_llm=args.skip_llm,
        verbose=args.verbose,
    )

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“Š Summary")
    print("=" * 70)
    print(f"  Best strategy: {summary.best_strategy}")
    print(
        f"  Best Top-K accuracy: {summary.best_metric * 100:.1f}%"
        if summary.best_metric
        else "  N/A"
    )
    print(f"  Target met: {'âœ… YES' if summary.target_met else 'âŒ NO'}")


if __name__ == "__main__":
    main()
