#!/usr/bin/env python3
"""
SAGE Agent Bench CLI - Agent èƒ½åŠ›è¯„æµ‹å‘½ä»¤è¡Œå…¥å£

è¿™æ˜¯ SAGE Agent Benchmark çš„å®˜æ–¹å…¥å£ï¼Œç”¨äºè¯„æµ‹ Agent å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚

Usage:
    sage-agent-bench <command> [options]

Commands:
    run         è¿è¡Œå®Œæ•´ Benchmark å®éªŒ
    eval        å·¥å…·é€‰æ‹©è¯„æµ‹ (è·¨æ•°æ®é›†)
    train       è®­ç»ƒæ–¹æ³•å¯¹æ¯”
    llm         LLM æœåŠ¡ç®¡ç†
    list        åˆ—å‡ºå¯ç”¨èµ„æº

Examples:
    # è¿è¡Œå®Œæ•´ benchmark
    sage-agent-bench run --quick
    sage-agent-bench run --section 5.2
    sage-agent-bench run --exp timing

    # å·¥å…·é€‰æ‹©è¯„æµ‹
    sage-agent-bench eval --dataset sage --samples 100
    sage-agent-bench eval --dataset acebench

    # è®­ç»ƒæ–¹æ³•å¯¹æ¯”
    sage-agent-bench train --methods A_baseline,D_combined
    sage-agent-bench train --dry-run

    # LLM æœåŠ¡ç®¡ç†
    sage-agent-bench llm status
    sage-agent-bench llm start
    sage-agent-bench llm stop

    # åˆ—å‡ºèµ„æº
    sage-agent-bench list datasets
    sage-agent-bench list methods
    sage-agent-bench list experiments
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path

# è·å–è„šæœ¬ç›®å½•
SCRIPT_DIR = Path(__file__).resolve().parent

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(SCRIPT_DIR.parent.parent.parent.parent))
sys.path.insert(0, str(SCRIPT_DIR / "experiments"))


# =============================================================================
# å­å‘½ä»¤å¤„ç†
# =============================================================================


def cmd_run(args):
    """è¿è¡Œ Benchmark å®éªŒ"""
    from experiments.run_paper1_experiments import main as run_main

    # æ„å»ºç­‰æ•ˆçš„ argparse args
    sys.argv = ["run_paper1_experiments.py"]

    if args.section:
        sys.argv.extend(["--section", args.section])
    if args.exp:
        sys.argv.extend(["--exp", args.exp])
    if args.quick:
        sys.argv.append("--quick")
    if args.skip_llm:
        sys.argv.append("--skip-llm")
    if args.generate_paper:
        sys.argv.append("--generate-paper")
    if args.output:
        sys.argv.extend(["--output-dir", args.output])

    run_main()
    return 0


def cmd_eval(args):
    """å·¥å…·é€‰æ‹©è¯„æµ‹"""
    from experiments.exp_cross_dataset import run_cross_dataset_evaluation
    from experiments.exp_main_selection import run_selection_experiment
    from experiments.exp_utils import setup_experiment_env

    setup_experiment_env()

    if args.dataset == "all":
        # è·¨æ•°æ®é›†è¯„æµ‹
        result = run_cross_dataset_evaluation(
            datasets=["sage", "acebench"],
            max_samples=args.samples,
            verbose=True,
        )
    else:
        # å•æ•°æ®é›†è¯„æµ‹
        result = run_selection_experiment(
            max_samples=args.samples,
            top_k=args.top_k,
            skip_llm=False,
            verbose=True,
        )

    return 0 if result else 1


def cmd_train(args):
    """è®­ç»ƒæ–¹æ³•å¯¹æ¯”"""
    from experiments.exp_training_comparison import run_training_comparison

    methods = args.methods.split(",") if args.methods else ["A_baseline", "D_combined"]

    run_training_comparison(
        methods=methods,
        base_model=args.model,
        quick=args.quick,
        dry_run=args.dry_run,
        verbose=True,
    )
    return 0


def cmd_llm(args):
    """LLM æœåŠ¡ç®¡ç†"""
    from experiments.llm_service import (
        print_llm_status,
        start_llm_service,
        stop_llm_service,
    )

    if args.llm_action == "status":
        print_llm_status()
        return 0

    elif args.llm_action == "start":
        success = start_llm_service(
            model=args.model,
            port=args.port,
            gpu_memory=args.gpu_memory,
        )
        return 0 if success else 1

    elif args.llm_action == "stop":
        success = stop_llm_service()
        return 0 if success else 1

    return 0


def cmd_list(args):
    """åˆ—å‡ºå¯ç”¨èµ„æº"""
    if args.resource == "datasets":
        print("\n" + "=" * 70)
        print("Available Datasets for Tool Selection Evaluation")
        print("=" * 70)
        print()
        print(f"{'Dataset':<15} {'Description':<50} {'Status'}")
        print("-" * 70)

        datasets = [
            ("sage", "SAGE Agent Bench (1200 synthetic tools)", "Built-in"),
            ("acebench", "ToolACE from HuggingFace", "HuggingFace"),
            ("apibank", "API-Bank (Microsoft/Alibaba)", "External"),
            ("toolalpaca", "ToolAlpaca (Microsoft)", "External"),
            ("bfcl", "BFCL (Berkeley Function Calling)", "External"),
            ("toolbench", "ToolBench (Tsinghua/OpenBMB)", "External"),
            ("all", "Evaluate on ALL datasets", "-"),
        ]
        for name, desc, status in datasets:
            print(f"{name:<15} {desc:<50} {status}")
        print()
        return 0

    elif args.resource == "methods":
        print("\n" + "=" * 70)
        print("Available Methods")
        print("=" * 70)

        print("\nğŸ“‹ Tool Selection Methods:")
        print("-" * 50)
        methods = [
            ("keyword", "BM25 keyword matching", "Classic"),
            ("embedding", "Semantic embedding similarity", "Common"),
            ("hybrid", "Keyword + Embedding fusion", "Common"),
            ("gorilla", "Retrieval + LLM reranking", "Berkeley"),
            ("dfsdt", "Tree search (ToolLLM)", "Tsinghua"),
        ]
        for name, desc, source in methods:
            print(f"  {name:<15} {desc:<35} {source}")

        print("\nğŸ“‹ Training Methods:")
        print("-" * 50)
        print("  Paper 1 (Benchmark):")
        paper1_methods = [
            ("A_baseline", "Standard SFT training"),
        ]
        for name, desc in paper1_methods:
            print(f"    {name:<20} {desc}")

        print("\n  Paper 2 (SIAS) - from sage.libs.sias:")
        sias_methods = [
            ("B1_coreset_loss", "[SIAS] Select high-loss samples"),
            ("B2_coreset_diversity", "[SIAS] Select diverse samples"),
            ("B3_coreset_hybrid", "[SIAS] 60% loss + 40% diversity"),
            ("C_continual", "[SIAS] Online learning with replay"),
            ("D_combined", "[SIAS] Coreset + Continual Learning"),
        ]
        for name, desc in sias_methods:
            print(f"    {name:<20} {desc}")
        print()
        return 0

    elif args.resource == "experiments":
        print("\n" + "=" * 70)
        print("Available Experiments (Paper 1: Benchmark)")
        print("=" * 70)

        print("\nğŸ“Š Section 5.2: Main Results")
        print("-" * 50)
        experiments = [
            ("timing", "RQ1: Timing Detection", "~10 min"),
            ("planning", "RQ2: Task Planning", "~15 min"),
            ("selection", "RQ3: Tool Selection", "~20 min"),
        ]
        for exp_id, name, time_est in experiments:
            print(f"  {exp_id:<20} {name:<30} {time_est}")

        print("\nğŸ”¬ Section 5.3: Analysis")
        print("-" * 50)
        experiments = [
            ("error", "Error Type Breakdown", "~5 min"),
            ("scaling", "Scaling Analysis", "~15 min"),
            ("robustness", "Robustness Analysis", "~10 min"),
            ("ablation", "Ablation Studies", "~10 min"),
        ]
        for exp_id, name, time_est in experiments:
            print(f"  {exp_id:<20} {name:<30} {time_est}")

        print("\nğŸŒ Section 5.4: Generalization")
        print("-" * 50)
        print(f"  {'cross-dataset':<20} {'Cross-Dataset Comparison':<30} ~30 min")

        print("\nğŸ“ Section 5.5: Training Comparison")
        print("-" * 50)
        print(f"  {'training':<20} {'Training Method Comparison':<30} ~2 hours")
        print()
        return 0

    return 1


# =============================================================================
# ä¸»å…¥å£
# =============================================================================


def print_banner():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     SAGE Agent Bench CLI v3.0                             â•‘
â•‘                                                                           â•‘
â•‘  Unified benchmark for evaluating Agent tool-calling capabilities         â•‘
â•‘  Paper 1: Agent Capability Evaluation Framework                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


def main():
    parser = argparse.ArgumentParser(
        prog="sage-agent-bench",
        description="SAGE Agent Bench CLI - Agent èƒ½åŠ›è¯„æµ‹å‘½ä»¤è¡Œå…¥å£",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    sage-agent-bench run --quick            # å¿«é€Ÿè¿è¡Œå®Œæ•´ Benchmark
    sage-agent-bench eval --dataset all     # è·¨æ•°æ®é›†è¯„æµ‹
    sage-agent-bench train --dry-run        # æ¨¡æ‹Ÿè®­ç»ƒå¯¹æ¯”
    sage-agent-bench llm start              # å¯åŠ¨ LLM æœåŠ¡
    sage-agent-bench list experiments       # åˆ—å‡ºå¯ç”¨å®éªŒ
        """,
    )

    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")

    # =========================================================================
    # run å­å‘½ä»¤
    # =========================================================================
    run_parser = subparsers.add_parser("run", help="è¿è¡Œ Benchmark å®éªŒ")
    run_parser.add_argument(
        "--section",
        choices=["5.2", "5.3", "5.4", "5.5", "all"],
        default="all",
        help="è¿è¡ŒæŒ‡å®šç« èŠ‚",
    )
    run_parser.add_argument(
        "--exp",
        choices=[
            "timing",
            "planning",
            "selection",
            "error",
            "scaling",
            "robustness",
            "ablation",
            "cross-dataset",
            "training",
            "all",
        ],
        help="è¿è¡ŒæŒ‡å®šå®éªŒ",
    )
    run_parser.add_argument("--quick", "-q", action="store_true", help="å¿«é€Ÿæ¨¡å¼")
    run_parser.add_argument("--skip-llm", action="store_true", help="è·³è¿‡ LLM æ–¹æ³•")
    run_parser.add_argument("--generate-paper", action="store_true", help="ç”Ÿæˆè®ºæ–‡ææ–™")
    run_parser.add_argument("--output", "-o", type=str, help="è¾“å‡ºç›®å½•")
    run_parser.set_defaults(func=cmd_run)

    # =========================================================================
    # eval å­å‘½ä»¤
    # =========================================================================
    eval_parser = subparsers.add_parser("eval", help="å·¥å…·é€‰æ‹©è¯„æµ‹")
    eval_parser.add_argument(
        "--dataset",
        "-d",
        default="sage",
        choices=["sage", "acebench", "apibank", "toolalpaca", "bfcl", "all"],
        help="è¯„æµ‹æ•°æ®é›†",
    )
    eval_parser.add_argument("--samples", "-n", type=int, default=100, help="æœ€å¤§æ ·æœ¬æ•°")
    eval_parser.add_argument("--top-k", "-k", type=int, default=5, help="Top-K è¯„æµ‹")
    eval_parser.set_defaults(func=cmd_eval)

    # =========================================================================
    # train å­å‘½ä»¤
    # =========================================================================
    train_parser = subparsers.add_parser("train", help="è®­ç»ƒæ–¹æ³•å¯¹æ¯”")
    train_parser.add_argument(
        "--methods",
        "-m",
        default="A_baseline,D_combined",
        help="è®­ç»ƒæ–¹æ³•ï¼Œé€—å·åˆ†éš”",
    )
    train_parser.add_argument("--model", default="Qwen/Qwen2.5-1.5B-Instruct", help="åŸºç¡€æ¨¡å‹")
    train_parser.add_argument("--quick", "-q", action="store_true", help="å¿«é€Ÿæ¨¡å¼")
    train_parser.add_argument("--dry-run", action="store_true", help="æ¨¡æ‹Ÿè¿è¡Œ")
    train_parser.set_defaults(func=cmd_train)

    # =========================================================================
    # llm å­å‘½ä»¤
    # =========================================================================
    llm_parser = subparsers.add_parser("llm", help="LLM æœåŠ¡ç®¡ç†")
    llm_parser.add_argument(
        "llm_action",
        choices=["start", "stop", "status"],
        help="æ“ä½œ: start/stop/status",
    )
    llm_parser.add_argument("--model", default="Qwen/Qwen2.5-0.5B-Instruct", help="LLM æ¨¡å‹")
    llm_parser.add_argument("--port", type=int, default=8901, help="ç«¯å£")
    llm_parser.add_argument("--gpu-memory", type=float, default=0.5, help="GPU æ˜¾å­˜æ¯”ä¾‹")
    llm_parser.set_defaults(func=cmd_llm)

    # =========================================================================
    # list å­å‘½ä»¤
    # =========================================================================
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºå¯ç”¨èµ„æº")
    list_parser.add_argument(
        "resource",
        choices=["datasets", "methods", "experiments"],
        help="èµ„æºç±»å‹",
    )
    list_parser.set_defaults(func=cmd_list)

    # =========================================================================
    # è§£æå¹¶æ‰§è¡Œ
    # =========================================================================
    args = parser.parse_args()

    if args.command is None:
        print_banner()
        parser.print_help()
        return 0

    return args.func(args)


if __name__ == "__main__":
    sys.exit(main())
