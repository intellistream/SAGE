#!/usr/bin/env python3
"""
Section 5.3.1: Error Analysis

æ·±å…¥åˆ†æå„æ–¹æ³•çš„å¤±è´¥æ¨¡å¼ï¼Œæ‰¾å‡ºæ”¹è¿›æ–¹å‘ã€‚

åˆ†æå†…å®¹:
1. Error Type Breakdown - æŒ‰ Challenge åˆ†è§£é”™è¯¯ç±»å‹
2. Failure Cascading Analysis - æ—©æœŸé”™è¯¯å¯¼è‡´çš„çº§è”å¤±è´¥

è¾“å‡º:
- figures/fig4_analysis_error_breakdown.pdf
- tables/table_error_breakdown.tex

Usage:
    python exp_analysis_error.py
    python exp_analysis_error.py --challenge timing
    python exp_analysis_error.py --challenge all
"""

from __future__ import annotations

import argparse
from collections import Counter, defaultdict
from typing import Any

from .exp_utils import (
    get_figures_dir,
    print_section_header,
    print_subsection_header,
    save_results,
    setup_experiment_env,
)

# =============================================================================
# Error Analysis Functions
# =============================================================================


def analyze_timing_errors(results: list[dict]) -> dict[str, Any]:
    """
    åˆ†æ Timing é”™è¯¯ç±»å‹ã€‚

    é”™è¯¯ç±»å‹:
    - false_positive: ä¸è¯¥è°ƒç”¨å´è°ƒç”¨ (è°ƒç”¨é¢‘ç‡è¿‡é«˜)
    - false_negative: è¯¥è°ƒç”¨å´æ²¡è°ƒç”¨ (é”™è¿‡å…³é”®æ—¶æœº)
    - confidence_miscalibration: é«˜ç½®ä¿¡åº¦ä½†é”™è¯¯
    """
    error_counts = defaultdict(lambda: defaultdict(int))
    confidence_errors = defaultdict(list)

    for r in results:
        strategy = r.get("strategy", "unknown")
        predictions = r.get("predictions", [])
        references = r.get("references", [])
        confidences = r.get("confidences", [])

        for i, (pred, ref) in enumerate(zip(predictions, references)):
            if pred != ref:
                if pred and not ref:
                    error_counts[strategy]["false_positive"] += 1
                elif not pred and ref:
                    error_counts[strategy]["false_negative"] += 1

                # ç½®ä¿¡åº¦æ ¡å‡†åˆ†æ
                if confidences and i < len(confidences):
                    conf = confidences[i]
                    if conf > 0.8:  # é«˜ç½®ä¿¡ä½†é”™è¯¯
                        error_counts[strategy]["high_conf_error"] += 1
                        confidence_errors[strategy].append(conf)

    return {
        "error_counts": {k: dict(v) for k, v in error_counts.items()},
        "confidence_errors": {
            k: {"count": len(v), "avg_conf": sum(v) / len(v) if v else 0}
            for k, v in confidence_errors.items()
        },
    }


def analyze_planning_errors(results: list[dict]) -> dict[str, Any]:
    """
    åˆ†æ Planning é”™è¯¯ç±»å‹ã€‚

    é”™è¯¯ç±»å‹:
    - step_missing: ç¼ºå¤±å…³é”®æ­¥éª¤
    - wrong_order: æ­¥éª¤é¡ºåºé”™è¯¯
    - invalid_step: æ­¥éª¤ä¸åˆç†/å¹»è§‰
    - extra_steps: å¤šä½™æ­¥éª¤
    """
    error_counts = defaultdict(lambda: defaultdict(int))
    first_error_indices = defaultdict(list)

    for r in results:
        strategy = r.get("strategy", "unknown")
        predictions = r.get("predictions", [])
        references = r.get("references", [])

        for pred_plan, ref_plan in zip(predictions, references):
            pred_steps = pred_plan if isinstance(pred_plan, list) else []
            ref_steps = ref_plan if isinstance(ref_plan, list) else []

            if not ref_steps:
                continue

            # åˆ†ç±»é”™è¯¯
            pred_set = set(pred_steps)
            ref_set = set(ref_steps)

            # ç¼ºå¤±æ­¥éª¤
            missing = ref_set - pred_set
            if missing:
                error_counts[strategy]["step_missing"] += len(missing)

            # å¤šä½™æ­¥éª¤ (å¯èƒ½æ˜¯å¹»è§‰)
            extra = pred_set - ref_set
            if extra:
                error_counts[strategy]["extra_steps"] += len(extra)

            # é¡ºåºé”™è¯¯ (å·¥å…·é›†åˆç›¸åŒä½†é¡ºåºä¸åŒ)
            if pred_set == ref_set and pred_steps != ref_steps:
                error_counts[strategy]["wrong_order"] += 1

            # é¦–æ¬¡é”™è¯¯ä½ç½®
            for i, (p, r) in enumerate(zip(pred_steps, ref_steps)):
                if p != r:
                    first_error_indices[strategy].append(i)
                    break

    # è®¡ç®—é¦–æ¬¡é”™è¯¯åˆ†å¸ƒ
    first_error_dist = {}
    for strategy, indices in first_error_indices.items():
        if indices:
            dist = Counter(indices)
            first_error_dist[strategy] = {
                "distribution": dict(dist),
                "mean_index": sum(indices) / len(indices),
                "total_errors": len(indices),
            }

    return {
        "error_counts": {k: dict(v) for k, v in error_counts.items()},
        "first_error_distribution": first_error_dist,
    }


def analyze_selection_errors(results: list[dict], k: int = 5) -> dict[str, Any]:
    """
    åˆ†æ Selection é”™è¯¯ç±»å‹ã€‚

    é”™è¯¯ç±»å‹:
    - top1_miss: ç¬¬ä¸€ä¸ªé€‰æ‹©å°±é”™
    - topk_miss: Top-K å†…å…¨é”™
    - rank_volatility: æ­£ç¡®ç­”æ¡ˆæ’åä¸ç¨³å®š
    - category_confusion: è·¨ç±»åˆ«æ··æ·†
    """
    error_counts = defaultdict(lambda: defaultdict(int))
    rank_positions = defaultdict(list)

    for r in results:
        strategy = r.get("strategy", "unknown")
        predictions = r.get("predictions", [])
        references = r.get("references", [])

        for pred_list, ref_list in zip(predictions, references):
            ref_set = set(ref_list) if isinstance(ref_list, list) else {ref_list}
            pred_top_k = pred_list[:k] if isinstance(pred_list, list) else [pred_list]

            # Top-1 é”™è¯¯
            if pred_top_k and pred_top_k[0] not in ref_set:
                error_counts[strategy]["top1_miss"] += 1

            # Top-K å…¨é”™
            if not (set(pred_top_k) & ref_set):
                error_counts[strategy]["topk_miss"] += 1

            # è®°å½•æ­£ç¡®ç­”æ¡ˆçš„æ’åä½ç½®
            for i, p in enumerate(pred_list if isinstance(pred_list, list) else [pred_list]):
                if p in ref_set:
                    rank_positions[strategy].append(i + 1)
                    break

            # ç±»åˆ«æ··æ·†åˆ†æ
            if pred_top_k:
                pred_categories = {_extract_category(p) for p in pred_top_k}
                ref_categories = {_extract_category(r) for r in ref_set}
                if pred_categories and ref_categories and not (pred_categories & ref_categories):
                    error_counts[strategy]["category_confusion"] += 1

    # è®¡ç®—æ’åç¨³å®šæ€§
    rank_analysis = {}
    for strategy, positions in rank_positions.items():
        if positions:
            rank_analysis[strategy] = {
                "mean_rank": sum(positions) / len(positions),
                "std_rank": (
                    sum((p - sum(positions) / len(positions)) ** 2 for p in positions)
                    / len(positions)
                )
                ** 0.5,
                "rank_1_count": sum(1 for p in positions if p == 1),
                "total": len(positions),
            }

    return {
        "error_counts": {k: dict(v) for k, v in error_counts.items()},
        "rank_analysis": rank_analysis,
    }


def _extract_category(tool_id: str) -> str:
    """ä»å·¥å…· ID æå–ç±»åˆ«ã€‚"""
    parts = tool_id.split("_")
    return parts[0] if parts else "unknown"


# =============================================================================
# Cascading Failure Analysis
# =============================================================================


def analyze_cascading_failures(results: list[dict]) -> dict[str, Any]:
    """
    åˆ†æçº§è”å¤±è´¥æ¨¡å¼ã€‚

    æ£€æŸ¥æ—©æœŸé”™è¯¯æ˜¯å¦å¯¼è‡´åç»­æ­¥éª¤å…¨éƒ¨å¤±è´¥ã€‚
    """
    cascade_stats = defaultdict(lambda: {"cascading": 0, "non_cascading": 0, "recovery": 0})

    for r in results:
        strategy = r.get("strategy", "unknown")
        predictions = r.get("predictions", [])
        references = r.get("references", [])

        for pred_plan, ref_plan in zip(predictions, references):
            if not isinstance(pred_plan, list) or not isinstance(ref_plan, list):
                continue

            if len(pred_plan) < 2 or len(ref_plan) < 2:
                continue

            # æ‰¾åˆ°é¦–æ¬¡é”™è¯¯ä½ç½®
            first_error_idx = None
            for i, (p, r) in enumerate(zip(pred_plan, ref_plan)):
                if p != r:
                    first_error_idx = i
                    break

            if first_error_idx is None:
                continue  # å…¨å¯¹

            # æ£€æŸ¥é¦–æ¬¡é”™è¯¯åçš„æƒ…å†µ
            remaining_pred = pred_plan[first_error_idx + 1 :]
            remaining_ref = ref_plan[first_error_idx + 1 :]

            if not remaining_ref:
                continue

            # è®¡ç®—å‰©ä½™æ­¥éª¤çš„æ­£ç¡®ç‡
            remaining_correct = sum(1 for p, r in zip(remaining_pred, remaining_ref) if p == r)
            remaining_acc = remaining_correct / len(remaining_ref) if remaining_ref else 0

            if remaining_acc < 0.2:  # çº§è”å¤±è´¥ï¼šåç»­å‡ ä¹å…¨é”™
                cascade_stats[strategy]["cascading"] += 1
            elif remaining_acc > 0.5:  # æ¢å¤ï¼šåç»­å¤§éƒ¨åˆ†æ­£ç¡®
                cascade_stats[strategy]["recovery"] += 1
            else:  # éƒ¨åˆ†å½±å“
                cascade_stats[strategy]["non_cascading"] += 1

    return {
        "cascade_statistics": {k: dict(v) for k, v in cascade_stats.items()},
        "insight": "High cascading rate indicates fragile design without rollback/recovery mechanism.",
    }


# =============================================================================
# Main Experiment
# =============================================================================


def run_error_analysis(challenge: str = "all", verbose: bool = True) -> dict[str, Any]:
    """
    è¿è¡Œé”™è¯¯åˆ†æå®éªŒã€‚

    Args:
        challenge: è¦åˆ†æçš„æŒ‘æˆ˜ ("timing", "planning", "selection", "all")
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        é”™è¯¯åˆ†æç»“æœå­—å…¸
    """
    setup_experiment_env(verbose=verbose)

    print_section_header("Section 5.3.1: Error Analysis")

    all_results = {}

    # åŠ è½½ä¹‹å‰å®éªŒçš„ç»“æœ
    # TODO: å®é™…åº”è¯¥ä»ä¿å­˜çš„ç»“æœæ–‡ä»¶åŠ è½½
    # è¿™é‡Œç”¨ç¤ºä¾‹æ•°æ®ç»“æ„

    challenges_to_analyze = []
    if challenge == "all":
        challenges_to_analyze = ["timing", "planning", "selection"]
    else:
        challenges_to_analyze = [challenge]

    for ch in challenges_to_analyze:
        print_subsection_header(f"Analyzing: {ch.title()}")

        # æ¨¡æ‹ŸåŠ è½½ç»“æœ (å®é™…åº”ä»æ–‡ä»¶åŠ è½½)
        results = _load_experiment_results(ch)

        if ch == "timing":
            analysis = analyze_timing_errors(results)
        elif ch == "planning":
            analysis = analyze_planning_errors(results)
            # æ·»åŠ çº§è”åˆ†æ
            analysis["cascading"] = analyze_cascading_failures(results)
        elif ch == "selection":
            analysis = analyze_selection_errors(results)

        all_results[ch] = analysis

        # æ‰“å°æ‘˜è¦
        if "error_counts" in analysis:
            print("    Error counts by strategy:")
            for strategy, counts in analysis["error_counts"].items():
                print(f"      {strategy}: {dict(counts)}")

    # ä¿å­˜ç»“æœ
    output_file = save_results(all_results, "5_3_analysis", "error_analysis")
    print(f"\n  Results saved to: {output_file}")

    # ç”Ÿæˆå›¾è¡¨
    _generate_error_figures(all_results)

    return all_results


def _load_experiment_results(challenge: str) -> list[dict]:
    """
    åŠ è½½å®éªŒç»“æœã€‚

    TODO: å®é™…å®ç°åº”ä» section_5_2_main/{challenge}_results.json åŠ è½½
    """
    # ç¤ºä¾‹æ•°æ®ç»“æ„
    return [
        {
            "strategy": f"{challenge}_strategy_1",
            "predictions": [],
            "references": [],
            "confidences": [],
        }
    ]


def _generate_error_figures(results: dict) -> None:
    """ç”Ÿæˆé”™è¯¯åˆ†æå›¾è¡¨ã€‚"""
    try:
        from figure_generator import plot_error_breakdown

        figures_dir = get_figures_dir()

        for challenge, analysis in results.items():
            if "error_counts" in analysis and analysis["error_counts"]:
                plot_error_breakdown(
                    analysis["error_counts"],
                    challenge=challenge,
                    output_path=figures_dir / f"fig4_analysis_error_{challenge}.pdf",
                )
                print(f"  Figure saved: fig4_analysis_error_{challenge}.pdf")

    except Exception as e:
        print(f"  Warning: Could not generate figures: {e}")


def main():
    parser = argparse.ArgumentParser(description="Section 5.3.1: Error Analysis")
    parser.add_argument(
        "--challenge",
        type=str,
        default="all",
        choices=["timing", "planning", "selection", "all"],
        help="Challenge to analyze",
    )
    parser.add_argument("--verbose", action="store_true", default=True, help="Verbose output")
    args = parser.parse_args()

    run_error_analysis(challenge=args.challenge, verbose=args.verbose)

    print("\n" + "=" * 70)
    print("ğŸ“Š Error Analysis Complete")
    print("=" * 70)


if __name__ == "__main__":
    main()
