#!/usr/bin/env python3
"""
Section 5.4: Cross-Dataset Generalization

éªŒè¯æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

æ•°æ®é›†:
- SAGE-Bench (ours): å†…éƒ¨åŸºå‡†
- ACE-Bench: å¤–éƒ¨å·¥å…·é€‰æ‹©æ•°æ®é›†
- ToolBench: Qin et al. å·¥å…·é€‰æ‹©
- API-Bank: API è°ƒç”¨æ•°æ®é›†
- BFCL: Gorilla Function Calling

è¾“å‡º:
- figures/fig9_generalization_cross_dataset.pdf
- tables/table_cross_dataset_results.tex

Usage:
    python exp_cross_dataset.py
    python exp_cross_dataset.py --datasets sage,acebench
    python exp_cross_dataset.py --strategies keyword,embedding,hybrid
"""

from __future__ import annotations

import argparse

from .exp_utils import (
    get_figures_dir,
    load_benchmark_data,
    print_section_header,
    print_subsection_header,
    save_results,
    setup_experiment_env,
)

# =============================================================================
# Dataset Configuration
# =============================================================================

DATASETS = {
    "sage": {
        "name": "SAGE-Bench",
        "source": "internal",
        "challenge": "selection",
        "loader": "load_benchmark_data",
    },
    "acebench": {
        "name": "ACE-Bench",
        "source": "external",
        "path": "acebench",
        "loader": "load_acebench_data",
    },
    "toolbench": {
        "name": "ToolBench",
        "source": "external",
        "path": "toolbench",
        "loader": "load_toolbench_data",
    },
    "apibank": {
        "name": "API-Bank",
        "source": "external",
        "path": "apibank",
        "loader": "load_apibank_data",
    },
    "bfcl": {
        "name": "BFCL",
        "source": "external",
        "path": "bfcl",
        "loader": "load_bfcl_data",
    },
}

DEFAULT_STRATEGIES = [
    "selector.keyword",
    "selector.embedding",
    "selector.hybrid",
]


# =============================================================================
# Data Loaders
# =============================================================================


def load_dataset(dataset_id: str, max_samples: int = 100) -> list[dict]:
    """
    åŠ è½½æŒ‡å®šæ•°æ®é›†ã€‚

    Args:
        dataset_id: æ•°æ®é›† ID
        max_samples: æœ€å¤§æ ·æœ¬æ•°

    Returns:
        æ ‡å‡†åŒ–çš„æ ·æœ¬åˆ—è¡¨
    """
    if dataset_id not in DATASETS:
        print(f"  âš ï¸  Unknown dataset: {dataset_id}")
        return []

    DATASETS[dataset_id]

    if dataset_id == "sage":
        return load_benchmark_data("selection", split="test", max_samples=max_samples)

    elif dataset_id == "acebench":
        return _load_acebench_data(max_samples)

    elif dataset_id == "toolbench":
        return _load_toolbench_data(max_samples)

    elif dataset_id == "apibank":
        return _load_apibank_data(max_samples)

    elif dataset_id == "bfcl":
        return _load_bfcl_data(max_samples)

    return []


def _load_acebench_data(max_samples: int) -> list[dict]:
    """åŠ è½½ ACE-Bench æ•°æ®ã€‚"""
    # TODO: å®ç°å®é™…çš„ ACE-Bench åŠ è½½
    # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®ç»“æ„
    try:
        from sage.benchmark.benchmark_agent.acebench_loader import load_acebench_samples

        samples = load_acebench_samples(max_samples=max_samples)
        # æ ‡å‡†åŒ–å­—æ®µ
        return [
            {
                "instruction": s.get("query", s.get("instruction", "")),
                "candidate_tools": s.get("tools", s.get("candidate_tools", [])),
                "ground_truth": s.get("expected", s.get("ground_truth", [])),
            }
            for s in samples
        ]
    except ImportError:
        print("  âš ï¸  ACE-Bench loader not available")
        return []


def _load_toolbench_data(max_samples: int) -> list[dict]:
    """åŠ è½½ ToolBench æ•°æ®ã€‚"""
    # TODO: å®ç°å®é™…çš„ ToolBench åŠ è½½
    print("  âš ï¸  ToolBench loader not implemented")
    return []


def _load_apibank_data(max_samples: int) -> list[dict]:
    """åŠ è½½ API-Bank æ•°æ®ã€‚"""
    # TODO: å®ç°å®é™…çš„ API-Bank åŠ è½½
    print("  âš ï¸  API-Bank loader not implemented")
    return []


def _load_bfcl_data(max_samples: int) -> list[dict]:
    """åŠ è½½ BFCL æ•°æ®ã€‚"""
    # TODO: å®ç°å®é™…çš„ BFCL åŠ è½½
    print("  âš ï¸  BFCL loader not implemented")
    return []


# =============================================================================
# Evaluation
# =============================================================================


def evaluate_on_dataset(
    strategy_name: str,
    samples: list[dict],
    top_k: int = 5,
    verbose: bool = True,
) -> dict[str, float]:
    """
    åœ¨å•ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°ç­–ç•¥ã€‚

    Returns:
        {metric: value}
    """
    if not samples:
        return {"top_k_accuracy": 0.0, "mrr": 0.0}

    try:
        from sage.benchmark.benchmark_agent import get_adapter_registry

        registry = get_adapter_registry()
        selector = registry.get(strategy_name)
    except Exception as e:
        if verbose:
            print(f"      âš ï¸  Failed to create selector: {e}")
        return {"top_k_accuracy": 0.0, "mrr": 0.0}

    hits = 0
    rr_sum = 0.0

    for sample in samples:
        query = sample.get("instruction", "")
        candidate_tools = sample.get("candidate_tools", [])
        ground_truth = sample.get("ground_truth", [])

        try:
            preds = selector.select(query, candidate_tools=candidate_tools, top_k=top_k)
            pred_ids = (
                [p.tool_id if hasattr(p, "tool_id") else str(p) for p in preds] if preds else []
            )

            ref_set = set(ground_truth) if isinstance(ground_truth, list) else {ground_truth}

            # Top-K accuracy
            if set(pred_ids[:top_k]) & ref_set:
                hits += 1

            # MRR
            for i, p in enumerate(pred_ids):
                if p in ref_set:
                    rr_sum += 1.0 / (i + 1)
                    break

        except Exception:
            pass

    n = len(samples)
    return {
        "top_k_accuracy": hits / n if n > 0 else 0.0,
        "mrr": rr_sum / n if n > 0 else 0.0,
    }


# =============================================================================
# Main Experiment
# =============================================================================


def run_cross_dataset_evaluation(
    datasets: list[str] = None,
    strategies: list[str] = None,
    max_samples: int = 100,
    top_k: int = 5,
    verbose: bool = True,
) -> dict[str, dict[str, dict[str, float]]]:
    """
    è¿è¡Œè·¨æ•°æ®é›†è¯„ä¼°ã€‚

    Args:
        datasets: è¦æµ‹è¯•çš„æ•°æ®é›†åˆ—è¡¨
        strategies: è¦æµ‹è¯•çš„ç­–ç•¥åˆ—è¡¨
        max_samples: æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°
        top_k: Top-K å‚æ•°
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        {strategy: {dataset: {metric: value}}}
    """
    setup_experiment_env(verbose=verbose)

    print_section_header("Section 5.4: Cross-Dataset Generalization")

    if datasets is None:
        datasets = ["sage", "acebench"]  # é»˜è®¤åªæµ‹è¯•å¯ç”¨çš„

    if strategies is None:
        strategies = DEFAULT_STRATEGIES

    print(f"   Datasets: {datasets}")
    print(f"   Strategies: {[s.split('.')[-1] for s in strategies]}")
    print(f"   Max samples per dataset: {max_samples}")

    all_results = {}

    for strategy_name in strategies:
        strategy_short = strategy_name.split(".")[-1]
        print_subsection_header(f"Strategy: {strategy_short}")

        all_results[strategy_name] = {}

        for dataset_id in datasets:
            dataset_config = DATASETS.get(dataset_id, {})
            dataset_name = dataset_config.get("name", dataset_id)

            print(f"\n      Dataset: {dataset_name}")

            # åŠ è½½æ•°æ®
            samples = load_dataset(dataset_id, max_samples=max_samples)

            if not samples:
                print("        No data available")
                all_results[strategy_name][dataset_id] = {"top_k_accuracy": 0.0, "mrr": 0.0}
                continue

            print(f"        Samples: {len(samples)}")

            # è¯„ä¼°
            metrics = evaluate_on_dataset(strategy_name, samples, top_k=top_k, verbose=verbose)
            all_results[strategy_name][dataset_id] = metrics

            if verbose:
                print(f"        Top-{top_k} Accuracy: {metrics['top_k_accuracy'] * 100:.1f}%")
                print(f"        MRR: {metrics['mrr'] * 100:.1f}%")

    # ä¿å­˜ç»“æœ
    output_file = save_results(all_results, "5_4_generalization", "cross_dataset")
    print(f"\n  Results saved to: {output_file}")

    # ç”Ÿæˆå›¾è¡¨
    _generate_cross_dataset_figures(all_results, datasets, top_k)

    # æ‰“å°æ±‡æ€»è¡¨
    _print_summary_table(all_results, datasets)

    return all_results


def _generate_cross_dataset_figures(results: dict, datasets: list[str], top_k: int) -> None:
    """ç”Ÿæˆè·¨æ•°æ®é›†å¯¹æ¯”å›¾è¡¨ã€‚"""
    try:
        from .figure_generator import plot_cross_dataset_comparison

        figures_dir = get_figures_dir()

        # è½¬æ¢æ•°æ®æ ¼å¼
        plot_data = {}
        for strategy, dataset_results in results.items():
            strategy_short = strategy.split(".")[-1]
            plot_data[strategy_short] = {
                d: dataset_results.get(d, {}).get("top_k_accuracy", 0) for d in datasets
            }

        plot_cross_dataset_comparison(
            plot_data,
            metric=f"top_{top_k}_accuracy",
            output_path=figures_dir / "fig9_generalization_cross_dataset.pdf",
        )
        print("  Figure saved: fig9_generalization_cross_dataset.pdf")

    except Exception as e:
        print(f"  Warning: Could not generate figures: {e}")


def _print_summary_table(results: dict, datasets: list[str]) -> None:
    """æ‰“å°æ±‡æ€»è¡¨æ ¼ã€‚"""
    print("\n" + "=" * 70)
    print("  Cross-Dataset Generalization Summary")
    print("=" * 70)

    # è¡¨å¤´
    header = f"{'Strategy':15s}"
    for d in datasets:
        header += f" | {DATASETS.get(d, {}).get('name', d):12s}"
    print(header)
    print("-" * 70)

    # æ¯ä¸ªç­–ç•¥ä¸€è¡Œ
    for strategy, dataset_results in results.items():
        strategy_short = strategy.split(".")[-1]
        row = f"{strategy_short:15s}"
        for d in datasets:
            acc = dataset_results.get(d, {}).get("top_k_accuracy", 0)
            row += f" | {acc * 100:10.1f}%"
        print(row)

    print("-" * 70)

    # è®¡ç®—æ³›åŒ–å¾—åˆ† (è·¨æ•°æ®é›†æ–¹å·®çš„å€’æ•°)
    print("\n  Generalization Scores (lower variance = better):")
    for strategy, dataset_results in results.items():
        strategy_short = strategy.split(".")[-1]
        accs = [dataset_results.get(d, {}).get("top_k_accuracy", 0) for d in datasets]
        if accs:
            mean_acc = sum(accs) / len(accs)
            variance = sum((a - mean_acc) ** 2 for a in accs) / len(accs)
            print(f"    {strategy_short:12s}: mean={mean_acc * 100:.1f}%, var={variance * 100:.2f}")


def main():
    parser = argparse.ArgumentParser(description="Section 5.4: Cross-Dataset Generalization")
    parser.add_argument(
        "--datasets", type=str, default="sage,acebench", help="Comma-separated dataset IDs"
    )
    parser.add_argument(
        "--strategies", type=str, default=None, help="Comma-separated strategy names"
    )
    parser.add_argument("--max-samples", type=int, default=100, help="Maximum samples per dataset")
    parser.add_argument("--top-k", type=int, default=5, help="Top-K parameter")
    parser.add_argument("--verbose", action="store_true", default=True, help="Verbose output")
    args = parser.parse_args()

    datasets = args.datasets.split(",") if args.datasets else None
    strategies = args.strategies.split(",") if args.strategies else None

    run_cross_dataset_evaluation(
        datasets=datasets,
        strategies=strategies,
        max_samples=args.max_samples,
        top_k=args.top_k,
        verbose=args.verbose,
    )

    print("\n" + "=" * 70)
    print("ğŸ“Š Cross-Dataset Evaluation Complete")
    print("=" * 70)


if __name__ == "__main__":
    main()
