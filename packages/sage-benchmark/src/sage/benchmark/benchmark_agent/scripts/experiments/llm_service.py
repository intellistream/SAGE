"""
LLM Service Manager - LLM æœåŠ¡ç®¡ç†æ¨¡å—

æä¾›ç»Ÿä¸€çš„ LLM æœåŠ¡ç®¡ç†åŠŸèƒ½:
- å¯åŠ¨/åœæ­¢ vLLM æœåŠ¡
- æ£€æŸ¥æœåŠ¡çŠ¶æ€
- å¤šç«¯å£ç®¡ç†
"""

from __future__ import annotations

import os
import signal
import subprocess
import sys
import time
from pathlib import Path

# ç«¯å£é…ç½®
try:
    from sage.common.config.ports import SagePorts

    DEFAULT_LLM_PORT = SagePorts.BENCHMARK_LLM
except ImportError:
    DEFAULT_LLM_PORT = 8901

DEFAULT_LLM_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
LLM_PID_FILE = Path.home() / ".sage" / "benchmark_llm.pid"

# LLM ä¸»æœºåœ°å€ï¼Œæ”¯æŒ Docker ç¯å¢ƒ
# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ LLM_HOSTï¼Œå¦åˆ™é»˜è®¤ localhost
DEFAULT_LLM_HOST = os.environ.get("LLM_HOST", "localhost")


def check_llm_service(port: int = DEFAULT_LLM_PORT, host: str | None = None) -> dict:
    """
    æ£€æŸ¥ LLM æœåŠ¡çŠ¶æ€ã€‚

    Args:
        port: æœåŠ¡ç«¯å£
        host: æœåŠ¡ä¸»æœºåœ°å€ï¼Œé»˜è®¤ä½¿ç”¨ LLM_HOST ç¯å¢ƒå˜é‡æˆ– localhost

    Returns:
        çŠ¶æ€å­—å…¸ {"running": bool, "port": int, "model": str, "error": str}
    """
    if host is None:
        host = DEFAULT_LLM_HOST

    try:
        import httpx
    except ImportError:
        return {"running": False, "port": port, "model": None, "error": "httpx not installed"}

    result = {"running": False, "port": port, "model": None, "error": None}

    try:
        response = httpx.get(f"http://{host}:{port}/v1/models", timeout=5.0)
        if response.status_code == 200:
            data = response.json()
            models = data.get("data", [])
            if models:
                result["running"] = True
                result["model"] = models[0].get("id", "unknown")
        else:
            result["error"] = f"HTTP {response.status_code}"
    except httpx.ConnectError:
        result["error"] = "Connection refused"
    except httpx.TimeoutException:
        result["error"] = "Timeout"
    except Exception as e:
        result["error"] = str(e)

    return result


def check_all_llm_services() -> dict[int, dict]:
    """
    æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„ LLM æœåŠ¡ç«¯å£ã€‚

    Returns:
        {port: status_dict, ...}
    """
    try:
        from sage.common.config.ports import SagePorts

        ports = [SagePorts.BENCHMARK_LLM] + SagePorts.get_llm_ports()
    except ImportError:
        ports = [DEFAULT_LLM_PORT, 8001, 8000]

    # å»é‡
    seen = set()
    unique_ports = []
    for port in ports:
        if port not in seen:
            seen.add(port)
            unique_ports.append(port)

    return {port: check_llm_service(port) for port in unique_ports}


def start_llm_service(
    model: str = DEFAULT_LLM_MODEL,
    port: int = DEFAULT_LLM_PORT,
    gpu_memory: float = 0.5,
    timeout: int = 120,
) -> bool:
    """
    å¯åŠ¨ vLLM æœåŠ¡ã€‚

    Args:
        model: æ¨¡å‹ ID
        port: æœåŠ¡ç«¯å£
        gpu_memory: GPU æ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹
        timeout: ç­‰å¾…å¯åŠ¨è¶…æ—¶ç§’æ•°

    Returns:
        æ˜¯å¦æˆåŠŸå¯åŠ¨
    """
    # æ£€æŸ¥æ˜¯å¦å·²è¿è¡Œ
    status = check_llm_service(port)
    if status["running"]:
        print(f"âœ… LLM æœåŠ¡å·²åœ¨è¿è¡Œ (port={port}, model={status['model']})")
        return True

    print("ğŸš€ å¯åŠ¨ LLM æœåŠ¡...")
    print(f"   æ¨¡å‹: {model}")
    print(f"   ç«¯å£: {port}")
    print(f"   GPU æ˜¾å­˜: {gpu_memory * 100:.0f}%")

    # ç¡®ä¿ PID æ–‡ä»¶ç›®å½•å­˜åœ¨
    LLM_PID_FILE.parent.mkdir(parents=True, exist_ok=True)

    # æ„å»ºå‘½ä»¤
    cmd = [
        sys.executable,
        "-m",
        "vllm.entrypoints.openai.api_server",
        "--model",
        model,
        "--port",
        str(port),
        "--gpu-memory-utilization",
        str(gpu_memory),
        "--trust-remote-code",
    ]

    try:
        # å¯åŠ¨åå°è¿›ç¨‹
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            start_new_session=True,
        )

        # ä¿å­˜ PID
        with open(LLM_PID_FILE, "w") as f:
            f.write(str(process.pid))

        print(f"   PID: {process.pid}")
        print("   ç­‰å¾…æœåŠ¡å¯åŠ¨...")

        # ç­‰å¾…æœåŠ¡å°±ç»ª
        for i in range(timeout):
            time.sleep(1)
            if check_llm_service(port)["running"]:
                print(f"\nâœ… LLM æœåŠ¡å·²å¯åŠ¨ (è€—æ—¶ {i + 1}s)")
                return True
            if i % 10 == 9:
                print(f"   å·²ç­‰å¾… {i + 1}s...")

        print("\nâŒ æœåŠ¡å¯åŠ¨è¶…æ—¶")
        return False

    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        return False


def stop_llm_service() -> bool:
    """
    åœæ­¢ LLM æœåŠ¡ã€‚

    Returns:
        æ˜¯å¦æˆåŠŸåœæ­¢
    """
    if not LLM_PID_FILE.exists():
        print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°è¿è¡Œä¸­çš„ LLM æœåŠ¡")
        return True

    try:
        with open(LLM_PID_FILE) as f:
            pid = int(f.read().strip())

        print(f"ğŸ›‘ åœæ­¢ LLM æœåŠ¡ (PID={pid})...")
        os.kill(pid, signal.SIGTERM)

        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        for _ in range(10):
            try:
                os.kill(pid, 0)  # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
                time.sleep(0.5)
            except OSError:
                break

        LLM_PID_FILE.unlink(missing_ok=True)
        print("âœ… LLM æœåŠ¡å·²åœæ­¢")
        return True

    except ProcessLookupError:
        # è¿›ç¨‹å·²ä¸å­˜åœ¨
        LLM_PID_FILE.unlink(missing_ok=True)
        print("âœ… LLM æœåŠ¡å·²åœæ­¢")
        return True
    except Exception as e:
        print(f"âŒ åœæ­¢å¤±è´¥: {e}")
        return False


def print_llm_status():
    """æ‰“å° LLM æœåŠ¡çŠ¶æ€ã€‚"""
    print("\nğŸ“¡ LLM æœåŠ¡çŠ¶æ€")
    print("=" * 50)

    statuses = check_all_llm_services()

    for port, status in statuses.items():
        if status["running"]:
            print(f"  âœ… Port {port}: è¿è¡Œä¸­")
            print(f"     æ¨¡å‹: {status['model']}")
        else:
            print(f"  âŒ Port {port}: {status['error'] or 'æœªè¿è¡Œ'}")


def ensure_llm_available(
    port: int = DEFAULT_LLM_PORT,
    model: str = DEFAULT_LLM_MODEL,
    auto_start: bool = True,
    allow_cloud: bool = True,
) -> bool:
    """
    ç¡®ä¿ LLM æœåŠ¡å¯ç”¨ã€‚

    å¦‚æœæœåŠ¡æœªè¿è¡Œä¸” auto_start=Trueï¼Œä¼šå°è¯•å¯åŠ¨æœåŠ¡ã€‚

    ä¼˜å…ˆçº§:
    1. æ£€æŸ¥ SAGE_CHAT_BASE_URL ç¯å¢ƒå˜é‡ï¼ˆç”¨æˆ·æ˜¾å¼æŒ‡å®šï¼‰
    2. æ£€æŸ¥æŒ‡å®šç«¯å£
    3. æ£€æŸ¥å…¶ä»–å¸¸ç”¨ç«¯å£
    4. æ£€æŸ¥äº‘ç«¯ API é…ç½®
    5. å°è¯•è‡ªåŠ¨å¯åŠ¨

    Args:
        port: æœåŠ¡ç«¯å£
        model: æ¨¡å‹ ID
        auto_start: æ˜¯å¦è‡ªåŠ¨å¯åŠ¨
        allow_cloud: æ˜¯å¦å…è®¸ä½¿ç”¨äº‘ç«¯ API (é»˜è®¤ True)

    Returns:
        æœåŠ¡æ˜¯å¦å¯ç”¨
    """
    # 1. é¦–å…ˆæ£€æŸ¥ç”¨æˆ·æ˜¯å¦é€šè¿‡ç¯å¢ƒå˜é‡æ˜¾å¼æŒ‡å®šäº† LLM ç«¯ç‚¹
    env_base_url = os.environ.get("SAGE_CHAT_BASE_URL")
    if env_base_url:
        print(f"  âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡ SAGE_CHAT_BASE_URL: {env_base_url}")
        # éªŒè¯ç«¯ç‚¹æ˜¯å¦å¯ç”¨
        try:
            import httpx

            response = httpx.get(f"{env_base_url}/models", timeout=5.0)
            if response.status_code == 200:
                data = response.json()
                models = data.get("data", [])
                if models:
                    print(f"     æ¨¡å‹: {models[0].get('id', 'unknown')}")
                    return True
            print(f"  âš ï¸  ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç«¯ç‚¹è¿”å›å¼‚å¸¸: HTTP {response.status_code}")
        except Exception as e:
            print(f"  âš ï¸  ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç«¯ç‚¹ä¸å¯ç”¨: {e}")
        # ç»§ç»­æ£€æŸ¥å…¶ä»–ç«¯å£

    # 2. æ£€æŸ¥æŒ‡å®šç«¯å£
    print(f"  ğŸ” Checking LLM service on port {port}...")
    status = check_llm_service(port)
    if status["running"]:
        print(f"  âœ… Found running service on port {port}")
        # è®¾ç½®ç¯å¢ƒå˜é‡ä¾›åç»­ä½¿ç”¨
        # ä½¿ç”¨ LLM_HOST ç¯å¢ƒå˜é‡ï¼ˆDocker ç¯å¢ƒå…¼å®¹ï¼‰
        llm_host = os.environ.get("LLM_HOST", "localhost")
        os.environ["SAGE_LLM_PORT"] = str(port)
        os.environ["SAGE_CHAT_BASE_URL"] = f"http://{llm_host}:{port}/v1"
        return True

    # 3. æ£€æŸ¥å…¶ä»–ç«¯å£
    print("  ğŸ” Checking other common ports...")
    all_statuses = check_all_llm_services()
    for p, s in all_statuses.items():
        if s["running"]:
            print(f"  â„¹ï¸  Found running service on port {p}")
            # è®¾ç½®ç¯å¢ƒå˜é‡ä¾›åç»­ä½¿ç”¨
            llm_host = os.environ.get("LLM_HOST", "localhost")
            os.environ["SAGE_LLM_PORT"] = str(p)
            os.environ["SAGE_CHAT_BASE_URL"] = f"http://{llm_host}:{p}/v1"
            return True

    # 4. æ£€æŸ¥äº‘ç«¯ API é…ç½®
    if allow_cloud and (os.environ.get("SAGE_CHAT_API_KEY") or os.environ.get("OPENAI_API_KEY")):
        print("  â„¹ï¸  æ£€æµ‹åˆ°äº‘ç«¯ API é…ç½®")
        return True

    # 5. å°è¯•è‡ªåŠ¨å¯åŠ¨
    if auto_start:
        print("  âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨çš„ LLM æœåŠ¡ï¼Œå°è¯•è‡ªåŠ¨å¯åŠ¨...")
        return start_llm_service(model=model, port=port)

    return False


# =============================================================================
# CLI å…¥å£
# =============================================================================


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="LLM Service Manager",
    )

    subparsers = parser.add_subparsers(dest="action", help="æ“ä½œ")

    # start
    start_parser = subparsers.add_parser("start", help="å¯åŠ¨ LLM æœåŠ¡")
    start_parser.add_argument("--model", default=DEFAULT_LLM_MODEL, help="æ¨¡å‹ ID")
    start_parser.add_argument("--port", type=int, default=DEFAULT_LLM_PORT, help="ç«¯å£")
    start_parser.add_argument("--gpu-memory", type=float, default=0.5, help="GPU æ˜¾å­˜æ¯”ä¾‹")

    # stop
    subparsers.add_parser("stop", help="åœæ­¢ LLM æœåŠ¡")

    # status
    subparsers.add_parser("status", help="æŸ¥çœ‹æœåŠ¡çŠ¶æ€")

    args = parser.parse_args()

    if args.action == "start":
        success = start_llm_service(
            model=args.model,
            port=args.port,
            gpu_memory=args.gpu_memory,
        )
        return 0 if success else 1

    elif args.action == "stop":
        success = stop_llm_service()
        return 0 if success else 1

    elif args.action == "status":
        print_llm_status()
        return 0

    else:
        parser.print_help()
        return 0


if __name__ == "__main__":
    sys.exit(main())
