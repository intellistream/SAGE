#!/usr/bin/env python3
"""
End-to-end benchmark for Tool Selection (Challenge 3)

This script tests the tool selection capability benchmark with:
- Different candidate pool sizes (100, 500, 1000)
- Multiple selector strategies (keyword, embedding, hybrid)
- Comprehensive metrics (top_k_accuracy, recall, precision, MRR)

Output directory: .sage/benchmark/results/tool_selection/

Usage:
    python run_tool_selection_benchmark.py
    python run_tool_selection_benchmark.py --quick  # Quick test with fewer samples
    python run_tool_selection_benchmark.py --visualize  # Generate comparison charts
"""

import argparse
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

# Setup paths
SCRIPT_DIR = Path(__file__).resolve().parent
BENCHMARK_AGENT_DIR = SCRIPT_DIR.parent.parent
BENCHMARK_ROOT = BENCHMARK_AGENT_DIR.parent.parent.parent  # sage-benchmark
SAGE_ROOT = BENCHMARK_ROOT.parent.parent  # SAGE repo root

# Default directories in .sage/
DEFAULT_DATA_DIR = SAGE_ROOT / ".sage" / "benchmark" / "data" / "tool_selection"
DEFAULT_OUTPUT_DIR = SAGE_ROOT / ".sage" / "benchmark" / "results" / "tool_selection"

# Ensure sage packages are importable
sys.path.insert(0, str(BENCHMARK_ROOT / "src"))


def setup_environment():
    """Setup environment variables and imports."""
    os.environ.setdefault("SAGE_TEST_MODE", "true")


def run_tool_selection_experiment(
    selector: str,
    profile: str = "tool_selection_eval",
    split: str = "test",
    max_samples: int = 100,
    top_k: int = 5,
    verbose: bool = True,
) -> dict[str, Any]:
    """
    Run a tool selection experiment with given parameters.

    Args:
        selector: Selector strategy name
        profile: Evaluation profile
        split: Data split (train/dev/test)
        max_samples: Maximum samples to evaluate
        top_k: Number of tools to select
        verbose: Print progress

    Returns:
        Dictionary with metrics and metadata
    """
    from sage.benchmark.benchmark_agent import (
        ToolSelectionConfig,
        ToolSelectionExperiment,
        get_adapter_registry,
    )
    from sage.benchmark.benchmark_agent.evaluation import compute_metrics
    from sage.data import DataManager

    if verbose:
        print(f"\n{'=' * 60}")
        print(f"Testing: {selector}")
        print(f"{'=' * 60}")

    start_time = time.time()

    # Initialize components
    dm = DataManager.get_instance()
    registry = get_adapter_registry()

    # Create config
    config = ToolSelectionConfig(
        profile=profile,
        split=split,
        selector=selector,
        top_k=top_k,
        max_samples=max_samples,
        verbose=verbose,
    )

    # Run experiment
    try:
        exp = ToolSelectionExperiment(config, dm, registry)
        exp.prepare()
        result = exp.run()

        # Compute metrics
        metrics = compute_metrics(
            task="tool_selection",
            predictions=result.predictions,
            references=result.references,
            metrics=["top_k_accuracy", "recall_at_k", "precision_at_k", "mrr"],
            k=top_k,
        )

        elapsed = time.time() - start_time

        # Print results
        if verbose:
            print("\nResults:")
            for name, value in metrics.items():
                if not name.endswith("_error"):
                    print(f"  {name}: {value:.4f}")
            print("\nTarget: top_k_accuracy >= 95%")
            print(f"Status: {'‚úÖ PASS' if metrics.get('top_k_accuracy', 0) >= 0.95 else '‚ùå FAIL'}")
            print(f"Time: {elapsed:.2f}s")

        return {
            "selector": selector,
            "metrics": metrics,
            "elapsed_seconds": elapsed,
            "num_samples": result.metadata.get("total_samples", 0),
            "num_failed": result.metadata.get("failed_samples", 0),
            "config": {
                "profile": profile,
                "split": split,
                "max_samples": max_samples,
                "top_k": top_k,
            },
        }

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback

        traceback.print_exc()
        return {
            "selector": selector,
            "error": str(e),
            "metrics": {},
            "elapsed_seconds": time.time() - start_time,
        }


def run_all_experiments(
    selectors: list[str],
    max_samples: int = 100,
    top_k: int = 5,
    verbose: bool = True,
) -> list[dict]:
    """Run experiments for all selectors."""
    results = []

    for selector in selectors:
        result = run_tool_selection_experiment(
            selector=selector,
            max_samples=max_samples,
            top_k=top_k,
            verbose=verbose,
        )
        results.append(result)

    return results


def generate_comparison_chart(results: list[dict], output_path: Path) -> None:
    """Generate comparison bar chart for different selectors."""
    try:
        import matplotlib

        matplotlib.use("Agg")  # Use non-interactive backend
        import matplotlib.pyplot as plt
        import numpy as np
    except ImportError:
        print("Warning: matplotlib not available, skipping chart generation")
        return

    # Extract data
    selectors = []
    accuracies = []
    recalls = []
    mrrs = []

    for r in results:
        if "error" not in r:
            selectors.append(r["selector"].replace("selector.", "").title())
            accuracies.append(r["metrics"].get("top_k_accuracy", 0) * 100)
            recalls.append(r["metrics"].get("recall_at_k", 0) * 100)
            mrrs.append(r["metrics"].get("mrr", 0) * 100)

    if not selectors:
        print("No valid results to plot")
        return

    # Create figure
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    x = np.arange(len(selectors))
    width = 0.6

    # Top-K Accuracy
    ax1 = axes[0]
    bars1 = ax1.bar(x, accuracies, width, color=["#2ecc71", "#3498db", "#9b59b6"])
    ax1.axhline(y=95, color="r", linestyle="--", label="Target (95%)")
    ax1.set_ylabel("Accuracy (%)")
    ax1.set_title("Top-K Accuracy")
    ax1.set_xticks(x)
    ax1.set_xticklabels(selectors)
    ax1.set_ylim(0, 100)
    ax1.legend()
    for bar, val in zip(bars1, accuracies):
        ax1.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{val:.1f}%",
            ha="center",
            va="bottom",
            fontsize=10,
        )

    # Recall@K
    ax2 = axes[1]
    bars2 = ax2.bar(x, recalls, width, color=["#2ecc71", "#3498db", "#9b59b6"])
    ax2.set_ylabel("Recall (%)")
    ax2.set_title("Recall@K")
    ax2.set_xticks(x)
    ax2.set_xticklabels(selectors)
    ax2.set_ylim(0, 100)
    for bar, val in zip(bars2, recalls):
        ax2.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{val:.1f}%",
            ha="center",
            va="bottom",
            fontsize=10,
        )

    # MRR
    ax3 = axes[2]
    bars3 = ax3.bar(x, mrrs, width, color=["#2ecc71", "#3498db", "#9b59b6"])
    ax3.set_ylabel("MRR (%)")
    ax3.set_title("Mean Reciprocal Rank")
    ax3.set_xticks(x)
    ax3.set_xticklabels(selectors)
    ax3.set_ylim(0, 100)
    for bar, val in zip(bars3, mrrs):
        ax3.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 1,
            f"{val:.1f}%",
            ha="center",
            va="bottom",
            fontsize=10,
        )

    plt.suptitle("Tool Selection Benchmark Results (Challenge 3)", fontsize=14, fontweight="bold")
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches="tight")
    print(f"\nüìä Chart saved to: {output_path}")


def generate_scale_comparison_chart(results_by_scale: dict, output_path: Path) -> None:
    """Generate chart comparing performance across different candidate pool sizes."""
    try:
        import matplotlib

        matplotlib.use("Agg")
        import matplotlib.pyplot as plt
    except ImportError:
        print("Warning: matplotlib not available, skipping chart generation")
        return

    fig, ax = plt.subplots(figsize=(10, 6))

    scales = sorted(results_by_scale.keys())
    selectors = set()
    for scale_results in results_by_scale.values():
        for r in scale_results:
            if "error" not in r:
                selectors.add(r["selector"].replace("selector.", ""))

    selectors = sorted(selectors)
    colors = {"keyword": "#2ecc71", "embedding": "#3498db", "hybrid": "#9b59b6"}
    markers = {"keyword": "o", "embedding": "s", "hybrid": "^"}

    for selector in selectors:
        accuracies = []
        for scale in scales:
            for r in results_by_scale[scale]:
                if selector in r["selector"] and "error" not in r:
                    accuracies.append(r["metrics"].get("top_k_accuracy", 0) * 100)
                    break
            else:
                accuracies.append(0)

        ax.plot(
            scales,
            accuracies,
            marker=markers.get(selector, "o"),
            color=colors.get(selector, "#666"),
            linewidth=2,
            markersize=8,
            label=selector.title(),
        )

    ax.axhline(y=95, color="r", linestyle="--", alpha=0.7, label="Target (95%)")
    ax.set_xlabel("Number of Candidate Tools")
    ax.set_ylabel("Top-K Accuracy (%)")
    ax.set_title("Tool Selection Accuracy vs Candidate Pool Size")
    ax.set_ylim(0, 100)
    ax.legend(loc="lower left")
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches="tight")
    print(f"\nüìä Scale comparison chart saved to: {output_path}")


def print_summary(all_results: list[dict]) -> None:
    """Print summary table of all results."""
    print("\n" + "=" * 80)
    print("TOOL SELECTION BENCHMARK SUMMARY (Challenge 3)")
    print("=" * 80)

    print(f"\n{'Selector':<25} {'Accuracy':<12} {'Recall':<12} {'MRR':<12} {'Status':<10}")
    print("-" * 80)

    for r in all_results:
        if "error" in r:
            print(f"{r['selector']:<25} {'ERROR':<12} {'-':<12} {'-':<12} {'‚ùå':<10}")
        else:
            acc = r["metrics"].get("top_k_accuracy", 0) * 100
            recall = r["metrics"].get("recall_at_k", 0) * 100
            mrr = r["metrics"].get("mrr", 0) * 100
            status = "‚úÖ PASS" if acc >= 95 else "‚ùå FAIL"
            print(f"{r['selector']:<25} {acc:>10.2f}% {recall:>10.2f}% {mrr:>10.2f}% {status:<10}")

    print("-" * 80)
    print("\nTarget: Top-K Accuracy ‚â• 95%")


def main():
    parser = argparse.ArgumentParser(description="Tool Selection E2E Test (Challenge 3)")
    parser.add_argument("--quick", action="store_true", help="Quick test with fewer samples")
    parser.add_argument("--visualize", action="store_true", help="Generate comparison charts")
    parser.add_argument(
        "--full", action="store_true", help="Full test with all strategies and scales"
    )
    parser.add_argument("--max-samples", type=int, default=100, help="Max samples per experiment")
    parser.add_argument("--top-k", type=int, default=5, help="Top-K for accuracy")
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help=f"Output directory for charts (default: {DEFAULT_OUTPUT_DIR})",
    )
    args = parser.parse_args()

    setup_environment()

    # Use .sage/benchmark/ by default
    output_dir = Path(args.output_dir) if args.output_dir else DEFAULT_OUTPUT_DIR
    output_dir.mkdir(parents=True, exist_ok=True)

    # Define selectors to test
    selectors = [
        "selector.keyword",
        "selector.embedding",
        "selector.hybrid",
    ]

    max_samples = 20 if args.quick else args.max_samples

    print("\nüöÄ Starting Tool Selection Benchmark (Challenge 3)")
    print(f"   Max samples: {max_samples}")
    print(f"   Top-K: {args.top_k}")
    print(f"   Selectors: {', '.join(s.replace('selector.', '') for s in selectors)}")

    # Run basic experiments
    results = run_all_experiments(
        selectors=selectors,
        max_samples=max_samples,
        top_k=args.top_k,
        verbose=True,
    )

    # Print summary
    print_summary(results)

    # Generate visualization
    if args.visualize or args.full:
        chart_path = (
            output_dir / f"tool_selection_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        )
        generate_comparison_chart(results, chart_path)

    # Save results to JSON
    results_path = (
        output_dir / f"tool_selection_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    with open(results_path, "w") as f:
        json.dump(
            {
                "timestamp": datetime.now().isoformat(),
                "config": {
                    "max_samples": max_samples,
                    "top_k": args.top_k,
                },
                "results": results,
            },
            f,
            indent=2,
        )
    print(f"\nüìÑ Results saved to: {results_path}")

    # Full test with scale comparison
    if args.full:
        print("\n\nüî¨ Running Scale Comparison Tests...")
        scales = [50, 100]  # Reduced for quick testing
        results_by_scale = {}

        for num_candidates in scales:
            print(f"\n--- Testing with {num_candidates} candidates ---")
            # Note: This would require creating splits with different candidate sizes
            # For now, we use the same data with max_samples limit
            scale_results = run_all_experiments(
                selectors=selectors,
                max_samples=min(max_samples, 50),  # Limit for speed
                top_k=args.top_k,
                verbose=False,
            )
            results_by_scale[num_candidates] = scale_results

        scale_chart_path = (
            output_dir / f"tool_selection_scale_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        )
        generate_scale_comparison_chart(results_by_scale, scale_chart_path)

    print("\n‚úÖ Tool Selection Benchmark Complete!")

    # Print result locations
    print("\n" + "=" * 70)
    print("üìÅ RESULT LOCATIONS")
    print("=" * 70)
    print(f"  üìÑ Results JSON: {results_path}")
    print(f"  üìä Charts:       {output_dir}/")
    print("=" * 70)

    # Exit with error code if no tests passed the target
    passed = any(
        r.get("metrics", {}).get("top_k_accuracy", 0) >= 0.95 for r in results if "error" not in r
    )
    if not passed:
        print("\n‚ö†Ô∏è  Warning: No selector achieved the 95% accuracy target")


if __name__ == "__main__":
    main()
