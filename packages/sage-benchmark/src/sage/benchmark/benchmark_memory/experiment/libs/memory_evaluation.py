"""è®°å¿†è¯„ä¼°æ¨¡å— - è´Ÿè´£ä½¿ç”¨ LLM å¯¹æ‰€æœ‰å¯è§é—®é¢˜è¿›è¡Œé—®ç­”è¯„ä¼°"""

import time

from sage.benchmark.benchmark_memory.experiment.utils import LLMGenerator
from sage.common.core import MapFunction


class MemoryEvaluation(MapFunction):
    """è®°å¿†è¯„ä¼°ç®—å­

    èŒè´£ï¼š
    1. ä½¿ç”¨å†å²å¯¹è¯ + LLM ç”Ÿæˆç­”æ¡ˆ
    2. è¿™æ˜¯ä¸€ä¸ªé€šç”¨ç®—å­ï¼Œä¸ä¾èµ–ç‰¹å®šæ•°æ®é›†çš„ç§æœ‰å±æ€§
    """

    def __init__(self, config):
        super().__init__()
        self.config = config

        # è·å–æ•°æ®é›†ç±»å‹ï¼ˆç”¨äºæ•°æ®é›†ç‰¹å®šçš„å¤„ç†ï¼‰
        self.dataset = config.get("runtime.dataset", "locomo")

        # ä»é…ç½®ä¸­è¯»å– prompt_templateï¼ˆé˜¶æ®µäºŒï¼šç»Ÿä¸€Promptï¼‰
        self.question_answer_prompt = self.config.get(
            "runtime.prompt_template",
            """Based on the above context, answer the following question concisely using exact words from the context whenever possible. If the information is not mentioned in the conversation, respond with "Not mentioned in the conversation".

Question: {question}
Answer:""",
        )

        # ç¬¬äº”ç±»é—®é¢˜ä¸“ç”¨ promptï¼ˆæ›´ç®€æ´ï¼Œé€‚åˆé€‰æ‹©é¢˜ï¼‰
        self.question_answer_prompt_category5 = self.config.get(
            "runtime.prompt_template_category5",
            """Based on the above context, answer the following question.

Question: {question}
Answer:""",
        )

        # åˆå§‹åŒ– LLM ç”Ÿæˆå™¨
        self.generator = LLMGenerator.from_config(config)

    def execute(self, data):
        """æ‰§è¡Œè®°å¿†æµ‹è¯•ï¼ˆç”Ÿæˆç­”æ¡ˆï¼‰

        Args:
            data: çº¯æ•°æ®å­—å…¸ï¼ˆå·²ç”± PipelineServiceSource è§£åŒ…ï¼‰

        Returns:
            åœ¨åŸå§‹æ•°æ®åŸºç¡€ä¸Šæ·»åŠ  "answers" å­—æ®µ
        """
        start_time = time.perf_counter()
        if not data:
            return None

        question = data.get("question")
        history_text = data.get("history_text", "")  # æ¥è‡ªPostRetrievalï¼ˆé˜¶æ®µä¸€ï¼‰
        question_metadata = data.get("question_metadata", {})

        # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œè¿”å›ç©º
        if not question:
            data["answer"] = None
            return data

        # ============================================================
        # æ•°æ®é›†ç‰¹å®šå¤„ç†ï¼šlocomo ç¬¬äº”ç±»é—®é¢˜ï¼ˆå¼ å† ææˆ´æµ‹è¯•ï¼‰
        # å¼ºåˆ¶å°†ç¬¬äº”ç±»é—®é¢˜æ ¼å¼åŒ–ä¸ºé€‰æ‹©é¢˜ï¼Œä»¥é¿å…åˆ†æ•°è™šé«˜
        # ============================================================
        # é»˜è®¤ä½¿ç”¨æ ‡å‡† prompt
        selected_prompt = self.question_answer_prompt

        if self.dataset == "locomo":
            category = question_metadata.get("category")

            if category == 5:
                # ä» question_metadata è·å– adversarial_answer
                adversarial_answer = question_metadata.get("adversarial_answer", "")
                if adversarial_answer:
                    # æ‹¼è£…é€‰æ‹©é¢˜æ ¼å¼
                    question = (
                        f"{question} Select the correct answer: "
                        f"(a) {adversarial_answer} "
                        f"(b) Not mentioned in the conversation."
                    )
                    # ç¬¬äº”ç±»é—®é¢˜ä½¿ç”¨ä¸“ç”¨ prompt
                    selected_prompt = self.question_answer_prompt_category5

        # æ„å»ºå®Œæ•´Promptï¼šhistory_textï¼ˆé˜¶æ®µä¸€ï¼‰ + question_answer_promptï¼ˆé˜¶æ®µäºŒï¼‰
        full_prompt = history_text
        if full_prompt:
            full_prompt += "\n\n"

        # æ‹¼æ¥é—®ç­”éƒ¨åˆ†ï¼ˆé˜¶æ®µäºŒï¼‰
        question_prompt = selected_prompt.replace("{question}", question)
        full_prompt += question_prompt

        prompt = full_prompt

        # ============ DEBUG: Promptå’Œç­”æ¡ˆæ‰“å° ============
        print("\n" + "=" * 80)
        print("ğŸ“ [MemoryEvaluation] ç”Ÿæˆç­”æ¡ˆ")
        print("=" * 80)
        print(f"é—®é¢˜: {question}")
        print(f"Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        # print("\nå®Œæ•´ Prompt:")
        # print("-" * 80)
        # print(prompt)
        # print("-" * 80)
        # ============ DEBUG END ============

        # è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
        llm_start = time.perf_counter()
        answer_text = self.generator.generate(prompt)
        llm_elapsed = (time.perf_counter() - llm_start) * 1000
        print(f"â±ï¸  [MemoryEvaluation] LLM ç­”æ¡ˆç”Ÿæˆè€—æ—¶: {llm_elapsed:.2f}ms")

        # ============ DEBUG: ç­”æ¡ˆæ‰“å° ============
        print(f"\nâœ… ç”Ÿæˆçš„ç­”æ¡ˆ: {answer_text}")
        print("=" * 80)
        # ============ DEBUG END ============

        # answer_text = "yes"

        # è¿”å›ç­”æ¡ˆå’Œå…ƒæ•°æ®
        data["answer"] = answer_text
        data["question_metadata"] = question_metadata

        # è®°å½•é˜¶æ®µè€—æ—¶
        elapsed_ms = (time.perf_counter() - start_time) * 1000
        data.setdefault("stage_timings", {})["memory_evaluation_ms"] = elapsed_ms
        print(f"â±ï¸  [MemoryEvaluation] æ€»è€—æ—¶: {elapsed_ms:.2f}ms (åŒ…å« LLM: {llm_elapsed:.2f}ms)")
        print("=" * 80 + "\n")

        return data
