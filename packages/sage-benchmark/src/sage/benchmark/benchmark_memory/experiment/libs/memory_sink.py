import json
import os

from sage.benchmark.benchmark_memory.experiment.utils.path_finder import get_project_root
from sage.benchmark.benchmark_memory.experiment.utils.time_geter import (
    get_runtime_timestamp,
    get_time_filename,
)
from sage.common.core import SinkFunction


class MemorySink(SinkFunction):
    """æ”¶é›†æµ‹è¯•ç»“æœå¹¶ä¿å­˜ä¸º JSON æ ¼å¼çš„ Sink"""

    def __init__(self, config):
        """åˆå§‹åŒ– MemorySink

        Args:
            config: RuntimeConfig å¯¹è±¡ï¼Œä»ä¸­è·å– dataset å’Œ task_id
        """
        self.dataset = config.get("dataset")
        self.task_id = config.get("task_id")
        self.test_segments = config.get("runtime.test_segments", 10)

        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = get_project_root()

        # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•ç»“æ„
        time_str = get_time_filename()
        self.output_dir = os.path.join(
            project_root, f".sage/benchmarks/benchmark_memory/{self.dataset}/{time_str}"
        )
        os.makedirs(self.output_dir, exist_ok=True)

        # è®¾ç½®è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆæ ¼å¼ï¼štask_id_HHMM.jsonï¼‰
        runtime_stamp = get_runtime_timestamp()
        self.output_file = os.path.join(self.output_dir, f"{self.task_id}_{runtime_stamp}.json")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {self.output_file}")

        # æ”¶é›†æ‰€æœ‰æµ‹è¯•ç»“æœ
        self.test_results = []

        # åˆå§‹åŒ– DataLoaderï¼ˆç”¨äºè·å–ç»Ÿè®¡ä¿¡æ¯ï¼‰
        self.loader = self._init_loader(self.dataset)

    def _init_loader(self, dataset):
        """æ ¹æ®æ•°æ®é›†ç±»å‹åˆå§‹åŒ– DataLoader

        Args:
            dataset: æ•°æ®é›†åç§°

        Returns:
            DataLoader å®ä¾‹
        """
        if dataset == "locomo":
            from sage.data.locomo.dataloader import LocomoDataLoader

            return LocomoDataLoader()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset}")

    def execute(self, data):
        """æ¥æ”¶å¹¶å¤„ç†æµ‹è¯•ç»“æœ

        Args:
            data: æ¥è‡ª PipelineCaller çš„çº¯æ•°æ®å­—å…¸
                - None: æœªè§¦å‘æµ‹è¯•
                - dict: æµ‹è¯•ç»“æœæˆ–å®Œæˆä¿¡å·
                    - completed: True è¡¨ç¤ºæœ€åä¸€ä¸ªåŒ…
                    - question_range, answers: æµ‹è¯•æ•°æ®
        """
        if not data:
            # None è¡¨ç¤ºæœªè§¦å‘æµ‹è¯•ï¼Œç›´æ¥è¿”å›
            return

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æµ‹è¯•ç»“æœ
        if "answers" in data:
            # æ”¶é›†æµ‹è¯•ç»“æœ
            test_result = {
                "test_index": len(self.test_results) + 1,
                "question_range": data.get("question_range"),
                "dialogs_inserted_count": data.get("dialogs_inserted"),
                "answers": data.get("answers", []),
            }
            self.test_results.append(test_result)

        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        if data.get("completed", False):
            self._save_results(data)

    def _save_results(self, data):
        """ä¿å­˜æœ€ç»ˆç»“æœ

        Args:
            data: åŒ…å« dataset å’Œ task_id çš„æ•°æ®
        """
        dataset = data.get("dataset", self.dataset)
        task_id = data.get("task_id", self.task_id)

        # ä» DataLoader è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        dataset_stats = self.loader.get_dataset_statistics(task_id)

        # æ„é€ è¾“å‡º JSON ç»“æ„
        output_data = {
            "experiment_info": {
                "dataset": dataset,
                "task_id": task_id,
            },
            "dataset_statistics": dataset_stats,
            "test_summary": {
                "total_tests": len(self.test_results),
                "test_segments": self.test_segments,
                "test_threshold": f"1/{self.test_segments} of total questions",
            },
            "test_results": self._format_test_results(self.test_results),
        }

        # ä¿å­˜ä¸º JSON
        with open(self.output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… æµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {self.output_file}")

    def _format_test_results(self, test_results):
        """æ ¼å¼åŒ–æµ‹è¯•ç»“æœä¸ºé€šç”¨æ ¼å¼

        Args:
            test_results: æµ‹è¯•ç»“æœåˆ—è¡¨

        Returns:
            list: æ ¼å¼åŒ–åçš„æµ‹è¯•ç»“æœ
        """
        formatted_results = []

        for test in test_results:
            formatted_test = {
                "test_index": test.get("test_index"),
                "question_range": test.get("question_range"),
                "dialogs_inserted_count": test.get("dialogs_inserted_count"),
                "questions": [],
            }

            # æ ¼å¼åŒ–æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆ
            for answer in test.get("answers", []):
                metadata = answer.get("metadata", {})

                question_data = {
                    "question_index": answer.get("question_index"),
                    "question_text": answer.get("question"),
                    "predicted_answer": answer.get("predicted_answer"),
                }

                # æ·»åŠ å‚è€ƒç­”æ¡ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if "answer" in metadata:
                    question_data["reference_answer"] = metadata["answer"]

                # æ·»åŠ è¯æ®ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if "evidence" in metadata:
                    question_data["evidence"] = metadata["evidence"]

                # æ·»åŠ åˆ†ç±»ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if "category" in metadata:
                    question_data["category"] = metadata["category"]

                # æ·»åŠ é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if "error" in answer:
                    question_data["error"] = answer["error"]

                formatted_test["questions"].append(question_data)

            formatted_results.append(formatted_test)

        return formatted_results
