"""PostRetrieval Multi-Tier Merge å®ç°

MemGPT é£æ ¼çš„å¤šå±‚è®°å¿†èåˆï¼š
1. Core memory: ç›´æ¥ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆä¸éœ€è¦æ’åºï¼‰
2. Archival + Recall: RRF èåˆåè¿”å› Top-K
3. æœ€ç»ˆè¾“å‡º: Coreï¼ˆå®Œæ•´ï¼‰ + Top-K(Archival+Recall)

è®ºæ–‡åŸæ–‡: Figure 3 - MemGPT System Architecture
"""

from typing import Any


class MemoryPressureMonitor:
    """Memory Pressure ç›‘æ§å™¨

    è®ºæ–‡åŸæ–‡ Section 2.2:
    "When the prompt tokens exceed the 'warning token count' of the underlying LLM's
    context window (e.g. 70% of the context window), the queue manager inserts a
    system message into the queue warning the LLM of an impending queue eviction."

    åŠŸèƒ½ï¼š
    1. ç›‘æ§å½“å‰ context çš„ token ä½¿ç”¨ç‡
    2. è¶…è¿‡é˜ˆå€¼æ—¶ç”Ÿæˆ Memory Pressure Warning
    3. å…è®¸ Agent ä¸»åŠ¨ä¿å­˜é‡è¦ä¿¡æ¯
    """

    def __init__(self, config: dict):
        """åˆå§‹åŒ–ç›‘æ§å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - context_window_size: LLM çš„ context window å¤§å°ï¼ˆtokensï¼‰
                - memory_pressure_threshold: è§¦å‘è­¦å‘Šçš„é˜ˆå€¼ï¼ˆ0-1ï¼‰
                - queue_flush_threshold: å¼ºåˆ¶ flush çš„é˜ˆå€¼ï¼ˆ0-1ï¼‰
        """
        self.context_window_size = config.get("runtime.context_window_size", 8192)
        self.memory_pressure_threshold = config.get("runtime.memory_pressure_threshold", 0.7)
        self.queue_flush_threshold = config.get("runtime.queue_flush_threshold", 1.0)
        self.pressure_warning_sent = False

    def estimate_token_count(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 4 charactersï¼‰"""
        return len(text) // 4

    def check_memory_pressure(
        self,
        core_memory_text: str,
        retrieved_memories: list[dict],
        conversation_history: list[dict] = None,
    ) -> dict[str, Any]:
        """æ£€æŸ¥ memory pressure å¹¶ç”Ÿæˆè­¦å‘Š"""
        # ä¼°ç®—å½“å‰ context çš„ token ä½¿ç”¨é‡
        core_tokens = self.estimate_token_count(core_memory_text)
        retrieved_tokens = sum(
            self.estimate_token_count(mem.get("text", "")) for mem in retrieved_memories
        )
        history_tokens = 0
        if conversation_history:
            for msg in conversation_history:
                history_tokens += self.estimate_token_count(msg.get("content", ""))

        total_tokens = core_tokens + retrieved_tokens + history_tokens
        usage_ratio = total_tokens / self.context_window_size

        result = {
            "has_pressure": False,
            "usage_ratio": usage_ratio,
            "estimated_tokens": total_tokens,
            "warning_message": None,
        }

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡è­¦å‘Šé˜ˆå€¼
        if usage_ratio >= self.memory_pressure_threshold and not self.pressure_warning_sent:
            result["has_pressure"] = True
            result["warning_message"] = self._generate_pressure_warning(usage_ratio, total_tokens)
            self.pressure_warning_sent = True

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ flush é˜ˆå€¼
        if usage_ratio >= self.queue_flush_threshold:
            result["should_flush"] = True
            result["flush_message"] = (
                f"WARNING: Context window is {usage_ratio * 100:.1f}% full "
                f"({total_tokens}/{self.context_window_size} tokens). "
                f"FIFO queue will be flushed to Recall Storage."
            )
        else:
            result["should_flush"] = False

        return result

    def _generate_pressure_warning(self, usage_ratio: float, total_tokens: int) -> str:
        """ç”Ÿæˆ Memory Pressure Warning æ¶ˆæ¯"""
        warning = f"""
================================================================
                  MEMORY PRESSURE WARNING
================================================================
  Context window usage: {usage_ratio * 100:.1f}%
  Current tokens: {total_tokens}/{self.context_window_size}

  The context window is approaching capacity.
  Consider using the following functions to preserve
  important information:

  - core_memory_append(label, content)
    Save critical facts to core memory

  - archival_memory_insert(content, tags)
    Archive detailed information for later retrieval

  If no action is taken, older messages in the conversation
  queue will be automatically moved to Recall Storage.
================================================================
"""
        return warning.strip()

    def reset(self):
        """é‡ç½®è­¦å‘ŠçŠ¶æ€"""
        self.pressure_warning_sent = False


class MultiTierMerge:
    """å¤šå±‚è®°å¿†èåˆå™¨ï¼ˆMemGPT é£æ ¼ï¼‰

    æ‰§è¡Œæµç¨‹ï¼š
    1. ä» memory_data ä¸­åˆ†ç¦»ä¸‰å±‚ç»“æœ
    2. Core memory ç›´æ¥ä½œä¸ºä¸Šä¸‹æ–‡
    3. Archival + Recall ä½¿ç”¨ RRF èåˆ
    4. æ£€æŸ¥ Memory Pressure å¹¶ç”Ÿæˆè­¦å‘Šï¼ˆè®ºæ–‡æ ¸å¿ƒç‰¹æ€§ï¼‰
    5. ç»„åˆæœ€ç»ˆç»“æœ
    """

    def __init__(self, config=None):
        """åˆå§‹åŒ–

        Args:
            config: RuntimeConfig å¯¹è±¡
        """
        self.config = config or {}

        # ä»é…ç½®è¯»å–å‚æ•°
        post_retrieval_config = config.get("operators.post_retrieval", {})
        self.top_k = post_retrieval_config.get("top_k", 10)
        self.rrf_k = post_retrieval_config.get("rrf_k", 60)
        self.vector_weight = post_retrieval_config.get("vector_weight", 0.5)
        self.fts_weight = post_retrieval_config.get("fts_weight", 0.5)

        # Memory Pressure ç›‘æ§ï¼ˆMemGPT æ ¸å¿ƒç‰¹æ€§ï¼‰
        self.enable_pressure_warning = post_retrieval_config.get(
            "enable_memory_pressure_warning", True
        )
        if self.enable_pressure_warning:
            self.pressure_monitor = MemoryPressureMonitor(config)
        else:
            self.pressure_monitor = None

        # è¯¦ç»†æ—¥å¿—å¼€å…³
        self.verbose = config.get("runtime.memory_test_verbose", False)

    def execute(self, data: dict[str, Any]) -> dict[str, Any]:
        """æ‰§è¡Œå¤šå±‚èåˆ

        Args:
            data: åŒ…å« memory_data çš„å­—å…¸

        Returns:
            data + enhanced_context
        """
        memory_data = data.get("memory_data", [])

        if not memory_data:
            data["enhanced_context"] = {
                "core_memory": "",
                "retrieved_memories": [],
                "stats": {"core_count": 0, "retrieved_count": 0},
            }
            return data

        # 1. æŒ‰ tier åˆ†ç»„
        core_memories = []
        archival_memories = []
        recall_memories = []

        for item in memory_data:
            tier = item.get("tier", "")
            if item.get("is_core_memory", False) or tier == "core":
                core_memories.append(item)
            elif tier == "archival":
                archival_memories.append(item)
            elif tier == "recall":
                recall_memories.append(item)

        if self.verbose:
            print("\n" + "=" * 80)
            print("ğŸ”„ [PostRetrieval] Multi-Tier Merge")
            print("=" * 80)
            print(f"Core memories: {len(core_memories)}")
            print(f"Archival memories: {len(archival_memories)}")
            print(f"Recall memories: {len(recall_memories)}")

        # 2. æ ¼å¼åŒ– Core Memory
        core_text = self._format_core_memory(core_memories)

        # 3. RRF èåˆ Archival + Recall
        fused_memories = self._rrf_fusion(archival_memories, recall_memories)

        # 4. æ£€æŸ¥ Memory Pressureï¼ˆMemGPT æ ¸å¿ƒç‰¹æ€§ï¼‰
        pressure_info = None
        if self.pressure_monitor:
            pressure_info = self.pressure_monitor.check_memory_pressure(
                core_memory_text=core_text,
                retrieved_memories=fused_memories[: self.top_k],
                conversation_history=None,  # TODO: ä» data ä¸­è·å–å¯¹è¯å†å²
            )

            # å¦‚æœæœ‰å‹åŠ›ï¼Œè¾“å‡ºè­¦å‘Š
            if pressure_info.get("has_pressure") and self.verbose:
                print("\n" + pressure_info["warning_message"] + "\n")

        # 5. ç»„åˆæœ€ç»ˆä¸Šä¸‹æ–‡
        enhanced_context = {
            "core_memory": core_text,
            "retrieved_memories": fused_memories[: self.top_k],
            "stats": {
                "core_count": len(core_memories),
                "archival_count": len(archival_memories),
                "recall_count": len(recall_memories),
                "fused_count": len(fused_memories),
                "final_count": min(len(fused_memories), self.top_k),
            },
        }

        # æ·»åŠ  Memory Pressure ä¿¡æ¯
        if pressure_info:
            enhanced_context["memory_pressure"] = {
                "has_pressure": pressure_info.get("has_pressure", False),
                "usage_ratio": pressure_info.get("usage_ratio", 0.0),
                "estimated_tokens": pressure_info.get("estimated_tokens", 0),
                "should_flush": pressure_info.get("should_flush", False),
            }

        if self.verbose:
            print("\nâœ… Fusion complete:")
            print(f"   Core memory: {len(core_text)} chars")
            print(f"   Retrieved: {len(fused_memories)} â†’ Top-{self.top_k}")
            print("=" * 80 + "\n")

        data["enhanced_context"] = enhanced_context
        return data

    def _format_core_memory(self, core_memories: list[dict]) -> str:
        """æ ¼å¼åŒ– Core Memory ä¸ºæ–‡æœ¬

        MemGPT çš„ Core Memory åŒ…å«å¤šä¸ª blocksï¼ˆpersona, human ç­‰ï¼‰

        Args:
            core_memories: Core memory åˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–çš„æ–‡æœ¬
        """
        if not core_memories:
            return ""

        # æŒ‰ label åˆ†ç»„
        blocks = {}
        for mem in core_memories:
            label = mem.get("metadata", {}).get("label", "unknown")
            text = mem.get("text", "")
            if label not in blocks:
                blocks[label] = []
            blocks[label].append(text)

        # æ ¼å¼åŒ–è¾“å‡º
        lines = []
        lines.append("<core_memory>")
        for label, texts in blocks.items():
            lines.append(f"<{label}>")
            lines.extend(texts)
            lines.append(f"</{label}>")
        lines.append("</core_memory>")

        return "\n".join(lines)

    def _rrf_fusion(self, archival_results: list[dict], recall_results: list[dict]) -> list[dict]:
        """RRF (Reciprocal Rank Fusion) èåˆ

        ä¸ Letta å’Œ HierarchicalMemoryService çš„å®ç°ä¸€è‡´ã€‚

        Args:
            archival_results: Archival memory æ£€ç´¢ç»“æœ
            recall_results: Recall memory æ£€ç´¢ç»“æœ

        Returns:
            èåˆåçš„ç»“æœåˆ—è¡¨
        """
        # æ„å»ºæ’åæ˜ å°„
        archival_ranks = {
            r.get("entry_id", r.get("id")): rank + 1 for rank, r in enumerate(archival_results)
        }
        recall_ranks = {
            r.get("entry_id", r.get("id")): rank + 1 for rank, r in enumerate(recall_results)
        }

        # åˆå¹¶æ‰€æœ‰å”¯ä¸€é¡¹
        all_items = {}
        for r in archival_results:
            item_id = r.get("entry_id", r.get("id"))
            all_items[item_id] = r
        for r in recall_results:
            item_id = r.get("entry_id", r.get("id"))
            if item_id not in all_items:
                all_items[item_id] = r

        # è®¡ç®— RRF åˆ†æ•°
        rrf_scores = {}
        for item_id in all_items:
            score = 0.0
            if item_id in archival_ranks:
                score += self.vector_weight / (self.rrf_k + archival_ranks[item_id])
            if item_id in recall_ranks:
                score += self.fts_weight / (self.rrf_k + recall_ranks[item_id])
            rrf_scores[item_id] = score

        # æ’åº
        sorted_ids = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

        result = []
        for item_id, score in sorted_ids:
            item = all_items[item_id].copy()
            item["score"] = score
            item["metadata"] = item.get("metadata", {})
            item["metadata"]["rrf_score"] = score
            item["metadata"]["archival_rank"] = archival_ranks.get(item_id)
            item["metadata"]["recall_rank"] = recall_ranks.get(item_id)
            item["metadata"]["fusion_method"] = "rrf"
            result.append(item)

        return result
