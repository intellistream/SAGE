"""ExpandAction - æŸ¥è¯¢æ‰©å±•ç­–ç•¥

ä½¿ç”¨è®°å¿†ä½“ï¼š
- MemGPT: hierarchical_memoryï¼Œé€šè¿‡LLMæ‰©å±•æŸ¥è¯¢ä»¥è¦†ç›–æ›´å¤šç›¸å…³è®°å¿†

ç‰¹ç‚¹ï¼š
- ä½¿ç”¨LLMç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢
- æ”¯æŒå¤šç§åˆå¹¶ç­–ç•¥ï¼ˆunion, intersectionï¼‰
- å¯é…ç½®æ‰©å±•æ•°é‡
"""

from sage.benchmark.benchmark_memory.experiment.utils import LLMGenerator

from ..base import BasePreRetrievalAction, PreRetrievalInput, PreRetrievalOutput


class ExpandAction(BasePreRetrievalAction):
    """æŸ¥è¯¢æ‰©å±•Action

    ä½¿ç”¨LLMç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢ï¼Œä»¥æ‰©å¤§æ£€ç´¢èŒƒå›´ã€‚
    """

    def _init_action(self) -> None:
        """åˆå§‹åŒ–æŸ¥è¯¢æ‰©å±•é…ç½®"""
        self.expand_prompt = self._get_config_value(
            "expand_prompt", required=True, context="optimize_type=expand"
        )

        self.expand_count = self._get_config_value(
            "expand_count", required=True, context="optimize_type=expand"
        )

        self.merge_strategy = self._get_config_value("merge_strategy", default="union")

        self.replace_original = self._get_config_value("replace_original", default=False)

        self.store_optimized = self._get_config_value("store_optimized", default=True)

        # LLMç”Ÿæˆå™¨å°†ç”±PreRetrievalä¸»ç±»æä¾›
        self._llm_generator = None

        # åˆå§‹åŒ– Embedding ç”Ÿæˆå™¨ï¼ˆç”¨äºŽæ‰©å±•æŸ¥è¯¢å‘é‡åŒ–ï¼‰
        from sage.benchmark.benchmark_memory.experiment.utils import EmbeddingGenerator

        self._embedding_generator = EmbeddingGenerator.from_config(self.config)

    def set_llm_generator(self, generator: LLMGenerator) -> None:
        """è®¾ç½®LLMç”Ÿæˆå™¨ï¼ˆç”±PreRetrievalä¸»ç±»è°ƒç”¨ï¼‰"""
        self._llm_generator = generator

    def execute(self, input_data: PreRetrievalInput) -> PreRetrievalOutput:
        """æ‰©å±•æŸ¥è¯¢

        Args:
            input_data: è¾“å…¥æ•°æ®

        Returns:
            åŒ…å«æ‰©å±•æŸ¥è¯¢çš„è¾“å‡ºæ•°æ®

        Raises:
            RuntimeError: LLMç”Ÿæˆå™¨æœªè®¾ç½®
        """
        if self._llm_generator is None:
            raise RuntimeError("LLM generator not set. Call set_llm_generator first.")

        question = input_data.question

        # ä½¿ç”¨LLMç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        prompt = self.expand_prompt.format(question=question, expand_count=self.expand_count)
        response = self._llm_generator.generate(prompt)

        # è§£æžæ‰©å±•æŸ¥è¯¢ï¼ˆå‡è®¾LLMè¿”å›žæ¢è¡Œåˆ†éš”çš„æŸ¥è¯¢åˆ—è¡¨ï¼‰
        expanded_queries = [q.strip() for q in response.split("\n") if q.strip()]
        expanded_queries = expanded_queries[: self.expand_count]

        # æ ¹æ®é…ç½®å†³å®šæœ€ç»ˆæŸ¥è¯¢
        if self.replace_original:
            # ä½¿ç”¨æ‰©å±•æŸ¥è¯¢æ›¿æ¢åŽŸæŸ¥è¯¢
            final_query = " | ".join(expanded_queries) if expanded_queries else question
            queries_for_retrieval = expanded_queries if expanded_queries else [question]
        else:
            # ä¿ç•™åŽŸæŸ¥è¯¢å¹¶æ·»åŠ æ‰©å±•
            final_query = question
            # å°†åŽŸæŸ¥è¯¢ä½œä¸ºç¬¬ä¸€ä¸ªæŸ¥è¯¢ï¼Œæ‰©å±•æŸ¥è¯¢è·Ÿåœ¨åŽé¢
            queries_for_retrieval = (
                [question] + expanded_queries if expanded_queries else [question]
            )

        # ä¸ºæ‰€æœ‰æŸ¥è¯¢ç”Ÿæˆ embeddingï¼ˆåŒ…æ‹¬åŽŸæŸ¥è¯¢ï¼‰
        all_embeddings = []
        if queries_for_retrieval and self._embedding_generator:
            print(f"\nðŸ”„ å¼€å§‹ä¸º {len(queries_for_retrieval)} ä¸ªæŸ¥è¯¢ç”Ÿæˆ embedding...")
            for idx, eq in enumerate(queries_for_retrieval, 1):
                try:
                    embedding = self._embedding_generator.embed(eq)
                    all_embeddings.append(embedding)
                    query_type = (
                        "åŽŸå§‹æŸ¥è¯¢"
                        if (not self.replace_original and idx == 1)
                        else f"æ‰©å±•æŸ¥è¯¢ {idx if self.replace_original else idx - 1}"
                    )
                    print(f"  âœ“ {query_type}: {eq[:50]}... (ç»´åº¦: {len(embedding)})")
                except Exception as e:
                    print(f"  âœ— æŸ¥è¯¢ {idx} embedding ç”Ÿæˆå¤±è´¥: {e}")
                    all_embeddings.append(None)
        else:
            all_embeddings = [None] * len(queries_for_retrieval)

        # æž„å»ºå…ƒæ•°æ®
        metadata = {
            "original_query": question,
            "expanded_queries": expanded_queries,
            "all_queries": queries_for_retrieval,
            "all_embeddings": all_embeddings,
            "merge_strategy": self.merge_strategy,
            "needs_embedding": True,
        }

        # å¦‚æžœé…ç½®äº†store_optimizedï¼Œå°†æ‰©å±•æŸ¥è¯¢å­˜å‚¨åˆ°metadata
        if self.store_optimized:
            metadata["optimized_queries"] = expanded_queries

        return PreRetrievalOutput(
            query=final_query,
            query_embedding=None,  # ç”±å¤–éƒ¨ç»Ÿä¸€ç”Ÿæˆ
            metadata=metadata,
            retrieve_mode="passive",
            retrieve_params={
                "multi_query": queries_for_retrieval if len(queries_for_retrieval) > 1 else None,
                "expanded_embeddings": all_embeddings,
                "merge_strategy": self.merge_strategy,
            },
        )
