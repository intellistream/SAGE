"""ExpandAction - æŸ¥è¯¢æ‰©å±•ç­–ç•¥

ä½¿ç”¨è®°å¿†ä½“ï¼š
- MemGPT: hierarchical_memoryï¼Œé€šè¿‡LLMæ‰©å±•æŸ¥è¯¢ä»¥è¦†ç›–æ›´å¤šç›¸å…³è®°å¿†

ç‰¹ç‚¹ï¼š
- ä½¿ç”¨LLMç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢
- æ”¯æŒå¤šç§åˆå¹¶ç­–ç•¥ï¼ˆunion, intersectionï¼‰
- å¯é…ç½®æ‰©å±•æ•°é‡
"""

from sage.benchmark.benchmark_memory.experiment.utils import LLMGenerator

from ..base import BasePreRetrievalAction, PreRetrievalInput, PreRetrievalOutput


class ExpandAction(BasePreRetrievalAction):
    """æŸ¥è¯¢æ‰©å±•Action

    ä½¿ç”¨LLMç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢ï¼Œä»¥æ‰©å¤§æ£€ç´¢èŒƒå›´ã€‚
    """

    def _init_action(self) -> None:
        """åˆå§‹åŒ–æŸ¥è¯¢æ‰©å±•é…ç½®"""
        self.expand_prompt = self._get_config_value(
            "expand_prompt", required=True, context="optimize_type=expand"
        )

        self.expand_count = self._get_config_value(
            "expand_count", required=True, context="optimize_type=expand"
        )

        self.merge_strategy = self._get_config_value("merge_strategy", default="union")

        self.replace_original = self._get_config_value("replace_original", default=False)

        self.store_optimized = self._get_config_value("store_optimized", default=True)

        # LLMç”Ÿæˆå™¨å°†ç”±PreRetrievalä¸»ç±»æä¾›
        self._llm_generator = None
        # Embeddingç”Ÿæˆå™¨å°†ç”±operatoré€šè¿‡set_embedding_generatorä¼ å…¥
        self._embedding_generator = None

    def set_llm_generator(self, generator: LLMGenerator) -> None:
        """è®¾ç½®LLMç”Ÿæˆå™¨ï¼ˆç”±PreRetrievalä¸»ç±»è°ƒç”¨ï¼‰"""
        self._llm_generator = generator

    def set_embedding_generator(self, generator) -> None:
        """è®¾ç½®Embeddingç”Ÿæˆå™¨ï¼ˆç”±PreRetrievalä¸»ç±»è°ƒç”¨ï¼‰"""
        self._embedding_generator = generator

    def execute(self, input_data: PreRetrievalInput) -> PreRetrievalOutput:
        """æ‰©å±•æŸ¥è¯¢

        Args:
            input_data: è¾“å…¥æ•°æ®

        Returns:
            åŒ…å«æ‰©å±•æŸ¥è¯¢çš„è¾“å‡ºæ•°æ®

        Raises:
            RuntimeError: LLMç”Ÿæˆå™¨æœªè®¾ç½®
        """
        if self._llm_generator is None:
            raise RuntimeError("LLM generator not set. Call set_llm_generator first.")

        question = input_data.question

        # ä½¿ç”¨LLMç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        prompt = self.expand_prompt.format(question=question, expand_count=self.expand_count)
        response = self._llm_generator.generate(prompt)

        # è§£æžæ‰©å±•æŸ¥è¯¢ï¼ˆè¿‡æ»¤æŽ‰è¯´æ˜Žæ–‡å­—å’Œæ ¼å¼æ ‡è®°ï¼‰
        lines = response.split("\n")
        expanded_queries = []
        for line in lines:
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œã€ä»£ç å—æ ‡è®°ã€è¯´æ˜Žæ€§æ–‡å­—
            if not line:
                continue
            if line.startswith("```"):
                continue
            if line in ["[", "]", "{", "}"]:
                continue
            # è¿‡æ»¤è¯´æ˜Žæ€§æ–‡å­—ï¼ˆä¸ä»¥é—®å·ç»“å°¾çš„é•¿å¥ï¼‰
            if any(
                keyword in line.lower()
                for keyword in [
                    "related queries",
                    "here are",
                    "this query explores",
                    "this focuses on",
                    "query explores",
                    "query focuses",
                ]
            ):
                continue
            # ç§»é™¤åˆ—è¡¨ç¼–å·å’Œæ ¼å¼æ ‡è®°
            import re

            line = re.sub(r"^[\d\-\*]+[\.)\]\s]+", "", line)  # 1. 2) 3] - *
            line = re.sub(
                r"^\*\*Query\s+\d+:?\s*[\"\']?", "", line, flags=re.IGNORECASE
            )  # **Query 1: "
            line = re.sub(r"[\"\']\*\*$", "", line)  # ç»“å°¾çš„ "**
            line = re.sub(r'^["\']|["\']$', "", line)  # å¼•å·
            line = line.strip()

            # åªä¿ç•™é•¿åº¦ > 15 ä¸”ä»¥é—®å·ç»“å°¾çš„æœ‰æ•ˆæŸ¥è¯¢
            if line and len(line) > 15 and line.endswith("?"):
                expanded_queries.append(line)

        expanded_queries = expanded_queries[: self.expand_count]

        # æ·»åŠ è°ƒè¯•è¾“å‡º
        print("\nðŸ“ LLM æ‰©å±•æŸ¥è¯¢ç”Ÿæˆ:")
        print(f"  åŽŸå§‹æŸ¥è¯¢: {question}")
        print(f"  LLM å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"  è§£æžå‡º {len(expanded_queries)} ä¸ªæ‰©å±•æŸ¥è¯¢:")
        for idx, eq in enumerate(expanded_queries, 1):
            print(f"    {idx}. {eq}")

        # æ ¹æ®é…ç½®å†³å®šæœ€ç»ˆæŸ¥è¯¢
        if self.replace_original:
            # ä½¿ç”¨æ‰©å±•æŸ¥è¯¢æ›¿æ¢åŽŸæŸ¥è¯¢
            final_query = " | ".join(expanded_queries) if expanded_queries else question
            queries_for_retrieval = expanded_queries if expanded_queries else [question]
        else:
            # ä¿ç•™åŽŸæŸ¥è¯¢å¹¶æ·»åŠ æ‰©å±•
            final_query = question
            # å°†åŽŸæŸ¥è¯¢ä½œä¸ºç¬¬ä¸€ä¸ªæŸ¥è¯¢ï¼Œæ‰©å±•æŸ¥è¯¢è·Ÿåœ¨åŽé¢
            queries_for_retrieval = (
                [question] + expanded_queries if expanded_queries else [question]
            )

        # ä¸ºæ‰€æœ‰æŸ¥è¯¢æ‰¹é‡ç”Ÿæˆ embeddingï¼ˆåŒ…æ‹¬åŽŸæŸ¥è¯¢ï¼‰
        all_embeddings = []
        if (
            queries_for_retrieval
            and self._embedding_generator
            and self._embedding_generator.is_available()
        ):
            print(f"\nðŸ”„ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(queries_for_retrieval)} ä¸ªæŸ¥è¯¢çš„ embedding...")
            try:
                all_embeddings = self._embedding_generator.embed_batch(queries_for_retrieval)
                if all_embeddings:
                    for idx, (eq, emb) in enumerate(zip(queries_for_retrieval, all_embeddings), 1):
                        query_type = (
                            "åŽŸå§‹æŸ¥è¯¢"
                            if (not self.replace_original and idx == 1)
                            else f"æ‰©å±•æŸ¥è¯¢ {idx if self.replace_original else idx - 1}"
                        )
                        if emb:
                            print(f"  âœ“ {query_type}: {eq[:50]}... (ç»´åº¦: {len(emb)})")
                        else:
                            print(f"  âœ— {query_type}: embedding ç”Ÿæˆå¤±è´¥")
                else:
                    print("  âš ï¸  embed_batch è¿”å›ž None")
                    all_embeddings = [None] * len(queries_for_retrieval)
            except Exception as e:
                print(f"  âœ— æ‰¹é‡ embedding ç”Ÿæˆå¤±è´¥: {e}")
                import traceback

                traceback.print_exc()
                all_embeddings = [None] * len(queries_for_retrieval)
        else:
            if not self._embedding_generator:
                print("âš ï¸  æœªåˆå§‹åŒ– EmbeddingGeneratorï¼ŒæŸ¥è¯¢å°†æ—  embedding")
            elif not self._embedding_generator.is_available():
                print("âš ï¸  EmbeddingGenerator ä¸å¯ç”¨ï¼ŒæŸ¥è¯¢å°†æ—  embedding")
            all_embeddings = [None] * len(queries_for_retrieval)

        # æž„å»ºå…ƒæ•°æ®
        metadata = {
            "original_query": question,
            "expanded_queries": expanded_queries,
            "all_queries": queries_for_retrieval,
            "all_embeddings": all_embeddings,
            "merge_strategy": self.merge_strategy,
            "needs_embedding": True,
        }

        # å¦‚æžœé…ç½®äº†store_optimizedï¼Œå°†æ‰©å±•æŸ¥è¯¢å­˜å‚¨åˆ°metadata
        if self.store_optimized:
            metadata["optimized_queries"] = expanded_queries

        return PreRetrievalOutput(
            query=final_query,
            query_embedding=None,  # ç”±å¤–éƒ¨ç»Ÿä¸€ç”Ÿæˆ
            metadata=metadata,
            retrieve_mode="passive",
            retrieve_params={
                "multi_query": queries_for_retrieval if len(queries_for_retrieval) > 1 else None,
                "expanded_embeddings": all_embeddings,
                "merge_strategy": self.merge_strategy,
            },
        )
