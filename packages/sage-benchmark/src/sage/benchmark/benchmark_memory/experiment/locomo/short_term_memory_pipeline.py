"""ç®€å•çš„ LLM Pipeline ç¤ºä¾‹ - ä½¿ç”¨æœ¬åœ° vLLM æœåŠ¡

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•ï¼š
1. ä½¿ç”¨æœ¬åœ°å¯åŠ¨çš„ vLLM æœåŠ¡ï¼ˆQwen/Qwen3-8B-AWQï¼‰
2. åˆ›å»ºä¸€ä¸ªç®€å•çš„é—®ç­” Pipeline
3. é¡ºåºå¤„ç†å¤šä¸ªé—®é¢˜å¹¶æ˜¾ç¤ºç­”æ¡ˆ
"""

import queue
import sys
import time
from pathlib import Path

import yaml
from sage.common.core.functions.map_function import MapFunction
from sage.common.core.functions.sink_function import SinkFunction
from sage.common.core.functions.source_function import SourceFunction
from sage.common.utils.logging.custom_logger import CustomLogger
from sage.kernel.api.local_environment import LocalEnvironment
from sage.middleware.operators.rag import OpenAIGenerator, QAPromptor
from sage.platform.service import BaseService


class PipelineBridge:
    """Pipeline ä¹‹é—´çš„é€šä¿¡æ¡¥æ¢"""

    def __init__(self):
        self._queue = queue.Queue()
        self._closed = False

    def submit(self, payload):
        """æäº¤è¯·æ±‚åˆ° Pipeline"""
        if self._closed:
            raise RuntimeError("PipelineBridge is closed")
        response_q = queue.Queue()
        self._queue.put({"payload": payload, "response_queue": response_q})
        return response_q

    def next(self, timeout=0.1):
        """ä» Pipeline è·å–ä¸‹ä¸€ä¸ªè¯·æ±‚"""
        if self._closed:
            return None
        try:
            return self._queue.get(timeout=timeout)
        except queue.Empty:
            return None

    def close(self):
        """å…³é—­ Bridge"""
        self._closed = True


# ==================== LLM é—®ç­”æœåŠ¡ Pipeline ====================


class LLMSource(SourceFunction):
    """ä» bridge æ¥æ”¶é—®ç­”è¯·æ±‚"""

    def __init__(self, bridge):
        super().__init__()
        self.bridge = bridge

    def execute(self, data=None):
        if self.bridge._closed:
            return None
        request = self.bridge.next(timeout=0.1)
        return request if request else None


class LLMMap(MapFunction):
    """ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ"""

    def __init__(self, config):
        super().__init__()
        self.config = config

    def execute(self, data):
        if not data:
            return None

        payload = data["payload"]
        question = payload["question"]

        print("ğŸ”§ LLMMap: å¼€å§‹å¤„ç†é—®é¢˜...")

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ promptor é…ç½®
        promptor_config = self.config.get("promptor", {})
        if not promptor_config.get("template"):
            promptor_config[
                "template"
            ] = """ä½ æ˜¯ä¸€ä½å‹å¥½çš„å¥åº·åŠ©æ‰‹ã€‚è¯·ç®€æ´ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {{ question }}

è¯·æä¾›æœ‰å¸®åŠ©çš„å»ºè®®ï¼š"""

        promptor = QAPromptor(promptor_config)
        prompted = promptor.execute({"question": question})

        print("ğŸ“ Prompt å‡†å¤‡å®Œæˆï¼Œå¼€å§‹è°ƒç”¨ LLM...")

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ generator é…ç½®
        generator_config = self.config.get("generator", {}).get("vllm", {})
        generator = OpenAIGenerator(generator_config)
        generator.ctx = self.ctx

        # ç”Ÿæˆç­”æ¡ˆ
        answer = generator.execute(prompted)

        print("âœ… LLM ç”Ÿæˆå®Œæˆ")

        return {
            "payload": {"question": question, "answer": answer},
            "response_queue": data["response_queue"],
        }


class LLMSink(SinkFunction):
    """å°†ç­”æ¡ˆè¿”å›ç»™è°ƒç”¨è€…"""

    def execute(self, data):
        if not data:
            return
        data["response_queue"].put(data["payload"])


class LLMService(BaseService):
    """LLM æœåŠ¡ï¼šæ¥æ”¶é—®é¢˜ï¼Œè¿”å›ç­”æ¡ˆ"""

    def __init__(self, bridge, config):
        super().__init__()
        self.bridge = bridge
        self.config = config

    def ask(self, question_data):
        """å¤„ç†å•ä¸ªé—®é¢˜"""
        response_q = self.bridge.submit(question_data)
        return response_q.get(timeout=60.0)


# ==================== Controller Pipelineï¼ˆé¡ºåºå‘é€é—®é¢˜ï¼‰====================


class QuestionController(SourceFunction):
    """é¡ºåºå‘é€é—®é¢˜ï¼Œæ¯æ¬¡åªå‘é€ä¸€ä¸ª"""

    def __init__(self, config):
        super().__init__()
        self.questions = config.get("questions")
        self.max = config.get("max_index", len(self.questions))
        self.index = 0

    def execute(self, data=None):
        if self.index >= self.max:
            return None

        q = self.questions[self.index]
        self.index += 1

        return {"question": q, "index": self.index, "total": self.max}


class ProcessQuestion(MapFunction):
    """è°ƒç”¨ LLM Service å¤„ç†é—®é¢˜"""

    def execute(self, data):
        if not data:
            return None

        question = data["question"]
        index = data["index"]
        total = data.get("total", index)

        # æ‰“å°é—®é¢˜
        print(f"\n{'='*60}")
        print(f"ğŸ“ é—®é¢˜ {index}/{total}: {question}")
        print(f"{'='*60}")

        print("ğŸ”„ è°ƒç”¨ LLM Service...")

        # è°ƒç”¨ LLM Serviceï¼ˆé˜»å¡ç­‰å¾…ç­”æ¡ˆï¼‰
        result = self.call_service(
            "llm_service", {"question": question}, method="ask", timeout=120.0
        )

        print("âœ… æ”¶åˆ° LLM Service çš„å›ç­”")

        result["index"] = index
        return result


class DisplayAnswer(SinkFunction):
    """æ˜¾ç¤ºç­”æ¡ˆ"""

    def __init__(self, bridges=None, total_questions=5):
        super().__init__()
        self.bridges = bridges or []
        self.total_questions = total_questions
        self.processed_count = 0

    @staticmethod
    def _render_markdown(text):
        """ç®€å•çš„ Markdown æ¸²æŸ“ï¼Œç”¨äºç»ˆç«¯æ˜¾ç¤º"""
        import re

        lines = text.split("\n")
        formatted_lines = []

        for line in lines:
            # å¤„ç† ### ä¸‰çº§æ ‡é¢˜
            if line.startswith("###"):
                title = line.replace("###", "").strip()
                line = f"\033[1m{title}\033[0m"

            # å¤„ç† **åŠ ç²—**
            elif "**" in line:
                line = re.sub(r"\*\*(.+?)\*\*", r"\033[1m\1\033[0m", line)

            # å¤„ç†æ•°å­—åˆ—è¡¨é¡¹
            if re.match(r"^\d+\.\s+", line):
                line = re.sub(r"^(\d+)\.\s+", r"\033[1m\1.\033[0m ", line)

            # å¤„ç†ç¼©è¿›çš„ç ´æŠ˜å·åˆ—è¡¨é¡¹
            elif re.match(r"^\s+-\s+", line):
                line = re.sub(r"^(\s+)-\s+", r"\1\033[1m-\033[0m ", line)

            formatted_lines.append(line)

        return "\n".join(formatted_lines)

    def execute(self, data):
        if not data:
            return

        answer_data = data.get("answer", {})

        # æ˜¾ç¤ºç­”æ¡ˆ
        if isinstance(answer_data, dict):
            answer_text = answer_data.get("generated", str(answer_data))
            generate_time = answer_data.get("generate_time", 0)
        else:
            answer_text = str(answer_data)
            generate_time = 0

        # æ¸²æŸ“ Markdown
        rendered_answer = self._render_markdown(answer_text)

        print(f"\n{'='*60}")
        print("ğŸ’¡ AI å›ç­”:")
        print(f"{'='*60}")
        print(rendered_answer)
        print(f"{'='*60}")
        if generate_time > 0:
            print(f"â±ï¸  ç”Ÿæˆè€—æ—¶: {generate_time:.2f}ç§’")
        print()

        # æ›´æ–°å¤„ç†è®¡æ•°
        self.processed_count += 1

        # å¦‚æœæ‰€æœ‰é—®é¢˜éƒ½å¤„ç†å®Œäº†ï¼Œå…³é—­æ‰€æœ‰ bridges
        if self.processed_count >= self.total_questions:
            print(f"\nâœ… æ‰€æœ‰ {self.total_questions} ä¸ªé—®é¢˜å·²å¤„ç†å®Œæˆï¼Œå…³é—­ bridges...")
            for bridge in self.bridges:
                bridge.close()
            print("âœ… Bridges å·²å…³é—­ï¼ŒPipeline å³å°†åœæ­¢...")


def main():
    """ä¸»å‡½æ•°"""
    print("=== å¯åŠ¨ç®€å• LLM Pipeline ç¤ºä¾‹ ===\n")

    script_dir = Path(__file__).parent
    config_file = script_dir / "config" / "short_term_memory_pipeline.yaml"

    if not config_file.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        sys.exit(1)

    print(f"ğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
    with open(config_file, "r") as f:
        config = yaml.safe_load(f)

    print("ğŸ”§ åˆ›å»ºç¯å¢ƒ...")
    env = LocalEnvironment("simple_llm_pipeline")

    try:
        print("ğŸŒ‰ åˆ›å»º Pipeline Bridge...")
        llm_bridge = PipelineBridge()

        print("ğŸ“ æ³¨å†Œ LLM æœåŠ¡...")
        env.register_service("llm_service", LLMService, llm_bridge, config)

        print("ğŸ”— åˆ›å»º LLM Pipelineï¼ˆç”Ÿæˆç­”æ¡ˆï¼‰...")
        env.from_source(LLMSource, llm_bridge).map(LLMMap, config).sink(LLMSink)

        print("ğŸ® åˆ›å»º Controller Pipelineï¼ˆé¡ºåºå‘é€é—®é¢˜ï¼‰...")
        total_questions = config["source"].get("max_index", 5)
        bridges = [llm_bridge]
        env.from_source(QuestionController, config["source"]).map(ProcessQuestion).sink(
            DisplayAnswer, bridges, total_questions
        )

        print("\n" + "=" * 60)
        print("ğŸš€ å¯åŠ¨ LLM Pipeline...")
        print("=" * 60 + "\n")

        env.submit(autostop=False)

        # ç­‰å¾…è¶³å¤Ÿçš„æ—¶é—´è®©æ‰€æœ‰é—®é¢˜å¤„ç†å®Œæˆ
        expected_time = total_questions * 30 + 10  # æ¯ä¸ªé—®é¢˜é¢„è®¡ 30 ç§’
        print(f"â³ ç­‰å¾…æœ€å¤š {expected_time} ç§’è®©æ‰€æœ‰é—®é¢˜å¤„ç†å®Œæˆ...")

        # åˆ†æ®µç­‰å¾…ï¼Œæ¯ 5 ç§’æ£€æŸ¥ä¸€æ¬¡
        for i in range(0, expected_time, 5):
            time.sleep(5)
            elapsed = i + 5
            print(f"â±ï¸  å·²ç­‰å¾… {elapsed}/{expected_time} ç§’...")

        print("\n" + "=" * 60)
        print("âœ… Pipeline æ‰§è¡Œå®Œæˆ!")
        print("=" * 60)

    finally:
        print("\nğŸ›‘ åœæ­¢ Pipeline...")
        env.stop()

        print("ğŸ§¹ æ¸…ç†ç¯å¢ƒèµ„æº...")
        env.close()
        print("âœ… ç¯å¢ƒå·²æ¸…ç†ï¼Œç¨‹åºæ­£å¸¸é€€å‡º\n")


if __name__ == "__main__":
    print("=== ç¨‹åºå¼€å§‹æ‰§è¡Œ ===\n")
    CustomLogger.disable_global_console_debug()
    main()
    print("\n=== ç¨‹åºæ‰§è¡Œå®Œæ¯• ===")
