# ============================================================
# HippoRAG Pipeline 配置
# 论文: HippoRAG: Neurobiologically Inspired Long-Term Memory for LLMs
# 特点: Knowledge Graph + Synonym Edges + PPR 检索
# ============================================================

runtime:
  dataset: "locomo"
  memory_insert_verbose: false
  memory_test_verbose: true
  test_segments: 10

  prompt_template: |
    Based on the above context, answer the following question concisely using exact words from the context whenever possible.

    Question: {question}
    Answer:

  # 第五类问题专用 Prompt（选择题格式，更简洁）
  prompt_template_category5: |
    Based on the above context, answer the following question.

    Question: {question}
    Answer:

  # LLM 配置
  api_key: "token-abc123"
  base_url: "http://sage2:8000/v1"
  model_name: "/home/cyb/Llama-3.1-8B-Instruct"
  max_tokens: 512
  temperature: 0.3
  seed: 42
  memory_name: "HippoRAG"

  # Embedding 配置
  embedding_base_url: "http://localhost:8091/v1"
  embedding_model: "BAAI/bge-m3"

# ============================================================
# 服务配置
# ============================================================
services:
  register_memory_service: "graph_memory"
  graph_memory:
    graph_type: "knowledge_graph"     # HippoRAG 使用知识图谱
    triple_store: "igraph"
    node_embedding_dim: 1024
    edge_types: ["relation", "synonym", "temporal"]
    # 检索参数：增加起始节点和遍历深度以获取更多上下文
    retrieval_top_k: 20              # 初始向量检索返回数量
    num_start_nodes: 10              # 使用更多种子节点启动图遍历
    max_depth: 3                     # 增加遍历深度
  memory_retrieval_adapter: "none"

# ============================================================
# Operator 配置
# ============================================================
operators:
  # ----------------------------------------------------------
  # PreInsert: 三元组抽取（OpenIE 风格）
  # HippoRAG 的核心是从文本抽取 (S, P, O) 三元组
  # ----------------------------------------------------------
  pre_insert:
    action: "extract.triple"
    triple_extraction_prompt: |
      You are a knowledge graph builder. Extract ALL factual triples from the dialogue, focusing on information that could answer questions about WHO, WHAT, WHEN, WHERE, WHY, and HOW.

      CRITICAL Rules:
      1. Output format: (Subject, Predicate, Object) - one triple per line
      2. Resolve ALL pronouns (he, she, it, they, etc.) to actual entity names
      3. Extract TEMPORAL information as separate triples:
         - (Event, happened_on, time/date)
         - (Person, did_action_at, time)
      4. Extract LOCATION information:
         - (Event, took_place_at, location)
         - (Person, was_at, place)
      5. Extract ATTRIBUTES and PROPERTIES:
         - (Entity, has_property, value)
         - (Person, is_a, role/occupation)
      6. Extract RELATIONSHIPS between people:
         - (Person1, relationship_with, Person2)
      7. Extract REASONS and PURPOSES:
         - (Action, was_for, purpose)
         - (Person, supports, cause/group)
      8. Be EXHAUSTIVE - extract every fact, even implicit ones
      9. Use specific predicates (e.g., "painted_at_lake" not just "painted")

      Example:
      Dialogue: "Yesterday, Sarah told me she painted a sunrise at the lake for her mom's birthday."
      Triples:
      (Sarah, painted, sunrise)
      (Sarah, painted_at, lake)
      (painting, happened_on, yesterday)
      (painting, was_for, mom's birthday)
      (Sarah, has_mom, mentioned)

      Dialogue:
      {dialogue}

      Triples:

  # ----------------------------------------------------------
  # PostInsert: Synonym Edge 建立
  # HippoRAG 原版在 indexing 阶段结束后才一次性创建 synonym edges
  # 现在 PostInsert 会自动检测：仅在 session 结束或最后一个 packet 时执行
  # ----------------------------------------------------------
  post_insert:
    action: "link_evolution"
    link_policy: "synonym_edge"
    knn_k: 10
    similarity_threshold: 0.7
    edge_weight: 1.0

  # ----------------------------------------------------------
  # PreRetrieval: Query Embedding
  # HippoRAG 需要 embedding 来通过向量相似度找到图中的起始节点
  # ----------------------------------------------------------
  pre_retrieval:
    action: "embedding"
    # embedding 配置（使用默认的 embedding 模型）

  # ----------------------------------------------------------
  # PostRetrieval: PPR 重排序（暂时禁用）
  # HippoRAG 使用 Personalized PageRank 排序检索结果
  # 注意：PPR 在服务单线程下容易超时，暂时禁用
  # ----------------------------------------------------------
  post_retrieval:
    action: "none"
    # action: "rerank"
    # rerank_type: "ppr"
    # damping_factor: 0.5
    # max_iterations: 100
    # convergence_threshold: 1e-6
    # personalization_nodes: "query_entities"
    top_k: 10
    conversation_format_prompt: |
      The following is relevant knowledge from the graph.

# ============================================================
# HippoRAG 复现说明
# ============================================================
# 论文核心特性：
#   1. 知识图谱存储：(Subject, Predicate, Object) 三元组
#   2. Synonym Edges: 连接语义相近的节点
#   3. PPR 检索：Personalized PageRank 图遍历
#   4. 两阶段检索：向量查找起始节点 + PPR 扩展
#
# SAGE 实现方式：
# ┌─────────────────────────────────────────────────────────┐
# │ Pipeline 阶段      │ 对应 HippoRAG 操作   │ 实现组件     │
# ├─────────────────────────────────────────────────────────┤
# │ PreInsert          │ OpenIE Triple Extract│ extract.triple│
# │  - 三元组提取      │ (S, P, O) 抽取       │ LLM Prompt   │
# │  - 节点 Embedding    │ 为每个实体生成向量   │ BGE-M3       │
# │  - 谓词关系标注    │ 边类型标记          │ metadata     │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryInsert       │ Graph Insert         │ GraphMemory  │
# │  - 节点创建        │ 添加 S/O 实体节点    │ igraph       │
# │  - 边创建          │ 添加 P 关系边       │ edge_types   │
# │  - 向量存储        │ 节点 embedding       │ FAISS        │
# ├─────────────────────────────────────────────────────────┤
# │ PostInsert         │ Synonym Edge Build   │ link_evol    │
# │  - 语义相似计算    │ 找到相近节点        │ knn_k=10     │
# │  - 建立 synonym 边  │ 连接语义相近实体    │ threshold    │
# │  - 批量处理        │ session 结束时执行   │ auto-detect  │
# ├─────────────────────────────────────────────────────────┤
# │ PreRetrieval       │ Query Embedding      │ embedding    │
# │  - 查询向量化      │ 用于找起始节点      │ BGE-M3       │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryRetrieval    │ Vector + Graph       │ GraphMemory  │
# │  - 向量查起始节点  │ top-k 最相关节点    │ top_k=20     │
# │  - 图遍历          │ 从起始节点扩展      │ max_depth=3  │
# │  - 收集上下文      │ 返回相关三元组      │ num_start=10 │
# ├─────────────────────────────────────────────────────────┤
# │ PostRetrieval      │ PPR Rerank (禁用)   │ none         │
# │  - PPR 重排序      │ PageRank 权重       │ 超时禁用     │
# │  - 基础格式化      │ 拼接三元组          │ top_k=10     │
# └─────────────────────────────────────────────────────────┘
#
# 关键配置参数：
#   - graph_type="knowledge_graph": 知识图谱类型
#   - retrieval_top_k=20: 初始向量检索数量
#   - num_start_nodes=10: 图遍历起始节点数
#   - max_depth=3: 图遍历最大深度
#   - link_policy="synonym_edge": Synonym 边建立策略
#
# 与论文的差异：
#   1. 论文使用 ColBERT，SAGE 使用 BGE-M3（更通用）
#   2. PPR 重排序暂时禁用（单线程易超时）
#   3. SAGE 增加了更详细的三元组提取 prompt
#   4. SAGE 支持自动检测 session 结束才建立 synonym 边
#
# 运行示例：
#   python packages/sage-benchmark/.../memory_test_pipeline.py \
#     --config .../locomo_hipporag_pipeline.yaml --task_id conv-26
