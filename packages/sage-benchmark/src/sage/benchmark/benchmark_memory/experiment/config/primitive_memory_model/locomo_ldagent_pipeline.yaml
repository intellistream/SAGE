# ============================================================
# LD-Agent Pipeline 配置
# 论文: LD-Agent for Long-term Dialogue
# 特点: STM 缓存 + 事件摘要 LTM + 话题重叠检索
# ============================================================

runtime:
  dataset: "locomo"
  memory_insert_verbose: false
  memory_test_verbose: true
  test_segments: 10

  prompt_template: |
    Based on the above context, answer the following question concisely.

    Question: {question}
    Answer:

  # 第五类问题专用 Prompt（选择题格式，更简洁）
  prompt_template_category5: |
    Based on the above context, answer the following question.

    Question: {question}
    Answer:

  # LLM 配置
  # 配置1（Llama-3.1-8B，用于对比）：
  api_key: "token-abc123"
  base_url: "http://sage2:8000/v1"
  model_name: "/home/cyb/Llama-3.1-8B-Instruct"
  max_tokens: 256
  temperature: 0
  seed: 42

  # 新配置（PanGu Embedded 1B）：
  # api_key: "iloveshuhao"
  # base_url: "http://172.17.0.1:1040/v1"
  # model_name: "pangu_embedded_1b"
  # max_tokens: 256
  # temperature: 0
  # seed: 42
  memory_name: "LD-Agent"

  # Embedding 配置
  embedding_base_url: "http://localhost:8091/v1"
  embedding_model: "BAAI/bge-m3"

# ============================================================
# 服务配置
# ============================================================
services:
  register_memory_service: "hierarchical_memory"
  hierarchical_memory:
    tier_mode: "two_tier"             # 双层模式会自动使用 ["stm", "ltm"]
    tier_capacities:
      stm: 50                         # 当前会话缓存
      ltm: -1                         # 事件摘要库
    migration_policy: "time"          # 按时间迁移
    embedding_dim: 1024               # BAAI/bge-m3 输出 1024 维向量
  memory_retrieval_adapter: "none"

# ============================================================
# Operator 配置
# ============================================================
operators:
  # ----------------------------------------------------------
  # PreInsert: 原文中无插入前操作
  # 注意：事件摘要应在 PostInsert 的迁移时生成，而非插入前
  # ----------------------------------------------------------
  pre_insert:
    action: "none"

  # ----------------------------------------------------------
  # PostInsert: 会话间隔触发迁移
  # LD-Agent 在会话间隔>1小时时触发 STM→LTM 迁移
  # 此时才生成事件摘要并提取名词集合
  # ----------------------------------------------------------
  post_insert:
    action: "migrate"
    migrate_policy: "time"
    session_gap: 3600                # 1小时 = 3600秒
    upgrade_transform: "summarize"   # STM→LTM时生成摘要
    downgrade_transform: "none"
    summarize_prompt: |
      Summarize the key events from this dialogue as a brief event description.
      Focus on: What happened? Who was involved? What was decided/planned?

      Dialogue:
      {dialogue}

      Event summary:

  # ----------------------------------------------------------
  # PreRetrieval: 名词/关键词提取（用于话题重叠）
  # LD-Agent 使用 spaCy 提取名词集合
  # ----------------------------------------------------------
  pre_retrieval:
    action: "optimize"
    optimize_type: "keyword_extract"
    extractor: "spacy"
    spacy_model: "en_core_web_sm"     # spaCy 模型名称
    extract_types: ["NOUN", "PROPN"]
    max_keywords: 15

  # ----------------------------------------------------------
  # PostRetrieval: 多因子重排序（语义+话题+时间）
  # LD-Agent 使用综合得分排序
  # ----------------------------------------------------------
  post_retrieval:
    action: "rerank"
    rerank_type: "weighted"
    factors:
    - name: "relevance"
      weight: 0.4
      source: "embedding_similarity"
    - name: "recency"
      weight: 0.3
      decay_type: "exponential"
      decay_rate: 1e-7              # 原文值：1E-7（衰减非常缓慢）
    - name: "topic_overlap"
      weight: 0.3
      source: "keyword_jaccard"
    # 原文：STM检索30条，LTM检索1条最优记忆
    tier_retrieval_limits:
      stm: 30                       # context_memory_number
      ltm: 1                        # relevance_memory_number
    conversation_format_prompt: |
      <CONTEXT>
      Recent conversation:
      {stm_memories}

      <MEMORY>
      Related long-term memories:
      {ltm_memories}

# ============================================================
# LD-Agent 复现说明
# ============================================================
# 论文核心特性：
#   1. 双层记忆: STM 缓存 + LTM 事件摘要库
#   2. 事件摘要: 对对话生成事件描述（在会话间隔触发时）
#   3. 话题重叠检索: 基于名词集合的 Jaccard 相似度
#   4. 多因子重排序: 语义 + 时间 + 话题
#
# SAGE 实现方式（已按原文修正）：
# ┌─────────────────────────────────────────────────────────┐
# │ Pipeline 阶段      │ 对应 LD-Agent 操作    │ 实现组件     │
# ├─────────────────────────────────────────────────────────┤
# │ PreInsert          │ None                  │ none         │
# │  【修正】原文无插入前操作                                │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryInsert       │ Dual-tier Insert      │ Hierarchical │
# │  - STM 插入        │ 当前会话缓存          │ stm=50       │
# │  - LTM 预留        │ 事件摘要库            │ ltm=-1       │
# ├─────────────────────────────────────────────────────────┤
# │ PostInsert         │ Session-gap Migrate   │ migrate      │
# │  - 时间间隔判断    │ >1小时触发迁移        │ gap=3600s    │
# │  - 生成事件摘要    │ 整个会话摘要          │ summarize    │
# │  - 提取名词集合    │ 用于 LTM 存储         │ topics       │
# │  - STM → LTM       │ 摘要迁移到长期记忆    │ time policy  │
# ├─────────────────────────────────────────────────────────┤
# │ PreRetrieval       │ Keyword Extraction    │ optimize     │
# │  - 名词提取        │ spaCy NOUN/PROPN      │ spacy        │
# │  - 话题集合        │ 用于 Jaccard 计算     │ max_kw=15    │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryRetrieval    │ Multi-tier Search     │ Hierarchical │
# │  - 同时检索 STM/LTM│ 合并结果              │ FAISS        │
# │  【修正】STM=30条，LTM=1条最优记忆                        │
# ├─────────────────────────────────────────────────────────┤
# │ PostRetrieval      │ Multi-factor Rerank   │ rerank       │
# │  - 语义相似度      │ embedding similarity  │ weight=0.4   │
# │  - 时间衰减        │ exponential decay     │ weight=0.3   │
# │  【修正】decay_rate=1e-7（原文值，衰减非常缓慢）          │
# │  - 话题重叠        │ keyword Jaccard       │ weight=0.3   │
# │  - 加权融合        │ 综合排序              │ stm+ltm      │
# └─────────────────────────────────────────────────────────┘
#
# 关键配置参数：
#   - tier_mode="two_tier": 双层架构（STM/LTM）
#   - session_gap=3600: 会话间隔阈值（秒）
#   - extract_types=["NOUN", "PROPN"]: 名词和专有名词
#   - rerank factors: relevance(0.4) + recency(0.3, rate=1e-7) + topic(0.3)
#   - tier_retrieval_limits: stm=30, ltm=1
#
# 与原文的主要修正（2025-12-24）：
#   1. 【关键】移除 pre_insert 的摘要生成
#      - 原因：原文中摘要在会话间隔触发时（PostInsert）生成
#      - 每条对话生成摘要开销过大且不符合原文设计
#
#   2. 【关键】修正时间衰减参数：0.995 → 1e-7
#      - 原因：原文使用 decay_temp=1E-7，衰减非常缓慢
#      - 原配置衰减过快，不符合长期对话场景
#
#   3. 【关键】区分 STM/LTM 检索数量：统一10 → 30/1
#      - 原因：原文 context_memory_number=30, relevance_memory_number=1
#      - STM 需要较多上下文，LTM 只返回最优记忆
#
#   4. 【改进】完善 prompt 格式
#      - 添加结构化 CONTEXT + MEMORY 分区
#      - 便于后续添加 USER_TRAITS（人物画像功能待实现）
#
# 论文参考：
#   Hello Again! LLM-powered Personalized Agent for Long-term Dialogue
#   NAACL 2025, https://aclanthology.org/2025.naacl-long.272/
#
# 运行示例：
#   python packages/sage-benchmark/.../memory_test_pipeline.py \
#     --config .../locomo_ldagent_pipeline.yaml --task_id conv-26
