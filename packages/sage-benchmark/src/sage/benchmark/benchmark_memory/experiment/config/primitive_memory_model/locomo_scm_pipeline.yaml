# ============================================================
# SCM (Self-Controlled Memory) Pipeline 配置
# 论文: SCM4LLMs - Enhancing Large Language Model with Self-Controlled Memory Framework
# 特点: Memory Stream + Token Budget + 三元决策 (drop/summary/raw)
# ============================================================

runtime:
  dataset: "locomo"
  memory_insert_verbose: false
  memory_test_verbose: true
  test_segments: 10

  prompt_template: |
    Based on the above context, answer the following question concisely.

    Question: {question}
    Answer:

  # 第五类问题专用 Prompt（选择题格式，更简洁）
  prompt_template_category5: |
    Based on the above context, answer the following question.

    Question: {question}
    Answer:

  # LLM 配置
  # 配置1（Llama-3.1-8B，用于对比）：
  api_key: "token-abc123"
  base_url: "http://sage2:8000/v1"
  model_name: "/home/cyb/Llama-3.1-8B-Instruct"
  max_tokens: 256
  temperature: 0
  seed: 42

  # 新配置（PanGu Embedded 1B）：
  # api_key: "iloveshuhao"
  # base_url: "http://172.17.0.1:1040/v1"
  # model_name: "pangu_embedded_1b"
  # max_tokens: 256
  # temperature: 0
  # seed: 42
  memory_name: "SCM"

  # Embedding 配置
  embedding_base_url: "http://localhost:8091/v1"
  embedding_model: "BAAI/bge-m3"

# ============================================================
# 服务配置
# SCM 使用 Memory Stream，本质是一个大容量的短期记忆
# ============================================================
services:
  register_memory_service: "short_term_memory"
  short_term_memory:
    max_dialog: 1000  # Memory Stream 容量
    embedding_dim: 1024  # BAAI/bge-m3 模型的向量维度
    retrieval_top_k: 6  # SCM 默认 similar_top_k=6
  memory_retrieval_adapter: "none"

# ============================================================
# Operator 配置
# ============================================================
operators:
  # ----------------------------------------------------------
  # PreInsert: SCM 样式 - 同时保存原文、摘要、embedding
  # 参考 SCM4LLMs/dialogue_test.py - summarize_embed_one_turn()
  # ----------------------------------------------------------
  pre_insert:
    action: "scm_embed"
    # SCM 摘要 prompt (参考 SCM4LLMs/prompts/dialogue.py)
    summarize_prompt: |
      Below is a conversation between a user and an AI assistant. Please write a summary for each of them in one sentence and list them in separate paragraphs, while trying to preserve the key information of the user's question and the assistant's answer as much as possible.

      conversation content:

      {dialogue}

      Summary:

    # 是否对摘要做 embedding（SCM 默认对原文）
    embed_summary: false
    # 原文 token 数超过此值才生成摘要（SCM 默认 300）
    summary_threshold: 300

  # ----------------------------------------------------------
  # PostInsert: 无（SCM 不做插入后优化）
  # ----------------------------------------------------------
  post_insert:
    action: "none"

  # ----------------------------------------------------------
  # PreRetrieval: 基础 embedding
  # SCM 检索范围是 [0, 上一轮)，上一轮直接拼接
  # ----------------------------------------------------------
  pre_retrieval:
    action: "embedding"

  # ----------------------------------------------------------
  # PostRetrieval: SCM 三元决策
  # 参考 SCM4LLMs/core/chat.py - get_related_turn() + judge_drop_or_summary()
  # 超过 token budget 时对每条记忆判断: drop / summary / raw
  # ----------------------------------------------------------
  post_retrieval:
    action: "scm_three_way"
    # Token budget 配置（SCM 默认值）
    max_history_tokens: 2500  # SCM: MAX_HISTORY_TOKENS = 2500
    max_pre_turn_tokens: 500  # SCM: MAX_PRE_TURN_TOKENS = 500
    token_counter: "char"  # char | word | tiktoken

    # 三元决策 prompt (参考 SCM4LLMs/prompts/dialogue.py - judge_answerable_prompt)
    judge_prompt: |
      Given the [Conversation Content] and [User Question], please answer the instruction question.

      [Conversation Content]:
      ```
      {content}
      ```

      [User Question]:
      ```
      {query}
      ```

      Instruction Question:
      ```
      Based on [Conversation Content], can you answer [User Question]? If yes, please answer `(A) Yes`, otherwise please answer `(B) No`.
      ```

      Please answer now. The output must strictly follow this format:

      [Answer]: The final answer is: (A) Yes / (B) No

    # 对话格式化 prompt
    conversation_format_prompt: |
      The following is a conversation between a user and an AI assistant. Please answer the current question based on the history of the conversation:

      Related conversation history:

# ============================================================
# SCM (Self-Controlled Memory) 复现说明
# ============================================================
# 论文核心特性：
#   1. Memory Stream: 大容量短期记忆流
#   2. Token Budget: 上下文长度限制管理
#   3. 三元决策: drop / summary / raw（每条记忆）
#   4. 同时存储原文和摘要: 灵活选择使用哪个版本
#
# SAGE 实现方式：
# ┌─────────────────────────────────────────────────────────┐
# │ Pipeline 阶段      │ 对应 SCM 操作         │ 实现组件     │
# ├─────────────────────────────────────────────────────────┤
# │ PreInsert          │ Dual Storage          │ scm_embed    │
# │  - 原文存储        │ 保存原始对话          │ raw_text     │
# │  - 摘要生成        │ 超过阈值生成摘要      │ summarize    │
# │  - 向量化          │ embedding 原文        │ BGE-M3       │
# │  - Token 计数      │ 判断是否需要摘要      │ threshold    │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryInsert       │ Stream Insert         │ ShortTermMem │
# │  - 大容量 STM      │ max_dialog=1000       │ Memory Stream│
# │  - 同时存储        │ raw + summary         │ metadata     │
# ├─────────────────────────────────────────────────────────┤
# │ PostInsert         │ None                  │ none         │
# │  - 无后处理        │ SCM 不做插入优化      │              │
# ├─────────────────────────────────────────────────────────┤
# │ PreRetrieval       │ Query Embedding       │ embedding    │
# │  - 查询向量化      │ 用于相似度检索        │ BGE-M3       │
# │  - 检索范围        │ [0, 上一轮)           │ exclude_last │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryRetrieval    │ Similar Top-k         │ ShortTermMem │
# │  - 向量检索        │ 相似度排序            │ top_k=6      │
# │  - 排除上一轮      │ 上一轮直接拼接        │ auto-exclude │
# ├─────────────────────────────────────────────────────────┤
# │ PostRetrieval      │ Three-way Decision    │ scm_three_way│
# │  - Token Budget    │ max_history=2500      │ token_limit  │
# │  - LLM 判断        │ (A) Yes = raw         │ judge_prompt │
# │  - LLM 判断        │ (B) No → drop/summary │ judge_prompt │
# │  - 上一轮直接加    │ 最近对话不判断        │ append_last  │
# └─────────────────────────────────────────────────────────┘
#
# 关键配置参数：
#   - max_dialog=1000: Memory Stream 容量
#   - retrieval_top_k=6: SCM 论文默认值
#   - summary_threshold=300: 超过 300 token 生成摘要
#   - max_history_tokens=2500: SCM 默认 context budget
#   - max_pre_turn_tokens=500: 上一轮最大 token 数
#
# 与论文的差异：
#   1. 论文使用 CharacterTextSplitter 分词，SAGE 使用 char 计数
#   2. SAGE 的 judge_prompt 直接采用论文原始 prompt
#   3. SAGE 支持配置化的 token budget 和阈值
#   4. SAGE 增加了 scm_embed 和 scm_three_way 专用算子
#
# 运行示例：
#   python packages/sage-benchmark/.../memory_test_pipeline.py \
#     --config .../locomo_scm_pipeline.yaml --task_id conv-26
