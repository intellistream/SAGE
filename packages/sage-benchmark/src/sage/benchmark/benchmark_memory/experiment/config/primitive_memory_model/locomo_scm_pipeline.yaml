# ============================================================
# SCM (Self-Controlled Memory) Pipeline 配置
# 论文: SCM4LLMs - Enhancing Large Language Model with Self-Controlled Memory Framework
# 特点: Memory Stream + Token Budget + 三元决策 (drop/summary/raw)
# ============================================================

runtime:
  dataset: "locomo"
  memory_insert_verbose: false
  memory_test_verbose: true
  test_segments: 10

  prompt_template: |
    Based on the context above, provide a brief, direct answer to the question. No explanation, no reasoning, no elaboration - just the answer.

    Question: {question}
    Answer:

  # 第五类问题专用 Prompt（选择题格式，更简洁）
  prompt_template_category5: |
    Answer the multiple choice question based on the context. Provide ONLY the answer in format (a) or (b) without any explanation.

    Question: {question}
    Answer:

  # LLM 配置
  # 配置1（Llama-3.1-8B，用于对比）：
  # api_key: "token-abc123"
  # base_url: "http://sage2:8000/v1"
  # model_name: "/home/cyb/Llama-3.1-8B-Instruct"
  # max_tokens: 256
  # temperature: 0
  # seed: 42

  # 新配置（PanGu Embedded 1B）：
  api_key: "iloveshuhao"
  base_url: "http://172.17.0.1:1040/v1"
  model_name: "pangu_embedded_1b"
  max_tokens: 256
  temperature: 0
  seed: 42
  memory_name: "SCM"

  # Embedding 配置
  embedding_base_url: "http://localhost:8091/v1"
  embedding_model: "BAAI/bge-m3"

# ============================================================
# 服务配置
# SCM 使用 Memory Stream，本质是一个大容量的短期记忆
# ============================================================
services:
  services_type: "partitional.fifo_queue"
  fifo_queue:
    max_size: 1000  # Memory Stream 容量（对应原 max_dialog）
    vector_dim: 1024  # BAAI/bge-m3 模型的向量维度
  memory_retrieval_adapter: "none"

# ============================================================
# Operator 配置
# ============================================================
operators:
  # ----------------------------------------------------------
  # PreInsert: SCM 样式 - 同时保存原文与摘要 + 向量化
  # 实现：使用 transform.summarize 动作，并通过配置对齐 SCM 行为
  # 参考 SCM4LLMs/dialogue_demo.py::summarize_embed_one_turn()
  # ----------------------------------------------------------
  pre_insert:
    action: "transform"
    transform_type: "summarize"
    # SCM 摘要 prompt (参考 SCM4LLMs/prompts/dialogue.py)
    summarize_prompt: |
      Below is a conversation between a user and an AI assistant. Please write a summary for each of them in one sentence and list them in separate paragraphs, while trying to preserve the key information of the user’s question and the assistant’s answer as much as possible.

      conversation content:

      {input}

      Summary:

    # 是否对摘要做 embedding（SCM 默认对原文，因此这里为 false）
    embed_summary: false
    # 原文长度超过此阈值才生成摘要（与 SCM 论文中 300 token 对齐；此处以字符近似）
    summary_threshold: 300
    # 是否仅在会话结束时生成摘要（SCM 每轮即可摘要，置为 false）
    only_on_session_end: false

  # ----------------------------------------------------------
  # PostInsert: 无（SCM 不做插入后优化）
  # ----------------------------------------------------------
  post_insert:
    action: "none"

  # ----------------------------------------------------------
  # PreRetrieval: 基础 embedding
  # SCM 检索范围是 [0, 上一轮)，上一轮直接拼接
  # ----------------------------------------------------------
  pre_retrieval:
    action: "embedding"

  # ----------------------------------------------------------
  # PostRetrieval: SCM 三元决策
  # 参考 SCM4LLMs/core/chat.py - get_related_turn() + judge_drop_or_summary()
  # 超过 token budget 时对每条记忆判断: drop / summary / raw
  # ----------------------------------------------------------
  post_retrieval:
    action: "scm_three_way"
    # Token budget 配置（SCM 默认值）
    max_history_tokens: 2500  # SCM: MAX_HISTORY_TOKENS = 2500
    max_pre_turn_tokens: 500  # SCM: MAX_PRE_TURN_TOKENS = 500
    token_counter: "tiktoken"  # char | word | tiktoken

    # 三元决策 prompt (参考 SCM4LLMs/prompts/dialogue.py - judge_answerable_prompt)
    judge_prompt: |
      Given the [Conversation Content] and [User Question], please answer the instruction question.

      [Conversation Content]:
      ```
      {content}
      ```

      [User Question]:
      ```
      {query}
      ```

      Instruction Question:
      ```
      Based on [Conversation Content], can you answer [User Question]? If yes, please answer `(A) Yes`, otherwise please answer `(B) No`.
      ```

      Please answer now. The output must strictly follow this format:

      [Answer]: The final answer is: (A) Yes / (B) No

    # 对话格式化 prompt
    conversation_format_prompt: |
      The following is a conversation between a user and an AI assistant. Please answer the current question based on the history of the conversation:

      Related conversation history:

# ============================================================
# SCM (Self-Controlled Memory) 复现说明
# ============================================================
# 论文核心特性：
#   1. Memory Stream: 大容量短期记忆流
#   2. Token Budget: 上下文长度限制管理
#   3. 三元决策: drop / summary / raw（每条记忆）
#   4. 同时存储原文和摘要: 灵活选择使用哪个版本
#
# SAGE 实现方式：
# ┌─────────────────────────────────────────────────────────┐
# │ Pipeline 阶段      │ 对应 SCM 操作         │ 实现组件     │
# ├─────────────────────────────────────────────────────────┤
# │ PreInsert          │ Dual Storage          │ transform    │
# │  - 原文存储        │ 保存原始对话          │ raw_text     │
# │  - 摘要生成        │ 超过阈值生成摘要      │ summarize    │
# │  - 向量化          │ embedding 原文        │ BGE-M3       │
# │  - Token 计数      │ 判断是否需要摘要      │ threshold    │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryInsert       │ Stream Insert         │ ShortTermMem │
# │  - 大容量 STM      │ max_dialog=1000       │ Memory Stream│
# │  - 同时存储        │ raw + summary         │ metadata     │
# ├─────────────────────────────────────────────────────────┤
# │ PostInsert         │ None                  │ none         │
# │  - 无后处理        │ SCM 不做插入优化      │              │
# ├─────────────────────────────────────────────────────────┤
# │ PreRetrieval       │ Query Embedding       │ embedding    │
# │  - 查询向量化      │ 用于相似度检索        │ BGE-M3       │
# │  - 检索范围        │ [0, 上一轮)           │ exclude_last │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryRetrieval    │ Similar Top-k         │ ShortTermMem │
# │  - 向量检索        │ 相似度排序            │ top_k=6      │
# │  - 排除上一轮      │ 上一轮直接拼接        │ exclude_last │
# ├─────────────────────────────────────────────────────────┤
# │ PostRetrieval      │ Three-way Decision    │ scm_three_way│
# │  - Token Budget    │ max_history=2500      │ token_limit  │
# │  - LLM 判断        │ (A) Yes = raw         │ judge_prompt │
# │  - LLM 判断        │ (B) No → drop/summary │ judge_prompt │
# │  - 上一轮直接加    │ 最近对话不判断        │ append_last  │
# └─────────────────────────────────────────────────────────┘
#
# 关键配置参数：
#   - max_dialog=1000: Memory Stream 容量
#   - retrieval_top_k=6: SCM 论文默认值
#   - summary_threshold=300: 超过 300 token 生成摘要
#   - max_history_tokens=2500: SCM 默认 context budget
#   - max_pre_turn_tokens=500: 上一轮最大 token 数
#
# 与论文的差异：
#   1. 论文使用 CharacterTextSplitter 分词，SAGE 使用 char 计数
#   2. SAGE 的 judge_prompt 直接采用论文原始 prompt
#   3. SAGE 支持配置化的 token budget 和阈值
#   4. SAGE 增加了 scm_embed 和 scm_three_way 专用算子

# 正确实现参考：
#   - 代码路径：SCM4LLMs
#   - 关键脚本：SCM4LLMs/core/chat.py、SCM4LLMs/prompts/dialogue.py、SCM4LLMs/dialogue_demo.py
#   - 行为基线：Memory Stream、Token Budget 管理、逐条记忆三元决策（drop/summary/raw）

# 与正确实现的差异：
#   - Token 计数：SCM4LLMs 使用模型 tokenizer/tiktoken；SAGE 支持 tiktoken，同时提供 char 近似以便在本地缺少 tokenizer 时运行
#   - 决策逻辑：SCM4LLMs 在 get_related_turn / judge_drop_or_summary 中综合多因素；SAGE 通过 scm_three_way 算子复刻核心流程但内部实现更轻量
#   - 检索策略：两者均将“上一轮对话”直接拼接、历史部分做相似检索；SAGE 的阈值与 top-k 可通过配置灵活修改
#
# 运行示例：
#   python packages/sage-benchmark/.../memory_test_pipeline.py \
#     --config .../locomo_scm_pipeline.yaml --task_id conv-26
