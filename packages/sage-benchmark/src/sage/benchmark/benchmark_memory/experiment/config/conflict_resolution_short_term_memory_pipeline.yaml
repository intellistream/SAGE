# Runtime parameters configuration
runtime:
  dataset: "conflict_resolution"  # Dataset name
  # task_id: "task_all"  # Task ID, can be overridden by --task_id command line argument
  memory_insert_verbose: false  # Whether to print memory insert details (Memory Source part)
  memory_test_verbose: true    # Whether to print memory test details (QA part)
  test_segments: 10             # Number of test segments, divide total questions into segments for testing

  # Prompt template configuration (Phase 2: MemoryTest)
  prompt_template: |
    Based on the above context, answer the following question concisely using exact words from the context whenever possible. If the information is not mentioned in the conversation, respond with "Not mentioned in the conversation".

    Question: {question}
    Answer:

  # LLM Generator configuration
  api_key: "token-abc123"
  base_url: "http://sage2:8000/v1"
  model_name: "/home/cyb/Llama-3.1-8B-Instruct"
  max_tokens: 80
  temperature: 0.3
  seed: 42
  memory_name: "stm"

# Service configuration
services:
  services_type: "partitional.fifo_queue"

  fifo_queue:
    # === 基础配置 ===
    max_capacity: 5530                # FIFO 队列最大容量（设置为总 facts 数，以便测试可以访问所有 facts）
    vector_dim: 1024                  # BGE-M3 向量维度（如果需要向量检索）

    # === 检索配置 ===
    retrieval_mode: "recent_first"    # 检索模式：recent_first（最新优先）
    retrieval_top_k: 10               # 默认检索数量

  memory_retrieval_adapter: "none"



# Operator configuration
operators:
  pre_insert:
    action: "none"  # Pre-insert action: none, validate, transform
  pre_retrieval:
    action: "none"  # Pre-retrieval action: none, optimize, validate
  post_insert:
    action: "none"  # Post-insert action
  post_retrieval:
    action: "none"  # Post-retrieval action: none (default will format conversation history)
    # Phase 1: Conversation history formatting Prompt (PostRetrieval)
    conversation_format_prompt: |
      The following is some history information.
