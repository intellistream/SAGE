# 运行时参数配置
runtime:
  dataset: "locomo"  # 数据集名称
  # task_id: "conv-26"  # 任务ID，可通过命令行 --task_id 覆盖
  memory_insert_verbose: false  # 是否打印记忆插入的详细信息（Memory Source部分）
  memory_test_verbose: true    # 是否打印记忆测试的详细信息（QA部分）
  test_segments: 10             # 测试分段数，将总问题数分成几段进行测试
  service_timeout: 600.0        # 服务调用超时时间（秒）- TiM需要LLM三元组提取和蒸馏操作，需要更长超时

  # Prompt 模板配置（阶段二：MemoryTest）
  prompt_template: |
    Based on the above context, answer the following question concisely.

    Question: {question}
    Answer:

  # 第五类问题专用 Prompt（选择题格式，更简洁）
  prompt_template_category5: |
    Based on the above context, answer the following question.

    Question: {question}
    Answer:

  # LLM Generator 配置
  # 配置1（Llama-3.1-8B，用于对比）：
  api_key: "token-abc123"
  base_url: "http://sage2:8000/v1"
  model_name: "/home/cyb/Llama-3.1-8B-Instruct"
  max_tokens: 256
  temperature: 0
  seed: 42

  # 新配置（PanGu Embedded 1B）：
  # api_key: "iloveshuhao"
  # base_url: "http://172.17.0.1:1040/v1"
  # model_name: "pangu_embedded_1b"
  # max_tokens: 256
  # temperature: 0
  # seed: 42
  memory_name: "TiM"

  # Embedding 配置
  embedding_base_url: "http://localhost:8091/v1"  # Embedding 服务器地址
  embedding_model: "BAAI/bge-m3"  # Embedding 模型名称



# 服务配置
services:
  register_memory_service: "vector_memory"  # 统一向量记忆服务（推荐）
  vector_memory:
    dim: 1024  # 向量维度（需要与 embedding 模型输出维度一致）
    index_type: "IndexLSH"  # TiM 使用 LSH 哈希索引
    index_config:
      nbits: 128  # LSH 哈希位数（推荐值：64-256，值越大精度越高但速度越慢）
      rotate_data: true  # 是否旋转数据（提高哈希质量）
      train_thresholds: false  # 是否训练阈值
  memory_retrieval_adapter: "none"



# Operator 配置
operators:
  pre_insert:
    action: "extract.triple"  # 先通过llm进行三元组提取实体后进行embedding
    # Use LLM-based triple extraction (falls back to simple if not available)
    extraction_method: "llm"
    max_triplets: 10
    keep_original: false  # TiM论文：只存储三元组，不存储原始对话
    triple_extraction_prompt: |
      You are a factual knowledge extractor.
      Analyze the dialogue below and extract all subject–predicate–object triples that represent explicitly stated or directly inferable real-world facts about people, organizations, objects, preferences, roles, locations, activities, or states.

      Guidelines:
      - Map "I", "me", "my" to the name of the speaker provided in the dialogue log.
      - Include personal states, plans, or situations if clearly described (e.g., "I'm busy" → (Speaker, is, busy)).
      - Handle colloquialisms and omitted subjects: If a subject is implied but omitted, infer the subject from context.
      - Resolve pronouns to specific entities; if the referent is ambiguous, skip the triple. Never output pronouns as subject or object.
      - Avoid redundancy by consolidating repeated or fragmented details into a single, most specific triple, and ignore non-informative conversational fillers.
      - Use natural-language predicates in active voice. Capture specific nuances like time and location directly in the predicate (e.g., "starts at 5 PM", "lives in Tokyo").
      - Do NOT assume external knowledge; base everything on what is said or immediately implied.

      Output ONLY in this format:

      If one or more facts exist:
      (Subject, Predicate, Object)
      ...

      If no extractable facts:
      None

      Dialogue:
      {dialogue}
  pre_retrieval:
    action: "embedding"  # 检索前操作: none, optimize, validate
  post_insert:
    action: "distillation"  # 插入后操作
    # TiM 数据量驱动的蒸馏策略（不依赖底层服务阈值实现）
    retrieve_count: 10      # 检索候选记忆的数量（降低以减少处理时间）
    min_merge_count: 5      # 最少需要多少条相似记忆才执行蒸馏（降低阈值，更早触发合并）
    merge_prompt: |
      You are a memory consolidation expert. Given a list of facts about a person, your job is to:
      1. REMOVE duplicates and near-duplicates (same meaning, different wording)
      2. MERGE related facts into more complete statements
      3. REMOVE logically subsumed facts (if A contains B's meaning, remove B)

      EXAMPLES of what to consolidate:
      - "Alice likes coffee" + "Alice likes coffee a lot" → keep only "Alice likes coffee a lot"
      - "Bob works at Google" + "Bob works at Google as engineer" → keep only "Bob works at Google as engineer"
      - "Carol plans to travel" + "Carol plans to travel to Japan" → keep only "Carol plans to travel to Japan"

      RULES:
      - to_delete: List EXACT original texts to remove (duplicates, subsumed facts)
      - to_insert: Only if you need to CREATE a new merged fact not in original list
      - Prefer keeping the most informative version rather than creating new text
      - If a fact appears multiple times, delete all but one

      Facts:
      {memories}

      Respond with JSON only:
      {"to_delete": ["exact text 1", "exact text 2"], "to_insert": []}
  post_retrieval:
    action: "none"  # 后检索操作: none（默认会格式化对话历史）
    # 阶段一：对话历史格式化Prompt（PostRetrieval）
    conversation_format_prompt: |
      The following is some history information.

# ============================================================
# TiM (Think-in-Memory) 复现说明
# ============================================================
# 论文核心特性：
#   1. 归纳性思想（Inductive Thoughts）：以三元组形式存储
#   2. LSH 哈希索引：快速定位语义相近的记忆
#   3. 后思考（Post-thinking）：插入后进行记忆优化
#   4. 两阶段检索：LSH 定位候选桶 + 桶内相似度计算
#
# SAGE 实现方式：
# ┌─────────────────────────────────────────────────────────┐
# │ Pipeline 阶段      │ 对应 TiM 操作        │ 实现组件     │
# ├─────────────────────────────────────────────────────────┤
# │ PreInsert          │ Post-thinking        │ extract.triple│
# │  - 三元组提取      │ 生成归纳性思想        │ LLM Prompt   │
# │  - Embedding       │ 向量化以备 LSH        │ BGE-M3       │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryInsert       │ Insert (LSH)         │ VectorMemory │
# │  - LSH 哈希        │ 计算哈希桶            │ index_type=  │
# │  - 存入对应桶      │ 追加到 bucket         │  IndexLSH    │
# ├─────────────────────────────────────────────────────────┤
# │ PostInsert         │ Forget + Merge       │ distillation │
# │  - 检索相似记忆    │ 查询同桶内容          │ topk=10      │
# │  - LLM 判断操作    │ 去重/合并/删除矛盾    │ LLM Prompt   │
# │  - 执行删除/插入   │ 一次性更新桶内容      │ Service      │
# ├─────────────────────────────────────────────────────────┤
# │ PreRetrieval       │ Query Embedding      │ embedding    │
# │  - 查询向量化      │ 用于 LSH 计算         │ BGE-M3       │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryRetrieval    │ Two-stage Retrieve   │ VectorMemory │
# │  - LSH 定位桶      │ 找到候选 bucket       │ Hamming Dist │
# │  - 桶内相似度计算  │ top-k 最相关记忆      │ Cosine Sim   │
# ├─────────────────────────────────────────────────────────┤
# │ PostRetrieval      │ Context Integration  │ none         │
# │  - 基础格式化      │ 拼接为 prompt         │ 默认格式     │
# └─────────────────────────────────────────────────────────┘
#
# 关键配置参数：
#   - index_type: IndexLSH (TiM 使用 LSH 哈希索引)
#   - nbits: 128 (LSH 哈希位数，影响精度和速度的平衡)
#   - distillation: 记忆蒸馏策略（去重/合并相似记忆）
#
# 索引类型对比：
#   - IndexLSH (TiM): 快速近似检索，适合大规模数据
#   - IndexHNSWFlat: 更高精度，但速度稍慢
#   - IndexFlatL2: 精确 KNN，适合小规模数据
#
# 运行示例：
#   python packages/sage-benchmark/.../memory_test_pipeline.py \
#     --config .../locomo_tim_pipeline.yaml --task_id conv-26
