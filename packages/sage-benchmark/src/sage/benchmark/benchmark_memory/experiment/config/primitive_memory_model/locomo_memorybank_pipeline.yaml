# ============================================================
# MemoryBank Pipeline 配置
# 论文: MemoryBank: Enhancing Large Language Models with Long-Term Memory
# 特点: 分层记忆 + Ebbinghaus 遗忘 + 多层摘要
# ============================================================

runtime:
  dataset: "locomo"
  memory_insert_verbose: false
  memory_test_verbose: true
  test_segments: 10

  prompt_template: |
    Based on the above context, answer the following question concisely.

    Question: {question}
    Answer:

  # 第五类问题专用 Prompt（选择题格式，更简洁）
  prompt_template_category5: |
    Based on the above context, answer the following question.

    Question: {question}
    Answer:

  # LLM 配置
  api_key: "token-abc123"
  base_url: "http://sage2:8000/v1"
  model_name: "/home/cyb/Llama-3.1-8B-Instruct"
  max_tokens: 512
  temperature: 0.3
  seed: 42
  memory_name: "MemoryBank"

  # Embedding 配置
  embedding_base_url: "http://localhost:8091/v1"
  embedding_model: "BAAI/bge-m3"

# ============================================================
# 服务配置
# ============================================================
services:
  register_memory_service: "hierarchical_memory"
  hierarchical_memory:
    tier_mode: "three_tier"           # 三层: STM / MTM / LTM
    tier_names: ["stm", "mtm", "ltm"]
    tier_capacities:
      stm: 50
      mtm: 500
      ltm: -1                         # 无限
    migration_policy: "heat"          # 基于热度迁移
    migration_threshold: 0.7
    embedding_dim: 1024               # bge-m3 模型输出 1024 维向量
  memory_retrieval_adapter: "none"

# ============================================================
# Operator 配置
# ============================================================
operators:
  # ----------------------------------------------------------
  # PreInsert: 生成摘要 + 重要性评分
  # MemoryBank 会对每轮对话生成 daily summary
  # ----------------------------------------------------------
  pre_insert:
    action: "transform"
    transform_type: "summarize"
    summarize_prompt: |
      Summarize the key events and information from this conversation.
      Focus on: facts mentioned, preferences expressed, plans discussed.
      Keep the summary concise but informative.

      Conversation:
      {dialogue}

      Summary:

  # ----------------------------------------------------------
  # PostInsert: Ebbinghaus 遗忘 + 层间迁移
  # MemoryBank 使用遗忘曲线管理记忆强度
  # ----------------------------------------------------------
  post_insert:
    action: "forgetting"
    decay_type: "ebbinghaus"          # 艾宾浩斯遗忘曲线
    initial_strength: 1.0
    forgetting_curve: "exponential"
    review_boost: 0.5                 # 被检索时增强
    retention_min: 50                 # 最少保留条数
    archive_before_delete: true

  # ----------------------------------------------------------
  # PreRetrieval: 基础 embedding
  # ----------------------------------------------------------
  pre_retrieval:
    action: "embedding"

  # ----------------------------------------------------------
  # PostRetrieval: 时间加权重排序 + 增强（添加全局画像）
  # MemoryBank 会同时返回相关记忆 + 全局用户画像
  # ----------------------------------------------------------
  post_retrieval:
    action: "rerank"
    rerank_type: "time_weighted"
    time_decay_rate: 0.1
    time_field: "timestamp"
    top_k: 10
    conversation_format_prompt: |
      The following is the user's conversation history and profile.

# ============================================================
# MemoryBank 复现说明
# ============================================================
# 论文核心特性：
#   1. 多层级存储：原始对话 + 事件摘要 + 用户画像
#   2. Ebbinghaus 遗忘曲线：基于记忆强度的衰减
#   3. 检索强化：被检索的记忆强度增加
#   4. 全局画像：每日人格分析 + 全局用户画像
#
# SAGE 实现方式：
# ┌─────────────────────────────────────────────────────────┐
# │ Pipeline 阶段      │ 对应 MemoryBank 操作 │ 实现组件     │
# ├─────────────────────────────────────────────────────────┤
# │ PreInsert          │ Event Summarization  │ transform    │
# │  - 对话摘要        │ Daily Event Summary  │ summarize    │
# │  - Embedding       │ 摘要向量化           │ BGE-M3       │
# │  - 主动插入标记    │ target_tier="ltm"    │ insert_mode  │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryInsert       │ Multi-tier Insert    │ Hierarchical │
# │  - STM: 原始对话   │ 存入短期记忆         │ capacity=50  │
# │  - LTM: 摘要       │ 摘要主动写入 LTM     │ capacity=-1  │
# ├─────────────────────────────────────────────────────────┤
# │ PostInsert         │ Forgetting Mechanism │ forgetting   │
# │  - 计算记忆强度    │ S = 初始强度 + 检索次数 │ ebbinghaus │
# │  - 应用遗忘曲线    │ R = exp(-t/S)        │ exponential  │
# │  - 删除低强度记忆  │ R < threshold 时删除 │ retention_min│
# │  - 检索增强        │ 被检索时 +0.5        │ review_boost │
# ├─────────────────────────────────────────────────────────┤
# │ PreRetrieval       │ Query Embedding      │ embedding    │
# │  - 查询向量化      │ 用于向量检索         │ BGE-M3       │
# ├─────────────────────────────────────────────────────────┤
# │ MemoryRetrieval    │ Dense Retrieval      │ Hierarchical │
# │  - 多层检索        │ 同时搜索 STM/MTM/LTM │ FAISS        │
# │  - 相似度排序      │ Cosine Similarity    │ top_k        │
# ├─────────────────────────────────────────────────────────┤
# │ PostRetrieval      │ Context Integration  │ rerank       │
# │  - 时间加权        │ 近期记忆权重更高     │ time_decay   │
# │  - 重排序          │ 综合相似度+时间      │ top_k=10     │
# └─────────────────────────────────────────────────────────┘
#
# 关键配置参数：
#   - tier_mode="three_tier": 三层架构（STM/MTM/LTM）
#   - decay_type="ebbinghaus": 艾宾浩斯遗忘曲线
#   - initial_strength=1.0: 初始记忆强度
#   - review_boost=0.5: 检索时的强度增量
#   - time_decay_rate=0.1: 时间衰减率
#
# 与论文的差异：
#   1. 论文使用 MiniLM/Text2vec，SAGE 使用 BGE-M3（更强）
#   2. 论文固定摘要策略，SAGE 支持可配置的摘要 prompt
#   3. SAGE 增加了热度迁移（migration_policy="heat"）
#
# 运行示例：
#   python packages/sage-benchmark/.../memory_test_pipeline.py \
#     --config .../locomo_memorybank_pipeline.yaml --task_id conv-26
