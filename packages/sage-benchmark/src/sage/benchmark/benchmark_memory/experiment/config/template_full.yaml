# ============================================================
# SAGE Memory Pipeline 完整配置模板
# ============================================================
# 说明：
# - 所有标记为 REQUIRED 的字段必须填写，缺失将导致程序启动失败
# - 所有 prompt 字段支持 {placeholder} 格式的变量替换
# - 注释中的 "when ..." 表示该字段在特定条件下必需
# ============================================================

# ============================================================
# 运行时基础配置
# ============================================================
runtime:
  dataset: ""                    # REQUIRED: 数据集名称
  # task_id: ""                  # 任务ID，可通过命令行覆盖
  memory_insert_verbose: false   # 是否打印记忆插入详细信息
  memory_test_verbose: true      # 是否打印记忆测试详细信息
  test_segments: 10              # 测试分段数

  # LLM 服务配置
  api_key: ""                    # REQUIRED: LLM API Key
  base_url: ""                   # REQUIRED: LLM 服务地址
  model_name: ""                 # REQUIRED: LLM 模型名称
  max_tokens: 512                # 生成最大 token 数
  temperature: 0.7               # 生成温度
  seed: 42                       # 随机种子

  # Embedding 服务配置
  embedding_base_url: ""         # REQUIRED when action needs embedding
  embedding_model: ""            # REQUIRED when action needs embedding
  
  # 测试 Prompt 模板
  prompt_template: |
    Based on the above context, answer the following question concisely using exact words from the context whenever possible. If the information is not mentioned in the conversation, respond with "Not mentioned in the conversation".

    Question: {question}
    Answer:

# ============================================================
# 服务配置
# ============================================================
services:
  register_memory_service: ""    # REQUIRED: 存储后端名称 (short_term_memory, long_term_memory, etc.)
  graph_memory_service: ""       # 图存储服务名称 (用于 link_evolution)
  stm_service: "short_term_memory"   # 短期记忆服务
  mtm_service: "mid_term_memory"     # 中期记忆服务
  ltm_service: "long_term_memory"    # 长期记忆服务

  short_term_memory:
    max_dialog: 3                # 短期记忆窗口大小

  memory_insert_adapter: "to_dialogs"
  memory_retrieval_adapter: "none"

# ============================================================
# D2: PreInsert 算子配置
# ============================================================
operators:
  pre_insert:
    # REQUIRED: 操作类型
    # - none: 直接透传
    # - tri_embed: HippoRAG 三元组提取 + embedding
    # - transform: 内容转换 (chunking, topic_segment, fact_extract, summarize, compress)
    # - extract: 信息抽取 (keyword, entity, noun, persona, all)
    # - score: 重要性评分 (importance, emotion)
    # - multi_embed: 多维向量编码
    # - validate: 输入验证
    action: "none"

    # -------------------- tri_embed action --------------------
    # REQUIRED when action=tri_embed
    triple_extraction_prompt: |
      Extract knowledge triples from the following conversation.
      Each triple should be in the format (subject, predicate, object).

      Conversation:
      {dialogue}

      Return a JSON list of triples.
      Example: [{"subject": "Alice", "predicate": "works at", "object": "Google"}]

      Triples:

    # -------------------- transform action --------------------
    # transform_type: chunking | topic_segment | fact_extract | summarize | compress
    transform_type: "chunking"

    # chunking 参数
    chunk_size: 512              # REQUIRED when transform_type=chunking
    chunk_overlap: 50            # REQUIRED when transform_type=chunking
    chunk_strategy: "fixed"      # fixed | sentence | paragraph

    # topic_segment 参数
    min_segment_size: 100
    max_segment_size: 500
    segment_prompt: |
      Identify topic boundaries in the following conversation.
      For each topic segment, provide the exchange indices that belong to that segment.

      Conversation:
      {dialogue}

      Return a JSON list of segments, each with "topic" (a short topic description) and "exchanges" (list of exchange indices, 0-based).
      Example: [{"topic": "greeting", "exchanges": [0, 1]}, {"topic": "weather", "exchanges": [2, 3, 4]}]

      Segments:

    # fact_extract 参数
    fact_format: "statement"     # statement | triple | json
    fact_prompt: |
      Extract factual statements from the following conversation.
      Each fact should be an objective piece of information about a speaker or topic mentioned.

      Conversation:
      {dialogue}

      Return a JSON list of facts, each with "fact" (the factual statement), "speaker" (who the fact is about, or "general"), and "dialog_id" (the dialog index where this fact was mentioned).
      Example: [{"fact": "Alice works at Google", "speaker": "Alice", "dialog_id": 2}]

      Facts:

    # summarize 参数
    summary_max_tokens: 200
    summary_prompt: |
      Summarize the following conversation concisely.
      Capture the main topics discussed and key points made by each speaker.

      Conversation:
      {dialogue}

      Summary:

    # compress 参数
    compression_ratio: 0.5
    compression_model: "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank"

    # -------------------- extract action --------------------
    # extract_type: keyword | entity | noun | persona | all
    extract_type: "keyword"
    add_to_metadata: true

    # keyword 参数
    max_keywords: 10
    keyword_prompt: |
      Extract key concepts and keywords from the following text.
      Focus on nouns, verbs, and key concepts that capture the main ideas.

      Text:
      {text}

      Return a JSON object with "keywords" (list of keywords, ordered from most to least important).
      Example: {"keywords": ["machine learning", "neural network", "training data"]}

      Keywords:

    # entity 参数
    ner_model: "spacy"           # spacy | llm
    entity_types:
      - "PERSON"
      - "ORG"
      - "LOC"
      - "EVENT"
      - "GPE"
      - "DATE"
      - "TIME"

    # persona 参数
    persona_prompt: |
      Extract personality traits and preferences from the following conversation.
      Focus on identifying characteristics, preferences, and facts about each speaker.

      Conversation:
      {dialogue}

      Return a JSON object with speaker names as keys, each containing:
      - "traits": list of personality traits
      - "preferences": list of likes/dislikes  
      - "facts": list of factual information about the person

      Example: {"Alice": {"traits": ["friendly"], "preferences": ["coffee"], "facts": ["works at Google"]}}

      Personas:

    # spacy 配置
    spacy_model: "en_core_web_sm"

    # -------------------- score action --------------------
    # score_type: importance | emotion
    score_type: "importance"

    importance_prompt: |
      On a scale of 1 to 10, rate the importance of this memory.
      Consider: Is it a significant life event? Does it reveal important information about a person?
      Will it likely be referenced in future conversations?

      Memory:
      {text}

      Return a JSON object with "score" (integer 1-10) and "reason" (brief explanation).
      Example: {"score": 7, "reason": "Reveals important career information"}

      Rating:

    emotion_prompt: |
      Analyze the emotional content of this memory.

      Memory:
      {text}

      Return a JSON object with:
      - "emotion": primary emotion (joy, sadness, anger, fear, surprise, disgust, neutral)
      - "intensity": 1-10 scale
      - "reason": brief explanation

      Analysis:

    # -------------------- multi_embed action --------------------
    embeddings:
      - name: "semantic"
        model: "BAAI/bge-m3"
        weight: 0.6
      - name: "emotion"
        model: "BAAI/bge-m3"
        weight: 0.4

    # -------------------- validate action --------------------
    rules:
      - type: "length"
        min: 1
        max: 10000
      - type: "duplicate"
        threshold: 0.95

  # ============================================================
  # D3: PostInsert 算子配置
  # ============================================================
  post_insert:
    # REQUIRED: 操作类型
    # - none: 无操作
    # - distillation: 记忆蒸馏 (SCM4LLMs)
    # - log: 日志记录
    # - stats: 统计分析
    # - reflection: 生成高阶反思 (Generative Agents, LoCoMo)
    # - link_evolution: 链接关系管理 (HippoRAG, A-mem)
    # - forgetting: 遗忘/淘汰机制 (MemoryBank, MemoryOS)
    # - summarize: 摘要压缩 (MemGPT, MemoryBank, SCM4LLMs)
    # - migrate: 层级迁移 (MemoryOS, LD-Agent)
    action: "none"

    # -------------------- distillation action --------------------
    distillation_topk: 10
    distillation_threshold: null
    distillation_prompt: |
      Given the following related memories, identify which memories can be consolidated or removed.
      Keep the most important and unique information.

      Memories:
      {memory_list}

      Return a JSON object with:
      - "to_delete": list of memory texts to remove
      - "to_insert": list of new consolidated memory texts

      Result:

    # -------------------- log action --------------------
    log_level: "INFO"

    # -------------------- stats action --------------------
    stats_fields:
      - "count"
      - "avg_len"

    # -------------------- reflection action --------------------
    # trigger_mode: threshold | periodic | count | manual
    trigger_mode: "threshold"

    # threshold 模式配置 (Generative Agents)
    importance_threshold: 100.0
    importance_field: "importance_score"
    reset_after_reflection: true

    # periodic 模式配置
    interval_minutes: 60

    # count 模式配置 (LoCoMo)
    memory_count: 50

    # 反思配置
    reflection_depth: 1
    max_reflections: 5

    # reflection_type: general | self | other
    reflection_type: "general"

    reflection_prompt: |
      Given only the information above, what are {max_reflections} most salient high-level questions we can answer about the subjects?

      Recent memories:
      {memory_list}

      Please output {max_reflections} high-level insights or reflections based on these memories.
      Format: Return a JSON object with a "reflections" array containing the insights.
      Example: {"reflections": ["insight 1", "insight 2", ...]}

    self_reflection_prompt: |
      Based on the following experiences, what have I learned about myself?

      Experiences:
      {memory_list}

      Please output {max_reflections} self-reflections.
      Format: Return a JSON object with a "reflections" array.

    other_reflection_prompt: |
      Based on the following interactions, what have I learned about others?

      Interactions:
      {memory_list}

      Please output {max_reflections} insights about others.
      Format: Return a JSON object with a "reflections" array.

    # 输出配置
    store_reflection: true
    reflection_importance: 8

    # -------------------- link_evolution action --------------------
    # link_policy: synonym_edge | strengthen | activate | auto_link
    link_policy: "synonym_edge"

    # synonym_edge 配置 (HippoRAG)
    knn_k: 10
    similarity_threshold: 0.7
    edge_weight: 1.0

    # strengthen 配置 (A-mem)
    strengthen_factor: 0.1
    decay_factor: 0.01
    max_weight: 10.0

    # activate 配置 (A-mem)
    activation_depth: 2
    activation_decay: 0.5

    # auto_link 配置 (A-mem)
    max_auto_links: 5
    auto_link_prompt: |
      Given this new memory and existing memories, identify which existing memories should be linked to this new memory.

      New memory:
      {new_memory}

      Existing memories:
      {existing_memories}

      Return a JSON object with "links" array containing the indices (0-based) of memories to link.
      Example: {"links": [0, 2, 5], "reasons": ["shared topic", "temporal relation", "causal link"]}

    # -------------------- forgetting action --------------------
    # decay_type: time_decay | lru | lfu | ebbinghaus | hybrid
    decay_type: "ebbinghaus"

    # time_decay 配置
    decay_rate: 0.1
    decay_floor: 0.1

    # lru 配置
    max_memories: 1000
    evict_count: 100

    # lfu 配置 (MemoryOS)
    heat_threshold: 0.3
    heat_decay: 0.1

    # ebbinghaus 配置 (MemoryBank)
    initial_strength: 1.0
    forgetting_curve: "exponential"  # exponential | power
    review_boost: 0.5

    # hybrid 配置
    factors:
      - type: "time"
        weight: 0.3
      - type: "frequency"
        weight: 0.3
      - type: "importance"
        weight: 0.4

    # 淘汰配置
    retention_min: 50
    archive_before_delete: true

    # -------------------- summarize action --------------------
    # trigger_condition: overflow | periodic | manual
    trigger_condition: "overflow"
    overflow_threshold: 100
    periodic_interval: 3600

    # summary_strategy: single | hierarchical | incremental
    summary_strategy: "hierarchical"

    # hierarchical 配置 (MemoryBank)
    hierarchy_levels:
      - name: "daily"
        window: 86400
        prompt: |
          Summarize today's conversations and events:
          {memory_list}

          Output JSON: {"summary": "..."}
      - name: "weekly"
        window: 604800
        prompt: |
          Summarize this week's key events and insights:
          {memory_list}

          Output JSON: {"summary": "..."}
      - name: "global"
        window: -1
        prompt: |
          Update the overall summary based on all information:
          {memory_list}

          Output JSON: {"summary": "..."}

    # incremental 配置 (SCM4LLMs)
    incremental_prompt: |
      Given the existing summary and new memories, update the summary incrementally.

      Existing summary:
      {existing_summary}

      New memories:
      {new_memories}

      Output a JSON object with "summary" field containing the updated summary.

    # single 配置
    summarize_prompt: |
      Summarize the following memories concisely while preserving key information:

      Memories:
      {memory_list}

      Output a JSON object with "summary" field containing the consolidated summary.

    # 输出配置
    replace_originals: false
    store_as_new: true
    summary_importance: 7

    # -------------------- migrate action --------------------
    # migrate_policy: heat | time | overflow | manual
    migrate_policy: "heat"

    # heat 配置 (MemoryOS)
    heat_upgrade_threshold: 0.7
    cold_threshold: 0.3

    # time 配置 (LD-Agent)
    session_gap: 3600

    # overflow 配置
    tier_capacities:
      stm: 100
      mtm: 1000

    # 迁移转换配置
    upgrade_transform: "none"
    downgrade_transform: "summarize"

  # ============================================================
  # D4: PreRetrieval 算子配置
  # ============================================================
  pre_retrieval:
    # REQUIRED: 操作类型
    # - none: 透传
    # - embedding: 基础向量化
    # - optimize: 查询优化 (关键词提取、扩展、改写、指令增强)
    # - multi_embed: 多维向量生成
    # - decompose: 复杂查询分解
    # - route: 检索路由
    # - validate: 查询验证
    action: "none"

    # -------------------- optimize action --------------------
    # optimize_type: keyword_extract | expand | rewrite | instruction
    optimize_type: "keyword_extract"

    # 关键词提取配置
    extractor: "spacy"           # spacy | nltk | llm
    extract_types:
      - "NOUN"
      - "PROPN"
    max_keywords: 10
    keyword_prompt: |
      Extract keywords from this query:
      {query}

      Return JSON: {"keywords": ["word1", "word2", ...]}

    # 查询扩展配置
    expand_count: 3
    merge_strategy: "union"
    expand_prompt: |
      Generate {count} alternative phrasings of this query:
      Query: {query}
      Alternatives:

    # 查询改写配置
    rewrite_prompt: |
      Rewrite this query to be more specific and searchable:
      Query: {query}
      Rewritten:

    # 指令增强配置 (HippoRAG)
    instruction_prefix: "Retrieve passages that contain information about: "
    instruction_suffix: ""

    # 通用配置
    replace_original: false
    store_optimized: true
    embed_optimized: false

    # spacy 配置
    spacy_model: "en_core_web_sm"

    # -------------------- multi_embed action --------------------
    embeddings:
      - name: "semantic"
        model: "BAAI/bge-m3"
        weight: 0.6
    output_format: "dict"
    match_insert_config: true

    # -------------------- decompose action --------------------
    # decompose_strategy: llm | rule | hybrid
    decompose_strategy: "llm"
    max_sub_queries: 5
    sub_query_action: "parallel"
    embed_sub_queries: false

    decompose_prompt: |
      Break down this complex question into simpler sub-questions that can be answered independently.
      Each sub-question should be self-contained and searchable.

      Question: {query}

      Return a JSON array of sub-questions. Example: ["sub-question 1", "sub-question 2"]
      Sub-questions:

    # rule 策略配置
    split_keywords:
      - "and"
      - "or"
      - "also"
      - "additionally"
      - "moreover"
      - "furthermore"
      - "besides"

    # -------------------- route action --------------------
    # route_strategy: keyword | classifier | llm
    route_strategy: "keyword"
    allow_multi_route: true
    max_routes: 2
    default_route: "long_term_memory"

    # keyword 路由规则
    keyword_rules:
      - keywords: ["remember", "recall", "last time", "before", "previously"]
        target: "long_term_memory"
      - keywords: ["just", "now", "recently", "today", "moment ago"]
        target: "short_term_memory"
      - keywords: ["know", "fact", "what is", "who is", "define"]
        target: "knowledge_base"

    # classifier 配置
    classifier_model: "intent-classifier"
    route_mapping:
      factual: "knowledge_base"
      personal: "long_term_memory"
      recent: "short_term_memory"

    # llm 路由配置
    route_prompt: |
      Determine which memory source(s) should be queried for this question.
      Available sources:
      - short_term_memory: Recent conversations and events
      - long_term_memory: Historical memories and past experiences
      - knowledge_base: Factual information and general knowledge

      Question: {query}

      Return a JSON array of source names to query. Example: ["long_term_memory", "knowledge_base"]
      Sources:

    # -------------------- validate action --------------------
    rules:
      - type: "length"
        min: 1
        max: 1000
    on_fail: "default"
    default_query: "Hello"

    preprocessing:
      strip_whitespace: true
      lowercase: false
      remove_punctuation: false

  # ============================================================
  # D5: PostRetrieval 算子配置
  # ============================================================
  post_retrieval:
    # REQUIRED: 操作类型
    # - none: 仅做基础格式化
    # - rerank: 结果重排序
    # - filter: 结果筛选
    # - merge: 多源结果融合
    # - augment: 结果增强
    # - compress: 结果压缩
    # - format: 格式化输出
    action: "none"

    # 对话格式化 Prompt
    conversation_format_prompt: |
      Below is a conversation between two people. 
      The conversation takes place over multiple days and the date of each 
      conversation is written at the beginning of the conversation.

    # -------------------- rerank action --------------------
    # rerank_type: semantic | time_weighted | ppr | weighted | cross_encoder
    rerank_type: "weighted"

    # semantic / cross_encoder 配置
    rerank_model: null
    cross_encoder_model: null
    batch_size: 32

    # time_weighted 配置
    time_decay_rate: 0.1
    time_field: "timestamp"

    # ppr 配置 (HippoRAG)
    damping_factor: 0.5
    max_iterations: 100
    convergence_threshold: 0.000001
    personalization_nodes: "query_entities"

    # weighted 配置 (Generative Agents)
    factors:
      - name: "recency"
        weight: 0.3
        decay_type: "exponential"
        decay_rate: 0.995
      - name: "importance"
        weight: 0.3
        field: "importance_score"
      - name: "relevance"
        weight: 0.4

    # 通用配置
    top_k: null
    score_field: "rerank_score"

    # -------------------- filter action --------------------
    # filter_type: token_budget | threshold | top_k | llm | dedup
    filter_type: "top_k"

    token_budget: 4096
    token_counter: "tiktoken"
    overflow_strategy: "truncate"
    priority_field: "score"

    score_threshold: 0.5
    k: 10

    dedup_field: "text"
    dedup_threshold: 0.95

    filter_llm_prompt: |
      Determine if this memory is relevant to the query.
      Query: {query}
      Memory: {memory}
      Return JSON: {"relevant": true/false, "reason": "..."}

    # -------------------- merge action --------------------
    # merge_strategy: weighted | rrf | interleave
    merge_strategy: "rrf"
    weights: []
    rrf_k: 60
    dedup: true

    # -------------------- augment action --------------------
    augment_types:
      - "reflection"
      - "context"
      - "metadata"
      - "temporal"
    context_window: 2
    metadata_fields:
      - "importance_score"
      - "source"
    temporal_format: "relative"

    # -------------------- compress action --------------------
    # compress_strategy: llmlingua | extractive | abstractive
    compress_strategy: "extractive"
    compression_ratio: 0.5
    compress_model: null
    compress_max_tokens: 1000
    preserve_order: true

    compress_abstractive_prompt: |
      Compress the following memories while preserving key information:
      {memories}
      
      Compressed result:

    # -------------------- format action --------------------
    # format_type: template | structured | chat | xml
    format_type: "template"

    template: |
      {memories}
    include_metadata: false
    separator: "\n"
    xml_root: "memories"

    chat_format:
      system_prefix: "Memory context:"
      memory_prefix: "- "
      memory_suffix: ""
