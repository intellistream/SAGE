# ============================================================
# SAGE Memory Pipeline 完整配置模板
# ============================================================
# 说明：
# - 所有标记为 REQUIRED 的字段必须填写，缺失将导致程序启动失败
# - 所有 prompt 字段支持 {placeholder} 格式的变量替换
# - 注释中的 "when ..." 表示该字段在特定条件下必需
# 更新时间：2025-12-11
# ============================================================

# ============================================================
# 运行时基础配置
# ============================================================
runtime:
  dataset: ""                    # REQUIRED: 数据集名称
  # task_id: ""                  # 任务ID，可通过命令行覆盖
  memory_insert_verbose: false   # 是否打印记忆插入详细信息
  memory_test_verbose: true      # 是否打印记忆测试详细信息
  test_segments: 10              # 测试分段数

  # LLM 服务配置
  api_key: ""                    # REQUIRED: LLM API Key
  base_url: ""                   # REQUIRED: LLM 服务地址
  model_name: ""                 # REQUIRED: LLM 模型名称
  max_tokens: 512                # 生成最大 token 数
  temperature: 0.7               # 生成温度
  seed: 42                       # 随机种子

  # Embedding 服务配置
  embedding_base_url: ""         # REQUIRED when action needs embedding
  embedding_model: ""            # REQUIRED when action needs embedding

  # 测试 Prompt 模板
  prompt_template: |
    Based on the above context, answer the following question concisely using exact words from the context whenever possible. If the information is not mentioned in the conversation, respond with "Not mentioned in the conversation".

    Question: {question}
    Answer:

# ============================================================
# 服务配置
# ============================================================
services:
  # REQUIRED: 主存储后端名称
  # 可选值: short_term_memory | neuromem_vdb | graph_memory | hierarchical_memory | hybrid_memory | vector_hash_memory | kv_memory
  register_memory_service: ""

  # 图存储服务名称 (用于 link_evolution)
  graph_memory_service: ""

  # 分层记忆服务名称
  stm_service: "short_term_memory"
  mtm_service: "mid_term_memory"
  ltm_service: "long_term_memory"

  # -------------------- short_term_memory 配置 --------------------
  short_term_memory:
    max_dialog: 3                # 短期记忆窗口大小
    embedding_dim: 1024          # 向量维度
    retrieval_top_k: 10          # 检索返回数量

  # -------------------- hierarchical_memory 配置 --------------------
  # 分层记忆服务 (MemGPT, MemoryOS, MemoryBank, LD-Agent)
  hierarchical_memory:
    # tier_mode: two_tier | three_tier | functional
    # - two_tier: [stm, ltm] 双层模式 (LD-Agent)
    # - three_tier: [stm, mtm, ltm] 三层模式 (MemoryOS, MemoryBank)
    # - functional: [core, recall, archival] 功能分层 (MemGPT)
    tier_mode: "three_tier"
    tier_capacities:
      stm: 20                    # 短期记忆容量
      mtm: 200                   # 中期记忆容量 (three_tier)
      ltm: -1                    # 长期记忆容量 (-1 表示无限)
      core: 20                   # 核心上下文容量 (functional)
      recall: 100                # 召回存储容量 (functional)
    migration_policy: "fifo"     # 迁移策略: fifo | heat | time
    embedding_dim: 1024

  # -------------------- hybrid_memory 配置 --------------------
  # 混合记忆服务 (Mem0, Mem0ᵍ)
  hybrid_memory:
    indexes:
    - name: "semantic"
      type: "vector"
      dim: 1024
    - name: "keyword"
      type: "bm25"
    fusion_strategy: "weighted"  # weighted | rrf
    fusion_weights:
      semantic: 0.7
      keyword: 0.3
    graph_enabled: false         # 是否启用图增强 (Mem0ᵍ)

  # -------------------- graph_memory 配置 --------------------
  # 图记忆服务 (HippoRAG, A-Mem)
  graph_memory:
    graph_type: "knowledge_graph" # knowledge_graph | link_graph
    triple_store: "igraph"
    node_embedding_dim: 1024
    edge_types: ["relation", "synonym", "temporal"]
    retrieval_top_k: 20
    num_start_nodes: 10
    max_depth: 3

  memory_retrieval_adapter: "none"

# ============================================================
# D2: PreInsert 算子配置
# ============================================================
operators:
  pre_insert:
    # REQUIRED: 操作类型
    # - none: 直接透传
    # - transform.chunking: 文本分块
    # - transform.segment: 主题分段
    # - transform.summarize: 摘要提取
    # - extract.keyword: 关键词提取
    # - extract.entity: 命名实体识别
    # - extract.noun: 名词短语提取
    # - extract.triple: 三元组提取（用于 TiM, HippoRAG, HippoRAG2）
    # - score.importance: 重要性评分
    # - score.heat: 热度评分
    # - tri_embed: [DEPRECATED] 使用 extract.triple 代替
    action: "none"

    # -------------------- extract.triple action --------------------
    # REQUIRED when action=extract.triple (或向后兼容的 tri_embed)
    triple_extraction_prompt: |
      Extract knowledge triples from the following conversation.
      Each triple should be in the format (subject, predicate, object).

      Conversation:
      {dialogue}

      Return a JSON list of triples.
      Example: [{"subject": "Alice", "predicate": "works at", "object": "Google"}]

      Triples:

    # -------------------- transform action --------------------
    # transform_type: chunking | topic_segment | fact_extract | summarize | compress
    transform_type: "chunking"

    # chunking 参数
    chunk_size: 512              # REQUIRED when transform_type=chunking
    chunk_overlap: 50            # REQUIRED when transform_type=chunking
    chunk_strategy: "fixed"      # fixed | sentence | paragraph

    # topic_segment 参数
    min_segment_size: 100
    max_segment_size: 500
    segment_prompt: |
      Identify topic boundaries in the following conversation.
      For each topic segment, provide the exchange indices that belong to that segment.

      Conversation:
      {dialogue}

      Return a JSON list of segments, each with "topic" (a short topic description) and "exchanges" (list of exchange indices, 0-based).
      Example: [{"topic": "greeting", "exchanges": [0, 1]}, {"topic": "weather", "exchanges": [2, 3, 4]}]

      Segments:

    # fact_extract 参数
    fact_format: "statement"     # statement | triple | json
    fact_prompt: |
      Extract factual statements from the following conversation.
      Each fact should be an objective piece of information about a speaker or topic mentioned.

      Conversation:
      {dialogue}

      Return a JSON list of facts, each with "fact" (the factual statement), "speaker" (who the fact is about, or "general"), and "dialog_id" (the dialog index where this fact was mentioned).
      Example: [{"fact": "Alice works at Google", "speaker": "Alice", "dialog_id": 2}]

      Facts:

    # summarize 参数
    summary_max_tokens: 200
    summary_prompt: |
      Summarize the following conversation concisely.
      Capture the main topics discussed and key points made by each speaker.

      Conversation:
      {dialogue}

      Summary:

    # compress 参数
    compression_ratio: 0.5
    compression_model: "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank"

    # -------------------- extract action --------------------
    # extract_type: keyword | entity | noun | persona | all
    extract_type: "keyword"
    add_to_metadata: true

    # keyword 参数
    max_keywords: 10
    keyword_prompt: |
      Extract key concepts and keywords from the following text.
      Focus on nouns, verbs, and key concepts that capture the main ideas.

      Text:
      {text}

      Return a JSON object with "keywords" (list of keywords, ordered from most to least important).
      Example: {"keywords": ["machine learning", "neural network", "training data"]}

      Keywords:

    # entity 参数
    ner_model: "spacy"           # spacy | llm
    entity_types:
    - "PERSON"
    - "ORG"
    - "LOC"
    - "EVENT"
    - "GPE"
    - "DATE"
    - "TIME"

    # persona 参数
    persona_prompt: |
      Extract personality traits and preferences from the following conversation.
      Focus on identifying characteristics, preferences, and facts about each speaker.

      Conversation:
      {dialogue}

      Return a JSON object with speaker names as keys, each containing:
      - "traits": list of personality traits
      - "preferences": list of likes/dislikes
      - "facts": list of factual information about the person

      Example: {"Alice": {"traits": ["friendly"], "preferences": ["coffee"], "facts": ["works at Google"]}}

      Personas:

    # spacy 配置
    spacy_model: "en_core_web_sm"

    # -------------------- score action --------------------
    # score_type: importance | emotion
    score_type: "importance"

    importance_prompt: |
      On a scale of 1 to 10, rate the importance of this memory.
      Consider: Is it a significant life event? Does it reveal important information about a person?
      Will it likely be referenced in future conversations?

      Memory:
      {text}

      Return a JSON object with "score" (integer 1-10) and "reason" (brief explanation).
      Example: {"score": 7, "reason": "Reveals important career information"}

      Rating:

    emotion_prompt: |
      Analyze the emotional content of this memory.

      Memory:
      {text}

      Return a JSON object with:
      - "emotion": primary emotion (joy, sadness, anger, fear, surprise, disgust, neutral)
      - "intensity": 1-10 scale
      - "reason": brief explanation

      Analysis:

    # -------------------- multi_embed action --------------------
    embeddings:
    - name: "semantic"
      model: "BAAI/bge-m3"
      weight: 0.6
    - name: "emotion"
      model: "BAAI/bge-m3"
      weight: 0.4

    # -------------------- scm_embed action [NEW] --------------------
    # SCM (Self-Controlled Memory) 样式预处理
    # 参考论文: SCM4LLMs - Enhancing Large Language Model with Self-Controlled Memory
    # 同时保存原文、摘要、embedding，用于后续三元决策
    scm_summarize_prompt: |
      Below is a conversation between a user and an AI assistant. Please write a summary for each of them in one sentence and list them in separate paragraphs, while trying to preserve the key information of the user's question and the assistant's answer as much as possible.

      conversation content:

      {dialogue}

      Summary:

    embed_summary: false          # 是否对摘要做 embedding（SCM 默认对原文）
    summary_threshold: 300        # 原文 token 数超过此值才生成摘要（SCM 默认 300）

  # ============================================================
  # D3: PostInsert 算子配置
  # ============================================================
  post_insert:
    # REQUIRED: 操作类型
    # - none: 无操作
    # - distillation: 记忆蒸馏 (SCM4LLMs, TiM)
    # - log: 日志记录
    # - stats: 统计分析
    # - reflection: 生成高阶反思 (Generative Agents, LoCoMo)
    # - link_evolution: 链接关系管理 (HippoRAG, A-mem)
    # - forgetting: 遗忘/淘汰机制 (MemoryBank, MemoryOS)
    # - summarize: 摘要压缩 (MemGPT, MemoryBank, SCM4LLMs)
    # - migrate: 层级迁移 (MemoryOS, LD-Agent)
    # - mem0_crud: [NEW] Mem0 CRUD 决策 (ADD/UPDATE/DELETE/NOOP)
    action: "none"

    # -------------------- distillation action --------------------
    distillation_topk: 10
    distillation_threshold:
    distillation_prompt: |
      Given the following related memories, identify which memories can be consolidated or removed.
      Keep the most important and unique information.

      Memories:
      {memory_list}

      Return a JSON object with:
      - "to_delete": list of memory texts to remove
      - "to_insert": list of new consolidated memory texts

      Result:

    # -------------------- log action --------------------
    log_level: "INFO"

    # -------------------- stats action --------------------
    stats_fields:
    - "count"
    - "avg_len"

    # -------------------- reflection action --------------------
    # trigger_mode: threshold | periodic | count | manual
    trigger_mode: "threshold"

    # threshold 模式配置 (Generative Agents)
    importance_threshold: 100.0
    importance_field: "importance_score"
    reset_after_reflection: true

    # periodic 模式配置
    interval_minutes: 60

    # count 模式配置 (LoCoMo)
    memory_count: 50

    # 反思配置
    reflection_depth: 1
    max_reflections: 5

    # reflection_type: general | self | other
    reflection_type: "general"

    reflection_prompt: |
      Given only the information above, what are {max_reflections} most salient high-level questions we can answer about the subjects?

      Recent memories:
      {memory_list}

      Please output {max_reflections} high-level insights or reflections based on these memories.
      Format: Return a JSON object with a "reflections" array containing the insights.
      Example: {"reflections": ["insight 1", "insight 2", ...]}

    self_reflection_prompt: |
      Based on the following experiences, what have I learned about myself?

      Experiences:
      {memory_list}

      Please output {max_reflections} self-reflections.
      Format: Return a JSON object with a "reflections" array.

    other_reflection_prompt: |
      Based on the following interactions, what have I learned about others?

      Interactions:
      {memory_list}

      Please output {max_reflections} insights about others.
      Format: Return a JSON object with a "reflections" array.

    # 输出配置
    store_reflection: true
    reflection_importance: 8

    # -------------------- link_evolution action --------------------
    # link_policy: synonym_edge | strengthen | activate | auto_link
    link_policy: "synonym_edge"

    # synonym_edge 配置 (HippoRAG)
    knn_k: 10
    similarity_threshold: 0.7
    edge_weight: 1.0

    # strengthen 配置 (A-mem)
    strengthen_factor: 0.1
    decay_factor: 0.01
    max_weight: 10.0

    # activate 配置 (A-mem)
    activation_depth: 2
    activation_decay: 0.5

    # auto_link 配置 (A-mem)
    max_auto_links: 5
    auto_link_prompt: |
      Given this new memory and existing memories, identify which existing memories should be linked to this new memory.

      New memory:
      {new_memory}

      Existing memories:
      {existing_memories}

      Return a JSON object with "links" array containing the indices (0-based) of memories to link.
      Example: {"links": [0, 2, 5], "reasons": ["shared topic", "temporal relation", "causal link"]}

    # -------------------- forgetting action --------------------
    # decay_type: time_decay | lru | lfu | ebbinghaus | hybrid
    decay_type: "ebbinghaus"

    # time_decay 配置
    decay_rate: 0.1
    decay_floor: 0.1

    # lru 配置
    max_memories: 1000
    evict_count: 100

    # lfu 配置 (MemoryOS)
    heat_threshold: 0.3
    heat_decay: 0.1

    # ebbinghaus 配置 (MemoryBank)
    initial_strength: 1.0
    forgetting_curve: "exponential"  # exponential | power
    review_boost: 0.5

    # hybrid 配置
    factors:
    - type: "time"
      weight: 0.3
    - type: "frequency"
      weight: 0.3
    - type: "importance"
      weight: 0.4

    # 淘汰配置
    retention_min: 50
    archive_before_delete: true

    # -------------------- summarize action --------------------
    # trigger_condition: overflow | periodic | manual
    trigger_condition: "overflow"
    overflow_threshold: 100
    periodic_interval: 3600

    # summary_strategy: single | hierarchical | incremental
    summary_strategy: "hierarchical"

    # hierarchical 配置 (MemoryBank)
    hierarchy_levels:
    - name: "daily"
      window: 86400
      prompt: |
        Summarize today's conversations and events:
        {memory_list}

        Output JSON: {"summary": "..."}
    - name: "weekly"
      window: 604800
      prompt: |
        Summarize this week's key events and insights:
        {memory_list}

        Output JSON: {"summary": "..."}
    - name: "global"
      window: -1
      prompt: |
        Update the overall summary based on all information:
        {memory_list}

        Output JSON: {"summary": "..."}

    # incremental 配置 (SCM4LLMs)
    incremental_prompt: |
      Given the existing summary and new memories, update the summary incrementally.

      Existing summary:
      {existing_summary}

      New memories:
      {new_memories}

      Output a JSON object with "summary" field containing the updated summary.

    # single 配置
    summarize_prompt: |
      Summarize the following memories concisely while preserving key information:

      Memories:
      {memory_list}

      Output a JSON object with "summary" field containing the consolidated summary.

    # 输出配置
    replace_originals: false
    store_as_new: true
    summary_importance: 7

    # -------------------- migrate action --------------------
    # migrate_policy: heat | time | overflow | manual
    migrate_policy: "heat"

    # heat 配置 (MemoryOS)
    heat_upgrade_threshold: 0.7
    cold_threshold: 0.3

    # time 配置 (LD-Agent)
    session_gap: 3600

    # overflow 配置
    tier_capacities:
      stm: 100
      mtm: 1000

    # 迁移转换配置
    upgrade_transform: "none"
    downgrade_transform: "summarize"

    # -------------------- mem0_crud action [NEW] --------------------
    # Mem0 CRUD 决策：对已有记忆执行 ADD/UPDATE/DELETE/NOOP
    # 参考论文: Mem0 - Building Production-Ready AI Agents with Scalable Long-Term Memory
    # 工作流程: 检索相似记忆 → LLM 判断冲突 → 执行对应操作
    mem0_crud_topk: 10            # 检索相似记忆的数量
    mem0_crud_threshold: 0.7      # 相似度阈值，超过才进行 CRUD 判断
    mem0_crud_prompt: |
      You are managing a fact database. Given a new fact and similar existing facts,
      decide the appropriate action:

      - ADD: The new fact is entirely new information
      - UPDATE: The new fact updates/extends an existing fact (delete old, keep new)
      - DELETE: The new fact contradicts an existing fact (delete old, keep new)
      - NOOP: The new fact is redundant (already covered by existing facts)

      New fact: {new_entry}

      Existing facts:
      {memory_list}

      Respond with JSON:
      {"action": "ADD|UPDATE|DELETE|NOOP", "to_delete": ["id1", "id2"], "reason": "brief explanation"}

  # ============================================================
  # D4: PreRetrieval 算子配置
  # ============================================================
  pre_retrieval:
    # REQUIRED: 操作类型
    # - none: 透传
    # - embedding: 基础向量化
    # - optimize: 查询优化 (关键词提取、扩展、改写、指令增强)
    # - multi_embed: 多维向量生成
    # - decompose: 复杂查询分解
    # - route: 检索路由
    # - scm_gate: [NEW] SCM 记忆激活判断 (是否需要检索)
    action: "none"

    # -------------------- optimize action --------------------
    # optimize_type: keyword_extract | expand | rewrite | instruction
    optimize_type: "keyword_extract"

    # 关键词提取配置
    extractor: "spacy"           # spacy | nltk | llm
    extract_types:
    - "NOUN"
    - "PROPN"
    max_keywords: 10
    keyword_prompt: |
      Extract keywords from this query:
      {query}

      Return JSON: {"keywords": ["word1", "word2", ...]}

    # 查询扩展配置
    expand_count: 3
    merge_strategy: "union"
    expand_prompt: |
      Generate {count} alternative phrasings of this query:
      Query: {query}
      Alternatives:

    # 查询改写配置
    rewrite_prompt: |
      Rewrite this query to be more specific and searchable:
      Query: {query}
      Rewritten:

    # 指令增强配置 (HippoRAG)
    instruction_prefix: "Retrieve passages that contain information about: "
    instruction_suffix: ""

    # 通用配置
    replace_original: false
    store_optimized: true
    embed_optimized: false

    # spacy 配置
    spacy_model: "en_core_web_sm"

    # -------------------- multi_embed action --------------------
    embeddings:
    - name: "semantic"
      model: "BAAI/bge-m3"
      weight: 0.6
    output_format: "dict"
    match_insert_config: true

    # -------------------- decompose action --------------------
    # decompose_strategy: llm | rule | hybrid
    decompose_strategy: "llm"
    max_sub_queries: 5
    sub_query_action: "parallel"
    embed_sub_queries: false

    decompose_prompt: |
      Break down this complex question into simpler sub-questions that can be answered independently.
      Each sub-question should be self-contained and searchable.

      Question: {query}

      Return a JSON array of sub-questions. Example: ["sub-question 1", "sub-question 2"]
      Sub-questions:

    # rule 策略配置
    split_keywords:
    - "and"
    - "or"
    - "also"
    - "additionally"
    - "moreover"
    - "furthermore"
    - "besides"

    # -------------------- route action --------------------
    # route_strategy: keyword | classifier | llm
    route_strategy: "keyword"
    allow_multi_route: true
    max_routes: 2
    default_route: "long_term_memory"

    # keyword 路由规则
    keyword_rules:
    - keywords: ["remember", "recall", "last time", "before", "previously"]
      target: "long_term_memory"
    - keywords: ["just", "now", "recently", "today", "moment ago"]
      target: "short_term_memory"
    - keywords: ["know", "fact", "what is", "who is", "define"]
      target: "knowledge_base"

    # classifier 配置
    classifier_model: "intent-classifier"
    route_mapping:
      factual: "knowledge_base"
      personal: "long_term_memory"
      recent: "short_term_memory"

    # llm 路由配置
    route_prompt: |
      Determine which memory source(s) should be queried for this question.
      Available sources:
      - short_term_memory: Recent conversations and events
      - long_term_memory: Historical memories and past experiences
      - knowledge_base: Factual information and general knowledge

      Question: {query}

      Return a JSON array of source names to query. Example: ["long_term_memory", "knowledge_base"]
      Sources:

    # -------------------- scm_gate action [NEW] --------------------
    # SCM 记忆激活判断：决定是否需要检索记忆
    # 参考论文: SCM4LLMs - Enhancing Large Language Model with Self-Controlled Memory
    # 某些简单问题不需要记忆检索，直接回答即可
    scm_gate_prompt: |
      Given the current question, determine if memory retrieval is needed.

      Question: {query}

      Consider:
      - Simple greetings or chitchat don't need memory
      - Questions about past events or personal info need memory
      - Follow-up questions may need context from memory

      Return JSON: {"need_memory": true/false, "reason": "brief explanation"}

  # ============================================================
  # D5: PostRetrieval 算子配置
  # ============================================================
  post_retrieval:
    # REQUIRED: 操作类型
    # - none: 仅做基础格式化
    # - rerank: 结果重排序
    # - filter: 结果筛选
    # - merge: 多源结果融合
    # - augment: 结果增强
    # - compress: 结果压缩
    # - format: 格式化输出
    # - scm_three_way: [NEW] SCM 三元决策 (drop/summary/raw)
    action: "none"

    # 对话格式化 Prompt
    conversation_format_prompt: |
      Below is a conversation between two people.
      The conversation takes place over multiple days and the date of each
      conversation is written at the beginning of the conversation.

    # -------------------- rerank action --------------------
    # rerank_type: semantic | time_weighted | ppr | weighted | cross_encoder
    rerank_type: "weighted"

    # semantic / cross_encoder 配置
    rerank_model:
    cross_encoder_model:
    batch_size: 32

    # time_weighted 配置
    time_decay_rate: 0.1
    time_field: "timestamp"

    # ppr 配置 (HippoRAG)
    damping_factor: 0.5
    max_iterations: 100
    convergence_threshold: 0.000001
    personalization_nodes: "query_entities"

    # weighted 配置 (Generative Agents)
    factors:
    - name: "recency"
      weight: 0.3
      decay_type: "exponential"
      decay_rate: 0.995
    - name: "importance"
      weight: 0.3
      field: "importance_score"
    - name: "relevance"
      weight: 0.4

    # 通用配置
    top_k:
    score_field: "rerank_score"

    # -------------------- filter action --------------------
    # filter_type: token_budget | threshold | top_k | llm | dedup
    filter_type: "top_k"

    token_budget: 4096
    token_counter: "tiktoken"
    overflow_strategy: "truncate"
    priority_field: "score"

    score_threshold: 0.5
    k: 10

    dedup_field: "text"
    dedup_threshold: 0.95

    filter_llm_prompt: |
      Determine if this memory is relevant to the query.
      Query: {query}
      Memory: {memory}
      Return JSON: {"relevant": true/false, "reason": "..."}

    # -------------------- merge action --------------------
    # merge_strategy: weighted | rrf | interleave
    merge_strategy: "rrf"
    weights: []
    rrf_k: 60
    dedup: true

    # -------------------- augment action --------------------
    augment_types:
    - "reflection"
    - "context"
    - "metadata"
    - "temporal"
    context_window: 2
    metadata_fields:
    - "importance_score"
    - "source"
    temporal_format: "relative"

    # -------------------- compress action --------------------
    # compress_strategy: llmlingua | extractive | abstractive
    compress_strategy: "extractive"
    compression_ratio: 0.5
    compress_model:
    compress_max_tokens: 1000
    preserve_order: true

    compress_abstractive_prompt: |
      Compress the following memories while preserving key information:
      {memories}

      Compressed result:

    # -------------------- format action --------------------
    # format_type: template | structured | chat | xml
    format_type: "template"

    template: |
      {memories}
    include_metadata: false
    separator: "\n"
    xml_root: "memories"

    chat_format:
      system_prefix: "Memory context:"
      memory_prefix: "- "
      memory_suffix: ""

    # -------------------- scm_three_way action [NEW] --------------------
    # SCM 三元决策：对每条检索结果判断 drop / summary / raw
    # 参考论文: SCM4LLMs - Enhancing Large Language Model with Self-Controlled Memory
    # 工作流程: 超过 token budget 时，对每条记忆 LLM 判断处理方式

    # Token budget 配置
    max_history_tokens: 2500      # SCM 默认: MAX_HISTORY_TOKENS = 2500
    max_pre_turn_tokens: 500      # SCM 默认: MAX_PRE_TURN_TOKENS = 500
    scm_token_counter: "char"     # char | word | tiktoken

    # 三元决策 prompt
    scm_judge_prompt: |
      Given the [Conversation Content] and [User Question], please answer the instruction question.

      [Conversation Content]:
      ```
      {content}
      ```

      [User Question]:
      ```
      {query}
      ```

      Instruction Question:
      ```
      Based on [Conversation Content], can you answer [User Question]? If yes, please answer `(A) Yes`, otherwise please answer `(B) No`.
      ```

      Please answer now. The output must strictly follow this format:

      [Answer]: The final answer is: (A) Yes / (B) No

    # 对话格式化 prompt (用于最终输出)
    scm_format_prompt: |
      The following is a conversation between a user and an AI assistant. Please answer the current question based on the history of the conversation:

      Related conversation history:
