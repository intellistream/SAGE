"""
åˆ†æžå™¨ - åè°ƒæŒ‡æ ‡è®¡ç®—å’Œç»“æžœå¯è§†åŒ–
"""

from pathlib import Path
from typing import Any

from sage.benchmark.benchmark_memory.evaluation.core.metric_interface import BaseMetric
from sage.benchmark.benchmark_memory.evaluation.core.result_loader import ResultLoader


class Analyzer:
    """å®žéªŒç»“æžœåˆ†æžå™¨

    åŠŸèƒ½ï¼š
    1. åè°ƒ ResultLoaderã€Metrics å’Œ DrawMethod
    2. å¯¹å•ä¸ªæˆ–å¤šä¸ªå®žéªŒç»“æžœè¿›è¡Œåˆ†æž
    3. ç”ŸæˆæŒ‡æ ‡æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨

    ä½¿ç”¨æµç¨‹ï¼š
    1. åŠ è½½ç»“æžœï¼šanalyzer.load_results(folder_path)
    2. æ³¨å†ŒæŒ‡æ ‡ï¼šanalyzer.register_metric(F1Score())
    3. è®¡ç®—æŒ‡æ ‡ï¼šanalyzer.compute_metrics()
    4. ç”ŸæˆæŠ¥å‘Šï¼šanalyzer.generate_report()
    5. ç»˜åˆ¶å›¾è¡¨ï¼šanalyzer.plot_metrics()
    """

    def __init__(self, output_dir: str = "./analysis_output"):
        """åˆå§‹åŒ–åˆ†æžå™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆæŠ¥å‘Šå’Œå›¾è¡¨ï¼‰
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.loader: ResultLoader | None = None
        self.metrics: list[BaseMetric] = []
        self.results_data: list[dict[str, Any]] = []
        self.metric_results: dict[str, Any] = {}

    def load_results(self, folder_path: str) -> None:
        """åŠ è½½å®žéªŒç»“æžœ

        Args:
            folder_path: ç»“æžœæ–‡ä»¶å¤¹è·¯å¾„
        """
        print(f"\n{'=' * 60}")
        print("ðŸ“‚ åŠ è½½å®žéªŒç»“æžœ")
        print(f"{'=' * 60}")

        self.loader = ResultLoader(folder_path)
        self.results_data = self.loader.get_all_results()

        summary = self.loader.get_summary()
        print(f"\næ€»è®¡: {summary['total_files']} ä¸ªç»“æžœæ–‡ä»¶")
        print(f"ä»»åŠ¡: {', '.join(summary['task_ids'])}")

    def register_metric(self, metric: BaseMetric) -> None:
        """æ³¨å†ŒæŒ‡æ ‡

        Args:
            metric: æŒ‡æ ‡å®žä¾‹ï¼ˆç»§æ‰¿è‡ª BaseMetricï¼‰
        """
        self.metrics.append(metric)
        print(f"âœ… å·²æ³¨å†ŒæŒ‡æ ‡: {metric.name}")

    def compute_metrics(self, mode: str = "independent") -> dict[str, Any]:
        """è®¡ç®—æŒ‡æ ‡

        Args:
            mode: åˆ†æžæ¨¡å¼
                - "independent": æ¯ä¸ªæ–‡ä»¶ç‹¬ç«‹åˆ†æž
                - "aggregate": èšåˆæ‰€æœ‰æ–‡ä»¶åˆ†æžï¼ˆæœªæ¥å®žçŽ°ï¼‰

        Returns:
            Dict: æŒ‡æ ‡è®¡ç®—ç»“æžœ
        """
        print(f"\n{'=' * 60}")
        print(f"ðŸ“Š è®¡ç®—æŒ‡æ ‡ (æ¨¡å¼: {mode})")
        print(f"{'=' * 60}\n")

        if mode == "independent":
            return self._compute_independent()
        elif mode == "aggregate":
            raise NotImplementedError("aggregate æ¨¡å¼å°šæœªå®žçŽ°")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}")

    def _compute_independent(self) -> dict[str, Any]:
        """ç‹¬ç«‹æ¨¡å¼ï¼šæ¯ä¸ªæ–‡ä»¶å•ç‹¬è®¡ç®—æŒ‡æ ‡"""
        results = {}

        for result_data in self.results_data:
            task_id = result_data.get("experiment_info", {}).get("task_id", "unknown")
            test_results = result_data.get("test_results", [])

            print(f"åˆ†æžä»»åŠ¡: {task_id}")

            task_metrics = {}
            for metric in self.metrics:
                # è®¡ç®—æ¯è½®çš„æŒ‡æ ‡å€¼
                round_scores = metric.compute_all_rounds(test_results)

                # è®¡ç®—æ•´ä½“ç»Ÿè®¡
                overall_stats = metric.compute_overall(test_results)

                task_metrics[metric.name] = {
                    "round_scores": round_scores,
                    "overall": overall_stats,
                }

                print(f"  - {metric.name}: å¹³å‡ {overall_stats['mean']:.4f}")

            results[task_id] = task_metrics

        self.metric_results = results
        return results

    def generate_report(self, output_file: str = "report.txt") -> None:
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š

        Args:
            output_file: æŠ¥å‘Šæ–‡ä»¶å
        """
        if not self.metric_results:
            print("âš ï¸  è¯·å…ˆè°ƒç”¨ compute_metrics()")
            return

        report_path = self.output_dir / output_file

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("=" * 60 + "\n")
            f.write("Memory Benchmark Analysis Report\n")
            f.write("=" * 60 + "\n\n")

            for task_id, task_metrics in self.metric_results.items():
                f.write(f"\nä»»åŠ¡: {task_id}\n")
                f.write("-" * 60 + "\n")

                for metric_name, metric_data in task_metrics.items():
                    overall = metric_data["overall"]
                    f.write(f"\n{metric_name}:\n")
                    f.write(f"  å¹³å‡å€¼: {overall['mean']:.4f}\n")
                    f.write(f"  æœ€å¤§å€¼: {overall['max']:.4f}\n")
                    f.write(f"  æœ€å°å€¼: {overall['min']:.4f}\n")
                    f.write(f"  æ ‡å‡†å·®: {overall['std']:.4f}\n")

                    # æ¯è½®è¯¦ç»†æ•°æ®
                    round_scores = metric_data["round_scores"]
                    f.write(f"  å„è½®å¾—åˆ†: {', '.join(f'{s:.4f}' for s in round_scores)}\n")

        print(f"\nðŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

    def plot_metrics(self, drawer_class=None) -> None:
        """ç»˜åˆ¶æŒ‡æ ‡å›¾è¡¨

        Args:
            drawer_class: ç»˜å›¾å™¨ç±»ï¼ˆå¦‚ LineChartï¼‰
        """
        if not self.metric_results:
            print("âš ï¸  è¯·å…ˆè°ƒç”¨ compute_metrics()")
            return

        if drawer_class is None:
            from sage.benchmark.benchmark_memory.evaluation.draw_method import LineChart

            drawer_class = LineChart

        drawer = drawer_class(output_dir=str(self.output_dir))

        print(f"\n{'=' * 60}")
        print("ðŸ“Š ç”Ÿæˆå›¾è¡¨")
        print(f"{'=' * 60}\n")

        # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆå›¾è¡¨
        for task_id, task_metrics in self.metric_results.items():
            # å•æŒ‡æ ‡å›¾è¡¨
            for metric_name, metric_data in task_metrics.items():
                round_scores = metric_data["round_scores"]
                drawer.plot_single_metric(
                    round_scores,
                    metric_name,
                    title=f"{task_id} - {metric_name}",
                    save_name=f"{task_id}_{metric_name}.png",
                )

            # å¤šæŒ‡æ ‡å¯¹æ¯”å›¾
            if len(task_metrics) > 1:
                metrics_data = {name: data["round_scores"] for name, data in task_metrics.items()}
                drawer.plot_multiple_metrics(
                    metrics_data,
                    title=f"{task_id} - å¤šæŒ‡æ ‡å¯¹æ¯”",
                    save_name=f"{task_id}_multiple_metrics.png",
                )
