# Memory Benchmark Evaluation Framework

å®éªŒç»“æœåˆ†æå’Œå¯è§†åŒ–æ¡†æ¶

## ğŸ“ ç›®å½•ç»“æ„

```
evaluation/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ data_analyze.py          # ä¸»å…¥å£è„šæœ¬
â”œâ”€â”€ README.md                # æœ¬æ–‡æ¡£
â”œâ”€â”€ core/                    # æ ¸å¿ƒç»„ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ result_loader.py     # ç»“æœåŠ è½½å™¨
â”‚   â”œâ”€â”€ metric_interface.py  # æŒ‡æ ‡åŸºç±»æ¥å£
â”‚   â””â”€â”€ analyzer.py          # åˆ†æå™¨ï¼ˆåè°ƒå™¨ï¼‰
â”œâ”€â”€ accuracy/                # å‡†ç¡®ç‡æŒ‡æ ‡
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ f1_score.py          # F1 åˆ†æ•°
â”‚   â”œâ”€â”€ precision.py         # ç²¾ç¡®ç‡ï¼ˆå¾…å®ç°ï¼‰
â”‚   â”œâ”€â”€ recall.py            # å¬å›ç‡ï¼ˆå¾…å®ç°ï¼‰
â”‚   â””â”€â”€ exact_match.py       # ç²¾ç¡®åŒ¹é…ï¼ˆå¾…å®ç°ï¼‰
â”œâ”€â”€ efficiency/              # æ•ˆç‡æŒ‡æ ‡
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ latency.py           # å»¶è¿Ÿï¼ˆå¾…å®ç°ï¼‰
â”‚   â””â”€â”€ throughput.py        # ååé‡ï¼ˆå¾…å®ç°ï¼‰
â””â”€â”€ draw_method/             # å¯è§†åŒ–æ–¹æ³•
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ line_chart.py        # æŠ˜çº¿å›¾
    â”œâ”€â”€ bar_chart.py         # æŸ±çŠ¶å›¾ï¼ˆå¾…å®ç°ï¼‰
    â”œâ”€â”€ heatmap.py           # çƒ­åŠ›å›¾ï¼ˆå¾…å®ç°ï¼‰
    â””â”€â”€ radar_chart.py       # é›·è¾¾å›¾ï¼ˆå¾…å®ç°ï¼‰
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```bash
# åˆ†æå•ä¸ªå®éªŒç»“æœæ–‡ä»¶å¤¹
python data_analyze.py --folder .sage/benchmarks/benchmark_memory/locomo/251121 --mode independent

# æŒ‡å®šè¾“å‡ºç›®å½•
python data_analyze.py --folder .sage/benchmarks/benchmark_memory/locomo/251121 --output ./my_analysis

# ä¸ç”Ÿæˆå›¾è¡¨
python data_analyze.py --folder .sage/benchmarks/benchmark_memory/locomo/251121 --no-plot
```

### é«˜çº§ç”¨æ³•

```bash
# æœªæ¥æ”¯æŒï¼šèšåˆæ¨¡å¼åˆ†æ
python data_analyze.py --folder .sage/benchmarks/benchmark_memory/locomo/251121 --mode aggregate

# æŒ‡å®šå¤šä¸ªæŒ‡æ ‡
python data_analyze.py --folder .sage/benchmarks/benchmark_memory/locomo/251121 --metrics F1 Precision Recall
```

## ğŸ“Š åˆ†ææ¨¡å¼

### 1. Independent æ¨¡å¼ï¼ˆç‹¬ç«‹åˆ†æï¼‰

æ¯ä¸ª JSON æ–‡ä»¶å•ç‹¬åˆ†æï¼Œç”Ÿæˆç‹¬ç«‹çš„æŒ‡æ ‡å’Œå›¾è¡¨ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š

- å¯¹æ¯”ä¸åŒé…ç½®ï¼ˆå¦‚ STM-3 vs STM-5ï¼‰
- åˆ†æå•ä¸ªä»»åŠ¡çš„æ€§èƒ½
- å¿«é€ŸæŸ¥çœ‹æ¯ä¸ªå®éªŒçš„ç»“æœ

**è¾“å‡º**ï¼š

```
analysis_output/
â”œâ”€â”€ report.txt                  # æ–‡æœ¬æŠ¥å‘Š
â”œâ”€â”€ conv-26_F1.png             # ä»»åŠ¡ conv-26 çš„ F1 æŠ˜çº¿å›¾
â”œâ”€â”€ conv-26_multiple_metrics.png  # å¤šæŒ‡æ ‡å¯¹æ¯”å›¾
â””â”€â”€ ...
```

### 2. Aggregate æ¨¡å¼ï¼ˆèšåˆåˆ†æï¼Œå¾…å®ç°ï¼‰

å°†æ‰€æœ‰ JSON æ–‡ä»¶æ±‡æ€»åˆ†æï¼Œç”Ÿæˆæ•´ä½“ç»Ÿè®¡ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š

- è·¨ä»»åŠ¡çš„å¹³å‡æ€§èƒ½
- æ•´ä½“è¶‹åŠ¿åˆ†æ
- æ•°æ®é›†çº§åˆ«çš„è¯„ä¼°

## ğŸ¯ æ ¸å¿ƒç»„ä»¶è¯´æ˜

### 1. ResultLoaderï¼ˆç»“æœåŠ è½½å™¨ï¼‰

è´Ÿè´£æ‰«æå’ŒåŠ è½½å®éªŒç»“æœæ–‡ä»¶ã€‚

```python
from sage.benchmark.benchmark_memory.evaluation.core import ResultLoader

loader = ResultLoader(".sage/benchmarks/benchmark_memory/locomo/251121")
results = loader.get_all_results()
```

**åŠŸèƒ½**ï¼š

- é€’å½’æ‰«æç›®å½•ä¸‹æ‰€æœ‰ JSON æ–‡ä»¶
- è§£æå¹¶éªŒè¯ JSON æ ¼å¼
- æä¾›ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ¥å£

### 2. BaseMetricï¼ˆæŒ‡æ ‡åŸºç±»ï¼‰

æ‰€æœ‰æŒ‡æ ‡å¿…é¡»ç»§æ‰¿æ­¤æ¥å£ã€‚

```python
from sage.benchmark.benchmark_memory.evaluation.core import BaseMetric

class MyMetric(BaseMetric):
    def compute_single_question(self, predicted, reference, metadata=None):
        # å®ç°å•ä¸ªé—®é¢˜çš„æŒ‡æ ‡è®¡ç®—
        return score
```

**å…³é”®æ–¹æ³•**ï¼š

- `compute_single_question()`: è®¡ç®—å•ä¸ªé—®é¢˜çš„æŒ‡æ ‡å€¼
- `compute_test_round()`: è®¡ç®—å•è½®æµ‹è¯•çš„å¹³å‡å€¼
- `compute_all_rounds()`: è®¡ç®—æ‰€æœ‰è½®æ¬¡çš„æŒ‡æ ‡å€¼
- `compute_overall()`: è®¡ç®—æ•´ä½“ç»Ÿè®¡ä¿¡æ¯

### 3. Analyzerï¼ˆåˆ†æå™¨ï¼‰

åè°ƒåŠ è½½ã€è®¡ç®—å’Œå¯è§†åŒ–çš„æ ¸å¿ƒç»„ä»¶ã€‚

```python
from sage.benchmark.benchmark_memory.evaluation.core import Analyzer
from sage.benchmark.benchmark_memory.evaluation.accuracy import F1Score

analyzer = Analyzer(output_dir="./analysis_output")
analyzer.load_results(".sage/benchmarks/benchmark_memory/locomo/251121")
analyzer.register_metric(F1Score())
analyzer.compute_metrics(mode="independent")
analyzer.generate_report()
analyzer.plot_metrics()
```

## ğŸ“ˆ æ·»åŠ æ–°æŒ‡æ ‡

### æ­¥éª¤ 1: åˆ›å»ºæŒ‡æ ‡ç±»

åœ¨ `accuracy/` æˆ– `efficiency/` ç›®å½•ä¸‹åˆ›å»ºæ–°æ–‡ä»¶ï¼š

```python
# accuracy/precision.py
from sage.benchmark.benchmark_memory.evaluation.core import BaseMetric

class PrecisionScore(BaseMetric):
    def __init__(self):
        super().__init__(
            name="Precision",
            description="ç²¾ç¡®ç‡ - é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹"
        )

    def compute_single_question(self, predicted_answer, reference_answer, metadata=None):
        # å®ç°ç²¾ç¡®ç‡è®¡ç®—é€»è¾‘
        pred_tokens = set(predicted_answer.lower().split())
        ref_tokens = set(reference_answer.lower().split())

        if not pred_tokens:
            return 0.0

        common = pred_tokens & ref_tokens
        return len(common) / len(pred_tokens)
```

### æ­¥éª¤ 2: æ³¨å†Œåˆ° `__init__.py`

```python
# accuracy/__init__.py
from .f1_score import F1Score
from .precision import PrecisionScore

__all__ = ["F1Score", "PrecisionScore"]
```

### æ­¥éª¤ 3: åœ¨ `data_analyze.py` ä¸­æ·»åŠ æ˜ å°„

```python
def get_metric_by_name(metric_name: str):
    metric_map = {
        "F1": F1Score,
        "Precision": PrecisionScore,  # æ–°å¢
    }
    # ...
```

## ğŸ¨ æ·»åŠ æ–°çš„å¯è§†åŒ–æ–¹æ³•

### æ­¥éª¤ 1: åˆ›å»ºç»˜å›¾ç±»

åœ¨ `draw_method/` ç›®å½•ä¸‹åˆ›å»ºæ–°æ–‡ä»¶ï¼š

```python
# draw_method/bar_chart.py
class BarChart:
    def __init__(self, output_dir="./output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def plot_comparison(self, data, title, save_name):
        # å®ç°æŸ±çŠ¶å›¾ç»˜åˆ¶é€»è¾‘
        pass
```

### æ­¥éª¤ 2: åœ¨ Analyzer ä¸­ä½¿ç”¨

```python
analyzer.plot_metrics(drawer_class=BarChart)
```

## ğŸ“ è¾“å‡ºæ ¼å¼

### æ–‡æœ¬æŠ¥å‘Š (report.txt)

```
============================================================
Memory Benchmark Analysis Report
============================================================

ä»»åŠ¡: conv-26
------------------------------------------------------------

F1:
  å¹³å‡å€¼: 0.8523
  æœ€å¤§å€¼: 0.9100
  æœ€å°å€¼: 0.6500
  æ ‡å‡†å·®: 0.0821
  å„è½®å¾—åˆ†: 0.6500, 0.7200, 0.7800, 0.8200, ...
```

### å›¾è¡¨

1. **å•æŒ‡æ ‡æŠ˜çº¿å›¾** (`{task_id}_{metric_name}.png`)

   - æ¨ªåæ ‡ï¼šæµ‹è¯•è½®æ¬¡
   - çºµåæ ‡ï¼šæŒ‡æ ‡å€¼

1. **å¤šæŒ‡æ ‡å¯¹æ¯”å›¾** (`{task_id}_multiple_metrics.png`)

   - å¤šæ¡æŠ˜çº¿å¯¹æ¯”ä¸åŒæŒ‡æ ‡

1. **å®éªŒå¯¹æ¯”å›¾** (`experiment_comparison.png`)

   - å¯¹æ¯”ä¸åŒé…ç½®çš„æ€§èƒ½

## ğŸ”§ æ‰©å±•ç‚¹

### 1. æ–°çš„åˆ†ææ¨¡å¼

åœ¨ `Analyzer` ç±»ä¸­æ·»åŠ æ–°æ–¹æ³•ï¼š

```python
def _compute_aggregate(self):
    """èšåˆæ¨¡å¼ï¼šæ±‡æ€»æ‰€æœ‰æ–‡ä»¶"""
    # å®ç°èšåˆé€»è¾‘
    pass
```

### 2. è‡ªå®šä¹‰æŒ‡æ ‡æƒé‡

```python
class WeightedF1(BaseMetric):
    def __init__(self, weights):
        self.weights = weights
        # ...
```

### 3. åˆ†ç±»åˆ«åˆ†æ

```python
def compute_by_category(self, test_results, category):
    """æŒ‰é—®é¢˜ç±»åˆ«è®¡ç®—æŒ‡æ ‡"""
    # è¿‡æ»¤ç‰¹å®šç±»åˆ«çš„é—®é¢˜
    # è®¡ç®—æŒ‡æ ‡
    pass
```

## ğŸ“‹ TODO

### é«˜ä¼˜å…ˆçº§

- [ ] å®ç° Precision æŒ‡æ ‡
- [ ] å®ç° Recall æŒ‡æ ‡
- [ ] å®ç° Exact Match æŒ‡æ ‡
- [ ] æ·»åŠ  aggregate æ¨¡å¼æ”¯æŒ

### ä¸­ä¼˜å…ˆçº§

- [ ] å®ç°æŸ±çŠ¶å›¾ç»˜åˆ¶
- [ ] å®ç°çƒ­åŠ›å›¾ç»˜åˆ¶ï¼ˆé—®é¢˜éš¾åº¦ vs å‡†ç¡®ç‡ï¼‰
- [ ] æ·»åŠ é…ç½®æ–‡ä»¶æ”¯æŒï¼ˆYAMLï¼‰
- [ ] æ”¯æŒæŒ‰é—®é¢˜ç±»åˆ«åˆ†æ

### ä½ä¼˜å…ˆçº§

- [ ] å®ç°é›·è¾¾å›¾ç»˜åˆ¶
- [ ] æ·»åŠ å»¶è¿Ÿå’Œååé‡æŒ‡æ ‡
- [ ] ç”Ÿæˆ HTML æŠ¥å‘Š
- [ ] æ”¯æŒå¯¼å‡ºåˆ° Excel

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. éµå¾ªç°æœ‰ä»£ç é£æ ¼
1. æ–°æŒ‡æ ‡å¿…é¡»ç»§æ‰¿ `BaseMetric`
1. æ·»åŠ å•å…ƒæµ‹è¯•ï¼ˆåœ¨ `__main__` å—ï¼‰
1. æ›´æ–° `__init__.py` å’Œ `data_analyze.py`
1. æ›´æ–°æœ¬ README

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»å›¢é˜Ÿã€‚
