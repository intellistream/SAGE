"""
Explore Metrics - æŒ‡æ ‡æ¢ç´¢è„šæœ¬

å¿«é€Ÿè®¡ç®—åŸºç¡€æŒ‡æ ‡ï¼Œç”¨äºæ¢ç´¢å“ªäº›æŒ‡æ ‡é€‚åˆè¯„ä¼°è®°å¿†å¢å¼ºç³»ç»Ÿã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python explore_metrics.py --input .sage/benchmarks/benchmark_memory/locomo/251121
    python explore_metrics.py --input .sage/benchmarks/benchmark_memory/locomo/251121 --task conv-26
"""

import argparse
import json
from pathlib import Path


def compute_token_f1(predicted: str, reference: str) -> dict:
    """è®¡ç®—åŸºäºtokençš„F1ã€ç²¾ç¡®ç‡ã€å¬å›ç‡"""
    if not predicted or not reference:
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0}

    pred_tokens = set(str(predicted).lower().split())
    ref_tokens = set(str(reference).lower().split())

    if not pred_tokens or not ref_tokens:
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0}

    common = pred_tokens & ref_tokens
    precision = len(common) / len(pred_tokens) if pred_tokens else 0.0
    recall = len(common) / len(ref_tokens) if ref_tokens else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

    return {"precision": precision, "recall": recall, "f1": f1}


def compute_exact_match(predicted: str, reference: str) -> float:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…ç‡"""
    pred_clean = str(predicted).strip().lower()
    ref_clean = str(reference).strip().lower()
    return 1.0 if pred_clean == ref_clean else 0.0


def analyze_single_file(file_path: Path) -> dict:
    """åˆ†æå•ä¸ªç»“æœæ–‡ä»¶"""
    with open(file_path, encoding="utf-8") as f:
        data = json.load(f)

    task_id = data.get("experiment_info", {}).get("task_id", "unknown")
    test_results = data.get("test_results", [])

    all_metrics = {
        "task_id": task_id,
        "total_rounds": len(test_results),
        "rounds": [],
        "overall": {},
    }

    # æŒ‰è½®æ¬¡è®¡ç®—æŒ‡æ ‡
    f1_scores = []
    em_scores = []

    for test_round in test_results:
        test_index = test_round.get("test_index", 0)
        questions = test_round.get("questions", [])

        round_f1 = []
        round_em = []

        for q in questions:
            pred = q.get("predicted_answer", "")
            ref = q.get("reference_answer", "")

            metrics = compute_token_f1(pred, ref)
            em = compute_exact_match(pred, ref)

            round_f1.append(metrics["f1"])
            round_em.append(em)

        avg_f1 = sum(round_f1) / len(round_f1) if round_f1 else 0.0
        avg_em = sum(round_em) / len(round_em) if round_em else 0.0

        all_metrics["rounds"].append(
            {"test_index": test_index, "f1": avg_f1, "exact_match": avg_em}
        )

        f1_scores.append(avg_f1)
        em_scores.append(avg_em)

    # è®¡ç®—æ•´ä½“ç»Ÿè®¡
    all_metrics["overall"] = {
        "avg_f1": sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,
        "avg_exact_match": sum(em_scores) / len(em_scores) if em_scores else 0.0,
        "max_f1": max(f1_scores) if f1_scores else 0.0,
        "min_f1": min(f1_scores) if f1_scores else 0.0,
    }

    return all_metrics


def main():
    parser = argparse.ArgumentParser(description="æŒ‡æ ‡æ¢ç´¢è„šæœ¬")
    parser.add_argument("--input", type=str, required=True, help="å®éªŒç»“æœæ–‡ä»¶å¤¹æˆ–æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--task", type=str, help="æŒ‡å®šä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--output", type=str, help="è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰")
    args = parser.parse_args()

    input_path = Path(args.input)

    if not input_path.exists():
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return

    # æ”¶é›†æ‰€æœ‰JSONæ–‡ä»¶
    if input_path.is_file():
        json_files = [input_path]
    else:
        json_files = list(input_path.rglob("*.json"))

    if args.task:
        # è¿‡æ»¤ç‰¹å®šä»»åŠ¡
        json_files = [f for f in json_files if args.task in f.name]

    if not json_files:
        print("âŒ æœªæ‰¾åˆ°JSONæ–‡ä»¶")
        return

    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š æŒ‡æ ‡æ¢ç´¢åˆ†æ")
    print(f"{'=' * 60}")
    print(f"æ–‡ä»¶æ•°: {len(json_files)}\n")

    all_results = []

    for json_file in json_files:
        print(f"åˆ†æ: {json_file.name}")
        try:
            result = analyze_single_file(json_file)
            all_results.append(result)

            # æ‰“å°ç»“æœ
            print(f"  ä»»åŠ¡ID: {result['task_id']}")
            print(f"  è½®æ¬¡æ•°: {result['total_rounds']}")
            print(f"  å¹³å‡F1: {result['overall']['avg_f1']:.4f}")
            print(f"  ç²¾ç¡®åŒ¹é…: {result['overall']['avg_exact_match']:.4f}")
            print()

        except Exception as e:
            print(f"  âŒ åˆ†æå¤±è´¥: {e}\n")

    # ä¿å­˜ç»“æœ
    if args.output:
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)

        output_file = output_dir / "metrics_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)

        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    print(f"\n{'=' * 60}")
    print("âœ¨ åˆ†æå®Œæˆ")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
