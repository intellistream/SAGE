"""
Quick Visualize - å¿«é€Ÿå¯è§†åŒ–è„šæœ¬

ç”ŸæˆåŸºç¡€å›¾è¡¨ï¼Œè§‚å¯ŸæŒ‡æ ‡éšè½®æ¬¡çš„å˜åŒ–è¶‹åŠ¿ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python quick_visualize.py --input .sage/benchmarks/benchmark_memory/locomo/251121
    python quick_visualize.py --input metrics_results.json --mode from_metrics
"""

import argparse
import json
from pathlib import Path

import matplotlib.pyplot as plt


def load_metrics_from_json(file_path: Path) -> list:
    """ä»explore_metrics.pyç”Ÿæˆçš„ç»“æœæ–‡ä»¶åŠ è½½æŒ‡æ ‡"""
    with open(file_path, encoding="utf-8") as f:
        return json.load(f)


def load_metrics_from_raw(folder_path: Path) -> list:
    """ä»åŸå§‹å®éªŒç»“æœè®¡ç®—æŒ‡æ ‡"""
    # è°ƒç”¨ explore_metrics çš„é€»è¾‘
    from explore_metrics import analyze_single_file

    json_files = list(folder_path.rglob("*.json"))
    results = []

    for json_file in json_files:
        try:
            result = analyze_single_file(json_file)
            results.append(result)
        except Exception as e:
            print(f"è·³è¿‡æ–‡ä»¶ {json_file.name}: {e}")

    return results


def plot_metric_by_rounds(metrics_data: dict, output_path: Path):
    """ç»˜åˆ¶æŒ‡æ ‡éšè½®æ¬¡å˜åŒ–çš„æŠ˜çº¿å›¾"""
    task_id = metrics_data["task_id"]
    rounds_data = metrics_data["rounds"]

    if not rounds_data:
        print(f"è·³è¿‡ {task_id}: æ— æ•°æ®")
        return

    test_indices = [r["test_index"] for r in rounds_data]
    f1_scores = [r["f1"] for r in rounds_data]
    em_scores = [r["exact_match"] for r in rounds_data]

    plt.figure(figsize=(10, 6))

    plt.plot(test_indices, f1_scores, marker="o", label="F1 Score", linewidth=2)
    plt.plot(test_indices, em_scores, marker="s", label="Exact Match", linewidth=2)

    plt.xlabel("Test Round", fontsize=12)
    plt.ylabel("Score", fontsize=12)
    plt.title(f"Metrics over Rounds - {task_id}", fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)

    # ä¿å­˜å›¾è¡¨
    output_file = output_path / f"{task_id}_metrics.png"
    plt.savefig(output_file, dpi=150, bbox_inches="tight")
    plt.close()

    print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {output_file}")


def plot_overall_comparison(all_metrics: list, output_path: Path):
    """ç»˜åˆ¶ä¸åŒä»»åŠ¡çš„æ•´ä½“å¯¹æ¯”"""
    if len(all_metrics) <= 1:
        return

    task_ids = [m["task_id"] for m in all_metrics]
    avg_f1s = [m["overall"]["avg_f1"] for m in all_metrics]
    avg_ems = [m["overall"]["avg_exact_match"] for m in all_metrics]

    x = range(len(task_ids))
    width = 0.35

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.bar([i - width / 2 for i in x], avg_f1s, width, label="Avg F1", alpha=0.8)
    ax.bar([i + width / 2 for i in x], avg_ems, width, label="Avg Exact Match", alpha=0.8)

    ax.set_xlabel("Task ID", fontsize=12)
    ax.set_ylabel("Score", fontsize=12)
    ax.set_title("Overall Metrics Comparison", fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(task_ids, rotation=45, ha="right")
    ax.legend()
    ax.grid(True, alpha=0.3, axis="y")

    output_file = output_path / "overall_comparison.png"
    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches="tight")
    plt.close()

    print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="å¿«é€Ÿå¯è§†åŒ–è„šæœ¬")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥è·¯å¾„ï¼ˆæ–‡ä»¶å¤¹æˆ–metrics_results.jsonï¼‰")
    parser.add_argument(
        "--mode",
        type=str,
        default="auto",
        choices=["auto", "from_raw", "from_metrics"],
        help="åŠ è½½æ¨¡å¼",
    )
    parser.add_argument("--output", type=str, default="./results/plots", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--metric", type=str, help="åªç”»ç‰¹å®šæŒ‡æ ‡ï¼ˆf1, emï¼‰")
    args = parser.parse_args()

    input_path = Path(args.input)
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"\n{'=' * 60}")
    print(f"ğŸ“ˆ å¿«é€Ÿå¯è§†åŒ–")
    print(f"{'=' * 60}\n")

    # åŠ è½½æ•°æ®
    if args.mode == "from_metrics" or (args.mode == "auto" and input_path.is_file()):
        print(f"ä»æŒ‡æ ‡æ–‡ä»¶åŠ è½½: {input_path}")
        all_metrics = load_metrics_from_json(input_path)
    else:
        print(f"ä»åŸå§‹ç»“æœåŠ è½½: {input_path}")
        all_metrics = load_metrics_from_raw(input_path)

    if not all_metrics:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®")
        return

    print(f"æ‰¾åˆ° {len(all_metrics)} ä¸ªä»»åŠ¡çš„æ•°æ®\n")

    # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆå›¾è¡¨
    for metrics_data in all_metrics:
        plot_metric_by_rounds(metrics_data, output_path)

    # ç”Ÿæˆæ•´ä½“å¯¹æ¯”å›¾
    if len(all_metrics) > 1:
        print()
        plot_overall_comparison(all_metrics, output_path)

    print(f"\n{'=' * 60}")
    print("âœ¨ å¯è§†åŒ–å®Œæˆ")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
