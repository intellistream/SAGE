# sageLLM - Self-Developed Inference Engine (Placeholder)

<p align="center">
  <strong>SAGE's Custom LLM Inference Engine Project</strong>
</p>

______________________________________________________________________

## âš ï¸ Status: Placeholder / Future Development

**sageLLM** is designed to be SAGE's **self-developed LLM inference engine** (similar in scope to
vLLM, TensorRT-LLM, etc.). However, the actual inference engine implementation is **not yet
developed**.

### What Happened?

The original sageLLM repository mistakenly contained **Control Plane** code, which has now been
**moved to the correct location**:

```
âŒ OLD (Incorrect):
packages/sage-llm-core/src/sage/llm/
â””â”€â”€ sageLLM/
    â”œâ”€â”€ control_plane/          # âŒ Should not be here
    â”œâ”€â”€ strategies/
    â””â”€â”€ executors/

âœ… NEW (Correct):
packages/sage-llm-core/src/sage/llm/
â”œâ”€â”€ control_plane/              # âœ… Engine-agnostic scheduling
â”‚   â”œâ”€â”€ manager.py
â”‚   â”œâ”€â”€ strategies/
â”‚   â””â”€â”€ executors/
â”œâ”€â”€ service.py                  # vLLM wrapper
â”œâ”€â”€ unified_client.py           # Unified API
â””â”€â”€ sageLLM/                    # ğŸ“¦ Future: Self-developed engine
    â””â”€â”€ (To be implemented)
```

## Architecture Overview

### Multi-Engine Design

SAGE supports **multiple inference engines** through a unified Control Plane:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          sage.llm.control_plane                     â”‚
â”‚   (Engine-agnostic scheduling & routing)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼             â–¼             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  vLLM   â”‚  â”‚ sageLLM â”‚  â”‚  Other   â”‚
   â”‚ (Ready) â”‚  â”‚(Future) â”‚  â”‚ (Future) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **vLLM**: Third-party engine (currently integrated)
- **sageLLM**: Self-developed engine (**this submodule**, not yet implemented)
- **Control Plane**: Unified scheduler for **all engines** (engine-agnostic)

### Why Separate Control Plane from Engines?

1. **Engine Independence**: Control Plane can schedule across different engines
1. **Fair Comparison**: Enables benchmarking vLLM vs sageLLM vs others
1. **Clear Responsibilities**:
   - Control Plane: Scheduling, routing, load balancing
   - Engines: Inference execution

## Current Status

### âœ… What Exists

- **Control Plane**: `packages/sage-llm-core/src/sage/llm/control_plane/`

  - Intelligent scheduling (FIFO, Priority, SLO-aware, Adaptive, etc.)
  - PD separation optimization
  - Multi-engine orchestration
  - Performance monitoring

- **vLLM Integration**: `packages/sage-llm-core/src/sage/llm/service.py`

  - VLLMService wrapper
  - API server integration

- **Unified Client**: `packages/sage-llm-core/src/sage/llm/unified_client.py`

  - Single entry point for all engines

### ğŸš§ What's Missing (This Submodule)

- **sageLLM Inference Engine**: Core inference implementation
- **Custom Kernels**: CUDA/Triton kernels for optimization
- **Model Loading**: Efficient model loading and caching
- **Attention Mechanisms**: PagedAttention or custom alternatives
- **Batching Logic**: Continuous batching implementation

## Migration Guide

If you have imports from the old structure:

```python
# âŒ OLD (Deprecated)
from sage.llm.sageLLM.control_plane import ControlPlaneManager
from sage.llm.sageLLM.control_plane.strategies import HybridSchedulingPolicy

# âœ… NEW (Correct)
from sage.llm.control_plane import ControlPlaneManager
from sage.llm.control_plane.strategies import HybridSchedulingPolicy
```

## How to Use (Current Functionality)

Even though sageLLM engine is not implemented, you can use the Control Plane with vLLM:

```python
from sage.llm import UnifiedInferenceClient

# Create client (uses Control Plane with vLLM backend)
client = UnifiedInferenceClient.create()

# LLM inference
response = client.chat([
    {"role": "user", "content": "Hello!"}
])

# Embedding
vectors = client.embed(["text1", "text2"])
```

## Future Development Roadmap

When sageLLM inference engine is implemented, it will include:

1. **Core Inference Engine**

   - Model loading and weight management
   - Efficient attention mechanisms (e.g., custom PagedAttention)
   - KV cache management

1. **Performance Optimizations**

   - Custom CUDA/Triton kernels
   - Continuous batching
   - Speculative decoding support

1. **Integration with Control Plane**

   - Register as an available engine
   - Expose metrics for scheduling decisions
   - Support PD separation hints

## Related Documentation

- **Control Plane**: `packages/sage-llm-core/src/sage/llm/control_plane/`
- **vLLM Integration**: `packages/sage-llm-core/src/sage/llm/service.py`
- **Unified Client**: `packages/sage-llm-core/src/sage/llm/unified_client.py`
- **Architecture Docs**: `docs-public/docs_src/dev-notes/l1-common/`

## Submodule Information

- **Repository**: https://github.com/intellistream/sageLLM.git
- **Branch**: main-dev
- **Purpose**: Self-developed inference engine (future implementation)
- **Parent Package**: sage-llm-core (L1)

## License

Apache License 2.0

______________________________________________________________________

**Note**: This is a placeholder submodule. The actual inference engine implementation is planned for
future development. For current LLM inference, use vLLM through the Control Plane.
max_parallel_requests=200, ), )

manager.register_instance(prefilling_instance) manager.register_instance(decoding_instance)

````

**æ€§èƒ½å¯¹æ¯”ï¼š**

| æŒ‡æ ‡              | å•å®ä¾‹   | PDåˆ†ç¦»  | æå‡    |
| ----------------- | -------- | ------- | ------- |
| ååé‡ (tokens/s) | 100      | 150-180 | +50-80% |
| P99å»¶è¿Ÿ (ms)      | 120      | 50-60   | -50-60% |
| GPUåˆ©ç”¨ç‡         | 75%      | 90%     | +15%    |
| æˆæœ¬æ•ˆç‡          | baseline | 1.8x    | +80%    |

### 3ï¸âƒ£ **åŠ¨æ€å¹¶è¡Œç­–ç•¥ï¼ˆ5ç§æ–¹æ¡ˆï¼‰**

è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„æ¨¡å‹å¹¶è¡Œæ–¹æ¡ˆï¼Œæ”¯æŒ TPã€PPã€DPã€EPã€Hybridï¼š

| å¹¶è¡Œç­–ç•¥                   | è¯´æ˜                   | é€‚ç”¨åœºæ™¯                |
| -------------------------- | ---------------------- | ----------------------- |
| **TP (Tensor Parallel)**   | å¼ é‡å¹¶è¡Œï¼Œæ¨¡å‹æƒé‡åˆ‡åˆ† | å•æ¨¡å‹å¤ªå¤§æ— æ³•æ”¾å…¥å•GPU |
| **PP (Pipeline Parallel)** | æµæ°´çº¿å¹¶è¡Œï¼Œæ¨¡å‹å±‚åˆ‡åˆ† | è¶…å¤§æ¨¡å‹ï¼ˆ70B+ï¼‰        |
| **DP (Data Parallel)**     | æ•°æ®å¹¶è¡Œï¼Œæ¨¡å‹å¤åˆ¶     | é«˜åååœºæ™¯              |
| **EP (Expert Parallel)**   | ä¸“å®¶å¹¶è¡Œï¼ŒMoEæ¨¡å‹      | Mixtralç­‰MoEæ¨¡å‹        |
| **Hybrid**                 | æ··åˆå¹¶è¡Œï¼Œç»„åˆå¤šç§ç­–ç•¥ | è¶…å¤§æ¨¡å‹+é«˜åå         |

```python
from control_plane import ParallelismConfig

# è‡ªåŠ¨ä¼˜åŒ–å¹¶è¡Œé…ç½®
config = ParallelismConfig(
    auto_optimize=True,
    supported_strategies=["TP", "PP", "Hybrid"],
)

# æ‰‹åŠ¨æŒ‡å®šå¹¶è¡Œé…ç½®
instance = ExecutionInstance(
    instance_id="hybrid-instance",
    tensor_parallel_size=4,     # TP=4
    pipeline_parallel_size=2,   # PP=2
    data_parallel_size=2,       # DP=2
    gpu_count=16,
)
````

**å¹¶è¡Œæ–¹æ¡ˆæ¨èï¼š**

| æ¨¡å‹å¤§å° | GPUæ•°é‡ | æ¨èç­–ç•¥            |
| -------- | ------- | ------------------- |
| \<10B    | 1-2     | TP=1 æˆ– TP=2        |
| 10B-30B  | 2-4     | TP=4                |
| 30B-70B  | 4-8     | TP=4 æˆ– TP=8        |
| 70B-175B | 8-16    | Hybrid (TP=4, PP=2) |
| >175B    | 16+     | Hybrid (TP=8, PP=4) |

### 4ï¸âƒ£ **è¯·æ±‚è·¯ç”±ç­–ç•¥**

æ”¯æŒå¤šç§è·¯ç”±ç®—æ³•ï¼Œä¼˜åŒ–è¯·æ±‚åˆ†å‘ï¼š

- **load_balanced**: è´Ÿè½½å‡è¡¡ï¼Œè·¯ç”±åˆ°è´Ÿè½½æœ€ä½çš„å®ä¾‹
- **round_robin**: è½®è¯¢
- **random**: éšæœºé€‰æ‹©
- **affinity**: ç”¨æˆ·äº²å’Œæ€§ï¼ŒåŒä¸€ç”¨æˆ·è¯·æ±‚è·¯ç”±åˆ°åŒä¸€å®ä¾‹ï¼ˆæé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼‰
- **locality**: åŸºäºå“ˆå¸Œçš„å±€éƒ¨æ€§è·¯ç”±ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡

```python
manager = ControlPlaneManager(
    routing_strategy="affinity",  # ç”¨æˆ·äº²å’Œæ€§è·¯ç”±
)
```

### 5ï¸âƒ£ **æ€§èƒ½ç›‘æ§ä¸æŒ‡æ ‡**

å®æ—¶æ”¶é›†å’Œåˆ†ææ€§èƒ½æŒ‡æ ‡ï¼š

```python
# è·å–æ€§èƒ½æŒ‡æ ‡
metrics = manager.get_metrics()

# è¯·æ±‚æŒ‡æ ‡
print(f"Total requests: {metrics.total_requests}")
print(f"Completed: {metrics.completed_requests}")
print(f"Active: {metrics.active_requests}")

# å»¶è¿ŸæŒ‡æ ‡
print(f"Avg latency: {metrics.avg_latency_ms}ms")
print(f"P95 latency: {metrics.p95_latency_ms}ms")
print(f"P99 latency: {metrics.p99_latency_ms}ms")

# ååæŒ‡æ ‡
print(f"Tokens/sec: {metrics.tokens_per_second}")
print(f"Requests/sec: {metrics.requests_per_second}")

# SLOæŒ‡æ ‡
print(f"SLO violations: {metrics.slo_violations}")
print(f"SLO compliance: {metrics.slo_compliance_rate:.2%}")

# èµ„æºæŒ‡æ ‡
print(f"GPU utilization: {metrics.avg_gpu_utilization:.2%}")
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/intellistream/sageLLM.git
cd sageLLM

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¼€å‘æ¨¡å¼å®‰è£…
pip install -e .
```

### åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from control_plane import (
    ControlPlaneManager,
    ExecutionInstance,
    RequestMetadata,
    RequestPriority,
)


async def main():
    # 1. åˆ›å»ºæ§åˆ¶å¹³é¢ç®¡ç†å™¨
    manager = ControlPlaneManager(
        scheduling_policy="adaptive",  # è‡ªé€‚åº”è°ƒåº¦
        routing_strategy="load_balanced",  # è´Ÿè½½å‡è¡¡
        enable_monitoring=True,
    )

    # 2. æ³¨å†Œ vLLM å®ä¾‹
    instance = ExecutionInstance(
        instance_id="vllm-1",
        host="localhost",
        port=8000,
        model_name="meta-llama/Llama-2-7b-chat-hf",
        tensor_parallel_size=2,
        gpu_count=2,
        max_concurrent_requests=100,
    )
    manager.register_instance(instance)

    # 3. å¯åŠ¨æ§åˆ¶å¹³é¢
    await manager.start()

    # 4. æäº¤æ¨ç†è¯·æ±‚
    request = RequestMetadata(
        request_id="req-001",
        user_id="user-123",
        priority=RequestPriority.HIGH,
        slo_deadline_ms=1000,  # 1ç§’SLO
        max_tokens=512,
        prompt="Explain quantum computing in simple terms.",
    )

    request_id = await manager.submit_request(request)
    print(f"Request submitted: {request_id}")

    # 5. ç­‰å¾…å¹¶è·å–ç»“æœ
    await asyncio.sleep(2)
    status = await manager.get_request_status(request_id)
    print(f"Request status: {status}")

    # 6. è·å–æ€§èƒ½æŒ‡æ ‡
    metrics = manager.get_metrics()
    print(f"Throughput: {metrics.requests_per_second:.2f} req/s")
    print(f"Avg Latency: {metrics.avg_latency_ms:.2f} ms")

    # 7. åœæ­¢æ§åˆ¶å¹³é¢
    await manager.stop()


if __name__ == "__main__":
    asyncio.run(main())
```

### é«˜çº§ä½¿ç”¨ç¤ºä¾‹

æ›´è¯¦ç»†çš„ä½¿ç”¨ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ï¼š

- **[HTTP å®¢æˆ·ç«¯æ¨¡å¼](./control_plane/examples/example_http_client.py)** - å®é™…éƒ¨ç½²åœºæ™¯ç¤ºä¾‹ï¼ˆå•æœºã€å¤šæœºã€æ··åˆéƒ¨ç½²ï¼‰
- **[å®Œæ•´æ¼”ç¤º](./control_plane/examples/demo_control_plane.py)** - åŠŸèƒ½æ¼”ç¤ºï¼ˆæ— éœ€ vLLM å®ä¾‹ï¼‰
- **[ç¤ºä¾‹æ–‡æ¡£](./control_plane/examples/README.md)** - ç¤ºä¾‹è¯´æ˜å’Œä½¿ç”¨æŒ‡å—
- **[é›†æˆæŒ‡å—](./dev-notes/INTEGRATION.md)** - ä¸åº”ç”¨é›†æˆçš„è¯¦ç»†æ­¥éª¤

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰ Control Plane æµ‹è¯•
cd tests/control_plane
python -m pytest -v

# è¿è¡Œç‰¹å®šæµ‹è¯•æ¨¡å—
python -m pytest test_scheduling.py -v      # è°ƒåº¦ç­–ç•¥æµ‹è¯•
python -m pytest test_pd_separation.py -v   # PD åˆ†ç¦»æµ‹è¯•
python -m pytest test_executor.py -v        # æ‰§è¡Œå™¨æµ‹è¯•
python -m pytest test_integration.py -v     # é›†æˆæµ‹è¯•

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
python -m pytest --cov=control_plane tests/control_plane/
```

**æµ‹è¯•ç»“æœï¼š** âœ… å…¨éƒ¨ 20 ä¸ªæµ‹è¯•é€šè¿‡

- âœ… 5 ä¸ªè°ƒåº¦æµ‹è¯• (`test_scheduling.py`)
- âœ… 5 ä¸ª PD åˆ†ç¦»æµ‹è¯• (`test_pd_separation.py`)
- âœ… 5 ä¸ªæ‰§è¡Œå™¨æµ‹è¯• (`test_executor.py`)
- âœ… 5 ä¸ªé›†æˆæµ‹è¯• (`test_integration.py`)

async def main(): # åˆ›å»ºæ§åˆ¶å¹³é¢ç®¡ç†å™¨ manager = ControlPlaneManager( scheduling_policy="adaptive",
enable_pd_separation=True, )

```
# æ³¨å†Œ vLLM å®ä¾‹
instance = ExecutionInstance(
    instance_id="llama-instance-1",
    model_name="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=2,
    gpu_count=2,
)
manager.register_instance(instance)

# å¤„ç†è¯·æ±‚
from vllm.sampling_params import SamplingParams

prompt = "Hello, how are you?"
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512,
)

output = await manager.process_request(
    prompt=prompt,
    sampling_params=sampling_params,
)

print(f"Output: {output}")

# è·å–æ€§èƒ½æŒ‡æ ‡
metrics = manager.get_metrics()
print(f"åå: {metrics.throughput} req/s")
print(f"å¹³å‡å»¶è¿Ÿ: {metrics.avg_latency} ms")
```

if __name__ == "__main__": asyncio.run(main())

````

æ›´è¯¦ç»†çš„ä½¿ç”¨ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ [`control_plane/examples/`](./control_plane/examples/) ç›®å½•ã€‚

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰ Control Plane æµ‹è¯•
cd tests/control_plane
python -m pytest -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
python -m pytest test_scheduling.py -v
python -m pytest test_pd_separation.py -v

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
python -m pytest --cov=control_plane tests/control_plane/
````

**æµ‹è¯•ç»“æœï¼š** âœ… å…¨éƒ¨ 17 ä¸ªæµ‹è¯•é€šè¿‡

- âœ… 5 ä¸ªè°ƒåº¦æµ‹è¯• (test_scheduling.py)
- âœ… 5 ä¸ª PD åˆ†ç¦»æµ‹è¯• (test_pd_separation.py)
- âœ… 5 ä¸ªæ‰§è¡Œå™¨æµ‹è¯• (test_executor.py)
- âœ… 2 ä¸ªé›†æˆæµ‹è¯• (test_integration.py)

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       User Application                           â”‚
â”‚                  (SAGE Apps, Custom Services)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ submit_request()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Control Plane (sageLLM)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Control Plane Manager (æ ¸å¿ƒç®¡ç†å™¨)              â”‚   â”‚
â”‚  â”‚  â€¢ è¯·æ±‚é˜Ÿåˆ—ç®¡ç† (pending_queue, running_requests)         â”‚   â”‚
â”‚  â”‚  â€¢ è°ƒåº¦å¾ªç¯ (scheduling_loop)                             â”‚   â”‚
â”‚  â”‚  â€¢ å¥åº·æ£€æŸ¥ (health_check_loop)                           â”‚   â”‚
â”‚  â”‚  â€¢ æ€§èƒ½ç›‘æ§ (performance monitoring)                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                  â”‚                  â”‚                â”‚
â”‚           â–¼                  â–¼                  â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Scheduling   â”‚  â”‚ Parallelism  â”‚  â”‚ PD Router &  â”‚          â”‚
â”‚  â”‚ Policies     â”‚  â”‚ Optimizer    â”‚  â”‚ Routing      â”‚          â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚          â”‚
â”‚  â”‚ â€¢ FIFO       â”‚  â”‚ â€¢ Auto TP/PP â”‚  â”‚ â€¢ Adaptive   â”‚          â”‚
â”‚  â”‚ â€¢ Priority   â”‚  â”‚ â€¢ DP/EP      â”‚  â”‚ â€¢ Hash-based â”‚          â”‚
â”‚  â”‚ â€¢ SLO-Aware  â”‚  â”‚ â€¢ Hybrid     â”‚  â”‚ â€¢ LB/Affinityâ”‚          â”‚
â”‚  â”‚ â€¢ Cost-Opt   â”‚  â”‚              â”‚  â”‚              â”‚          â”‚
â”‚  â”‚ â€¢ Adaptive   â”‚  â”‚              â”‚  â”‚              â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                  â”‚                  â”‚                â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                  â”‚ Execution Coordinator â”‚                        â”‚
â”‚                  â”‚  â€¢ Instance Registry  â”‚                        â”‚
â”‚                  â”‚  â€¢ Health Monitoring  â”‚                        â”‚
â”‚                  â”‚  â€¢ Metrics Collection â”‚                        â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚ HTTP API calls
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Execution Layer (vLLM)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ vLLM     â”‚  â”‚ vLLM     â”‚  â”‚ vLLM     â”‚  â”‚ vLLM     â”‚        â”‚
â”‚  â”‚ Instance â”‚  â”‚ Instance â”‚  â”‚ Instance â”‚  â”‚ Instance â”‚        â”‚
â”‚  â”‚    1     â”‚  â”‚    2     â”‚  â”‚    3     â”‚  â”‚    N     â”‚        â”‚
â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚        â”‚
â”‚  â”‚ TP=4     â”‚  â”‚ TP=2,PP=2â”‚  â”‚ DP=2     â”‚  â”‚ Hybrid   â”‚        â”‚
â”‚  â”‚ Prefill  â”‚  â”‚ Decode   â”‚  â”‚ Decode   â”‚  â”‚ General  â”‚        â”‚
â”‚  â”‚ ä¼˜åŒ–åå  â”‚  â”‚ ä¼˜åŒ–å»¶è¿Ÿ  â”‚  â”‚ é«˜å¹¶å‘   â”‚  â”‚ æ··åˆè´Ÿè½½ â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                   â”‚
â”‚  â€¢ GPU Memory: PagedAttention, KV Cache Management              â”‚
â”‚  â€¢ Kernels: CUDA, FlashAttention, FlashInfer                    â”‚
â”‚  â€¢ Quantization: GPTQ, AWQ, FP8, INT8                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è¯·æ±‚å¤„ç†æµç¨‹

```
1. User App submits RequestMetadata
          â†“
2. Control Plane Manager receives request
          â†“
3. Scheduling Policy determines priority/order
          â†“
4. PD Router (if enabled) determines request phase
   â€¢ Prefilling phase (long input)
   â€¢ Decoding phase (short input)
          â†“
5. Request Router selects appropriate instance
   â€¢ Load balancing
   â€¢ Affinity/Locality
   â€¢ Health check
          â†“
6. Execution Coordinator executes via HTTP API
   â€¢ POST /v1/completions or /v1/chat/completions
   â€¢ Stream or batch response
          â†“
7. vLLM Instance processes request
   â€¢ AsyncLLMEngine execution
   â€¢ KV cache management
   â€¢ GPU scheduling
          â†“
8. Response returns to Control Plane
          â†“
9. Metrics collected and updated
          â†“
10. Result returns to User App
```

### æ ¸å¿ƒç»„ä»¶è¯´æ˜

#### 1. Control Plane Manager (`manager.py`)

- æ ¸å¿ƒåè°ƒå±‚ï¼Œç®¡ç†æ•´ä¸ªè¯·æ±‚ç”Ÿå‘½å‘¨æœŸ
- ç»´æŠ¤è¯·æ±‚é˜Ÿåˆ—å’Œè¿è¡ŒçŠ¶æ€
- åè°ƒå„ä¸ªå­ç»„ä»¶å·¥ä½œ

#### 2. Scheduling Strategies (`strategies/`)

- 5ç§è°ƒåº¦ç­–ç•¥ï¼šFIFOã€Priorityã€SLO-Awareã€Cost-Optimizedã€Adaptive
- æ¨¡å—åŒ–è®¾è®¡ï¼Œæ¯ä¸ªç­–ç•¥ç‹¬ç«‹æ–‡ä»¶
- æ”¯æŒè‡ªå®šä¹‰ç­–ç•¥å¼€å‘ï¼ˆå‚è§ `docs/CUSTOM_SCHEDULING.md`ï¼‰

#### 3. PD Router (`pd_routing.py`)

- Prefilling/Decoding åˆ†ç¦»è·¯ç”±
- æ ¹æ®è¯·æ±‚ç‰¹å¾ï¼ˆè¾“å…¥é•¿åº¦ã€è¾“å‡ºé•¿åº¦ï¼‰åˆ¤æ–­é˜¶æ®µ
- å°†è¯·æ±‚è·¯ç”±åˆ°ä¸“é—¨ä¼˜åŒ–çš„å®ä¾‹

#### 4. Request Router (`router.py`)

- è¯·æ±‚è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
- æ”¯æŒå¤šç§è·¯ç”±ç­–ç•¥ï¼šload_balancedã€round_robinã€affinityã€locality
- è€ƒè™‘å®ä¾‹å¥åº·çŠ¶æ€å’Œå½“å‰è´Ÿè½½

#### 5. Parallelism Optimizer (`parallelism.py`)

- è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¹¶è¡Œç­–ç•¥
- æ”¯æŒ TPã€PPã€DPã€EPã€Hybrid
- æ ¹æ®æ¨¡å‹å¤§å°å’Œ GPU æ•°é‡æ¨èé…ç½®

#### 6. Execution Coordinator (`executor.py`)

- ç®¡ç†æ‰€æœ‰ vLLM å®ä¾‹
- æ‰§è¡Œ HTTP API è°ƒç”¨
- å¥åº·æ£€æŸ¥å’ŒæŒ‡æ ‡æ”¶é›†

#### 7. Types (`types.py`)

- æ•°æ®æ¨¡å‹å®šä¹‰
- æšä¸¾ç±»å‹
- é…ç½®ç±»

## ğŸ“š æ–‡æ¡£

- **[é›†æˆæŒ‡å—](./dev-notes/INTEGRATION.md)** - Control Plane é›†æˆæ¶æ„å’Œä½¿ç”¨æŒ‡å—
- **[éƒ¨ç½²æŒ‡å—](./dev-notes/DEPLOYMENT.md)** - vLLM å®ä¾‹éƒ¨ç½²é…ç½®
- **[é¡¹ç›®ç»“æ„](./STRUCTURE.md)** - è¯¦ç»†çš„ç›®å½•ç»“æ„è¯´æ˜
- **[æµ‹è¯•æ–‡æ¡£](./tests/control_plane/README.md)** - æµ‹è¯•å¥—ä»¶è¯´æ˜

## âš™ï¸ ç¯å¢ƒè®¾ç½®

### GPU æ”¯æŒ (ç”Ÿäº§ç¯å¢ƒæ¨è)

```bash
# å®‰è£… CUDA Toolkit (Ubuntu/Debian)
sudo apt update && sudo apt install -y nvidia-cuda-toolkit

# éªŒè¯ CUDA å®‰è£…
nvcc --version
nvidia-smi

# å®‰è£… vLLMï¼ˆä¼šè‡ªåŠ¨ç¼–è¯‘ CUDA å†…æ ¸ï¼‰
pip install vllm

# é‡æ–°å®‰è£… sageLLM (å¦‚éœ€ç¼–è¯‘æ‰©å±•)
pip install -e .
```

### CPU æµ‹è¯•ç¯å¢ƒ

```bash
# æµ‹è¯•å¯ä»¥åœ¨æ²¡æœ‰ GPU çš„ç¯å¢ƒä¸‹è¿è¡Œï¼ˆä»…ç”¨äºå•å…ƒæµ‹è¯•ï¼‰
cd tests/control_plane
python -m pytest -v

# æ³¨æ„ï¼šå®é™…æ¨ç†éœ€è¦ GPU
```

## ğŸ”— ä¾èµ–å…³ç³»

### æ ¸å¿ƒä¾èµ–

- **vLLM** (>= 0.3.0): LLM æ¨ç†å¼•æ“
- **PyTorch** (>= 2.0.0): æ·±åº¦å­¦ä¹ æ¡†æ¶
- **Python** (>= 3.8): ç¼–ç¨‹è¯­è¨€

### å¯é€‰ä¾èµ–

- **asyncio**: å¼‚æ­¥ç¼–ç¨‹ï¼ˆPython å†…ç½®ï¼‰
- **pydantic**: æ•°æ®éªŒè¯
- **pytest**: å•å…ƒæµ‹è¯•
- **pytest-cov**: æµ‹è¯•è¦†ç›–ç‡

è¯¦è§ `requirements.txt` å’Œ `requirements-dev.txt`

## ğŸš¢ éƒ¨ç½²

### æœ¬åœ°å¼€å‘

```bash
# å¯åŠ¨å•ä¸ª vLLM å®ä¾‹
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1

# å¯åŠ¨ Control Plane
python -m control_plane.example
```

### ç”Ÿäº§ç¯å¢ƒ

å‚è€ƒ [éƒ¨ç½²æŒ‡å—](./dev-notes/DEPLOYMENT.md) äº†è§£ï¼š

- å¤šå®ä¾‹éƒ¨ç½²
- PD åˆ†ç¦»é…ç½®
- è´Ÿè½½å‡è¡¡è®¾ç½®
- ç›‘æ§å’Œæ—¥å¿—

## ğŸ“ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: é«˜ååæ‰¹å¤„ç†

```python
# ä½¿ç”¨ DP (æ•°æ®å¹¶è¡Œ) æé«˜åå
manager = ControlPlaneManager(scheduling_policy="fifo")
instance = ExecutionInstance(
    instance_id="batch-instance",
    data_parallel_size=4,
    gpu_count=4,
)
```

### åœºæ™¯ 2: ä½å»¶è¿Ÿåœ¨çº¿æœåŠ¡

```python
# ä½¿ç”¨ SLO-Aware è°ƒåº¦ + PD åˆ†ç¦»
manager = ControlPlaneManager(
    scheduling_policy="slo_aware",
    enable_pd_separation=True,
)
# æ³¨å†Œ decoding ä¼˜åŒ–å®ä¾‹ï¼ˆä½å»¶è¿Ÿï¼‰
```

### åœºæ™¯ 3: æ··åˆä¼˜å…ˆçº§

```python
# ä½¿ç”¨ Priority è°ƒåº¦
manager = ControlPlaneManager(scheduling_policy="priority")

# é«˜ä¼˜å…ˆçº§è¯·æ±‚
high_priority_request = RequestMetadata(
    priority=RequestPriority.CRITICAL,
    slo_deadline_ms=500,
)

# ä½ä¼˜å…ˆçº§è¯·æ±‚
low_priority_request = RequestMetadata(
    priority=RequestPriority.LOW,
)
```

### åœºæ™¯ 4: æˆæœ¬ä¼˜åŒ–

```python
# ä½¿ç”¨ Cost-Optimized è°ƒåº¦
manager = ControlPlaneManager(scheduling_policy="cost_optimized")

# è®¾ç½®æˆæœ¬é¢„ç®—
request = RequestMetadata(
    cost_budget=0.01,  # æœ€å¤šèŠ±è´¹ $0.01
)
```

## ğŸ“„ è®¸å¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ï¼Œè¯¦è§ [LICENSE](./LICENSE)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](../../../../../../CONTRIBUTING.md) äº†è§£å¦‚ä½•å‚ä¸å¼€å‘ã€‚

### å¿«é€Ÿå¼€å§‹è´¡çŒ®

```bash
# Fork å’Œ Clone
git clone https://github.com/yourusername/SAGE.git
cd packages/sage-common/src/sage/common/components/sage_llm/sageLLM

# åˆ›å»ºç‰¹æ€§åˆ†æ”¯
git checkout -b feature/your-feature

# ä¿®æ”¹ä»£ç å¹¶æäº¤
git add .
git commit -m "feat: your feature description"

# Push å¹¶åˆ›å»º PR
git push origin feature/your-feature
```

## ğŸ“® è”ç³»æ–¹å¼

- ğŸ“§ é‚®ä»¶ï¼šè¯·é€šè¿‡ GitHub Issues è”ç³»
- ğŸ’¬ è®¨è®ºï¼šä½¿ç”¨ GitHub Discussions
- ğŸ› Bug æŠ¥å‘Šï¼šGitHub Issues

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢ [vLLM é¡¹ç›®](https://github.com/vllm-project/vllm) æä¾›ä¼˜ç§€çš„ LLM æ¨ç†å¼•æ“
- æ„Ÿè°¢ SAGE é¡¹ç›®å›¢é˜Ÿçš„æ”¯æŒå’ŒæŒ‡å¯¼
