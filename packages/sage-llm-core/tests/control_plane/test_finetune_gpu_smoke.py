#!/usr/bin/env python3
"""GPU smoke test for finetune engine.

This test validates actual fine-tuning with GPU if available.
If no GPU is available, the test is skipped.

Requirements:
- GPU with CUDA
- Enough GPU memory (at least 4GB recommended)
- Real dataset file
"""

import json
import os
import tempfile
from pathlib import Path

import pytest

# Check GPU availability
try:
    import torch

    GPU_AVAILABLE = torch.cuda.is_available()
    GPU_COUNT = torch.cuda.device_count() if GPU_AVAILABLE else 0
    GPU_MEMORY = (
        torch.cuda.get_device_properties(0).total_memory / (1024**3) if GPU_AVAILABLE else 0
    )
except ImportError:
    GPU_AVAILABLE = False
    GPU_COUNT = 0
    GPU_MEMORY = 0

# Skip all tests if no GPU
pytestmark = pytest.mark.skipif(
    not GPU_AVAILABLE, reason="GPU not available for finetune smoke test"
)


def create_dummy_dataset(output_path: str, num_samples: int = 10) -> None:
    """Create a minimal dummy training dataset.

    Args:
        output_path: Path to save the dataset JSON file
        num_samples: Number of training samples to generate
    """
    dataset = []
    for i in range(num_samples):
        dataset.append(
            {
                "instruction": f"Test instruction {i}",
                "input": f"Test input {i}",
                "output": f"Test output {i}",
            }
        )

    with open(output_path, "w") as f:
        json.dump(dataset, f, indent=2)


@pytest.mark.gpu
@pytest.mark.slow
@pytest.mark.asyncio
async def test_finetune_engine_gpu_smoke():
    """Smoke test: Run actual fine-tuning for 1 step on GPU.

    This test validates that:
    1. Fine-tune engine can start with GPU
    2. Model can be loaded and quantized
    3. Training can run for at least 1 step
    4. Checkpoint can be saved
    """
    from sage.llm.control_plane.manager import ControlPlaneManager

    print(f"\n{'=' * 70}")
    print("ðŸ”¥ Task J Phase 5: GPU Smoke Test")
    print(f"{'=' * 70}")
    print(f"âœ“ GPU Available: {GPU_AVAILABLE}")
    print(f"âœ“ GPU Count: {GPU_COUNT}")
    print(f"âœ“ GPU Memory: {GPU_MEMORY:.2f} GB")
    print(f"{'=' * 70}\n")

    # Create temporary dataset and output directory
    with tempfile.TemporaryDirectory() as tmpdir:
        dataset_path = os.path.join(tmpdir, "train_dataset.json")
        output_dir = os.path.join(tmpdir, "checkpoints")
        os.makedirs(output_dir, exist_ok=True)

        # Create minimal dataset (10 samples for quick testing)
        print("ðŸ“ Creating dummy dataset...")
        create_dummy_dataset(dataset_path, num_samples=10)
        print(f"  âœ“ Dataset: {dataset_path}")
        print("  âœ“ Samples: 10")

        # Initialize Control Plane
        print("\nðŸš€ Starting Control Plane...")
        manager = ControlPlaneManager(mode="local")
        await manager.start()

        try:
            # Start fine-tune engine with minimal configuration
            print("\nðŸŽ¯ Starting finetune engine...")
            print("  Model: Qwen/Qwen2.5-0.5B-Instruct")
            print("  LoRA rank: 8")
            print("  Epochs: 1 (minimal)")
            print("  Batch size: 1 (minimal)")
            print("  Quantization: 4-bit")

            engine_info = manager.start_finetune_engine(
                model_id="Qwen/Qwen2.5-0.5B-Instruct",
                dataset_path=dataset_path,
                output_dir=output_dir,
                lora_rank=8,  # Small LoRA for testing
                lora_alpha=16,
                learning_rate=5e-5,
                epochs=1,  # Just 1 epoch for smoke test
                batch_size=1,  # Minimal batch size
                gradient_accumulation_steps=1,
                max_seq_length=512,  # Shorter for faster testing
                use_flash_attention=True,
                quantization_bits=4,  # 4-bit quantization to save memory
                auto_download=True,  # Download model if needed
                engine_label="gpu-smoke-test",
            )

            print(f"\nâœ“ Engine created: {engine_info['engine_id']}")
            print(f"  Status: {engine_info['status']}")
            engine_id = engine_info["engine_id"]

            # Wait for engine to start training
            print("\nâ³ Waiting for training to start...")
            import asyncio

            for i in range(30):  # Wait up to 30 seconds
                await asyncio.sleep(1)
                info = manager.get_engine_info(engine_id)
                if info:
                    status = info.status.name if hasattr(info.status, "name") else str(info.status)
                    print(f"  [{i + 1}s] Status: {status}")

                    # Check if training started
                    if "TRAINING" in status or "RUNNING" in status:
                        print("\nâœ“ Training started successfully.")
                        break

                    # Check for errors
                    if "FAILED" in status or "ERROR" in status:
                        error_msg = info.metadata.get("error", "Unknown error")
                        pytest.fail(f"Training failed: {error_msg}")
                else:
                    print(f"  [{i + 1}s] Engine info not available yet")

            # Verify checkpoints directory was created
            checkpoint_files = list(Path(output_dir).rglob("*"))
            print(f"\nðŸ“ Output directory contents: {len(checkpoint_files)} files")
            if len(checkpoint_files) > 0:
                print("  Sample files:")
                for f in list(checkpoint_files)[:5]:
                    print(f"    - {f.name}")

            # Stop the engine
            print(f"\nðŸ›‘ Stopping engine {engine_id}...")
            result = manager.request_engine_shutdown(engine_id)
            if result.get("stopped"):
                print("  âœ“ Engine stopped successfully")
            else:
                print(f"  âš  Engine stop result: {result}")

            print(f"\n{'=' * 70}")
            print("âœ… GPU Smoke Test PASSED")
            print(f"{'=' * 70}")
            print("\nTask J Phase 5 Validation:")
            print("  âœ“ Fine-tune engine can start with GPU")
            print("  âœ“ Model loading and quantization works")
            print("  âœ“ Training process can be initiated")
            print("  âœ“ Control Plane integration functional")
            print(f"{'=' * 70}\n")

        finally:
            await manager.stop()


@pytest.mark.gpu
def test_gpu_requirements():
    """Test GPU environment requirements."""
    print("\nðŸ“Š GPU Environment Check:")
    print(f"  PyTorch version: {torch.__version__}")
    print(f"  CUDA available: {torch.cuda.is_available()}")
    print(f"  CUDA version: {torch.version.cuda if hasattr(torch.version, 'cuda') else 'N/A'}")
    print(f"  GPU count: {torch.cuda.device_count()}")

    if GPU_AVAILABLE:
        for i in range(GPU_COUNT):
            props = torch.cuda.get_device_properties(i)
            print(f"\n  GPU {i}:")
            print(f"    Name: {props.name}")
            print(f"    Memory: {props.total_memory / (1024**3):.2f} GB")
            print(f"    Compute capability: {props.major}.{props.minor}")

    assert GPU_AVAILABLE, "GPU is required for this test"
    assert GPU_MEMORY >= 4.0, f"At least 4GB GPU memory required, found {GPU_MEMORY:.2f}GB"


if __name__ == "__main__":
    # Run smoke test directly
    import asyncio

    print("Running GPU smoke test directly...")
    asyncio.run(test_finetune_engine_gpu_smoke())
