import os
import time
from typing import TYPE_CHECKING, Any

from sage.common.utils.logging.custom_logger import CustomLogger
from sage.kernel.fault_tolerance.factory import (
    create_fault_handler_from_config,
    create_lifecycle_manager,
)
from sage.kernel.runtime.heartbeat_monitor import HeartbeatMonitor
from sage.kernel.scheduler.api import BaseScheduler
from sage.kernel.utils.ray.actor import ActorWrapper
from sage.kernel.utils.ray.ray_utils import ensure_ray_initialized

if TYPE_CHECKING:
    from sage.kernel.api.base_environment import BaseEnvironment
    from sage.kernel.runtime.context.service_context import ServiceContext
    from sage.kernel.runtime.graph.execution_graph import ExecutionGraph
    from sage.kernel.runtime.service.local_service_task import LocalServiceTask
    from sage.kernel.runtime.task.local_task import LocalTask


# è¿™ä¸ªdispatcherå¯ä»¥ç›´æ¥æ‰“åŒ…ä¼ ç»™ray sage daemon service
class Dispatcher:
    def __init__(self, graph: "ExecutionGraph", env: "BaseEnvironment"):
        self.total_stop_signals = graph.total_stop_signals
        self.received_stop_signals = 0
        self.graph = graph
        self.env = env
        self.name: str = env.name
        self.remote = env.platform == "remote"
        # self.nodes: Dict[str, Union[ActorHandle, LocalDAGNode]] = {}
        self.tasks: dict[str, LocalTask | ActorWrapper] = {}
        self.services: dict[str, LocalServiceTask | ActorWrapper] = {}  # å­˜å‚¨æœåŠ¡å®ä¾‹
        self.is_running: bool = False
        # HeartbeatMonitor å®ä¾‹ (ç›‘æ§çº¿ç¨‹)
        self.heartbeat_monitor: HeartbeatMonitor | None = None

        # å®¹é”™é…ç½®
        self.fault_tolerance_config = {
            "enabled": False,  # æ˜¯å¦å¯ç”¨å®¹é”™
            "heartbeat_interval": 5.0,  # å¿ƒè·³é—´éš” (ç§’)
            "heartbeat_timeout": 15.0,  # è¶…æ—¶é˜ˆå€¼ (ç§’)
            "max_restart_attempts": 3,  # æœ€å¤§é‡å¯æ¬¡æ•°
        }

        # ä½¿ç”¨è°ƒåº¦å™¨å’Œå®¹é”™ç®¡ç†å™¨ï¼ˆé‡æ„åæ¶æ„ï¼‰
        # è°ƒåº¦å™¨ï¼šçº¯å†³ç­–è€…ï¼ˆè¿”å› PlacementDecisionï¼‰
        # PlacementExecutorï¼šçº¯æ‰§è¡Œè€…ï¼ˆæ¥æ”¶å†³ç­–ï¼Œæ‰§è¡Œæ”¾ç½®ï¼‰
        # Dispatcherï¼šåè°ƒè€…ï¼ˆå†³ç­– â†’ æ‰§è¡Œï¼‰

        # åˆå§‹åŒ–è°ƒåº¦å™¨
        self.scheduler: BaseScheduler
        if hasattr(env, "scheduler") and env.scheduler is not None:
            self.scheduler = env.scheduler
        else:
            from sage.kernel.scheduler.impl import FIFOScheduler

            self.scheduler = FIFOScheduler(platform=env.platform)

        # åˆå§‹åŒ–æ”¾ç½®æ‰§è¡Œå™¨ï¼ˆç”± Dispatcher æŒæœ‰ï¼Œä¸åœ¨ Scheduler ä¸­ï¼‰
        from sage.kernel.scheduler.placement import PlacementExecutor

        self.placement_executor = PlacementExecutor()

        # ä» Environment é…ç½®åˆ›å»ºå®¹é”™å¤„ç†å™¨
        fault_tolerance_config = env.config.get("fault_tolerance", {})
        self.fault_handler = create_fault_handler_from_config(fault_tolerance_config)
        self.lifecycle_manager = create_lifecycle_manager()

        self.setup_logging_system()

        # æ³¨å…¥ logger åˆ°å®¹é”™ç®¡ç†å™¨
        self.fault_handler.logger = self.logger
        if hasattr(self.lifecycle_manager, "logger"):
            self.lifecycle_manager.logger = self.logger  # type: ignore
        self.fault_handler.dispatcher = self

        self.logger.info(f"Dispatcher '{self.name}' construction complete")
        if fault_tolerance_config:
            strategy = fault_tolerance_config.get("strategy", "restart")
            self.logger.info(f"Fault tolerance enabled: strategy={strategy}")
        if env.platform == "remote":
            self.logger.info(f"Dispatcher '{self.name}' is running in remote mode")
            ensure_ray_initialized()

    def enable_fault_tolerance(
        self,
        heartbeat_interval: float = 5.0,
        heartbeat_timeout: float = 15.0,
        max_restart_attempts: int = 3,
    ):
        """
        å¯ç”¨ Remote ç¯å¢ƒæ•…éšœå®¹é”™

        Args:
            heartbeat_interval: å¿ƒè·³å‘é€é—´éš” (ç§’)
            heartbeat_timeout: å¿ƒè·³è¶…æ—¶é˜ˆå€¼ (ç§’)
            max_restart_attempts: æœ€å¤§é‡å¯å°è¯•æ¬¡æ•°
        """
        self.fault_tolerance_config.update(
            {
                "enabled": True,
                "heartbeat_interval": heartbeat_interval,
                "heartbeat_timeout": heartbeat_timeout,
                "max_restart_attempts": max_restart_attempts,
            }
        )

        self.logger.info(
            f"ğŸ›¡ï¸  Fault tolerance enabled: "
            f"interval={heartbeat_interval}s, timeout={heartbeat_timeout}s"
        )

    def _init_heartbeat_monitor(self):
        """
        åˆå§‹åŒ– HeartbeatMonitor ç›‘æ§çº¿ç¨‹

        åœ¨æ‰€æœ‰ä»»åŠ¡åˆ›å»ºåè°ƒç”¨,å¼€å§‹å¿ƒè·³ç›‘æ§
        """
        if not self.fault_tolerance_config["enabled"]:
            return

        if self.heartbeat_monitor is not None:
            self.logger.warning("HeartbeatMonitor already initialized")
            return

        try:
            from sage.kernel.runtime.heartbeat_monitor import HeartbeatMonitor

            # åˆ›å»º HeartbeatMonitor
            self.heartbeat_monitor = HeartbeatMonitor(
                dispatcher=self,
                check_interval=self.fault_tolerance_config["heartbeat_interval"],
            )
            # å¯åŠ¨ç›‘æ§çº¿ç¨‹
            self.heartbeat_monitor.start()

            self.logger.info("ğŸ” HeartbeatMonitor started")

        except Exception as e:
            self.logger.error(
                f"âŒ Failed to initialize HeartbeatMonitor: {e}", exc_info=True
            )

    def receive_stop_signal(self):
        """
        æ¥æ”¶åœæ­¢ä¿¡å·å¹¶å¤„ç†
        """
        self.logger.info("Dispatcher received stop signal.")
        self.received_stop_signals += 1
        if self.received_stop_signals >= self.total_stop_signals:
            self.logger.info(
                f"Received all {self.total_stop_signals} stop signals, stopping dispatcher for batch job."
            )
            self.cleanup()
            return True
        else:
            return False

    def receive_node_stop_signal(self, node_name: str) -> bool:
        """
        æ¥æ”¶å•ä¸ªèŠ‚ç‚¹çš„åœæ­¢ä¿¡å·

        Args:
            node_name: åœæ­¢çš„èŠ‚ç‚¹åç§°

        Returns:
            bool: å¦‚æœæ‰€æœ‰èŠ‚ç‚¹éƒ½å·²åœæ­¢è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        self.logger.info(f"Dispatcher received node stop signal from: {node_name}")

        # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
        if node_name not in self.tasks:
            self.logger.warning(f"Node {node_name} not found in tasks")
            return False

        # å¦‚æœè¿™æ˜¯ä¸€ä¸ªæºèŠ‚ç‚¹ï¼Œç›´æ¥é€šçŸ¥æ‰€æœ‰ç›¸å…³çš„ JoinOperator
        self._notify_join_operators_on_source_stop(node_name)

        # åœæ­¢å¹¶æ¸…ç†æŒ‡å®šèŠ‚ç‚¹
        try:
            task = self.tasks[node_name]
            task.stop()
            task.cleanup()

            # ä»ä»»åŠ¡åˆ—è¡¨ä¸­ç§»é™¤
            del self.tasks[node_name]

            self.logger.info(f"Node {node_name} stopped and cleaned up")

        except Exception as e:
            self.logger.error(f"Error stopping node {node_name}: {e}", exc_info=True)
            return False

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹éƒ½å·²åœæ­¢
        if len(self.tasks) == 0:
            self.logger.info(
                "All computation nodes stopped, batch processing completed"
            )
            self.is_running = False

            # å½“æ‰€æœ‰è®¡ç®—èŠ‚ç‚¹åœæ­¢åï¼Œä¹Ÿåº”è¯¥æ¸…ç†æœåŠ¡
            if len(self.services) > 0:
                self.logger.info(
                    f"Cleaning up {len(self.services)} services after batch completion"
                )
                self._cleanup_services_after_batch_completion()

            return True
        else:
            self.logger.info(
                f"Remaining nodes: {len(self.tasks)}, services: {len(self.services)}"
            )
            return False

    def _notify_join_operators_on_source_stop(self, source_node_name: str):
        """å½“æºèŠ‚ç‚¹åœæ­¢æ—¶ï¼Œç›´æ¥é€šçŸ¥ç›¸å…³çš„ JoinOperator"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯æºèŠ‚ç‚¹ï¼ˆä»¥ "Source" å¼€å¤´ï¼‰
        if not source_node_name.startswith("Source"):
            return

        # æŸ¥æ‰¾æ‰€æœ‰çš„ JoinOperator
        for task_name, task in self.tasks.items():
            if (
                hasattr(task, "operator")
                and hasattr(task.operator, "handle_stop_signal")
                and hasattr(task.operator, "__class__")
                and "JoinOperator" in task.operator.__class__.__name__
            ):
                # è¿™æ˜¯ä¸€ä¸ª JoinOperatorï¼Œåˆ›å»ºä¸€ä¸ªåœæ­¢ä¿¡å·å¹¶ç›´æ¥å‘é€
                from sage.kernel.runtime.communication.router.packet import StopSignal

                stop_signal = StopSignal(source_node_name)

                try:
                    # ç›´æ¥è°ƒç”¨ JoinOperator çš„ handle_stop_signal æ–¹æ³•
                    if hasattr(task.operator, "handle_stop_signal"):
                        task.operator.handle_stop_signal(stop_signal)  # type: ignore
                        self.logger.info(
                            f"Notified JoinOperator {task_name} about source {source_node_name} stopping"
                        )
                except Exception as e:
                    self.logger.error(f"Failed to notify JoinOperator {task_name}: {e}")

    def _cleanup_services_after_batch_completion(self):
        """åœ¨æ‰¹å¤„ç†å®Œæˆåæ¸…ç†æ‰€æœ‰æœåŠ¡"""
        self.logger.info("Cleaning up services after batch completion")

        if self.remote:
            # æ¸…ç† Ray æœåŠ¡ (ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨)
            try:
                self.lifecycle_manager.cleanup_all(
                    tasks={}, services=self.services, cleanup_timeout=5.0
                )
            except Exception as e:
                self.logger.error(f"Error cleaning up Ray services: {e}")
        else:
            # æ¸…ç†æœ¬åœ°æœåŠ¡
            for service_name, service_task in list(self.services.items()):
                try:
                    # å…ˆåœæ­¢æœåŠ¡ï¼ˆå¦‚æœè¿˜åœ¨è¿è¡Œï¼‰
                    if hasattr(service_task, "is_running") and service_task.is_running:
                        self.logger.debug(f"Stopping service task: {service_name}")
                        if hasattr(service_task, "stop"):
                            service_task.stop()

                    # æ¸…ç†æœåŠ¡ï¼ˆæ— è®ºæ˜¯å¦åœ¨è¿è¡Œï¼‰
                    if hasattr(service_task, "cleanup"):
                        self.logger.debug(f"Cleaning up service task: {service_name}")
                        service_task.cleanup()

                    self.logger.info(
                        f"Service task '{service_name}' cleaned up successfully"
                    )
                except Exception as e:
                    self.logger.error(
                        f"Error cleaning up service task {service_name}: {e}"
                    )

        # æ¸…ç©ºæœåŠ¡å­—å…¸
        self.services.clear()
        self.logger.info("All services cleaned up")

    def setup_logging_system(self):
        base_dir = self.env.env_base_dir if self.env.env_base_dir is not None else "."
        self.logger = CustomLogger(
            [
                ("console", "INFO"),  # æ§åˆ¶å°æ˜¾ç¤ºé‡è¦ä¿¡æ¯
                (
                    os.path.join(base_dir, "Dispatcher.log"),
                    "DEBUG",
                ),  # è¯¦ç»†æ—¥å¿—
                (os.path.join(base_dir, "Error.log"), "ERROR"),  # é”™è¯¯æ—¥å¿—
            ],
            name=f"Dispatcher_{self.name}",
        )

    def start(self):
        # ç¬¬ä¸‰æ­¥ï¼šå¯åŠ¨æ‰€æœ‰æœåŠ¡ä»»åŠ¡
        for service_name, service_task in self.services.items():
            try:
                if hasattr(service_task, "start_running"):
                    service_task.start_running()
                elif hasattr(service_task, "_actor"):
                    # ActorWrapperåŒ…è£…çš„æœåŠ¡
                    import ray

                    actor_ref = service_task._actor
                    if hasattr(actor_ref, "start_running"):
                        ray.get(actor_ref.start_running.remote())  # type: ignore
                self.logger.debug(f"Started service task: {service_name}")
            except Exception as e:
                self.logger.error(
                    f"Failed to start service task {service_name}: {e}", exc_info=True
                )

        # ç¬¬å››æ­¥ï¼šæäº¤æ‰€æœ‰èŠ‚ç‚¹å¼€å§‹è¿è¡Œ
        for node_name, task in list(self.tasks.items()):
            try:
                task.start_running()
                self.logger.debug(f"Started node: {node_name}")
            except Exception as e:
                self.logger.error(
                    f"Failed to start node {node_name}: {e}", exc_info=True
                )

        self.logger.info(
            f"Job submission completed: {len(self.tasks)} nodes, {len(self.services)} service tasks"
        )
        if self.fault_tolerance_config["enabled"] and self.remote:
            self._init_heartbeat_monitor()
        self.is_running = True

    def _create_service_context(self, service_name: str) -> "ServiceContext | None":
        """
        è·å–service taskçš„ServiceContextï¼ˆä»execution graphä¸­å·²åˆ›å»ºçš„service nodeè·å–ï¼‰

        Args:
            service_name: æœåŠ¡åç§°

        Returns:
            ä»execution graphä¸­è·å–çš„ServiceContextï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        try:
            # ä»execution graphçš„service_nodesä¸­æŸ¥æ‰¾å¯¹åº”çš„service_node
            service_node = None
            for _node_name, node in self.graph.service_nodes.items():
                # é€šè¿‡service_factoryçš„åç§°åŒ¹é…
                if (
                    hasattr(node, "service_factory")
                    and node.service_factory
                    and node.service_factory.service_name == service_name
                ):
                    service_node = node
                    break

            if service_node is None:
                self.logger.error(
                    f"Service node for service '{service_name}' not found in execution graph"
                )
                return None

            # ç›´æ¥è¿”å›å·²ç»åˆ›å»ºå¥½çš„ServiceContext
            if not hasattr(service_node, "ctx") or service_node.ctx is None:
                self.logger.error(
                    f"ServiceContext not found in service node for service '{service_name}'"
                )
                return None

            self.logger.debug(
                f"Retrieved ServiceContext for service '{service_name}' from execution graph"
            )
            return service_node.ctx

        except Exception as e:
            self.logger.error(
                f"Failed to retrieve ServiceContext for service {service_name}: {e}",
                exc_info=True,
            )
            return None

    # Dispatcher will submit the job to LocalEngine or Ray Server.
    def submit(self):
        """
        ç¼–è¯‘å›¾ç»“æ„ï¼Œåˆ›å»ºèŠ‚ç‚¹å¹¶å»ºç«‹è¿æ¥

        é‡æ„åçš„æµç¨‹ï¼š
        1. è°ƒç”¨ Scheduler è·å–è°ƒåº¦å†³ç­–
        2. æ ¹æ®å†³ç­–ç­‰å¾…ï¼ˆå¦‚æœéœ€è¦å»¶è¿Ÿè°ƒåº¦ï¼‰
        3. è°ƒç”¨ PlacementExecutor æ‰§è¡Œç‰©ç†æ”¾ç½®
        4. å¯åŠ¨æ‰€æœ‰ä»»åŠ¡
        """
        self.logger.info(f"Compiling Job for graph: {self.name}")

        # ç¬¬ä¸€æ­¥ï¼šè°ƒåº¦æ‰€æœ‰æœåŠ¡ä»»åŠ¡
        for service_node_name, service_node in self.graph.service_nodes.items():
            service_name = None
            try:
                service_name = service_node.service_name

                # ä¸ºserviceåˆ›å»ºä¸“ç”¨çš„runtime context
                service_ctx = self._create_service_context(service_name)

                # === æ–°æ¶æ„ï¼šScheduler â†’ Decision â†’ Placement ===
                # 1. è·å–è°ƒåº¦å†³ç­–
                decision = self.scheduler.make_service_decision(service_node)

                self.logger.debug(
                    f"Service scheduling decision for '{service_name}': {decision}"
                )

                # 2. æ ¹æ®å†³ç­–ç­‰å¾…ï¼ˆå¦‚æœéœ€è¦å»¶è¿Ÿï¼‰
                if decision.delay > 0:
                    self.logger.debug(
                        f"Delaying service placement of '{service_name}' by {decision.delay}s"
                    )
                    time.sleep(decision.delay)

                # 3. æ‰§è¡Œç‰©ç†æ”¾ç½®
                service_task = self.placement_executor.place_service(
                    service_node=service_node,
                    decision=decision,
                    runtime_ctx=service_ctx,
                )
                self.services[service_name] = service_task

                self.logger.debug(
                    f"Placed service task '{service_name}' of type '{service_task.__class__.__name__}'"
                )
            except Exception as e:
                error_service = service_name if service_name else service_node_name
                self.logger.error(
                    f"Failed to schedule and place service task {error_service}: {e}",
                    exc_info=True,
                )
                # å¯ä»¥é€‰æ‹©ç»§ç»­æˆ–åœæ­¢ï¼Œè¿™é‡Œé€‰æ‹©ç»§ç»­ä½†è®°å½•é”™è¯¯

        # ç¬¬äºŒæ­¥ï¼šè°ƒåº¦æ‰€æœ‰è®¡ç®—ä»»åŠ¡èŠ‚ç‚¹
        for node_name, graph_node in self.graph.nodes.items():
            try:
                # === æ–°æ¶æ„ï¼šScheduler â†’ Decision â†’ Placement ===
                # æ³¨å…¥ dispatcher å¼•ç”¨åˆ° context (ç”¨äºå®¹é”™å¤„ç†)
                graph_node.ctx.dispatcher = self

                # 1. è·å–è°ƒåº¦å†³ç­–
                decision = self.scheduler.make_decision(graph_node)

                self.logger.debug(
                    f"Task scheduling decision for '{node_name}': {decision}"
                )

                # 2. æ ¹æ®å†³ç­–ç­‰å¾…ï¼ˆå¦‚æœéœ€è¦å»¶è¿Ÿè°ƒåº¦ï¼‰
                if decision.delay > 0:
                    self.logger.debug(
                        f"Delaying task placement of '{node_name}' by {decision.delay}s"
                    )
                    time.sleep(decision.delay)

                # 3. æ‰§è¡Œç‰©ç†æ”¾ç½®
                task = self.placement_executor.place_task(
                    task_node=graph_node, decision=decision, runtime_ctx=graph_node.ctx
                )
                self.tasks[node_name] = task

                self.logger.debug(
                    f"Placed task '{node_name}' of type '{task.__class__.__name__}' "
                    f"on node '{decision.target_node or 'default'}'"
                )
            except Exception as e:
                self.logger.error(
                    f"Failed to schedule and place task {node_name}: {e}", exc_info=True
                )
                raise e

        # è¿æ¥å…³ç³»å·²ç»åœ¨execution graphå±‚é€šè¿‡task contextè®¾ç½®å¥½äº†ï¼Œæ— éœ€åœ¨æ­¤å¤„è®¾ç½®

        try:
            self.start()
        except Exception as e:
            self.logger.error(f"Error starting dispatcher: {e}", exc_info=True)
            raise e

    def stop(self):
        """åœæ­¢æ‰€æœ‰ä»»åŠ¡å’ŒæœåŠ¡"""
        if self.heartbeat_monitor is not None:
            self.logger.info("ğŸ” Stopping HeartbeatMonitor...")
            self.heartbeat_monitor.stop()
            self.heartbeat_monitor = None

        if not self.is_running:
            self.logger.warning("Dispatcher is not running")
            return

        self.logger.info(f"Stopping dispatcher '{self.name}'")

        # å‘é€åœæ­¢ä¿¡å·ç»™æ‰€æœ‰ä»»åŠ¡
        for node_name, node_instance in self.tasks.items():
            try:
                node_instance.stop()
                self.logger.debug(f"Sent stop signal to node: {node_name}")
            except Exception as e:
                self.logger.error(f"Error stopping node {node_name}: {e}")

        # åœæ­¢æ‰€æœ‰æœåŠ¡ä»»åŠ¡
        for service_name, service_task in self.services.items():
            try:
                service_task.stop()
                self.logger.debug(f"Stopped service task: {service_name}")
            except Exception as e:
                self.logger.error(f"Error stopping service task {service_name}: {e}")

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡åœæ­¢ï¼ˆæœ€å¤šç­‰å¾…10ç§’ï¼‰
        self._wait_for_tasks_stop(timeout=10.0)

        self.is_running = False
        self.logger.info("Dispatcher stopped")

    def _wait_for_tasks_stop(self, timeout: float = 10.0):
        """ç­‰å¾…æ‰€æœ‰ä»»åŠ¡åœæ­¢"""
        start_time = time.time()

        while time.time() - start_time < timeout:
            all_stopped = True

            for _node_name, task in self.tasks.items():
                if hasattr(task, "is_running") and task.is_running:
                    all_stopped = False
                    break

            if all_stopped:
                self.logger.debug("All tasks stopped")
                return

            time.sleep(0.1)

        self.logger.warning(f"Timeout waiting for tasks to stop after {timeout}s")

    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        self.logger.info(f"Cleaning up dispatcher '{self.name}'")

        try:
            # åœæ­¢æ‰€æœ‰ä»»åŠ¡å’ŒæœåŠ¡
            if self.is_running:
                self.stop()

            if self.remote:
                # ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨æ¸…ç†æ‰€æœ‰Rayèµ„æº
                self.lifecycle_manager.cleanup_all(
                    tasks=self.tasks, services=self.services, cleanup_timeout=5.0
                )
            else:
                # æ¸…ç†æœ¬åœ°ä»»åŠ¡
                for node_name, task in self.tasks.items():
                    try:
                        task.cleanup()
                        self.logger.debug(f"Cleaned up task: {node_name}")
                    except Exception as e:
                        self.logger.error(f"Error cleaning up task {node_name}: {e}")

                # æ¸…ç†æœ¬åœ°æœåŠ¡ä»»åŠ¡
                for service_name, service_task in self.services.items():
                    try:
                        if hasattr(service_task, "cleanup"):
                            service_task.cleanup()
                        self.logger.debug(f"Cleaned up service task: {service_name}")
                    except Exception as e:
                        self.logger.error(
                            f"Error cleaning up service task {service_name}: {e}"
                        )

            # æ¸…ç©ºä»»åŠ¡å’ŒæœåŠ¡å­—å…¸
            self.tasks.clear()
            self.services.clear()

            self.logger.info("Dispatcher cleanup completed")

        except Exception as e:
            self.logger.error(f"Error during dispatcher cleanup: {e}")

    def get_task_status(self) -> dict[str, Any]:
        """è·å–æ‰€æœ‰ä»»åŠ¡çš„çŠ¶æ€"""
        status = {}

        for node_name, task in self.tasks.items():
            try:
                task_status = {
                    "name": node_name,
                    "running": getattr(task, "is_running", False),
                    "processed_count": getattr(task, "_processed_count", 0),
                    "error_count": getattr(task, "_error_count", 0),
                }
                status[node_name] = task_status
            except Exception as e:
                status[node_name] = {"name": node_name, "error": str(e)}

        return status

    def get_service_status(self) -> dict[str, Any]:
        """è·å–æ‰€æœ‰æœåŠ¡ä»»åŠ¡çš„çŠ¶æ€"""
        status = {}

        for service_name, service_task in self.services.items():
            try:
                if hasattr(service_task, "get_statistics"):
                    service_status = service_task.get_statistics()
                elif hasattr(service_task, "_actor"):
                    # ActorWrapperåŒ…è£…çš„æœåŠ¡
                    actor_ref = service_task._actor
                    if hasattr(actor_ref, "get_statistics"):
                        service_status = actor_ref.get_statistics()  # type: ignore
                    else:
                        service_status = {
                            "service_name": service_name,
                            "type": service_task.__class__.__name__,
                            "status": "unknown",
                        }
                else:
                    service_status = {
                        "service_name": service_name,
                        "type": service_task.__class__.__name__,
                        "status": "unknown",
                    }
                status[service_name] = service_status
            except Exception as e:
                status[service_name] = {"service_name": service_name, "error": str(e)}

        return status

    def restart_task(self, task_id: str, restore_state: dict | None = None) -> bool:
        """
        é‡å¯ä»»åŠ¡ï¼ˆç”¨äºå®¹é”™æ¢å¤ï¼‰

        Args:
            task_id: è¦é‡å¯çš„ä»»åŠ¡ ID
            restore_state: å¯é€‰çš„çŠ¶æ€ï¼Œå¦‚æœæä¾›åˆ™åœ¨å¯åŠ¨å‰æ¢å¤

        Returns:
            True å¦‚æœé‡å¯æˆåŠŸ
        """
        self.logger.info(f"ğŸ”„ Restarting task {task_id}")

        if task_id not in self.tasks:
            self.logger.error(f"âŒ Task {task_id} not found")
            return False

        try:
            task = self.tasks[task_id]

            # === æ­¥éª¤ 1: åœæ­¢æ—§ä»»åŠ¡ ===
            if hasattr(task, "is_running") and task.is_running:
                self.logger.debug(f"Stopping old task {task_id}...")
                if hasattr(task, "stop"):
                    task.stop()

                # ç­‰å¾…ä»»åŠ¡åœæ­¢
                import time

                max_wait = 5.0
                waited = 0.0
                while (
                    hasattr(task, "is_running")
                    and task.is_running
                    and waited < max_wait
                ):
                    time.sleep(0.1)
                    waited += 0.1

                if hasattr(task, "is_running") and task.is_running:
                    self.logger.warning(f"âš ï¸ Task {task_id} did not stop gracefully")

            # === æ­¥éª¤ 2: æ¸…ç†æ—§ä»»åŠ¡èµ„æº ===
            if hasattr(task, "cleanup"):
                try:
                    self.logger.debug(f"Cleaning up old task {task_id}...")
                    task.cleanup()
                except Exception as cleanup_error:
                    self.logger.warning(f"âš ï¸ Error during cleanup: {cleanup_error}")

            # === æ­¥éª¤ 3: è·å– graph node å¹¶é‡æ–°åˆ›å»ºä»»åŠ¡ ===
            graph_node = self.graph.nodes.get(task_id)
            if not graph_node:
                self.logger.error(f"âŒ Graph node for {task_id} not found")
                return False

            # é‡æ–°æ³¨å…¥ dispatcher å¼•ç”¨åˆ° context
            if graph_node.ctx is not None:
                graph_node.ctx.dispatcher = self

            self.logger.debug(f"Creating new task instance for {task_id}...")

            decision = self.scheduler.make_decision(graph_node)
            new_task = self.placement_executor.place_task(
                task_node=graph_node, decision=decision, runtime_ctx=graph_node.ctx
            )

            # æ›¿æ¢æ—§ä»»åŠ¡
            self.tasks[task_id] = new_task

            self.logger.info(f"âœ… New task instance created for {task_id}")

            # === æ­¥éª¤ 4: å¦‚æœæœ‰çŠ¶æ€ï¼Œå…ˆæ¢å¤çŠ¶æ€ ===
            if restore_state and hasattr(new_task, "restore_state"):
                self.logger.debug(f"Restoring state for {task_id}...")
                try:
                    new_task.restore_state(restore_state)
                    self.logger.info(f"âœ… State restored for task {task_id}")
                except Exception as restore_error:
                    self.logger.error(
                        f"âŒ Failed to restore state for {task_id}: {restore_error}",
                        exc_info=True,
                    )
                    return False

            # === æ­¥éª¤ 5: å¯åŠ¨æ–°ä»»åŠ¡ ===
            self.logger.debug(f"Starting new task {task_id}...")
            new_task.start_running()

            self.logger.info(f"ğŸ‰ Task {task_id} restarted successfully")
            return True

        except Exception as e:
            self.logger.error(
                f"âŒ Failed to restart task {task_id}: {e}", exc_info=True
            )
            return False

    def restart_task_with_state(self, task_id: str, state: dict) -> bool:
        """
        é‡å¯ä»»åŠ¡å¹¶æ¢å¤çŠ¶æ€ï¼ˆä¸“é—¨ç”¨äº checkpoint æ¢å¤ï¼‰

        è¿™æ˜¯ restart_task çš„ä¾¿æ·æ–¹æ³•ï¼Œæ˜ç¡®è¡¨ç¤ºè¦æ¢å¤çŠ¶æ€

        Args:
            task_id: è¦é‡å¯çš„ä»»åŠ¡ ID
            state: è¦æ¢å¤çš„çŠ¶æ€

        Returns:
            True å¦‚æœé‡å¯å’Œæ¢å¤æˆåŠŸ
        """
        return self.restart_task(task_id, restore_state=state)
