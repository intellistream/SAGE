import logging
import threading
import time
from typing import Any, Dict, List

import pytest
from sage.core.api.function.base_function import BaseFunction
from sage.core.api.function.sink_function import SinkFunction
from sage.core.api.function.source_function import SourceFunction
from sage.core.api.local_environment import LocalEnvironment


class DocumentSource(SourceFunction):
    """ç”Ÿæˆæ–‡æ¡£æ•°æ®æº"""

    def __init__(self, documents=None, **kwargs):
        super().__init__(**kwargs)
        self.counter = 0
        self.documents = documents or [
            {
                "content": "This is a sample document with some text content.",
                "id": "doc1",
            },
            {
                "content": "Another document containing different information and data.",
                "id": "doc2",
            },
            {
                "content": "Large document with extensive content that needs chunking for processing.",
                "id": "doc3",
            },
            {"content": "Short text document.", "id": "doc4"},
            {
                "content": "Medium sized document with reasonable content length.",
                "id": "doc5",
            },
        ]

    def execute(self):
        if self.counter >= len(self.documents):
            return None

        data = self.documents[self.counter]
        self.counter += 1
        self.logger.info(f"DocumentSource generated: {data['id']}")
        return data


class CharacterSplitter(BaseFunction):
    """å­—ç¬¦çº§æ–‡æœ¬åˆ†å—ç®—å­ - ä½¿ç”¨parallelism hintsè¿›è¡Œæ€§èƒ½ä¼˜åŒ–"""

    def __init__(self, chunk_size=50, overlap=10, **kwargs):
        super().__init__(**kwargs)
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.instance_id = id(self)
        self.thread_id = threading.get_ident()
        logging.info(
            f"ðŸ”§ CharacterSplitter instance {self.instance_id} created in thread {self.thread_id}"
        )

    def execute(self, document):
        """å•æ–‡æ¡£å¤„ç†é€»è¾‘ï¼Œæ¡†æž¶ä¼šè‡ªåŠ¨å¹¶è¡ŒåŒ–"""
        content = document.get("content", "")
        doc_id = document.get("id", "unknown")
        chunks = []

        current_thread = threading.get_ident()
        instance_id = id(self)

        logging.info(
            f"âš™ï¸ CharacterSplitter[{instance_id}]: Processing {doc_id} (thread: {current_thread})"
        )

        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.01)

        for i in range(0, len(content), self.chunk_size - self.overlap):
            chunk_text = content[i : i + self.chunk_size]
            if chunk_text.strip():  # è¿‡æ»¤ç©ºå—
                chunks.append(
                    {
                        "content": chunk_text,
                        "doc_id": doc_id,
                        "start_idx": i,
                        "end_idx": min(i + self.chunk_size, len(content)),
                        "chunk_id": f"{doc_id}_chunk_{len(chunks)}",
                    }
                )

        logging.info(
            f"âœ… CharacterSplitter[{instance_id}]: Generated {len(chunks)} chunks for {doc_id}"
        )
        return chunks


class ChunkCollector(SinkFunction):
    """æ”¶é›†åˆ†å—ç»“æžœçš„sinkç®—å­"""

    _collected_chunks: List[Dict] = []
    _lock = threading.Lock()

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.instance_id = id(self)
        self.thread_id = threading.get_ident()
        logging.info(
            f"ðŸ”§ ChunkCollector instance {self.instance_id} created in thread {self.thread_id}"
        )

    def execute(self, chunks):
        current_thread = threading.get_ident()
        instance_id = id(self)

        with self._lock:
            if isinstance(chunks, list):
                self._collected_chunks.extend(chunks)
                logging.info(
                    f"ðŸŽ¯ ChunkCollector[{instance_id}]: Collected {len(chunks)} chunks (thread: {current_thread})"
                )
            else:
                self._collected_chunks.append(chunks)
                logging.info(
                    f"ðŸŽ¯ ChunkCollector[{instance_id}]: Collected 1 chunk (thread: {current_thread})"
                )

    @classmethod
    def get_collected_chunks(cls):
        """èŽ·å–æ”¶é›†åˆ°çš„æ‰€æœ‰chunks"""
        return cls._collected_chunks.copy()

    @classmethod
    def clear_collected_chunks(cls):
        """æ¸…ç©ºæ”¶é›†åˆ°çš„chunks"""
        cls._collected_chunks.clear()


class TestChunkParallelism:
    """æµ‹è¯•chunkç®—å­çš„å¹¶è¡Œæ€§ä¼˜åŒ–"""

    def test_chunk_parallelism_hints_basic(self):
        """æµ‹è¯•åŸºæœ¬çš„chunkå¹¶è¡Œæ€§hintsåŠŸèƒ½"""
        logging.info("\n" + "=" * 70)
        logging.info("TEST: Basic Chunk Parallelism Hints")
        logging.info("=" * 70)

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        ChunkCollector.clear_collected_chunks()

        env = LocalEnvironment(name="chunk_parallelism_test")

        # æµ‹è¯•æ–‡æ¡£
        documents = [
            {"content": "This is document one with some content.", "id": "doc1"},
            {"content": "This is document two with different content.", "id": "doc2"},
            {"content": "This is document three with more content here.", "id": "doc3"},
        ]

        # ä½¿ç”¨parallelismå‚æ•°ç›´æŽ¥è®¾ç½®2ä¸ªå¹¶è¡Œchunkå®žä¾‹
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .map(CharacterSplitter, chunk_size=20, overlap=5, parallelism=2)
            .sink(ChunkCollector, parallelism=1)
        )

        # æ‰§è¡Œ
        env.submit()

        # ç­‰å¾…å¤„ç†å®Œæˆ
        time.sleep(2)
        env.close()

        # éªŒè¯ç»“æžœ
        collected_chunks = ChunkCollector.get_collected_chunks()
        assert len(collected_chunks) > 0
        assert all(chunk.get("chunk_id") for chunk in collected_chunks)

        # éªŒè¯æ‰€æœ‰æ–‡æ¡£éƒ½è¢«å¤„ç†
        processed_doc_ids = set(chunk["doc_id"] for chunk in collected_chunks)
        expected_doc_ids = set(doc["id"] for doc in documents)
        assert processed_doc_ids == expected_doc_ids

        logging.info(
            f"âœ… Collected {len(collected_chunks)} chunks from {len(processed_doc_ids)} documents"
        )

    def test_chunk_parallelism_hints_multiple_levels(self):
        """æµ‹è¯•å¤šçº§å¹¶è¡Œåº¦è®¾ç½®"""
        logging.info("\n" + "=" * 70)
        logging.info("TEST: Multi-level Chunk Parallelism Hints")
        logging.info("=" * 70)

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        ChunkCollector.clear_collected_chunks()

        env = LocalEnvironment(name="multi_level_parallelism_test")

        # æ›´å¤šæµ‹è¯•æ–‡æ¡£
        documents = [
            {"content": "Document one content here.", "id": "doc1"},
            {"content": "Document two content here.", "id": "doc2"},
            {"content": "Document three content here.", "id": "doc3"},
            {"content": "Document four content here.", "id": "doc4"},
            {"content": "Document five content here.", "id": "doc5"},
        ]

        # æµ‹è¯•ä¸åŒçš„å¹¶è¡Œåº¦è®¾ç½®
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .map(CharacterSplitter, chunk_size=15, overlap=3, parallelism=3)
            .sink(ChunkCollector, parallelism=2)
        )

        env.submit()

        # éªŒè¯ç»“æžœ
        collected_chunks = ChunkCollector.get_collected_chunks()
        assert len(collected_chunks) > 0

        # éªŒè¯chunkç»“æž„
        for chunk in collected_chunks:
            assert "content" in chunk
            assert "doc_id" in chunk
            assert "start_idx" in chunk
            assert "end_idx" in chunk
            assert "chunk_id" in chunk
            assert len(chunk["content"]) > 0

        logging.info(
            f"âœ… Multi-level parallelism test completed with {len(collected_chunks)} chunks"
        )

    def test_chunk_parallelism_hints_large_documents(self):
        """æµ‹è¯•å¤§æ–‡æ¡£çš„chunkå¹¶è¡Œå¤„ç†"""
        logging.info("\n" + "=" * 70)
        logging.info("TEST: Large Document Chunk Parallelism")
        logging.info("=" * 70)

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        ChunkCollector.clear_collected_chunks()

        env = LocalEnvironment(name="large_doc_test")

        # æ¨¡æ‹Ÿå¤§æ–‡æ¡£
        large_content = "This is a large document with extensive content. " * 50
        documents = [
            {"content": large_content, "id": "large_doc1"},
            {"content": large_content, "id": "large_doc2"},
        ]

        # ä½¿ç”¨æ›´é«˜çš„å¹¶è¡Œåº¦å¤„ç†å¤§æ–‡æ¡£
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .map(CharacterSplitter, chunk_size=100, overlap=20, parallelism=4)
            .sink(ChunkCollector, parallelism=1)
        )

        env.submit()

        # éªŒè¯å¤§æ–‡æ¡£è¢«æ­£ç¡®åˆ†å—
        collected_chunks = ChunkCollector.get_collected_chunks()
        assert len(collected_chunks) > 0

        # æ£€æŸ¥å¤§æ–‡æ¡£äº§ç”Ÿäº†å¤šä¸ªchunk
        large_doc_chunks = [c for c in collected_chunks if c["doc_id"] == "large_doc1"]
        assert len(large_doc_chunks) > 1  # å¤§æ–‡æ¡£åº”è¯¥è¢«åˆ†æˆå¤šä¸ªchunk

        logging.info(f"âœ… Large document test: {len(large_doc_chunks)} chunks from large_doc1")

    def test_chunk_parallelism_hints_vs_manual_parallelization(self):
        """å¯¹æ¯”parallelism hintsä¸Žæ‰‹åŠ¨å¹¶è¡ŒåŒ–çš„åŒºåˆ«"""
        logging.info("\n" + "=" * 70)
        logging.info("TEST: Parallelism Hints vs Manual Parallelization")
        logging.info("=" * 70)

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        ChunkCollector.clear_collected_chunks()

        # ä½¿ç”¨parallelism hintsçš„æ–¹å¼
        env1 = LocalEnvironment(name="hints_test")
        documents = [
            {"content": "Test document one.", "id": "test1"},
            {"content": "Test document two.", "id": "test2"},
        ]

        result1 = (
            env1.from_collection(DocumentSource, documents)
            .map(CharacterSplitter, chunk_size=10, overlap=2, parallelism=2)
            .sink(ChunkCollector)
        )
        env1.submit()

        # éªŒè¯hintsæ–¹å¼å·¥ä½œæ­£å¸¸
        collected_chunks = ChunkCollector.get_collected_chunks()
        assert len(collected_chunks) > 0

        logging.info("âœ… Parallelism hints approach works correctly")
        logging.info("ðŸ’¡ Key advantage: Framework manages parallelism, code stays simple")
