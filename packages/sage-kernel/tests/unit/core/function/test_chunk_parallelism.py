import threading
import time
from typing import Any, Dict, List

import pytest

from sage.core.api.function.base_function import BaseFunction
from sage.core.api.function.sink_function import SinkFunction
from sage.core.api.function.source_function import SourceFunction
from sage.core.api.local_environment import LocalEnvironment


class DocumentSource(SourceFunction):
    """ç”Ÿæˆæ–‡æ¡£æ•°æ®æº"""

    def __init__(self, documents=None, **kwargs):
        super().__init__(**kwargs)
        self.counter = 0
        self.documents = documents or [
            {"content": "This is a sample document with some text content.", "id": "doc1"},
            {"content": "Another document containing different information and data.", "id": "doc2"},
            {"content": "Large document with extensive content that needs chunking for processing.", "id": "doc3"},
            {"content": "Short text document.", "id": "doc4"},
            {"content": "Medium sized document with reasonable content length.", "id": "doc5"},
        ]

    def execute(self):
        if self.counter >= len(self.documents):
            return None

        data = self.documents[self.counter]
        self.counter += 1
        self.logger.info(f"DocumentSource generated: {data['id']}")
        return data


class CharacterSplitter(BaseFunction):
    """å­—ç¬¦çº§æ–‡æœ¬åˆ†å—ç®—å­ - ä½¿ç”¨parallelism hintsè¿›è¡Œæ€§èƒ½ä¼˜åŒ–"""

    def __init__(self, chunk_size=50, overlap=10, **kwargs):
        super().__init__(**kwargs)
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.instance_id = id(self)
        self.thread_id = threading.get_ident()
        print(f"ðŸ”§ CharacterSplitter instance {self.instance_id} created in thread {self.thread_id}")

    def execute(self, document):
        """å•æ–‡æ¡£å¤„ç†é€»è¾‘ï¼Œæ¡†æž¶ä¼šè‡ªåŠ¨å¹¶è¡ŒåŒ–"""
        content = document.get("content", "")
        doc_id = document.get("id", "unknown")
        chunks = []

        current_thread = threading.get_ident()
        instance_id = id(self)

        print(f"âš™ï¸ CharacterSplitter[{instance_id}]: Processing {doc_id} (thread: {current_thread})")

        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.01)

        for i in range(0, len(content), self.chunk_size - self.overlap):
            chunk_text = content[i:i + self.chunk_size]
            if chunk_text.strip():  # è¿‡æ»¤ç©ºå—
                chunks.append({
                    "content": chunk_text,
                    "doc_id": doc_id,
                    "start_idx": i,
                    "end_idx": min(i + self.chunk_size, len(content)),
                    "chunk_id": f"{doc_id}_chunk_{len(chunks)}"
                })

        print(f"âœ… CharacterSplitter[{instance_id}]: Generated {len(chunks)} chunks for {doc_id}")
        return chunks


class ChunkCollector(SinkFunction):
    """æ”¶é›†åˆ†å—ç»“æžœçš„sinkç®—å­"""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.collected_chunks = []
        self.instance_id = id(self)
        self.thread_id = threading.get_ident()
        print(f"ðŸ”§ ChunkCollector instance {self.instance_id} created in thread {self.thread_id}")

    def execute(self, chunks):
        current_thread = threading.get_ident()
        instance_id = id(self)

        if isinstance(chunks, list):
            self.collected_chunks.extend(chunks)
            print(f"ðŸŽ¯ ChunkCollector[{instance_id}]: Collected {len(chunks)} chunks (thread: {current_thread})")
        else:
            self.collected_chunks.append(chunks)
            print(f"ðŸŽ¯ ChunkCollector[{instance_id}]: Collected 1 chunk (thread: {current_thread})")


class TestChunkParallelism:
    """æµ‹è¯•chunkç®—å­çš„å¹¶è¡Œæ€§ä¼˜åŒ–"""

    def test_chunk_parallelism_hints_basic(self):
        """æµ‹è¯•åŸºæœ¬çš„chunkå¹¶è¡Œæ€§hintsåŠŸèƒ½"""
        print("\n" + "=" * 70)
        print("TEST: Basic Chunk Parallelism Hints")
        print("=" * 70)

        env = LocalEnvironment(name="chunk_parallelism_test")

        # æµ‹è¯•æ–‡æ¡£
        documents = [
            {"content": "This is document one with some content.", "id": "doc1"},
            {"content": "This is document two with different content.", "id": "doc2"},
            {"content": "This is document three with more content here.", "id": "doc3"},
        ]

        # ä½¿ç”¨parallelism hintsè®¾ç½®2ä¸ªå¹¶è¡Œchunkå®žä¾‹
        collector = ChunkCollector()
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .set_parallelism(2)  # è®¾ç½®2ä¸ªå¹¶è¡ŒCharacterSplitterå®žä¾‹
            .map(CharacterSplitter, chunk_size=20, overlap=5)
            .set_parallelism(1)  # sinkä½¿ç”¨1ä¸ªå®žä¾‹
            .sink(collector)
        )

        # æ‰§è¡Œ
        env.execute()

        # éªŒè¯ç»“æžœ
        assert len(collector.collected_chunks) > 0
        assert all(chunk.get("chunk_id") for chunk in collector.collected_chunks)

        # éªŒè¯æ‰€æœ‰æ–‡æ¡£éƒ½è¢«å¤„ç†
        processed_doc_ids = set(chunk["doc_id"] for chunk in collector.collected_chunks)
        expected_doc_ids = set(doc["id"] for doc in documents)
        assert processed_doc_ids == expected_doc_ids

        print(f"âœ… Collected {len(collector.collected_chunks)} chunks from {len(processed_doc_ids)} documents")

    def test_chunk_parallelism_hints_multiple_levels(self):
        """æµ‹è¯•å¤šçº§å¹¶è¡Œåº¦è®¾ç½®"""
        print("\n" + "=" * 70)
        print("TEST: Multi-level Chunk Parallelism Hints")
        print("=" * 70)

        env = LocalEnvironment(name="multi_level_parallelism_test")

        # æ›´å¤šæµ‹è¯•æ–‡æ¡£
        documents = [
            {"content": "Document one content here.", "id": "doc1"},
            {"content": "Document two content here.", "id": "doc2"},
            {"content": "Document three content here.", "id": "doc3"},
            {"content": "Document four content here.", "id": "doc4"},
            {"content": "Document five content here.", "id": "doc5"},
        ]

        collector = ChunkCollector()

        # æµ‹è¯•ä¸åŒçš„å¹¶è¡Œåº¦è®¾ç½®
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .set_parallelism(3)  # 3ä¸ªå¹¶è¡Œsource->splitter
            .map(CharacterSplitter, chunk_size=15, overlap=3)
            .set_parallelism(2)  # 2ä¸ªå¹¶è¡Œsplitter->sink
            .sink(collector)
        )

        env.execute()

        # éªŒè¯ç»“æžœ
        assert len(collector.collected_chunks) > 0

        # éªŒè¯chunkç»“æž„
        for chunk in collector.collected_chunks:
            assert "content" in chunk
            assert "doc_id" in chunk
            assert "start_idx" in chunk
            assert "end_idx" in chunk
            assert "chunk_id" in chunk
            assert len(chunk["content"]) > 0

        print(f"âœ… Multi-level parallelism test completed with {len(collector.collected_chunks)} chunks")

    def test_chunk_parallelism_hints_large_documents(self):
        """æµ‹è¯•å¤§æ–‡æ¡£çš„chunkå¹¶è¡Œå¤„ç†"""
        print("\n" + "=" * 70)
        print("TEST: Large Document Chunk Parallelism")
        print("=" * 70)

        env = LocalEnvironment(name="large_doc_test")

        # æ¨¡æ‹Ÿå¤§æ–‡æ¡£
        large_content = "This is a large document with extensive content. " * 50
        documents = [
            {"content": large_content, "id": "large_doc1"},
            {"content": large_content, "id": "large_doc2"},
        ]

        collector = ChunkCollector()

        # ä½¿ç”¨æ›´é«˜çš„å¹¶è¡Œåº¦å¤„ç†å¤§æ–‡æ¡£
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .set_parallelism(4)  # 4ä¸ªå¹¶è¡Œå®žä¾‹å¤„ç†å¤§æ–‡æ¡£
            .map(CharacterSplitter, chunk_size=100, overlap=20)
            .set_parallelism(1)
            .sink(collector)
        )

        env.execute()

        # éªŒè¯å¤§æ–‡æ¡£è¢«æ­£ç¡®åˆ†å—
        assert len(collector.collected_chunks) > 0

        # æ£€æŸ¥å¤§æ–‡æ¡£äº§ç”Ÿäº†å¤šä¸ªchunk
        large_doc_chunks = [c for c in collector.collected_chunks if c["doc_id"] == "large_doc1"]
        assert len(large_doc_chunks) > 1  # å¤§æ–‡æ¡£åº”è¯¥è¢«åˆ†æˆå¤šä¸ªchunk

        print(f"âœ… Large document test: {len(large_doc_chunks)} chunks from large_doc1")

    def test_chunk_parallelism_hints_vs_manual_parallelization(self):
        """å¯¹æ¯”parallelism hintsä¸Žæ‰‹åŠ¨å¹¶è¡ŒåŒ–çš„åŒºåˆ«"""
        print("\n" + "=" * 70)
        print("TEST: Parallelism Hints vs Manual Parallelization")
        print("=" * 70)

        # ä½¿ç”¨parallelism hintsçš„æ–¹å¼
        env1 = LocalEnvironment(name="hints_test")
        documents = [
            {"content": "Test document one.", "id": "test1"},
            {"content": "Test document two.", "id": "test2"},
        ]

        collector1 = ChunkCollector()
        result1 = (
            env1.from_collection(DocumentSource, documents)
            .set_parallelism(2)  # å£°æ˜Žå¼å¹¶è¡Œ
            .map(CharacterSplitter, chunk_size=10, overlap=2)
            .sink(collector1)
        )
        env1.execute()

        # æ‰‹åŠ¨å¹¶è¡ŒåŒ–ï¼ˆé”™è¯¯æ–¹å¼ - ä»…ç”¨äºŽå¯¹æ¯”è¯´æ˜Žï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä¸å®žçŽ°æ‰‹åŠ¨å¹¶è¡ŒåŒ–ï¼Œå› ä¸ºSAGEçš„è®¾è®¡ç†å¿µæ˜¯é¿å…è¿™ç§åšæ³•

        # éªŒè¯hintsæ–¹å¼å·¥ä½œæ­£å¸¸
        assert len(collector1.collected_chunks) > 0

        print("âœ… Parallelism hints approach works correctly")
        print("ðŸ’¡ Key advantage: Framework manages parallelism, code stays simple")