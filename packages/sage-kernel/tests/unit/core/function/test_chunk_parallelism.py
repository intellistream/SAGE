import threading
import time
from typing import Any, Dict, List

import pytest
from sage.core.api.function.base_function import BaseFunction
from sage.core.api.function.sink_function import SinkFunction
from sage.core.api.function.source_function import SourceFunction
from sage.core.api.local_environment import LocalEnvironment

# æ·»åŠ å…¨å±€æ‰“å°é”æ¥é˜²æ­¢å¹¶å‘è¾“å‡ºæ··ä¹±
_print_lock = threading.Lock()


def thread_safe_print(*args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°å‡½æ•°"""
    with _print_lock:
        print(*args, **kwargs)


class DocumentSource(SourceFunction):
    """ç”Ÿæˆæ–‡æ¡£æ•°æ®æº"""

    def __init__(self, documents=None, **kwargs):
        super().__init__(**kwargs)
        self.counter = 0
        self.documents = documents or [
            {
                "content": "This is a sample document with some text content.",
                "id": "doc1",
            },
            {
                "content": "Another document containing different information and data.",
                "id": "doc2",
            },
            {
                "content": "Large document with extensive content that needs chunking for processing.",
                "id": "doc3",
            },
            {"content": "Short text document.", "id": "doc4"},
            {
                "content": "Medium sized document with reasonable content length.",
                "id": "doc5",
            },
        ]

    def execute(self):
        if self.counter >= len(self.documents):
            return None

        data = self.documents[self.counter]
        self.counter += 1
        self.logger.info(f"DocumentSource generated: {data['id']}")
        return data


class CharacterSplitter(BaseFunction):
    """å­—ç¬¦çº§æ–‡æœ¬åˆ†å—ç®—å­ - ä½¿ç”¨parallelism hintsè¿›è¡Œæ€§èƒ½ä¼˜åŒ–"""

    def __init__(self, chunk_size=50, overlap=10, **kwargs):
        super().__init__(**kwargs)
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.instance_id = id(self)
        self.thread_id = threading.get_ident()
        thread_safe_print(
            f":gear: CharacterSplitter instance {self.instance_id} created in thread {self.thread_id}"
        )

    def execute(self, document):
        """å•æ–‡æ¡£å¤„ç†é€»è¾‘ï¼Œæ¡†æž¶ä¼šè‡ªåŠ¨å¹¶è¡ŒåŒ–"""
        content = document.get("content", "")
        doc_id = document.get("id", "unknown")
        chunks = []

        current_thread = threading.get_ident()
        instance_id = id(self)

        thread_safe_print(
            f":gear: CharacterSplitter[{instance_id}]: Processing {doc_id} (thread: {current_thread})"
        )

        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.01)

        for i in range(0, len(content), self.chunk_size - self.overlap):
            chunk_text = content[i : i + self.chunk_size]
            if chunk_text.strip():  # è¿‡æ»¤ç©ºå—
                chunks.append(
                    {
                        "content": chunk_text,
                        "doc_id": doc_id,
                        "start_idx": i,
                        "end_idx": min(i + self.chunk_size, len(content)),
                        "chunk_id": f"{doc_id}_chunk_{len(chunks)}",
                    }
                )

        thread_safe_print(
            f":white_check_mark: CharacterSplitter[{instance_id}]: Generated {len(chunks)} chunks for {doc_id}"
        )
        return chunks


class SlowCharacterSplitter(CharacterSplitter):
    """å»¶é•¿å¤„ç†æ—¶é—´çš„CharacterSplitterï¼Œç”¨äºŽæ¨¡æ‹Ÿåœ¨é€”æ•°æ®"""

    def execute(self, document):
        time.sleep(0.05)
        return super().execute(document)


class ChunkCollector(SinkFunction):
    """æ”¶é›†åˆ†å—ç»“æžœçš„sinkç®—å­"""

    # ç±»çº§åˆ«çš„ç»“æžœæ”¶é›† - åªç”¨äºŽæµ‹è¯•ç›®çš„
    _collected_chunks: List[Dict] = None
    _lock = threading.Lock()

    @classmethod
    def _ensure_chunks_list(cls):
        """ç¡®ä¿chunksåˆ—è¡¨è¢«åˆå§‹åŒ–"""
        if cls._collected_chunks is None:
            cls._collected_chunks = []

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.instance_id = id(self)
        self.thread_id = threading.get_ident()
        thread_safe_print(
            f":gear: ChunkCollector instance {self.instance_id} created in thread {self.thread_id}"
        )

    def execute(self, chunks):
        current_thread = threading.get_ident()
        instance_id = id(self)

        with self._lock:
            self._ensure_chunks_list()

            if chunks is None:  # Stop signal
                return

            if isinstance(chunks, list):
                self._collected_chunks.extend(chunks)
                thread_safe_print(
                    f":dart: ChunkCollector[{instance_id}]: Collected {len(chunks)} chunks (thread: {current_thread})"
                )
            else:
                self._collected_chunks.append(chunks)
                thread_safe_print(
                    f":dart: ChunkCollector[{instance_id}]: Collected 1 chunk (thread: {current_thread})"
                )

    def close(self):
        """SinkFunctionçš„å…³é—­æ–¹æ³•ï¼Œåœ¨åœæ­¢ä¿¡å·æ—¶è¢«è°ƒç”¨"""
        current_thread = threading.get_ident()
        instance_id = id(self)
        thread_safe_print(
            f":stop_sign: ChunkCollector[{instance_id}]: Received close signal (thread: {current_thread})"
        )

    @classmethod
    def get_collected_chunks(cls):
        """èŽ·å–æ”¶é›†åˆ°çš„æ‰€æœ‰chunks"""
        with cls._lock:
            cls._ensure_chunks_list()
            return cls._collected_chunks.copy()

    @classmethod
    def clear_collected_chunks(cls):
        """æ¸…ç©ºæ”¶é›†åˆ°çš„chunks"""
        with cls._lock:
            cls._collected_chunks = []


class LaggyChunkCollector(ChunkCollector):
    """å¤„ç†å»¶è¿Ÿçš„Sinkç®—å­ï¼Œæ¨¡æ‹Ÿæ…¢é€Ÿæ¶ˆè´¹è€…"""

    def execute(self, chunks):
        time.sleep(0.05)
        super().execute(chunks)


class TestChunkParallelism:
    """æµ‹è¯•chunkç®—å­çš„å¹¶è¡Œæ€§ä¼˜åŒ–"""

    def test_chunk_parallelism_hints_basic(self):
        """æµ‹è¯•åŸºæœ¬çš„chunkå¹¶è¡Œæ€§hintsåŠŸèƒ½"""
        print("\n" + "=" * 70)
        print("TEST: Basic Chunk Parallelism Hints")
        print("=" * 70)

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        ChunkCollector.clear_collected_chunks()

        env = LocalEnvironment(name="chunk_parallelism_test")

        # æµ‹è¯•æ–‡æ¡£
        documents = [
            {"content": "This is document one with some content.", "id": "doc1"},
            {"content": "This is document two with different content.", "id": "doc2"},
            {"content": "This is document three with more content here.", "id": "doc3"},
        ]

        # ä½¿ç”¨parallelismå‚æ•°ç›´æŽ¥è®¾ç½®2ä¸ªå¹¶è¡Œchunkå®žä¾‹
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .map(CharacterSplitter, chunk_size=20, overlap=5, parallelism=2)
            .sink(ChunkCollector, parallelism=1)
        )

        # ä½¿ç”¨autostop=Trueè®©SAGEè‡ªåŠ¨æ£€æµ‹æ‰¹å¤„ç†å®Œæˆ
        env.submit(autostop=True)

        # èŽ·å–æ”¶é›†çš„ç»“æžœ
        collected_chunks = ChunkCollector.get_collected_chunks()

        # éªŒè¯ç»“æžœ
        assert len(collected_chunks) > 0
        assert all(chunk.get("chunk_id") for chunk in collected_chunks)

        # éªŒè¯æ‰€æœ‰æ–‡æ¡£éƒ½è¢«å¤„ç†
        processed_doc_ids = set(chunk["doc_id"] for chunk in collected_chunks)
        expected_doc_ids = set(doc["id"] for doc in documents)
        assert processed_doc_ids == expected_doc_ids

        print(
            f"âœ… Collected {len(collected_chunks)} chunks from {len(processed_doc_ids)} documents"
        )

    def test_chunk_parallelism_hints_multiple_levels(self):
        """æµ‹è¯•å¤šçº§å¹¶è¡Œåº¦è®¾ç½®"""
        print("\n" + "=" * 70)
        print("TEST: Multi-level Chunk Parallelism Hints")
        print("=" * 70)

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        ChunkCollector.clear_collected_chunks()

        env = LocalEnvironment(name="multi_level_parallelism_test")

        # æ›´å¤šæµ‹è¯•æ–‡æ¡£
        documents = [
            {"content": "Document one content here.", "id": "doc1"},
            {"content": "Document two content here.", "id": "doc2"},
            {"content": "Document three content here.", "id": "doc3"},
            {"content": "Document four content here.", "id": "doc4"},
            {"content": "Document five content here.", "id": "doc5"},
        ]

        # æµ‹è¯•ä¸åŒçš„å¹¶è¡Œåº¦è®¾ç½®
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .map(CharacterSplitter, chunk_size=15, overlap=3, parallelism=3)
            .sink(ChunkCollector, parallelism=2)
        )

        # ä½¿ç”¨autostop=Trueè®©SAGEè‡ªåŠ¨æ£€æµ‹æ‰¹å¤„ç†å®Œæˆ
        env.submit(autostop=True)

        # éªŒè¯ç»“æžœ
        collected_chunks = ChunkCollector.get_collected_chunks()
        assert len(collected_chunks) > 0

        # éªŒè¯chunkç»“æž„
        for chunk in collected_chunks:
            assert "content" in chunk
            assert "doc_id" in chunk
            assert "start_idx" in chunk
            assert "end_idx" in chunk
            assert "chunk_id" in chunk
            assert len(chunk["content"]) > 0

        print(
            f"âœ… Multi-level parallelism test completed with {len(collected_chunks)} chunks"
        )

    def test_chunk_parallelism_hints_large_documents(self):
        """æµ‹è¯•å¤§æ–‡æ¡£çš„chunkå¹¶è¡Œå¤„ç†"""
        print("\n" + "=" * 70)
        print("TEST: Large Document Chunk Parallelism")
        print("=" * 70)

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        ChunkCollector.clear_collected_chunks()

        env = LocalEnvironment(name="large_doc_test")

        # æ¨¡æ‹Ÿå¤§æ–‡æ¡£
        large_content = "This is a large document with extensive content. " * 50
        documents = [
            {"content": large_content, "id": "large_doc1"},
            {"content": large_content, "id": "large_doc2"},
        ]

        # ä½¿ç”¨æ›´é«˜çš„å¹¶è¡Œåº¦å¤„ç†å¤§æ–‡æ¡£
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .map(CharacterSplitter, chunk_size=100, overlap=20, parallelism=4)
            .sink(ChunkCollector, parallelism=1)
        )

        # ä½¿ç”¨autostop=Trueè®©SAGEè‡ªåŠ¨æ£€æµ‹æ‰¹å¤„ç†å®Œæˆ
        env.submit(autostop=True)

        # éªŒè¯å¤§æ–‡æ¡£è¢«æ­£ç¡®åˆ†å—
        collected_chunks = ChunkCollector.get_collected_chunks()
        assert len(collected_chunks) > 0

        # æ£€æŸ¥å¤§æ–‡æ¡£äº§ç”Ÿäº†å¤šä¸ªchunk
        large_doc_chunks = [c for c in collected_chunks if c["doc_id"] == "large_doc1"]
        assert len(large_doc_chunks) > 1  # å¤§æ–‡æ¡£åº”è¯¥è¢«åˆ†æˆå¤šä¸ªchunk

        print(f"âœ… Large document test: {len(large_doc_chunks)} chunks from large_doc1")

    def test_mixed_document_chunk_parallelism(self):
        """æµ‹è¯•æ··åˆæ–‡æ¡£ï¼ˆæ™®é€šæ–‡æ¡£+å¤§æ–‡æ¡£ï¼‰çš„å¹¶è¡Œå¤„ç†åœºæ™¯"""
        print("\n" + "=" * 70)
        print("TEST: Mixed Document Chunk Parallelism")
        print("=" * 70)

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        ChunkCollector.clear_collected_chunks()

        env = LocalEnvironment(name="mixed_doc_test")

        # åˆ›å»ºå®Œæ•´çš„æµ‹è¯•æ–‡æ¡£é›†åˆï¼ŒåŒ…æ‹¬æ™®é€šæ–‡æ¡£å’Œå¤§æ–‡æ¡£
        large_content = "This is a large document with extensive content. " * 50
        documents = [
            {
                "content": "Another document containing different information and data.",
                "id": "doc2",
            },
            {
                "content": "Large document with extensive content that needs chunking for processing.",
                "id": "doc3",
            },
            {"content": "Short text document.", "id": "doc4"},
            {
                "content": "Medium sized document with reasonable content length.",
                "id": "doc5",
            },
            {"content": large_content, "id": "large_doc1"},
            {"content": large_content, "id": "large_doc2"},
        ]

        # ä½¿ç”¨é«˜å¹¶è¡Œåº¦å¤„ç†æ··åˆæ–‡æ¡£ç±»åž‹
        result_stream = (
            env.from_collection(DocumentSource, documents)
            .map(CharacterSplitter, chunk_size=50, overlap=10, parallelism=4)
            .sink(ChunkCollector, parallelism=2)
        )

        # ä½¿ç”¨autostop=Trueè®©SAGEè‡ªåŠ¨æ£€æµ‹æ‰¹å¤„ç†å®Œæˆ
        env.submit(autostop=True)

        # éªŒè¯å¤„ç†ç»“æžœ
        collected_chunks = ChunkCollector.get_collected_chunks()
        assert len(collected_chunks) > 0

        # éªŒè¯æ‰€æœ‰æ–‡æ¡£éƒ½è¢«å¤„ç†
        processed_doc_ids = set(chunk["doc_id"] for chunk in collected_chunks)
        expected_doc_ids = {"doc2", "doc3", "doc4", "doc5", "large_doc1", "large_doc2"}

        assert (
            processed_doc_ids == expected_doc_ids
        ), f"Expected documents {expected_doc_ids}, got {processed_doc_ids}"

        # éªŒè¯å¤§æ–‡æ¡£äº§ç”Ÿäº†è¶³å¤Ÿçš„chunksï¼ˆå¦‚æžœè¢«å¤„ç†çš„è¯ï¼‰
        large_doc1_chunks = [c for c in collected_chunks if c["doc_id"] == "large_doc1"]
        large_doc2_chunks = [c for c in collected_chunks if c["doc_id"] == "large_doc2"]

        # å¤§æ–‡æ¡£åº”è¯¥äº§ç”Ÿå¤§é‡chunks
        assert (
            len(large_doc1_chunks) > 30
        ), f"large_doc1 should generate many chunks, got {len(large_doc1_chunks)}"
        assert (
            len(large_doc2_chunks) > 30
        ), f"large_doc2 should generate many chunks, got {len(large_doc2_chunks)}"

        print(f"âœ… Mixed document test completed: {len(collected_chunks)} total chunks")
        print(f"   - large_doc1: {len(large_doc1_chunks)} chunks")
        print(f"   - large_doc2: {len(large_doc2_chunks)} chunks")

    def test_chunk_parallelism_hints_vs_manual_parallelization(self):
        """å¯¹æ¯”parallelism hintsä¸Žæ‰‹åŠ¨å¹¶è¡ŒåŒ–çš„åŒºåˆ«"""
        print("\n" + "=" * 70)
        print("TEST: Parallelism Hints vs Manual Parallelization")
        print("=" * 70)

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        ChunkCollector.clear_collected_chunks()

        # ä½¿ç”¨parallelism hintsçš„æ–¹å¼
        env1 = LocalEnvironment(name="hints_test")
        documents = [
            {"content": "Test document one.", "id": "test1"},
            {"content": "Test document two.", "id": "test2"},
        ]

        result1 = (
            env1.from_collection(DocumentSource, documents)
            .map(CharacterSplitter, chunk_size=10, overlap=2, parallelism=2)
            .sink(ChunkCollector)
        )
        # ä½¿ç”¨autostop=Trueè®©SAGEè‡ªåŠ¨æ£€æµ‹æ‰¹å¤„ç†å®Œæˆ
        env1.submit(autostop=True)

        # éªŒè¯hintsæ–¹å¼å·¥ä½œæ­£å¸¸
        collected_chunks = ChunkCollector.get_collected_chunks()
        assert len(collected_chunks) > 0

        print("âœ… Parallelism hints approach works correctly")
        print("ðŸ’¡ Key advantage: Framework manages parallelism, code stays simple")

    def test_autostop_graceful_shutdown_drains_sink(self):
        """éªŒè¯autostopåœ¨å­˜åœ¨åœ¨é€”æ•°æ®æ—¶ä¸ä¼šä¸¢å¤±æ•°æ®"""
        print("\n" + "=" * 70)
        print("TEST: Autostop Graceful Shutdown Drains Sink")
        print("=" * 70)

        ChunkCollector.clear_collected_chunks()
        LaggyChunkCollector.clear_collected_chunks()

        env = LocalEnvironment(name="graceful_shutdown_drain_test")

        large_content = (
            "Synthetic large document content to simulate heavy processing. " * 60
        )
        documents = [
            {"content": large_content, "id": "slow_doc1"},
            {"content": large_content, "id": "slow_doc2"},
        ]

        result_stream = (
            env.from_collection(DocumentSource, documents)
            .map(SlowCharacterSplitter, chunk_size=60, overlap=15, parallelism=2)
            .sink(LaggyChunkCollector, parallelism=1)
        )

        env.submit(autostop=True)

        collected_chunks = LaggyChunkCollector.get_collected_chunks()
        assert len(collected_chunks) > 0

        expected_doc_ids = {doc["id"] for doc in documents}
        processed_doc_ids = {chunk["doc_id"] for chunk in collected_chunks}

        assert (
            processed_doc_ids == expected_doc_ids
        ), f"Expected documents {expected_doc_ids}, got {processed_doc_ids}"

        for doc_id in expected_doc_ids:
            doc_chunks = [c for c in collected_chunks if c["doc_id"] == doc_id]
            assert (
                len(doc_chunks) > 20
            ), f"Document {doc_id} should generate many chunks, got {len(doc_chunks)}"

        print(
            f"âœ… Graceful shutdown drained sink with {len(collected_chunks)} total chunks across {len(expected_doc_ids)} documents"
        )
