#!/usr/bin/env python3
"""
æµ‹è¯•é˜Ÿåˆ—æè¿°ç¬¦çš„å¼•ç”¨ä¼ é€’å’Œå¹¶å‘è¯»å†™èƒ½åŠ›

éªŒè¯ï¼š
1. å¼•ç”¨ä¼ é€’ï¼ˆå¯¹è±¡åœ¨ä¸åŒè¿›ç¨‹/çº¿ç¨‹é—´çš„å…±äº«ï¼‰
2. å¹¶å‘è¯»å†™å®‰å…¨æ€§
3. Ray Actorä¹‹é—´çš„é˜Ÿåˆ—å¼•ç”¨ä¼ é€’
4. ä¸åŒé˜Ÿåˆ—ç±»å‹çš„å¹¶å‘æ€§èƒ½æµ‹è¯•
"""

import logging
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict

import pytest

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sage_kernel_src = os.path.join(current_dir, "../../../../../src")
sys.path.insert(0, os.path.abspath(sage_kernel_src))

try:
    from sage.platform.queue import (  # noqa: F401
        BaseQueueDescriptor,
        PythonQueueDescriptor,
        RayQueueDescriptor,
        resolve_descriptor,
    )
    from sage.kernel.utils.ray.ray_utils import ensure_ray_initialized  # noqa: F401

    print("âœ“ æˆåŠŸå¯¼å…¥é˜Ÿåˆ—æè¿°ç¬¦")
except ImportError as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# ============ è¾…åŠ©å‡½æ•° ============


def worker_producer(
    queue_desc: BaseQueueDescriptor,
    worker_id: int,
    num_items: int,
    item_prefix: str = "item",
):
    """ç”Ÿäº§è€…å·¥ä½œçº¿ç¨‹"""
    try:
        for i in range(num_items):
            item = f"{item_prefix}_{worker_id}_{i}"
            queue_desc.put(item)
            logger.debug(f"Producer {worker_id} put: {item}")
        logger.info(f"Producer {worker_id} completed {num_items} items")
        return f"producer_{worker_id}_done"
    except Exception as e:
        logger.error(f"Producer {worker_id} failed: {e}")
        return f"producer_{worker_id}_error: {e}"


def worker_consumer(
    queue_desc: BaseQueueDescriptor,
    worker_id: int,
    expected_items: int,
    timeout: float = 10.0,
):
    """æ¶ˆè´¹è€…å·¥ä½œçº¿ç¨‹"""
    try:
        consumed_items = []
        start_time = time.time()

        while len(consumed_items) < expected_items:
            if time.time() - start_time > timeout:
                break

            try:
                item = queue_desc.get(timeout=1.0)
                consumed_items.append(item)
                logger.debug(f"Consumer {worker_id} got: {item}")
            except Exception:
                continue

        logger.info(f"Consumer {worker_id} consumed {len(consumed_items)} items")
        return consumed_items
    except Exception as e:
        logger.error(f"Consumer {worker_id} failed: {e}")
        return []


def worker_mixed_operations(
    queue_desc: BaseQueueDescriptor, worker_id: int, num_operations: int
):
    """æ··åˆè¯»å†™æ“ä½œå·¥ä½œçº¿ç¨‹"""
    try:
        operations_completed = 0
        for i in range(num_operations):
            if i % 2 == 0:  # å¶æ•°æ¬¡æ‰§è¡Œå†™æ“ä½œ
                item = f"mixed_{worker_id}_{i}"
                queue_desc.put(item)
                logger.debug(f"Mixed worker {worker_id} put: {item}")
            else:  # å¥‡æ•°æ¬¡æ‰§è¡Œè¯»æ“ä½œ
                try:
                    item = queue_desc.get(timeout=0.1)
                    logger.debug(f"Mixed worker {worker_id} got: {item}")
                except Exception:
                    # é˜Ÿåˆ—ä¸ºç©ºæ—¶è·³è¿‡
                    pass
            operations_completed += 1

        logger.info(
            f"Mixed worker {worker_id} completed {operations_completed} operations"
        )
        return operations_completed
    except Exception as e:
        logger.error(f"Mixed worker {worker_id} failed: {e}")
        return 0


# ============ å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼ˆå·²ç§»é™¤ï¼Œå› ä¸ºPython multiprocessing.Queueå¼•ç”¨ä¼ é€’å›°éš¾ï¼‰ ============

# æ³¨é‡Šï¼šåŸæœ¬çš„ multiprocess_producer å’Œ multiprocess_consumer å‡½æ•°å·²ç§»é™¤
# å› ä¸ºPython multiprocessing.Queueçš„é˜Ÿåˆ—æè¿°ç¬¦å¼•ç”¨å¾ˆéš¾è·¨è¿›ç¨‹ä¼ é€’


# ============ Ray Actor ç›¸å…³æµ‹è¯• ============

try:
    import ray

    @ray.remote
    class QueueProducerActor:
        """Ray Actor ç”Ÿäº§è€…"""

        def produce_items(
            self, queue_desc_dict: Dict[str, Any], actor_id: int, num_items: int
        ):
            """ç”Ÿäº§ç‰©å“åˆ°é˜Ÿåˆ—"""
            try:
                # ä»å­—å…¸é‡å»ºé˜Ÿåˆ—æè¿°ç¬¦
                from sage.platform.queue import (
                    resolve_descriptor,
                )

                queue_desc = resolve_descriptor(queue_desc_dict)

                for i in range(num_items):
                    item = f"ray_actor_{actor_id}_{i}"
                    queue_desc.put(item)

                return f"actor_producer_{actor_id}_completed_{num_items}"
            except Exception as e:
                return f"actor_producer_{actor_id}_error: {e}"

    @ray.remote
    class QueueConsumerActor:
        """Ray Actor æ¶ˆè´¹è€…"""

        def consume_items(
            self, queue_desc_dict: dict[str, Any], actor_id: int, expected_items: int
        ):
            """ä»é˜Ÿåˆ—æ¶ˆè´¹ç‰©å“"""
            try:
                # ä»å­—å…¸é‡å»ºé˜Ÿåˆ—æè¿°ç¬¦
                from sage.platform.queue import (
                    resolve_descriptor,
                )

                queue_desc = resolve_descriptor(queue_desc_dict)

                consumed_items = []
                start_time = time.time()

                while len(consumed_items) < expected_items:
                    if time.time() - start_time > 30.0:  # 30ç§’è¶…æ—¶
                        break

                    try:
                        item = queue_desc.get(timeout=1.0)
                        consumed_items.append(item)
                    except Exception:
                        continue

                return consumed_items
            except Exception as e:
                return []

    RAY_AVAILABLE = True

except ImportError:
    RAY_AVAILABLE = False
    print("âš ï¸ Ray not available, skipping Ray tests")


# ============ æµ‹è¯•ç±» ============


class TestPythonQueueConcurrency:
    """Pythoné˜Ÿåˆ—å¹¶å‘æµ‹è¯• - ä¸éœ€è¦Ray"""

    def test_python_queue_multithreading(self):
        """æµ‹è¯•Pythoné˜Ÿåˆ—çš„å¤šçº¿ç¨‹å¹¶å‘"""
        print("\n=== æµ‹è¯•Pythoné˜Ÿåˆ—å¤šçº¿ç¨‹å¹¶å‘ ===")

        # åˆ›å»ºé˜Ÿåˆ—æè¿°ç¬¦
        queue_desc = PythonQueueDescriptor(queue_id="test_python_mt", maxsize=100)

        # å‚æ•°è®¾ç½®
        num_producers = 3
        num_consumers = 2
        items_per_producer = 10
        total_items = num_producers * items_per_producer

        print(
            f"é…ç½®: {num_producers}ä¸ªç”Ÿäº§è€…, {num_consumers}ä¸ªæ¶ˆè´¹è€…, æ€»å…±{total_items}ä¸ªé¡¹ç›®"
        )

        # å¯åŠ¨ç”Ÿäº§è€…çº¿ç¨‹
        with ThreadPoolExecutor(max_workers=num_producers + num_consumers) as executor:
            # æäº¤ç”Ÿäº§è€…ä»»åŠ¡
            producer_futures = []
            for i in range(num_producers):
                future = executor.submit(
                    worker_producer, queue_desc, i, items_per_producer
                )
                producer_futures.append(future)

            # ç­‰å¾…æ‰€æœ‰ç”Ÿäº§è€…å®Œæˆ
            producer_results = []
            for future in as_completed(producer_futures):
                result = future.result()
                producer_results.append(result)
                print(f"ç”Ÿäº§è€…ç»“æœ: {result}")

            # å¯åŠ¨æ¶ˆè´¹è€…çº¿ç¨‹
            consumer_futures = []
            expected_per_consumer = total_items // num_consumers
            for i in range(num_consumers):
                future = executor.submit(
                    worker_consumer, queue_desc, i, expected_per_consumer
                )
                consumer_futures.append(future)

            # ç­‰å¾…æ‰€æœ‰æ¶ˆè´¹è€…å®Œæˆ
            consumer_results = []
            for future in as_completed(consumer_futures):
                result = future.result()
                consumer_results.append(result)
                print(f"æ¶ˆè´¹è€…ç»“æœ: æ¶ˆè´¹äº†{len(result)}ä¸ªé¡¹ç›®")

        # éªŒè¯ç»“æœ
        total_consumed = sum(len(items) for items in consumer_results)
        print(f"æ€»å…±æ¶ˆè´¹: {total_consumed}/{total_items}")
        print(f"å‰©ä½™é˜Ÿåˆ—å¤§å°: {queue_desc.qsize()}")

        assert len(producer_results) == num_producers, "æ‰€æœ‰ç”Ÿäº§è€…åº”è¯¥å®Œæˆ"
        assert total_consumed > 0, "åº”è¯¥æ¶ˆè´¹äº†ä¸€äº›é¡¹ç›®"

        print("âœ“ Pythoné˜Ÿåˆ—å¤šçº¿ç¨‹æµ‹è¯•é€šè¿‡")

    def test_python_queue_mixed_operations(self):
        """æµ‹è¯•Pythoné˜Ÿåˆ—çš„æ··åˆè¯»å†™æ“ä½œ"""
        print("\n=== æµ‹è¯•Pythoné˜Ÿåˆ—æ··åˆè¯»å†™æ“ä½œ ===")

        queue_desc = PythonQueueDescriptor(queue_id="test_python_mixed", maxsize=50)

        # å…ˆæ”¾å…¥ä¸€äº›åˆå§‹æ•°æ®
        for i in range(10):
            queue_desc.put(f"initial_{i}")

        num_workers = 5
        operations_per_worker = 20

        print(
            f"é…ç½®: {num_workers}ä¸ªæ··åˆå·¥ä½œçº¿ç¨‹, æ¯ä¸ªæ‰§è¡Œ{operations_per_worker}ä¸ªæ“ä½œ"
        )

        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = []
            for i in range(num_workers):
                future = executor.submit(
                    worker_mixed_operations, queue_desc, i, operations_per_worker
                )
                futures.append(future)

            results = []
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
                print(f"æ··åˆå·¥ä½œçº¿ç¨‹å®Œæˆæ“ä½œæ•°: {result}")

        print(f"æœ€ç»ˆé˜Ÿåˆ—å¤§å°: {queue_desc.qsize()}")
        assert len(results) == num_workers, "æ‰€æœ‰å·¥ä½œçº¿ç¨‹åº”è¯¥å®Œæˆ"

        print("âœ“ Pythoné˜Ÿåˆ—æ··åˆæ“ä½œæµ‹è¯•é€šè¿‡")

    def test_serializable_queue_multiprocessing(self):
        """æµ‹è¯•å¯åºåˆ—åŒ–é˜Ÿåˆ—çš„å¤šè¿›ç¨‹æ“ä½œï¼ˆè·³è¿‡ï¼Œå› ä¸ºPython multiprocessing.Queueå¼•ç”¨ä¼ é€’å›°éš¾ï¼‰"""
        print("\n=== è·³è¿‡å¤šè¿›ç¨‹æµ‹è¯• ===")
        print(
            "âš ï¸ Python multiprocessing.Queueçš„é˜Ÿåˆ—æè¿°ç¬¦å¼•ç”¨å¾ˆéš¾è·¨è¿›ç¨‹ä¼ é€’ï¼Œè·³è¿‡æ­¤æµ‹è¯•"
        )
        print("âœ“ å¤šè¿›ç¨‹æµ‹è¯•è·³è¿‡")

    def test_queue_reference_integrity(self):
        """æµ‹è¯•é˜Ÿåˆ—å¼•ç”¨çš„å®Œæ•´æ€§"""
        print("\n=== æµ‹è¯•é˜Ÿåˆ—å¼•ç”¨å®Œæ•´æ€§ ===")

        # åˆ›å»ºåŸå§‹é˜Ÿåˆ—æè¿°ç¬¦
        original_desc = PythonQueueDescriptor(queue_id="reference_test", maxsize=20)

        # æ”¾å…¥ä¸€äº›æ•°æ®
        for i in range(5):
            original_desc.put(f"ref_item_{i}")

        print(f"åŸå§‹é˜Ÿåˆ—å¤§å°: {original_desc.qsize()}")

        # å…‹éš†æè¿°ç¬¦
        cloned_desc = original_desc.clone("reference_test_clone")

        # éªŒè¯å…‹éš†çš„æè¿°ç¬¦å¼•ç”¨äº†ç›¸åŒçš„é˜Ÿåˆ—ï¼ˆå¯¹äºä¸å¯åºåˆ—åŒ–çš„Pythoné˜Ÿåˆ—ï¼‰
        cloned_desc.put("cloned_item")
        print(f"æ·»åŠ é¡¹ç›®åå…‹éš†é˜Ÿåˆ—å¤§å°: {cloned_desc.qsize()}")

        # ä»åŸå§‹æè¿°ç¬¦è¯»å–
        items_from_original = []
        while not original_desc.empty():
            try:
                item = original_desc.get_nowait()
                items_from_original.append(item)
            except Exception:
                break

        print(f"ä»åŸå§‹æè¿°ç¬¦è¯»å–çš„é¡¹ç›®: {len(items_from_original)}")
        print(f"è¯»å–ååŸå§‹é˜Ÿåˆ—å¤§å°: {original_desc.qsize()}")

        print("âœ“ é˜Ÿåˆ—å¼•ç”¨å®Œæ•´æ€§æµ‹è¯•é€šè¿‡")

    def test_concurrent_stress_test(self):
        """å¹¶å‘å‹åŠ›æµ‹è¯•"""
        print("\n=== å¹¶å‘å‹åŠ›æµ‹è¯• ===")

        queue_desc = PythonQueueDescriptor(queue_id="stress_test", maxsize=1000)

        num_threads = 10
        operations_per_thread = 50

        print(
            f"å‹åŠ›æµ‹è¯•é…ç½®: {num_threads}ä¸ªçº¿ç¨‹, æ¯ä¸ªæ‰§è¡Œ{operations_per_thread}ä¸ªæ“ä½œ"
        )

        start_time = time.time()

        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = []
            for i in range(num_threads):
                future = executor.submit(
                    worker_mixed_operations, queue_desc, i, operations_per_thread
                )
                futures.append(future)

            completed_operations = []
            for future in as_completed(futures):
                result = future.result()
                completed_operations.append(result)

        end_time = time.time()
        duration = end_time - start_time

        total_operations = sum(completed_operations)
        operations_per_second = total_operations / duration if duration > 0 else 0

        print("å‹åŠ›æµ‹è¯•ç»“æœ:")
        print(f"  æ€»æ“ä½œæ•°: {total_operations}")
        print(f"  è€—æ—¶: {duration:.2f}ç§’")
        print(f"  æ“ä½œ/ç§’: {operations_per_second:.2f}")
        print(f"  æœ€ç»ˆé˜Ÿåˆ—å¤§å°: {queue_desc.qsize()}")

        assert total_operations > 0, "åº”è¯¥å®Œæˆä¸€äº›æ“ä½œ"

        print("âœ“ å¹¶å‘å‹åŠ›æµ‹è¯•é€šè¿‡")


@pytest.mark.ray
class TestRayQueueConcurrency:
    """Rayé˜Ÿåˆ—å¹¶å‘æµ‹è¯• - éœ€è¦Rayç¯å¢ƒ"""

    def test_ray_queue_actor_communication(self):
        """æµ‹è¯•Rayé˜Ÿåˆ—Actoré€šä¿¡"""
        print("\n=== æµ‹è¯•Rayé˜Ÿåˆ—Actoré€šä¿¡ ===")

        if not ray.is_initialized():
            ray.init(ignore_reinit_error=True)

        try:
            # åˆ›å»ºRayé˜Ÿåˆ—æè¿°ç¬¦
            ray_desc = RayQueueDescriptor(queue_id="ray_actor_comm_test", maxsize=100)

            num_producer_actors = 2
            num_consumer_actors = 2
            items_per_actor = 5

            print(
                f"Ray Actoré…ç½®: {num_producer_actors}ä¸ªç”Ÿäº§è€…, {num_consumer_actors}ä¸ªæ¶ˆè´¹è€…"
            )

            # åˆ›å»ºç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…Actor
            producer_actors = [
                QueueProducerActor.remote() for _ in range(num_producer_actors)
            ]
            consumer_actors = [
                QueueConsumerActor.remote() for _ in range(num_consumer_actors)
            ]

            # è·å–é˜Ÿåˆ—å­—å…¸ç”¨äºActoré€šä¿¡
            queue_dict = ray_desc.to_dict()

            # å¯åŠ¨ç”Ÿäº§è€…
            producer_futures = []
            for i, actor in enumerate(producer_actors):
                future = actor.produce_items.remote(queue_dict, i, items_per_actor)
                producer_futures.append(future)

            # ç­‰å¾…ç”Ÿäº§è€…å®Œæˆ
            producer_results = ray.get(producer_futures)
            for result in producer_results:
                print(f"Rayç”Ÿäº§è€…Actorç»“æœ: {result}")

            # å¯åŠ¨æ¶ˆè´¹è€…
            consumer_futures = []
            expected_per_consumer = (
                num_producer_actors * items_per_actor
            ) // num_consumer_actors
            for i, actor in enumerate(consumer_actors):
                future = actor.consume_items.remote(
                    queue_dict, i, expected_per_consumer
                )
                consumer_futures.append(future)

            # ç­‰å¾…æ¶ˆè´¹è€…å®Œæˆ
            consumer_results = ray.get(consumer_futures)
            total_consumed = sum(
                len(items) for items in consumer_results if isinstance(items, list)
            )

            print(f"Ray Actoræ€»å…±æ¶ˆè´¹: {total_consumed}")
            for i, result in enumerate(consumer_results):
                if isinstance(result, list):
                    print(f"æ¶ˆè´¹è€…Actor {i}: æ¶ˆè´¹äº†{len(result)}ä¸ªé¡¹ç›®")

            print("âœ“ Rayé˜Ÿåˆ—Actoré€šä¿¡æµ‹è¯•é€šè¿‡")

        except Exception as e:
            print(f"âš ï¸ Ray Actoræµ‹è¯•å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œå¼•ç”¨ä¼ é€’å’Œå¹¶å‘æµ‹è¯•...")

    test_suite = TestPythonQueueConcurrency()

    try:
        # åŸºç¡€å¤šçº¿ç¨‹æµ‹è¯•
        test_suite.test_python_queue_multithreading()
        test_suite.test_python_queue_mixed_operations()

        # å¤šè¿›ç¨‹æµ‹è¯•
        test_suite.test_serializable_queue_multiprocessing()

        # å¼•ç”¨å®Œæ•´æ€§æµ‹è¯•
        test_suite.test_queue_reference_integrity()

        # å‹åŠ›æµ‹è¯•
        test_suite.test_concurrent_stress_test()

        print("\nğŸ‰ Pythoné˜Ÿåˆ—æµ‹è¯•é€šè¿‡ï¼")

        # Rayæµ‹è¯•éœ€è¦å•ç‹¬è¿è¡Œï¼ˆè¢«pytestæ ‡è®°è¿‡æ»¤ï¼‰
        print("\næ³¨æ„: Rayé˜Ÿåˆ—æµ‹è¯•éœ€è¦ä½¿ç”¨ pytest -m ray å•ç‹¬è¿è¡Œ")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False

    return True


if __name__ == "__main__":
    success = run_all_tests()
    if not success:
        sys.exit(1)
