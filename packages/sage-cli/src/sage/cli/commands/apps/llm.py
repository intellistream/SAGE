#!/usr/bin/env python3
"""LLM service management commands for SAGE.

All LLM services should be managed through sageLLM (LLMAPIServer),
NOT by directly calling vLLM entrypoints.
"""

from __future__ import annotations

import json
import time
from pathlib import Path
from typing import Any
from urllib.parse import urlparse

import httpx
import typer
import yaml
from rich.console import Console
from rich.table import Table

from sage.common.components.sage_llm.presets import (
    EnginePreset,
    get_builtin_preset,
    list_builtin_presets,
    load_preset_file,
)
from sage.common.config import ensure_hf_mirror_configured
from sage.common.config.ports import SagePorts
from sage.common.model_registry import fetch_recommended_models, vllm_registry

try:  # Optional dependency: middleware is not required for every CLI install
    from sage.common.components.sage_llm import VLLMService
except Exception:  # pragma: no cover - handled gracefully at runtime
    VLLMService = None  # type: ignore

try:
    from sage.common.components.sage_llm import (
        LLMAPIServer,
        LLMLauncher,
        LLMServerConfig,
    )
except Exception:  # pragma: no cover
    LLMAPIServer = None  # type: ignore
    LLMLauncher = None  # type: ignore
    LLMServerConfig = None  # type: ignore

# sage-gateway is now the unified gateway (includes Control Plane)
# UnifiedAPIServer has been removed from sage-common
GATEWAY_AVAILABLE = True
try:
    from sage.gateway.server import main as gateway_main  # noqa: F401
except ImportError:  # pragma: no cover
    GATEWAY_AVAILABLE = False

# Import config subcommands
from sage.cli.commands.platform.llm_config import app as config_app

console = Console()
app = typer.Typer(help="ğŸ¤– LLM æœåŠ¡ç®¡ç†")
model_app = typer.Typer(help="ğŸ“¦ æ¨¡å‹ç®¡ç†")
engine_app = typer.Typer(help="âš™ï¸ å¼•æ“ç®¡ç†")
preset_app = typer.Typer(help="ğŸ›ï¸ é¢„è®¾ç¼–æ’")

# PID file for tracking background service
SAGE_DIR = Path.home() / ".sage"
LOG_DIR = SAGE_DIR / "logs"


def _ensure_dirs():
    """Ensure required directories exist."""
    SAGE_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)


def _resolve_api_base(api_base: str | None, port: int | None) -> str:
    """Return the control plane base URL (including /v1)."""
    if api_base:
        return api_base.rstrip("/")
    target_port = port or SagePorts.GATEWAY_DEFAULT
    return f"http://localhost:{target_port}/v1"


def _print_management_api_hint(api_base: str) -> None:
    """Provide guidance when the management API cannot be reached."""

    parsed = urlparse(api_base)
    host = parsed.hostname or "localhost"
    port = parsed.port or SagePorts.GATEWAY_DEFAULT

    console.print(
        "[yellow]ğŸ’¡ æ§åˆ¶å¹³é¢ç®¡ç† API æœªè¿è¡Œæˆ–ä¸å¯è¾¾ã€‚[/yellow]",
    )
    console.print(
        "   è¯·å…ˆå¯åŠ¨ Gateway æœåŠ¡ï¼Œè¿è¡Œ [cyan]sage gateway start[/cyan]",
    )
    console.print(
        f"   é»˜è®¤ç®¡ç†åœ°å€: http://{host}:{port}/v1ï¼Œå¯ç”¨ --api-port æˆ– --api-base è‡ªè¡Œè¦†ç›–ã€‚",
    )


def _extract_error_detail(resp: httpx.Response) -> str:
    try:
        payload = resp.json()
    except ValueError:
        return resp.text.strip() or resp.reason_phrase

    if isinstance(payload, dict):
        for key in ("detail", "message", "error"):
            if key in payload:
                value = payload[key]
                if isinstance(value, (dict, list)):
                    return json.dumps(value, ensure_ascii=False)
                return str(value)
        return json.dumps(payload, ensure_ascii=False)
    return str(payload)


def _management_request(
    method: str,
    endpoint: str,
    *,
    api_base: str,
    timeout: float,
    payload: dict[str, Any] | None = None,
) -> dict[str, Any]:
    endpoint_path = endpoint if endpoint.startswith("/") else f"/{endpoint}"
    url = f"{api_base.rstrip('/')}{endpoint_path}"

    request_kwargs: dict[str, Any] = {"timeout": timeout}
    if payload is not None:
        request_kwargs["json"] = payload

    try:
        response = httpx.request(method, url, **request_kwargs)
    except httpx.RequestError as exc:
        console.print(f"[red]âŒ æ— æ³•è¿æ¥åˆ°ç®¡ç† API: {exc}[/red]")
        _print_management_api_hint(api_base)
        raise typer.Exit(1) from exc

    if response.status_code >= 400:
        detail = _extract_error_detail(response)
        console.print(f"[red]âŒ ç®¡ç† API è¯·æ±‚å¤±è´¥ ({response.status_code}): {detail}[/red]")
        raise typer.Exit(1)

    if not response.content:
        return {}

    try:
        return response.json()
    except ValueError as exc:  # pragma: no cover - defensive
        console.print(f"[red]âŒ æ— æ³•è§£ææœåŠ¡å“åº”: {exc}[/red]")
        raise typer.Exit(1)


def _load_preset_source(name: str | None, file_path: Path | None) -> EnginePreset:
    """Resolve preset definition from builtin registry or local file."""

    if file_path is not None:
        return load_preset_file(file_path)
    if name:
        preset = get_builtin_preset(name)
        if preset is None:
            console.print(f"[red]æœªçŸ¥é¢„è®¾ '{name}'ã€‚ä½¿ç”¨ 'sage llm preset list' æŸ¥çœ‹å¯ç”¨é¡¹ã€‚[/red]")
            raise typer.Exit(1)
        return preset
    console.print("[red]è¯·æŒ‡å®šé¢„è®¾åç§°æˆ– --fileã€‚[/red]")
    raise typer.Exit(1)


def _print_preset_plan(preset: EnginePreset) -> None:
    table = Table(show_header=True, header_style="bold", title=f"é¢„è®¾: {preset.name}")
    table.add_column("åºå·", justify="center")
    table.add_column("åç§°", overflow="fold")
    table.add_column("ç±»å‹", justify="center")
    table.add_column("æ¨¡å‹", overflow="fold")
    table.add_column("TP/PP", justify="center")
    table.add_column("ç«¯å£", justify="center")
    table.add_column("æ ‡ç­¾", overflow="fold")
    for idx, engine in enumerate(preset.engines, start=1):
        table.add_row(
            str(idx),
            engine.name,
            engine.kind,
            engine.model,
            f"{engine.tensor_parallel}/{engine.pipeline_parallel}",
            str(engine.port or "auto"),
            engine.label or "-",
        )
    console.print(table)


def _fetch_cluster_status(api_base: str, timeout: float) -> dict[str, Any]:
    return _management_request(
        "GET",
        "/management/status",
        api_base=api_base,
        timeout=timeout,
    )


def _ensure_dict_list(data: Any) -> list[dict[str, Any]]:
    if isinstance(data, list):
        return [item for item in data if isinstance(item, dict)]
    if isinstance(data, dict):
        return [item for item in data.values() if isinstance(item, dict)]
    return []


def _normalize_memory_gb(value: Any) -> float | None:
    if value is None:
        return None
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        return None

    if numeric > 1_000_000:  # assume bytes
        return numeric / (1024**3)
    return numeric


def _format_memory_gb(value: Any) -> str:
    amount = _normalize_memory_gb(value)
    if amount is None:
        return "-"
    return f"{amount:.1f} GB"


def _format_uptime(value: Any) -> str:
    try:
        seconds = float(value)
    except (TypeError, ValueError):
        return "-"

    if seconds < 60:
        return f"{int(seconds)}s"

    minutes, remaining = divmod(int(seconds), 60)
    if minutes < 60:
        return f"{minutes}m{remaining:02d}s"

    hours, minutes = divmod(minutes, 60)
    return f"{hours}h{minutes:02d}m"


# Add subcommands
app.add_typer(config_app, name="config")
app.add_typer(model_app, name="model")
app.add_typer(engine_app, name="engine")
app.add_typer(preset_app, name="preset")


# ---------------------------------------------------------------------------
# Preset orchestration commands
# ---------------------------------------------------------------------------
@preset_app.command("list")
def list_presets(json_output: bool = typer.Option(False, "--json", help="JSON è¾“å‡º")):
    """åˆ—å‡ºå†…ç½®é¢„è®¾ã€‚"""

    presets = list_builtin_presets()
    if not presets:
        console.print("[yellow]å½“å‰æ²¡æœ‰å®šä¹‰ä»»ä½•å†…ç½®é¢„è®¾ã€‚[/yellow]")
        return

    if json_output:
        typer.echo(
            json.dumps([preset.to_dict() for preset in presets], ensure_ascii=False, indent=2)
        )
        return

    table = Table(show_header=True, header_style="bold", title="LLM é¢„è®¾åˆ—è¡¨")
    table.add_column("åç§°", overflow="fold")
    table.add_column("æè¿°", overflow="fold")
    table.add_column("å¼•æ“æ•°é‡", justify="center")

    for preset in presets:
        table.add_row(
            preset.name,
            preset.description or "-",
            str(len(preset.engines)),
        )

    console.print(table)


@preset_app.command("show")
def show_preset(
    name: str | None = typer.Option(None, "--name", "-n", help="é¢„è®¾åç§°"),
    file: Path | None = typer.Option(None, "--file", "-f", help="è‡ªå®šä¹‰é¢„è®¾æ–‡ä»¶"),
    json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON è¾“å‡º"),
):
    """å±•ç¤ºé¢„è®¾è¯¦æƒ…ã€‚"""

    preset = _load_preset_source(name, file)
    data = preset.to_dict()
    if json_output:
        typer.echo(json.dumps(data, ensure_ascii=False, indent=2))
    else:
        typer.echo(yaml.safe_dump(data, sort_keys=False, allow_unicode=True))


def _rollback_engines(engine_ids: list[str], api_base: str, timeout: float) -> None:
    for engine_id in engine_ids:
        try:
            _management_request(
                "DELETE",
                f"/management/engines/{engine_id}",
                api_base=api_base,
                timeout=timeout,
            )
            console.print(f"[yellow]â†©ï¸ å·²å›æ»šå¼•æ“ {engine_id}[/yellow]")
        except typer.Exit:
            console.print(f"[red]âš ï¸ å›æ»š {engine_id} å¤±è´¥[/red]")


@preset_app.command("apply")
def apply_preset(
    name: str | None = typer.Option(None, "--name", "-n", help="é¢„è®¾åç§°"),
    file: Path | None = typer.Option(None, "--file", "-f", help="è‡ªå®šä¹‰é¢„è®¾æ–‡ä»¶"),
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(None, "--api-base", help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€"),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
    assume_yes: bool = typer.Option(False, "--yes", "-y", help="æ— éœ€ç¡®è®¤ç›´æ¥æ‰§è¡Œ"),
    dry_run: bool = typer.Option(False, "--dry-run", help="ä»…å±•ç¤ºè®¡åˆ’ï¼Œä¸æ‰§è¡Œ"),
    no_rollback: bool = typer.Option(False, "--no-rollback", help="å¤±è´¥æ—¶ä¸å›æ»šå·²å¯åŠ¨çš„å¼•æ“"),
):
    """æ ¹æ®é¢„è®¾å¯åŠ¨ä¸€ç»„å¼•æ“ã€‚"""

    preset = _load_preset_source(name, file)
    _print_preset_plan(preset)

    if dry_run:
        console.print("[blue]ğŸ” Dry-run æ¨¡å¼ï¼Œä»…å±•ç¤ºè®¡åˆ’ã€‚[/blue]")
        return

    if not assume_yes and not typer.confirm("ç¡®è®¤æŒ‰ç…§ä»¥ä¸Šè®¡åˆ’å¯åŠ¨å¼•æ“?", default=True):
        typer.echo("å·²å–æ¶ˆã€‚")
        return

    base_url = _resolve_api_base(api_base, api_port)
    started_ids: list[str] = []
    results: list[dict[str, Any]] = []
    rollback_enabled = not no_rollback

    for engine in preset.engines:
        console.print(f"[cyan]ğŸš€ å¯åŠ¨ {engine.name} ({engine.kind}) -> {engine.model}[/cyan]")
        payload = engine.to_payload()
        try:
            response = _management_request(
                "POST",
                "/management/engines",
                api_base=base_url,
                timeout=timeout,
                payload=payload,
            )
        except typer.Exit as exc:
            if rollback_enabled and started_ids:
                console.print("[yellow]âš ï¸ å¯åŠ¨å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š...[/yellow]")
                _rollback_engines(started_ids, base_url, timeout)
            raise exc

        engine_id = response.get("engine_id") or response.get("id")
        if engine_id:
            started_ids.append(engine_id)
        results.append(
            {
                "engine_id": engine_id or "(pending)",
                "model": response.get("model_id") or engine.model,
                "port": response.get("port") or payload.get("port") or "auto",
                "status": response.get("status") or "STARTING",
                "kind": response.get("engine_kind") or engine.kind,
            }
        )

    table = Table(show_header=True, header_style="bold", title="å¯åŠ¨ç»“æœ")
    table.add_column("Engine ID", overflow="fold")
    table.add_column("ç±»å‹", justify="center")
    table.add_column("æ¨¡å‹", overflow="fold")
    table.add_column("ç«¯å£", justify="center")
    table.add_column("çŠ¶æ€", justify="center")

    for item in results:
        table.add_row(
            item["engine_id"],
            item["kind"],
            item["model"],
            str(item["port"]),
            item["status"],
        )

    console.print("[green]âœ… é¢„è®¾å·²åº”ç”¨ã€‚[/green]")
    console.print(table)


# ---------------------------------------------------------------------------
# Model management commands
# ---------------------------------------------------------------------------
@model_app.command("show")
def show_models(json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON æ ¼å¼è¾“å‡º")):
    """åˆ—å‡ºæœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    infos = vllm_registry.list_models()
    if json_output:
        payload = [
            {
                "model_id": info.model_id,
                "revision": info.revision,
                "path": str(info.path),
                "size_bytes": info.size_bytes,
                "size_mb": round(info.size_mb, 2),
                "last_used": info.last_used_iso,
                "tags": info.tags,
            }
            for info in infos
        ]
        typer.echo(json.dumps(payload, ensure_ascii=False, indent=2))
        return

    if not infos:
        typer.echo(
            "ğŸ“­ æœ¬åœ°å°šæœªç¼“å­˜ä»»ä½• vLLM æ¨¡å‹ã€‚ä½¿ç”¨ 'sage llm model download --model <name>' å¼€å§‹ä¸‹è½½ã€‚"
        )
        return

    header = f"{'æ¨¡å‹ID':48} {'Revision':12} {'Size(MB)':>10} {'Last Used':>20}"
    typer.echo(header)
    typer.echo("-" * len(header))
    for info in infos:
        typer.echo(
            f"{info.model_id[:48]:48} {str(info.revision or '-'):12} {info.size_mb:>10.2f} {info.last_used_iso or '-':>20}"
        )


@model_app.command("list-remote")
def list_remote_models(
    json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON æ ¼å¼è¾“å‡º"),
    timeout: float = typer.Option(5.0, "--timeout", help="è¿œç¨‹è¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """å±•ç¤ºå®˜æ–¹æ¨èçš„å¸¸ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆè‡ªåŠ¨ä» GitHub æ‹‰å–ï¼‰ã€‚"""

    models = fetch_recommended_models(timeout=timeout)
    if not models:
        typer.echo("âš ï¸ æœªèƒ½è·å–æ¨èæ¨¡å‹åˆ—è¡¨ã€‚è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œã€‚")
        return

    if json_output:
        typer.echo(json.dumps(models, ensure_ascii=False, indent=2))
        return

    table = Table(show_header=True, header_style="bold")
    table.add_column("æ¨¡å‹ID", overflow="fold")
    table.add_column("æ˜¾å­˜éœ€æ±‚", justify="center")
    table.add_column("æ ‡ç­¾", justify="center")
    table.add_column("ç®€ä»‹", overflow="fold")

    for item in models:
        tags = ", ".join(item.get("tags", [])) or "-"
        memory = item.get("min_gpu_memory_gb")
        memory_str = f"{memory} GB" if memory else "-"
        table.add_row(
            item.get("model_id", "-"),
            memory_str,
            tags,
            item.get("description", ""),
        )

    console.print(table)
    typer.echo(
        "ğŸ’¡ å¦‚éœ€æ·»åŠ æ–°çš„æ¨èæ¨¡å‹ï¼Œè¯·æ›´æ–° packages/sage-common/src/sage/common/model_registry/recommended_llm_models.jsonï¼Œ"
        "æˆ–è®¾ç½® SAGE_LLM_MODEL_INDEX_URL æŒ‡å‘è‡ªå®šä¹‰ JSONã€‚"
    )


@model_app.command("download")
def download_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦ä¸‹è½½çš„æ¨¡å‹åç§°"),
    revision: str | None = typer.Option(None, "--revision", help="æ¨¡å‹ revision"),
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½"),
    no_progress: bool = typer.Option(False, "--no-progress", help="éšè—ä¸‹è½½è¿›åº¦"),
):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ã€‚"""

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    try:
        info = vllm_registry.download_model(
            model,
            revision=revision,
            force=force,
            progress=not no_progress,
        )
    except Exception as exc:  # pragma: no cover - huggingface errors
        typer.echo(f"âŒ ä¸‹è½½å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo("âœ… ä¸‹è½½å®Œæˆ")
    typer.echo(f"ğŸ“ è·¯å¾„: {info.path}")
    typer.echo(f"ğŸ“¦ å¤§å°: {info.size_mb:.2f} MB")


@model_app.command("delete")
def delete_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦åˆ é™¤çš„æ¨¡å‹åç§°"),
    assume_yes: bool = typer.Option(False, "--yes", "-y", help="æ— éœ€ç¡®è®¤ç›´æ¥åˆ é™¤"),
):
    """åˆ é™¤æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    if not assume_yes and not typer.confirm(f"ç¡®è®¤åˆ é™¤æœ¬åœ°æ¨¡å‹ '{model}'?"):
        raise typer.Exit(0)

    try:
        vllm_registry.delete_model(model)
    except Exception as exc:  # pragma: no cover - filesystem errors
        typer.echo(f"âš ï¸ åˆ é™¤å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo(f"ğŸ—‘ï¸ å·²åˆ é™¤æ¨¡å‹ {model}")


# ---------------------------------------------------------------------------
# Engine management commands
# ---------------------------------------------------------------------------


@engine_app.command("list")
def list_engines(
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(
        None,
        "--api-base",
        help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€ (é»˜è®¤ http://localhost:<api-port>/v1)",
    ),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """åˆ—å‡ºå½“å‰ç”±æ§åˆ¶å¹³é¢ç®¡ç†çš„å¼•æ“ã€‚"""

    base_url = _resolve_api_base(api_base, api_port)
    cluster_status = _fetch_cluster_status(base_url, timeout)
    engines = _ensure_dict_list(
        cluster_status.get("engines")
        or cluster_status.get("engine_instances")
        or cluster_status.get("instances")
        or []
    )

    if not engines:
        console.print("[yellow]å½“å‰æ²¡æœ‰ç”±æ§åˆ¶å¹³é¢ç®¡ç†çš„å¼•æ“ã€‚[/yellow]")
        return

    table = Table(show_header=True, header_style="bold")
    table.add_column("Engine ID", overflow="fold")
    table.add_column("æ¨¡å‹", overflow="fold")
    table.add_column("ç±»å‹", justify="center")
    table.add_column("çŠ¶æ€", justify="center")
    table.add_column("ç«¯å£", justify="center")
    table.add_column("GPU", justify="center")
    table.add_column("PID", justify="center")
    table.add_column("Uptime", justify="center")

    for engine in engines:
        engine_id = engine.get("engine_id") or engine.get("id") or "-"
        model_name = engine.get("model_id") or engine.get("model") or "-"
        runtime_kind = engine.get("runtime") or engine.get("engine_kind")
        if not runtime_kind:
            metadata = engine.get("metadata") or {}
            runtime_kind = metadata.get("engine_kind")
        runtime_kind = runtime_kind or "llm"
        status_text = engine.get("status") or engine.get("state") or "-"
        listen_port = engine.get("port") or engine.get("listen_port") or "-"
        pid = engine.get("pid") or engine.get("process_id") or "-"
        uptime = engine.get("uptime_seconds") or engine.get("uptime") or engine.get("uptime_s")

        gpu_ids = engine.get("gpu_ids") or engine.get("gpus") or engine.get("devices")
        if isinstance(gpu_ids, list):
            gpu_text = ",".join(str(item) for item in gpu_ids) or "-"
        else:
            gpu_text = str(gpu_ids) if gpu_ids is not None else "-"

        table.add_row(
            str(engine_id),
            str(model_name),
            str(runtime_kind),
            str(status_text),
            str(listen_port),
            gpu_text,
            str(pid),
            _format_uptime(uptime),
        )

    console.print(table)
    console.print(f"[green]å…± {len(engines)} ä¸ªå¼•æ“ã€‚[/green]")


@engine_app.command("start")
def start_engine(
    model_id: str = typer.Argument(..., help="è¦å¯åŠ¨çš„æ¨¡å‹ ID"),
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(
        None,
        "--api-base",
        help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€",
    ),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
    engine_port: int | None = typer.Option(
        None,
        "--engine-port",
        help="æ˜¾å¼æŒ‡å®šæ–°å¼•æ“ç›‘å¬ç«¯å£",
    ),
    tensor_parallel: int | None = typer.Option(
        None,
        "--tensor-parallel",
        "-tp",
        help="Tensor å¹¶è¡Œåº¦ (ç›´æ¥é€ä¼ ç»™æ§åˆ¶å¹³é¢)",
    ),
    required_memory_gb: float | None = typer.Option(
        None,
        "--required-memory-gb",
        help="æœŸæœ›çš„æ˜¾å­˜éœ€æ±‚ (GB)",
    ),
    engine_label: str | None = typer.Option(
        None,
        "--label",
        help="è‡ªå®šä¹‰æ ‡ç­¾ï¼Œä¾¿äºè¯†åˆ«å¼•æ“",
    ),
    pipeline_parallel: int | None = typer.Option(
        None,
        "--pipeline-parallel",
        "-pp",
        help="Pipeline å¹¶è¡Œåº¦",
    ),
    max_concurrent: int | None = typer.Option(
        None,
        "--max-concurrent",
        help="æœ€å¤§å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤ 256)",
    ),
    engine_kind: str = typer.Option(
        "llm",
        "--engine-kind",
        help="å¼•æ“ç±»å‹ (llm æˆ– embedding)",
    ),
    use_gpu: bool | None = typer.Option(
        None,
        "--use-gpu/--no-gpu",
        help="æ˜¾å¼æŒ‡å®šæ˜¯å¦ä½¿ç”¨ GPU (é»˜è®¤: LLM ä½¿ç”¨ GPU, Embedding ä¸ä½¿ç”¨)",
    ),
):
    """è¯·æ±‚å¯åŠ¨æ–°çš„ LLM å¼•æ“ã€‚"""

    base_url = _resolve_api_base(api_base, api_port)
    payload: dict[str, Any] = {"model_id": model_id}
    engine_kind_value = engine_kind.strip().lower()
    if engine_kind_value not in {"llm", "embedding"}:
        console.print("[red]engine-kind ä»…æ”¯æŒ 'llm' æˆ– 'embedding'.[/red]")
        raise typer.Exit(1)

    if engine_port is not None:
        payload["port"] = engine_port
    if tensor_parallel is not None:
        payload["tensor_parallel_size"] = tensor_parallel
    if pipeline_parallel is not None:
        payload["pipeline_parallel_size"] = pipeline_parallel
    if required_memory_gb is not None:
        payload["required_memory_gb"] = required_memory_gb
    if engine_label:
        payload["engine_label"] = engine_label
    if max_concurrent is not None:
        payload["max_concurrent_requests"] = max_concurrent
    payload["engine_kind"] = engine_kind_value
    if use_gpu is not None:
        payload["use_gpu"] = use_gpu

    response = _management_request(
        "POST",
        "/management/engines",
        api_base=base_url,
        timeout=timeout,
        payload=payload,
    )

    engine_id = response.get("engine_id") or response.get("id") or "(pending)"
    model_name = response.get("model_id") or model_id
    status_text = response.get("status") or response.get("state") or "CREATED"
    assigned_port = response.get("port") or response.get("listen_port") or payload.get("port")

    console.print("[green]âœ… å·²æäº¤å¼•æ“å¯åŠ¨è¯·æ±‚[/green]")
    console.print(f"  Engine ID : {engine_id}")
    console.print(f"  æ¨¡å‹       : {model_name}")
    console.print(f"  çŠ¶æ€       : {status_text}")
    console.print(f"  ç«¯å£       : {assigned_port or '-'}")


@engine_app.command("stop")
def stop_engine(
    engine_id: str = typer.Argument(..., help="è¦åœæ­¢çš„å¼•æ“ ID"),
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(
        None,
        "--api-base",
        help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€",
    ),
    drain: bool = typer.Option(
        False,
        "--drain",
        "-d",
        help="ä¼˜é›…å…³é—­ï¼šç­‰å¾…ç°æœ‰è¯·æ±‚å®Œæˆåå†åœæ­¢å¼•æ“",
    ),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """è¯·æ±‚åœæ­¢æŒ‡å®šçš„ LLM å¼•æ“ã€‚

    ä½¿ç”¨ --drain é€‰é¡¹å¯ä»¥ä¼˜é›…å…³é—­å¼•æ“ï¼šå¼•æ“å°†åœæ­¢æ¥å—æ–°è¯·æ±‚ï¼Œ
    ç­‰å¾…ç°æœ‰è¯·æ±‚å¤„ç†å®Œæˆåå†åœæ­¢ã€‚
    """
    base_url = _resolve_api_base(api_base, api_port)

    # Build URL with drain query parameter
    endpoint = f"/management/engines/{engine_id}"
    if drain:
        endpoint += "?drain=true"

    response = _management_request(
        "DELETE",
        endpoint,
        api_base=base_url,
        timeout=timeout,
    )

    status_text = response.get("status") or response.get("state") or "STOPPED"
    drained = response.get("drained", False)

    if drained:
        console.print(f"[green]âœ… å¼•æ“ {engine_id} å·²ä¼˜é›…å…³é—­ (çŠ¶æ€: {status_text}).[/green]")
    else:
        console.print(f"[green]âœ… å·²è¯·æ±‚åœæ­¢å¼•æ“ {engine_id} (çŠ¶æ€: {status_text}).[/green]")


@app.command("gpu")
def gpu_status(
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(
        None,
        "--api-base",
        help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€",
    ),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """å±•ç¤ºæ§åˆ¶å¹³é¢æ„ŸçŸ¥åˆ°çš„ GPU çŠ¶æ€ã€‚"""

    base_url = _resolve_api_base(api_base, api_port)
    cluster_status = _fetch_cluster_status(base_url, timeout)
    gpu_entries = _ensure_dict_list(
        cluster_status.get("gpus")
        or cluster_status.get("gpu_status")
        or cluster_status.get("system_status")
        or cluster_status.get("gpu")
        or []
    )

    if not gpu_entries:
        console.print("[yellow]æ§åˆ¶å¹³é¢æœªè¿”å› GPU ä¿¡æ¯ã€‚[/yellow]")
        return

    table = Table(title="GPU èµ„æº", show_header=True, header_style="bold")
    table.add_column("GPU", overflow="fold")
    table.add_column("å†…å­˜ (å·²ç”¨/æ€»é‡)", justify="center")
    table.add_column("ç©ºé—²", justify="center")
    table.add_column("åˆ©ç”¨ç‡", justify="center")
    table.add_column("å…³è”å¼•æ“", overflow="fold")

    for gpu in gpu_entries:
        idx = gpu.get("index")
        name = gpu.get("name") or "GPU"
        label = f"{idx}: {name}" if idx is not None else name

        used = gpu.get("memory_used_gb") or gpu.get("memory_used")
        total = gpu.get("memory_total_gb") or gpu.get("memory_total")
        free = gpu.get("memory_free_gb") or gpu.get("memory_free")

        util = gpu.get("utilization") or gpu.get("gpu_utilization")
        if isinstance(util, (int, float)):
            util_str = f"{util:.0f}%"
        else:
            util_str = str(util) if util is not None else "-"

        engines = gpu.get("engines") or gpu.get("engine_ids") or gpu.get("allocations")
        if isinstance(engines, list):
            engines_str = ", ".join(str(item) for item in engines) or "-"
        else:
            engines_str = str(engines) if engines is not None else "-"

        table.add_row(
            label,
            f"{_format_memory_gb(used)} / {_format_memory_gb(total)}",
            _format_memory_gb(free),
            util_str,
            engines_str,
        )

    console.print(table)


# ---------------------------------------------------------------------------
# Blocking service runner & fine-tune stub
# ---------------------------------------------------------------------------
@app.command("run")
def run_vllm_service(
    model: str = typer.Option("Qwen/Qwen2.5-1.5B-Instruct", "--model", "-m", help="ç”Ÿæˆæ¨¡å‹"),
    embedding_model: str | None = typer.Option(
        None, "--embedding-model", help="åµŒå…¥æ¨¡å‹ï¼ˆé»˜è®¤åŒç”Ÿæˆæ¨¡å‹ï¼‰"
    ),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="ç¼ºå¤±æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"
    ),
    temperature: float = typer.Option(0.7, "--temperature", help="é‡‡æ ·æ¸©åº¦"),
    top_p: float = typer.Option(0.95, "--top-p", help="Top-p é‡‡æ ·"),
    max_tokens: int = typer.Option(512, "--max-tokens", help="æœ€å¤§ç”Ÿæˆ token æ•°"),
):
    """ä»¥é˜»å¡æ¨¡å¼è¿è¡Œ vLLM æœåŠ¡ï¼Œå¹¶æä¾›äº¤äº’å¼ä½“éªŒã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-common[vllm]ï¼Œæ— æ³•åŠ è½½å†…ç½®æœåŠ¡ã€‚")
        typer.echo("   è¯·è¿è¡Œ `pip install isage-common[vllm]` åé‡è¯•ã€‚")
        raise typer.Exit(1)

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    config_dict: dict[str, Any] = {
        "model_id": model,
        "embedding_model_id": embedding_model,
        "auto_download": auto_download,
        "sampling": {
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens,
        },
    }

    service = VLLMService(config_dict)

    try:
        service.setup()
        typer.echo("âœ… vLLM æœåŠ¡å·²åŠ è½½å®Œæˆã€‚è¾“å…¥ç©ºè¡Œé€€å‡ºï¼Œæˆ– Ctrl+C ç»“æŸã€‚")
        while True:
            prompt = typer.prompt("ğŸ’¬ Prompt", default="")
            if not prompt.strip():
                break
            outputs = service.generate(prompt)
            if not outputs:
                typer.echo("âš ï¸ æœªè·å¾—ç”Ÿæˆç»“æœã€‚")
                continue
            choice = outputs[0]["generations"][0]
            typer.echo(f"ğŸ§  {choice['text'].strip()}")
    except KeyboardInterrupt:
        typer.echo("\nğŸ›‘ å·²ä¸­æ–­ã€‚")
    except Exception as exc:
        typer.echo(f"âŒ è¿è¡Œå¤±è´¥: {exc}")
        raise typer.Exit(1)
    finally:
        try:
            service.cleanup()
        except Exception:  # pragma: no cover - cleanup best-effort
            pass


@app.command("fine-tune")
def fine_tune_stub(
    base_model: str = typer.Option(..., "--base-model", help="åŸºç¡€æ¨¡å‹åç§°"),
    dataset_path: str = typer.Option(..., "--dataset", help="è®­ç»ƒæ•°æ®è·¯å¾„"),
    output_dir: str = typer.Option(..., "--output", help="è¾“å‡ºç›®å½•"),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="è‡ªåŠ¨ç¡®ä¿åŸºç¡€æ¨¡å‹å°±ç»ª"
    ),
):
    """æäº¤ fine-tune è¯·æ±‚ï¼ˆå½“å‰ä¸ºå ä½å®ç°ï¼‰ã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-common[vllm]ï¼Œæ— æ³•è°ƒç”¨ fine-tune æ¥å£ã€‚")
        raise typer.Exit(1)

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    service = VLLMService({"model_id": base_model, "auto_download": auto_download})
    try:
        try:
            service.fine_tune(
                {
                    "base_model": base_model,
                    "dataset_path": dataset_path,
                    "output_dir": output_dir,
                }
            )
        except NotImplementedError as exc:
            typer.echo(f"â„¹ï¸ {exc}")
        else:
            typer.echo("âœ… fine-tune è¯·æ±‚å·²æäº¤")
    finally:
        service.cleanup()


# ---------------------------------------------------------------------------
# Service lifecycle commands (via Control Plane)
# ---------------------------------------------------------------------------

# PID file for Control Plane Gateway
GATEWAY_PID_FILE = SAGE_DIR / "gateway.pid"
GATEWAY_CONFIG_FILE = SAGE_DIR / "gateway.json"


def _save_gateway_info(pid: int, config: dict[str, Any]) -> None:
    """Save gateway process info for later management."""
    _ensure_dirs()
    GATEWAY_PID_FILE.write_text(str(pid))
    GATEWAY_CONFIG_FILE.write_text(json.dumps(config, indent=2))


def _load_gateway_info() -> tuple[int | None, dict[str, Any] | None]:
    """Load gateway process info."""
    if not GATEWAY_PID_FILE.exists():
        return None, None
    try:
        pid = int(GATEWAY_PID_FILE.read_text().strip())
        config = json.loads(GATEWAY_CONFIG_FILE.read_text()) if GATEWAY_CONFIG_FILE.exists() else {}
        return pid, config
    except Exception:
        return None, None


def _clear_gateway_info() -> None:
    """Clear gateway process info."""
    GATEWAY_PID_FILE.unlink(missing_ok=True)
    GATEWAY_CONFIG_FILE.unlink(missing_ok=True)


def _is_gateway_running(pid: int | None = None) -> bool:
    """Check if gateway is running."""
    import psutil

    if pid is None:
        pid, _ = _load_gateway_info()
    if pid is None:
        return False
    return psutil.pid_exists(pid)


def _check_existing_gateway(port: int) -> bool:
    """Check if there's already a SAGE Gateway running on the given port.

    Returns:
        True if a SAGE Gateway is running and healthy on this port
    """
    try:
        resp = httpx.get(f"http://localhost:{port}/health", timeout=2.0)
        if resp.status_code == 200:
            data = resp.json()
            # Check if it's a SAGE Gateway (has 'status' field)
            return data.get("status") == "healthy"
    except Exception:
        pass
    return False


def _wait_for_gateway(port: int, timeout: float = 30.0) -> bool:
    """Wait for gateway to be ready."""
    import time

    start = time.time()
    while time.time() - start < timeout:
        try:
            resp = httpx.get(f"http://localhost:{port}/health", timeout=2.0)
            if resp.status_code == 200:
                return True
        except Exception:
            pass
        time.sleep(0.5)
    return False


def _start_engine_via_api(
    api_base: str,
    model_id: str,
    engine_kind: str = "llm",
    port: int | None = None,
    tensor_parallel_size: int = 1,
    use_gpu: bool | None = None,
    extra_args: list[str] | None = None,
    timeout: float = 10.0,
) -> dict[str, Any] | None:
    """Start an engine via Control Plane management API."""
    payload = {
        "model_id": model_id,
        "engine_kind": engine_kind,
        "tensor_parallel_size": tensor_parallel_size,
    }
    if port is not None:
        payload["port"] = port
    if use_gpu is not None:
        payload["use_gpu"] = use_gpu
    if extra_args:
        payload["extra_args"] = extra_args

    try:
        resp = httpx.post(
            f"{api_base}/management/engines",
            json=payload,
            timeout=timeout,
        )
        if resp.status_code == 200:
            return resp.json()
        else:
            console.print(f"[red]âŒ å¯åŠ¨å¼•æ“å¤±è´¥: {_extract_error_detail(resp)}[/red]")
            return None
    except Exception as e:
        console.print(f"[red]âŒ å¯åŠ¨å¼•æ“å¤±è´¥: {e}[/red]")
        return None


@app.command("serve")
def serve_llm(
    model: str = typer.Option(
        "Qwen/Qwen2.5-0.5B-Instruct",
        "--model",
        "-m",
        help="LLM æ¨¡å‹åç§°",
    ),
    gateway_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--gateway-port",
        "-g",
        help=f"Control Plane Gateway ç«¯å£ (é»˜è®¤: {SagePorts.GATEWAY_DEFAULT})",
    ),
    llm_port: int = typer.Option(
        SagePorts.BENCHMARK_LLM,
        "--llm-port",
        "-p",
        help=f"LLM å¼•æ“ç«¯å£ (é»˜è®¤: {SagePorts.BENCHMARK_LLM})",
    ),
    host: str = typer.Option(
        "0.0.0.0",
        "--host",
        help="æœåŠ¡ä¸»æœºåœ°å€",
    ),
    gpu_memory: float = typer.Option(
        0.7,
        "--gpu-memory",
        help="GPU å†…å­˜ä½¿ç”¨ç‡ (0.1-1.0)ï¼Œé»˜è®¤ 0.7 ä»¥å…¼å®¹æ¶ˆè´¹çº§æ˜¾å¡",
    ),
    max_model_len: int = typer.Option(
        4096,
        "--max-model-len",
        help="æœ€å¤§æ¨¡å‹åºåˆ—é•¿åº¦",
    ),
    tensor_parallel: int = typer.Option(
        1,
        "--tensor-parallel",
        "-tp",
        help="Tensor å¹¶è¡Œ GPU æ•°é‡",
    ),
    background: bool = typer.Option(
        True,
        "--background/--foreground",
        help="åå°è¿è¡Œï¼ˆé»˜è®¤ï¼‰æˆ–å‰å°è¿è¡Œ",
    ),
    with_embedding: bool = typer.Option(
        True,
        "--with-embedding/--no-embedding",
        help="åŒæ—¶å¯åŠ¨ Embedding æœåŠ¡ï¼ˆé»˜è®¤å¯ç”¨ï¼‰",
    ),
    embedding_model: str = typer.Option(
        "BAAI/bge-small-zh-v1.5",
        "--embedding-model",
        "-e",
        help="Embedding æ¨¡å‹åç§°",
    ),
    embedding_port: int = typer.Option(
        SagePorts.EMBEDDING_DEFAULT,
        "--embedding-port",
        help=f"Embedding æœåŠ¡ç«¯å£ (é»˜è®¤: {SagePorts.EMBEDDING_DEFAULT})",
    ),
):
    """å¯åŠ¨ LLM æ¨ç†æœåŠ¡ï¼ˆé€šè¿‡ Control Planeï¼‰ã€‚

    ä½¿ç”¨ sageLLM Control Plane å¯åŠ¨ç»Ÿä¸€çš„ API Gateway å’Œæ¨ç†å¼•æ“ã€‚
    é»˜è®¤åå°è¿è¡Œï¼Œå¯é€šè¿‡ 'sage llm stop' åœæ­¢ã€‚

    æ¶æ„:
        Gateway (8000) â†’ LLM Engine (8901) + Embedding Engine (8090)

    ç¤ºä¾‹:
        sage llm serve                              # å¯åŠ¨ Gateway + LLM + Embedding
        sage llm serve -m Qwen/Qwen2.5-7B-Instruct  # æŒ‡å®š LLM æ¨¡å‹
        sage llm serve --no-embedding               # ä»…å¯åŠ¨ LLMï¼Œä¸å¯åŠ¨ Embedding
        sage llm serve --foreground                 # å‰å°è¿è¡Œï¼ˆé˜»å¡ï¼‰

    å¯åŠ¨åå¯é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨:

        from sage.common.components.sage_llm import UnifiedInferenceClient

        client = UnifiedInferenceClient.create()
        response = client.chat([{"role": "user", "content": "Hello"}])
    """
    import os
    import subprocess
    import sys

    if not GATEWAY_AVAILABLE:
        console.print("[red]âŒ sage-gateway ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿å·²å®‰è£… sage-gateway[/red]")
        raise typer.Exit(1)

    _ensure_dirs()
    ensure_hf_mirror_configured()

    # Check if gateway is already running (by our PID file)
    pid, config = _load_gateway_info()
    if pid and _is_gateway_running(pid):
        console.print(f"[yellow]âš ï¸  Control Plane Gateway å·²åœ¨è¿è¡Œ (PID: {pid})[/yellow]")
        console.print(f"   ç«¯å£: {config.get('gateway_port', gateway_port)}")
        console.print("   ä½¿ç”¨ 'sage llm stop' åœæ­¢åé‡è¯•ï¼Œæˆ–ä½¿ç”¨ 'sage llm engine start' æ·»åŠ å¼•æ“")
        raise typer.Exit(0)

    # Check if there's an existing SAGE Gateway on the port (started by another user)
    existing_gateway = _check_existing_gateway(gateway_port)
    if existing_gateway:
        console.print(f"[green]âœ“[/green] æ£€æµ‹åˆ°ç°æœ‰ Gateway è¿è¡Œåœ¨ç«¯å£ {gateway_port}")
        console.print("   å°†å¤ç”¨ç°æœ‰ Gatewayï¼Œç›´æ¥å¯åŠ¨å¼•æ“...")
        # Skip gateway startup, just start engines
        api_base = f"http://localhost:{gateway_port}/v1"

        # Start LLM engine (let Control Plane auto-assign port if needed)
        console.print("\n[blue]ğŸ¯ å¯åŠ¨ LLM å¼•æ“[/blue]")
        console.print(f"   æ¨¡å‹: {model}")

        # Check if the specified LLM port is available
        llm_port_to_use: int | None = llm_port
        if not SagePorts.is_available(llm_port):
            console.print(f"   [yellow]ç«¯å£ {llm_port} å·²å ç”¨ï¼Œå°†è‡ªåŠ¨åˆ†é…å¯ç”¨ç«¯å£[/yellow]")
            llm_port_to_use = None  # Let Control Plane auto-assign

        extra_args = [
            f"--gpu-memory-utilization={gpu_memory}",
            f"--max-model-len={max_model_len}",
        ]

        llm_result = _start_engine_via_api(
            api_base=api_base,
            model_id=model,
            engine_kind="llm",
            port=llm_port_to_use,
            tensor_parallel_size=tensor_parallel,
            extra_args=extra_args,
            timeout=120.0,
        )

        if llm_result:
            actual_port = llm_result.get("port", llm_port_to_use)
            engine_id = llm_result.get("engine_id", "unknown")
            console.print(
                f"   [green]âœ“[/green] LLM å¼•æ“å·²å¯åŠ¨ (ID: {engine_id}, ç«¯å£: {actual_port})"
            )
        else:
            console.print("[yellow]âš ï¸  LLM å¼•æ“å¯åŠ¨å¤±è´¥[/yellow]")

        # Optionally start Embedding engine
        if with_embedding:
            console.print("\n[blue]ğŸ¯ å¯åŠ¨ Embedding å¼•æ“[/blue]")
            console.print(f"   æ¨¡å‹: {embedding_model}")

            embed_port_to_use: int | None = embedding_port
            if not SagePorts.is_available(embedding_port):
                console.print(
                    f"   [yellow]ç«¯å£ {embedding_port} å·²å ç”¨ï¼Œå°†è‡ªåŠ¨åˆ†é…å¯ç”¨ç«¯å£[/yellow]"
                )
                embed_port_to_use = None

            embed_result = _start_engine_via_api(
                api_base=api_base,
                model_id=embedding_model,
                engine_kind="embedding",
                port=embed_port_to_use,
                use_gpu=False,
                timeout=60.0,
            )

            if embed_result:
                actual_port = embed_result.get("port", embed_port_to_use)
                engine_id = embed_result.get("engine_id", "unknown")
                console.print(
                    f"   [green]âœ“[/green] Embedding å¼•æ“å·²å¯åŠ¨ (ID: {engine_id}, ç«¯å£: {actual_port})"
                )
            else:
                console.print("[yellow]âš ï¸  Embedding å¼•æ“å¯åŠ¨å¤±è´¥[/yellow]")

        console.print("\n[green]âœ… å¼•æ“å¯åŠ¨å®Œæˆ[/green]")
        console.print(f"   API Gateway: http://localhost:{gateway_port}/v1")
        console.print("\n[dim]ä½¿ç”¨ 'sage llm engine list' æŸ¥çœ‹æ‰€æœ‰å¼•æ“[/dim]")
        return

    # Build extra args for vLLM
    extra_args = [
        f"--gpu-memory-utilization={gpu_memory}",
        f"--max-model-len={max_model_len}",
    ]

    console.print("[blue]ğŸš€ å¯åŠ¨ SAGE Gateway (Control Plane)[/blue]")
    console.print(f"   Gateway ç«¯å£: {gateway_port}")
    console.print(f"   ä¸»æœº: {host}")

    # Start sage-gateway as subprocess
    gateway_log = LOG_DIR / "gateway.log"
    gateway_cmd = [
        sys.executable,
        "-m",
        "sage.gateway.server",
    ]
    # Set environment variables for gateway configuration
    gateway_env = {
        **dict(os.environ),
        "SAGE_GATEWAY_ENABLE_CONTROL_PLANE": "true",
        "SAGE_GATEWAY_HOST": host,
        "SAGE_GATEWAY_PORT": str(gateway_port),
    }

    if background:
        with open(gateway_log, "w") as log_file:
            proc = subprocess.Popen(
                gateway_cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                start_new_session=True,
                env=gateway_env,
            )
        gateway_pid = proc.pid
        console.print(f"   [green]âœ“[/green] Gateway è¿›ç¨‹å·²å¯åŠ¨ (PID: {gateway_pid})")
        console.print(f"   æ—¥å¿—: {gateway_log}")

        # Wait for gateway to be ready
        console.print("   [dim]ç­‰å¾… Gateway å°±ç»ª...[/dim]")
        if not _wait_for_gateway(gateway_port, timeout=30.0):
            console.print("[red]âŒ Gateway å¯åŠ¨è¶…æ—¶[/red]")
            console.print(f"   è¯·æ£€æŸ¥æ—¥å¿—: {gateway_log}")
            raise typer.Exit(1)
        console.print("   [green]âœ“[/green] Gateway å·²å°±ç»ª")

        # Save gateway info
        gateway_config = {
            "gateway_port": gateway_port,
            "host": host,
            "llm_model": model,
            "llm_port": llm_port,
            "embedding_model": embedding_model if with_embedding else None,
            "embedding_port": embedding_port if with_embedding else None,
            "engines": [],
        }
        _save_gateway_info(gateway_pid, gateway_config)

        # Start LLM engine via Control Plane API
        api_base = f"http://localhost:{gateway_port}/v1"
        console.print("\n[blue]ğŸ¯ å¯åŠ¨ LLM å¼•æ“[/blue]")
        console.print(f"   æ¨¡å‹: {model}")
        console.print(f"   TP: {tensor_parallel}")

        # Check if the specified LLM port is available
        llm_port_to_use: int | None = llm_port
        if not SagePorts.is_available(llm_port):
            console.print(f"   [yellow]ç«¯å£ {llm_port} å·²å ç”¨ï¼Œå°†è‡ªåŠ¨åˆ†é…å¯ç”¨ç«¯å£[/yellow]")
            llm_port_to_use = None  # Let Control Plane auto-assign
        else:
            console.print(f"   ç«¯å£: {llm_port}")

        llm_result = _start_engine_via_api(
            api_base=api_base,
            model_id=model,
            engine_kind="llm",
            port=llm_port_to_use,
            tensor_parallel_size=tensor_parallel,
            extra_args=extra_args,
            timeout=120.0,  # LLM å¯åŠ¨å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        )

        if llm_result:
            actual_port = llm_result.get("port", llm_port_to_use)
            engine_id = llm_result.get("engine_id", "unknown")
            console.print(
                f"   [green]âœ“[/green] LLM å¼•æ“å·²å¯åŠ¨ (ID: {engine_id}, ç«¯å£: {actual_port})"
            )
            gateway_config["engines"].append(
                {"id": engine_id, "kind": "llm", "model": model, "port": actual_port}
            )
        else:
            console.print("[yellow]âš ï¸  LLM å¼•æ“å¯åŠ¨å¤±è´¥ï¼ŒGateway ä»åœ¨è¿è¡Œ[/yellow]")

        # Optionally start Embedding engine
        if with_embedding:
            console.print("\n[blue]ğŸ¯ å¯åŠ¨ Embedding å¼•æ“[/blue]")
            console.print(f"   æ¨¡å‹: {embedding_model}")

            embed_port_to_use: int | None = embedding_port
            if not SagePorts.is_available(embedding_port):
                console.print(
                    f"   [yellow]ç«¯å£ {embedding_port} å·²å ç”¨ï¼Œå°†è‡ªåŠ¨åˆ†é…å¯ç”¨ç«¯å£[/yellow]"
                )
                embed_port_to_use = None
            else:
                console.print(f"   ç«¯å£: {embedding_port}")

            embed_result = _start_engine_via_api(
                api_base=api_base,
                model_id=embedding_model,
                engine_kind="embedding",
                port=embed_port_to_use,
                use_gpu=False,  # Embedding é»˜è®¤ä¸ä½¿ç”¨ GPU
                timeout=60.0,
            )

            if embed_result:
                actual_port = embed_result.get("port", embed_port_to_use)
                engine_id = embed_result.get("engine_id", "unknown")
                console.print(
                    f"   [green]âœ“[/green] Embedding å¼•æ“å·²å¯åŠ¨ (ID: {engine_id}, ç«¯å£: {actual_port})"
                )
                gateway_config["engines"].append(
                    {
                        "id": engine_id,
                        "kind": "embedding",
                        "model": embedding_model,
                        "port": actual_port,
                    }
                )
            else:
                console.print("[yellow]âš ï¸  Embedding å¼•æ“å¯åŠ¨å¤±è´¥[/yellow]")

        # Update gateway config with engine info
        _save_gateway_info(gateway_pid, gateway_config)

        console.print("\n[green]âœ… æœåŠ¡å¯åŠ¨å®Œæˆ[/green]")
        console.print(f"   API Gateway: http://localhost:{gateway_port}/v1")
        console.print("\n[dim]ä½¿ç”¨ 'sage llm status' æŸ¥çœ‹çŠ¶æ€[/dim]")
        console.print("[dim]ä½¿ç”¨ 'sage llm stop' åœæ­¢æœåŠ¡[/dim]")

    else:
        # Foreground mode - run gateway directly (blocking)
        console.print("[dim]å‰å°æ¨¡å¼ï¼ŒCtrl+C é€€å‡º[/dim]")

        import uvicorn

        from sage.gateway.server import app as gateway_app

        try:
            uvicorn.run(
                gateway_app,
                host=host,
                port=gateway_port,
                log_level="info",
            )
        except KeyboardInterrupt:
            console.print("\n[yellow]æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...[/yellow]")


@app.command("stop")
def stop_llm(
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶åœæ­¢"),
):
    """åœæ­¢ LLM æ¨ç†æœåŠ¡ï¼ˆControl Plane Gateway å’Œæ‰€æœ‰å¼•æ“ï¼‰ã€‚"""
    import os
    import signal

    import psutil

    pid, config = _load_gateway_info()

    if pid is None:
        # Fallback to legacy LLMLauncher
        if LLMLauncher is not None:
            legacy_pid, _ = LLMLauncher.load_service_info()
            if legacy_pid:
                console.print("[dim]æ£€æµ‹åˆ°æ—§ç‰ˆæœåŠ¡ï¼Œä½¿ç”¨ LLMLauncher åœæ­¢[/dim]")
                success = LLMLauncher.stop(verbose=True)
                if not success:
                    raise typer.Exit(1)
                return
        console.print("[yellow]âš ï¸  æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„æœåŠ¡[/yellow]")
        return

    if not _is_gateway_running(pid):
        console.print("[yellow]âš ï¸  Gateway è¿›ç¨‹ä¸å­˜åœ¨ï¼Œæ¸…ç† PID æ–‡ä»¶[/yellow]")
        _clear_gateway_info()
        return

    console.print(f"[blue]ğŸ›‘ åœæ­¢ Control Plane Gateway (PID: {pid})[/blue]")

    try:
        process = psutil.Process(pid)
        # First try graceful shutdown
        os.kill(pid, signal.SIGTERM)
        try:
            process.wait(timeout=10)
            console.print("[green]âœ“[/green] Gateway å·²åœæ­¢")
        except psutil.TimeoutExpired:
            if force:
                console.print("[yellow]âš ï¸  å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹[/yellow]")
                os.kill(pid, signal.SIGKILL)
                process.wait(timeout=5)
                console.print("[green]âœ“[/green] Gateway å·²å¼ºåˆ¶åœæ­¢")
            else:
                console.print("[yellow]âš ï¸  è¿›ç¨‹æœªèƒ½åœ¨è¶…æ—¶æ—¶é—´å†…åœæ­¢ï¼Œä½¿ç”¨ --force å¼ºåˆ¶ç»ˆæ­¢[/yellow]")
                raise typer.Exit(1)
    except psutil.NoSuchProcess:
        console.print("[dim]è¿›ç¨‹å·²ä¸å­˜åœ¨[/dim]")
    except Exception as e:
        console.print(f"[red]âŒ åœæ­¢å¤±è´¥: {e}[/red]")
        raise typer.Exit(1)

    _clear_gateway_info()
    console.print("[green]âœ… æœåŠ¡å·²åœæ­¢[/green]")


@app.command("restart")
def restart_llm():
    """é‡å¯ LLM æ¨ç†æœåŠ¡ï¼ˆä½¿ç”¨ä¸Šæ¬¡çš„é…ç½®ï¼‰ã€‚"""
    pid, config = _load_gateway_info()

    if not config:
        # Fallback to legacy LLMLauncher
        if LLMLauncher is not None:
            legacy_pid, legacy_config = LLMLauncher.load_service_info()
            if legacy_config:
                console.print("[dim]æ£€æµ‹åˆ°æ—§ç‰ˆé…ç½®ï¼Œä½¿ç”¨ LLMLauncher é‡å¯[/dim]")
                LLMLauncher.stop(verbose=False)
                time.sleep(1)
                model = legacy_config.get("model", "Qwen/Qwen2.5-0.5B-Instruct")
                port = legacy_config.get("port", SagePorts.BENCHMARK_LLM)
                result = LLMLauncher.launch(model=model, port=port, background=True, verbose=True)
                if result.success:
                    console.print("[green]âœ… LLM æœåŠ¡é‡å¯æˆåŠŸ[/green]")
                else:
                    console.print(f"[red]âŒ é‡å¯å¤±è´¥: {result.error}[/red]")
                    raise typer.Exit(1)
                return
        console.print("[yellow]âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä¹‹å‰çš„æœåŠ¡é…ç½®ï¼Œè¯·ä½¿ç”¨ 'sage llm serve' å¯åŠ¨[/yellow]")
        raise typer.Exit(1)

    console.print("[blue]ğŸ”„ é‡å¯ Control Plane æœåŠ¡...[/blue]")

    # Stop current service
    stop_llm(force=False)
    time.sleep(2)  # Wait for ports to be released

    # Restart with saved config
    serve_llm(
        model=config.get("llm_model", "Qwen/Qwen2.5-0.5B-Instruct"),
        gateway_port=config.get("gateway_port", SagePorts.GATEWAY_DEFAULT),
        llm_port=config.get("llm_port", SagePorts.BENCHMARK_LLM),
        host=config.get("host", "0.0.0.0"),
        with_embedding=config.get("embedding_model") is not None,
        embedding_model=config.get("embedding_model", "BAAI/bge-small-zh-v1.5"),
        embedding_port=config.get("embedding_port", SagePorts.EMBEDDING_DEFAULT),
        background=True,
    )


@app.command("status")
def status_llm():
    """æŸ¥çœ‹ LLM æœåŠ¡çŠ¶æ€ã€‚"""
    import socket

    import psutil

    pid, config = _load_gateway_info()

    # Check for legacy service
    legacy_pid, legacy_config = None, None
    if LLMLauncher is not None:
        legacy_pid, legacy_config = LLMLauncher.load_service_info()

    table = Table(title="Control Plane æœåŠ¡çŠ¶æ€", show_header=True, header_style="bold")
    table.add_column("å±æ€§")
    table.add_column("å€¼")

    # Check gateway process status
    gateway_running = False
    if pid and psutil.pid_exists(pid):
        try:
            proc = psutil.Process(pid)
            gateway_running = proc.is_running()
        except psutil.NoSuchProcess:
            pass

    # Check gateway port
    gateway_port = (
        config.get("gateway_port", SagePorts.GATEWAY_DEFAULT)
        if config
        else SagePorts.GATEWAY_DEFAULT
    )
    gateway_port_in_use = False
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        gateway_port_in_use = sock.connect_ex(("localhost", gateway_port)) == 0

    # Determine gateway status
    if gateway_running and gateway_port_in_use:
        gateway_status = "[green]è¿è¡Œä¸­[/green]"
    elif gateway_port_in_use:
        gateway_status = "[yellow]ç«¯å£è¢«å ç”¨ (å¤–éƒ¨è¿›ç¨‹)[/yellow]"
    else:
        gateway_status = "[red]å·²åœæ­¢[/red]"

    table.add_row("Gateway çŠ¶æ€", gateway_status)
    table.add_row("Gateway PID", str(pid) if pid else "-")
    table.add_row("Gateway ç«¯å£", str(gateway_port))

    if config:
        table.add_row("LLM æ¨¡å‹", config.get("llm_model", "-"))
        table.add_row("LLM ç«¯å£", str(config.get("llm_port", "-")))
        if config.get("embedding_model"):
            table.add_row("Embedding æ¨¡å‹", config.get("embedding_model", "-"))
            table.add_row("Embedding ç«¯å£", str(config.get("embedding_port", "-")))
        table.add_row("API ç«¯ç‚¹", f"http://localhost:{gateway_port}/v1")

        # Show engines
        engines = config.get("engines", [])
        if engines:
            engine_info = ", ".join(f"{e['kind']}:{e.get('id', 'unknown')}" for e in engines)
            table.add_row("å¼•æ“", engine_info)

    console.print(table)

    # Try to get detailed status from Control Plane API
    if gateway_port_in_use:
        try:
            resp = httpx.get(f"http://localhost:{gateway_port}/v1/management/status", timeout=5)
            if resp.status_code == 200:
                cluster_status = resp.json()
                console.print("\n[green]âœ“[/green] Control Plane å¥åº·æ£€æŸ¥é€šè¿‡")

                # Show registered instances
                instances = cluster_status.get("instances", [])
                if instances:
                    inst_table = Table(title="æ³¨å†Œçš„å¼•æ“å®ä¾‹", show_header=True)
                    inst_table.add_column("ID")
                    inst_table.add_column("ç±»å‹")
                    inst_table.add_column("æ¨¡å‹")
                    inst_table.add_column("ç«¯å£")
                    inst_table.add_column("çŠ¶æ€")

                    for inst in instances:
                        inst_table.add_row(
                            inst.get("instance_id", "-"),
                            inst.get("instance_type", "-"),
                            inst.get("model_name", "-"),
                            str(inst.get("port", "-")),
                            "[green]è¿è¡Œä¸­[/green]"
                            if inst.get("is_healthy")
                            else "[red]å¼‚å¸¸[/red]",
                        )
                    console.print(inst_table)
        except Exception:
            # Control Plane API not available, try basic health check
            try:
                resp = httpx.get(f"http://localhost:{gateway_port}/health", timeout=5)
                if resp.status_code == 200:
                    console.print("\n[green]âœ“[/green] Gateway å¥åº·æ£€æŸ¥é€šè¿‡")
            except Exception as e:
                console.print(f"\n[yellow]âš ï¸  å¥åº·æ£€æŸ¥å¤±è´¥: {e}[/yellow]")

    # Legacy service status
    if legacy_pid and not pid:
        console.print("\n[dim]æ£€æµ‹åˆ°æ—§ç‰ˆæœåŠ¡é…ç½®:[/dim]")
        if psutil.pid_exists(legacy_pid):
            console.print(f"  PID: {legacy_pid} [green](è¿è¡Œä¸­)[/green]")
        else:
            console.print(f"  PID: {legacy_pid} [red](å·²åœæ­¢)[/red]")
        if legacy_config:
            console.print(f"  æ¨¡å‹: {legacy_config.get('model', '-')}")
            console.print(f"  ç«¯å£: {legacy_config.get('port', '-')}")


def _show_embedding_status():
    """æ˜¾ç¤º Embedding æœåŠ¡çŠ¶æ€ã€‚"""
    import socket

    embedding_port = SagePorts.EMBEDDING_DEFAULT
    embedding_log = LOG_DIR / "embedding.log"

    # Check port status
    embedding_port_in_use = False
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        embedding_port_in_use = sock.connect_ex(("localhost", embedding_port)) == 0

    # Build table
    embed_table = Table(title="Embedding æœåŠ¡çŠ¶æ€", show_header=True, header_style="bold")
    embed_table.add_column("å±æ€§")
    embed_table.add_column("å€¼")

    if embedding_port_in_use:
        embed_status = "[green]è¿è¡Œä¸­[/green]"
    else:
        embed_status = "[red]å·²åœæ­¢[/red]"

    embed_table.add_row("çŠ¶æ€", embed_status)
    embed_table.add_row("ç«¯å£", str(embedding_port))
    embed_table.add_row("æ—¥å¿—", str(embedding_log) if embedding_log.exists() else "-")
    embed_table.add_row("API ç«¯ç‚¹", f"http://localhost:{embedding_port}/v1")

    console.print()
    console.print(embed_table)

    # Health check for embedding
    if embedding_port_in_use:
        try:
            import httpx

            resp = httpx.get(f"http://localhost:{embedding_port}/v1/models", timeout=5)
            if resp.status_code == 200:
                models = resp.json().get("data", [])
                if models:
                    console.print("\n[green]âœ“[/green] Embedding å¥åº·æ£€æŸ¥é€šè¿‡")
                    console.print(f"  åŠ è½½çš„æ¨¡å‹: {models[0].get('id', 'unknown')}")
        except Exception as e:
            console.print(f"\n[yellow]âš ï¸  Embedding å¥åº·æ£€æŸ¥å¤±è´¥: {e}[/yellow]")


@app.command("logs")
def view_logs(
    follow: bool = typer.Option(False, "--follow", "-f", help="å®æ—¶è·Ÿè¸ªæ—¥å¿—"),
    lines: int = typer.Option(50, "--lines", "-n", help="æ˜¾ç¤ºæœ€å N è¡Œ"),
):
    """æŸ¥çœ‹ LLM æœåŠ¡æ—¥å¿—ã€‚"""
    import os

    if LLMLauncher is None:
        console.print("[red]âŒ LLMLauncher ä¸å¯ç”¨[/red]")
        raise typer.Exit(1)

    _, config = LLMLauncher.load_service_info()

    if config and config.get("log_file"):
        log_file = Path(config["log_file"])
    else:
        # Try default log file
        log_file = LOG_DIR / f"llm_api_server_{SagePorts.BENCHMARK_LLM}.log"

    if not log_file.exists():
        console.print(f"[yellow]æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}[/yellow]")
        return

    console.print(f"[blue]ğŸ“„ æ—¥å¿—æ–‡ä»¶: {log_file}[/blue]\n")

    if follow:
        import shlex

        os.system(f"tail -f {shlex.quote(str(log_file))}")
    else:
        try:
            content = log_file.read_text()
            log_lines = content.strip().split("\n")
            for line in log_lines[-lines:]:
                console.print(line)
        except Exception as e:
            console.print(f"[red]æ— æ³•è¯»å–æ—¥å¿—: {e}[/red]")
