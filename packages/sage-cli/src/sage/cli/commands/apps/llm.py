#!/usr/bin/env python3
"""LLM service management commands for SAGE."""

from __future__ import annotations

import json
import os
import subprocess
import time
from typing import Any

import psutil
import typer

from sage.common.model_registry import vllm_registry

try:  # Optional dependency: middleware is not required for every CLI install
    from sage.common.components.sage_vllm import VLLMService
except Exception:  # pragma: no cover - handled gracefully at runtime
    VLLMService = None  # type: ignore

# Import config subcommands
from sage.cli.commands.platform.llm_config import app as config_app

app = typer.Typer(help="ğŸ¤– LLM æœåŠ¡ç®¡ç†")
model_app = typer.Typer(help="ğŸ“¦ æ¨¡å‹ç®¡ç†")

# Add subcommands
app.add_typer(config_app, name="config")
app.add_typer(model_app, name="model")


# ---------------------------------------------------------------------------
# Model management commands
# ---------------------------------------------------------------------------
@model_app.command("show")
def show_models(json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON æ ¼å¼è¾“å‡º")):
    """åˆ—å‡ºæœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    infos = vllm_registry.list_models()
    if json_output:
        payload = [
            {
                "model_id": info.model_id,
                "revision": info.revision,
                "path": str(info.path),
                "size_bytes": info.size_bytes,
                "size_mb": round(info.size_mb, 2),
                "last_used": info.last_used_iso,
                "tags": info.tags,
            }
            for info in infos
        ]
        typer.echo(json.dumps(payload, ensure_ascii=False, indent=2))
        return

    if not infos:
        typer.echo(
            "ğŸ“­ æœ¬åœ°å°šæœªç¼“å­˜ä»»ä½• vLLM æ¨¡å‹ã€‚ä½¿ç”¨ 'sage llm model download --model <name>' å¼€å§‹ä¸‹è½½ã€‚"
        )
        return

    header = f"{'æ¨¡å‹ID':48} {'Revision':12} {'Size(MB)':>10} {'Last Used':>20}"
    typer.echo(header)
    typer.echo("-" * len(header))
    for info in infos:
        typer.echo(
            f"{info.model_id[:48]:48} {str(info.revision or '-'):12} {info.size_mb:>10.2f} {info.last_used_iso or '-':>20}"
        )


@model_app.command("download")
def download_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦ä¸‹è½½çš„æ¨¡å‹åç§°"),
    revision: str | None = typer.Option(None, "--revision", help="æ¨¡å‹ revision"),
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½"),
    no_progress: bool = typer.Option(False, "--no-progress", help="éšè—ä¸‹è½½è¿›åº¦"),
):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ã€‚"""

    try:
        info = vllm_registry.download_model(
            model,
            revision=revision,
            force=force,
            progress=not no_progress,
        )
    except Exception as exc:  # pragma: no cover - huggingface errors
        typer.echo(f"âŒ ä¸‹è½½å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo("âœ… ä¸‹è½½å®Œæˆ")
    typer.echo(f"ğŸ“ è·¯å¾„: {info.path}")
    typer.echo(f"ğŸ“¦ å¤§å°: {info.size_mb:.2f} MB")


@model_app.command("delete")
def delete_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦åˆ é™¤çš„æ¨¡å‹åç§°"),
    assume_yes: bool = typer.Option(False, "--yes", "-y", help="æ— éœ€ç¡®è®¤ç›´æ¥åˆ é™¤"),
):
    """åˆ é™¤æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    if not assume_yes and not typer.confirm(f"ç¡®è®¤åˆ é™¤æœ¬åœ°æ¨¡å‹ '{model}'?"):
        raise typer.Exit(0)

    try:
        vllm_registry.delete_model(model)
    except Exception as exc:  # pragma: no cover - filesystem errors
        typer.echo(f"âš ï¸ åˆ é™¤å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo(f"ğŸ—‘ï¸ å·²åˆ é™¤æ¨¡å‹ {model}")


# ---------------------------------------------------------------------------
# Blocking service runner & fine-tune stub
# ---------------------------------------------------------------------------
@app.command("run")
def run_vllm_service(
    model: str = typer.Option("meta-llama/Llama-3.1-8B-Instruct", "--model", "-m", help="ç”Ÿæˆæ¨¡å‹"),
    embedding_model: str | None = typer.Option(
        None, "--embedding-model", help="åµŒå…¥æ¨¡å‹ï¼ˆé»˜è®¤åŒç”Ÿæˆæ¨¡å‹ï¼‰"
    ),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="ç¼ºå¤±æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"
    ),
    temperature: float = typer.Option(0.7, "--temperature", help="é‡‡æ ·æ¸©åº¦"),
    top_p: float = typer.Option(0.95, "--top-p", help="Top-p é‡‡æ ·"),
    max_tokens: int = typer.Option(512, "--max-tokens", help="æœ€å¤§ç”Ÿæˆ token æ•°"),
):
    """ä»¥é˜»å¡æ¨¡å¼è¿è¡Œ vLLM æœåŠ¡ï¼Œå¹¶æä¾›äº¤äº’å¼ä½“éªŒã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-middleware[vllm]ï¼Œæ— æ³•åŠ è½½å†…ç½®æœåŠ¡ã€‚")
        typer.echo("   è¯·è¿è¡Œ `pip install isage-middleware[vllm]` åé‡è¯•ã€‚")
        raise typer.Exit(1)

    config_dict: dict[str, Any] = {
        "model_id": model,
        "embedding_model_id": embedding_model,
        "auto_download": auto_download,
        "sampling": {
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens,
        },
    }

    service = VLLMService(config_dict)

    try:
        service.setup()
        typer.echo("âœ… vLLM æœåŠ¡å·²åŠ è½½å®Œæˆã€‚è¾“å…¥ç©ºè¡Œé€€å‡ºï¼Œæˆ– Ctrl+C ç»“æŸã€‚")
        while True:
            prompt = typer.prompt("ğŸ’¬ Prompt", default="")
            if not prompt.strip():
                break
            outputs = service.generate(prompt)
            if not outputs:
                typer.echo("âš ï¸ æœªè·å¾—ç”Ÿæˆç»“æœã€‚")
                continue
            choice = outputs[0]["generations"][0]
            typer.echo(f"ğŸ§  {choice['text'].strip()}")
    except KeyboardInterrupt:
        typer.echo("\nğŸ›‘ å·²ä¸­æ–­ã€‚")
    except Exception as exc:
        typer.echo(f"âŒ è¿è¡Œå¤±è´¥: {exc}")
        raise typer.Exit(1)
    finally:
        try:
            service.cleanup()
        except Exception:  # pragma: no cover - cleanup best-effort
            pass


@app.command("fine-tune")
def fine_tune_stub(
    base_model: str = typer.Option(..., "--base-model", help="åŸºç¡€æ¨¡å‹åç§°"),
    dataset_path: str = typer.Option(..., "--dataset", help="è®­ç»ƒæ•°æ®è·¯å¾„"),
    output_dir: str = typer.Option(..., "--output", help="è¾“å‡ºç›®å½•"),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="è‡ªåŠ¨ç¡®ä¿åŸºç¡€æ¨¡å‹å°±ç»ª"
    ),
):
    """æäº¤ fine-tune è¯·æ±‚ï¼ˆå½“å‰ä¸ºå ä½å®ç°ï¼‰ã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-middleware[vllm]ï¼Œæ— æ³•è°ƒç”¨ fine-tune æ¥å£ã€‚")
        raise typer.Exit(1)

    service = VLLMService({"model_id": base_model, "auto_download": auto_download})
    try:
        try:
            service.fine_tune(
                {
                    "base_model": base_model,
                    "dataset_path": dataset_path,
                    "output_dir": output_dir,
                }
            )
        except NotImplementedError as exc:
            typer.echo(f"â„¹ï¸ {exc}")
        else:
            typer.echo("âœ… fine-tune è¯·æ±‚å·²æäº¤")
    finally:
        service.cleanup()


# ---------------------------------------------------------------------------
# Legacy process-based controls (kept for backwards compatibility)
# ---------------------------------------------------------------------------
@app.command("start")
def start_llm_service(
    service: str = typer.Argument("vllm", help="è¦å¯åŠ¨çš„æœåŠ¡ç±»å‹ (é»˜è®¤: vllm)"),
    model: str = typer.Option("microsoft/DialoGPT-small", "--model", "-m", help="è¦åŠ è½½çš„æ¨¡å‹åç§°"),
    port: int = typer.Option(8000, "--port", "-p", help="æœåŠ¡ç›‘å¬ç«¯å£"),
    auth_token: str = typer.Option("token-abc123", "--auth-token", "-t", help="APIè®¤è¯token"),
    gpu_memory_utilization: float = typer.Option(
        0.5, "--gpu-memory", help="GPUå†…å­˜ä½¿ç”¨ç‡ (0.1-1.0)"
    ),
    max_model_len: int = typer.Option(512, "--max-model-len", help="æ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦"),
    offline: bool = typer.Option(True, "--offline/--online", help="ç¦»çº¿æ¨¡å¼ï¼ˆä¸ä¸‹è½½æ¨¡å‹ï¼‰"),
    background: bool = typer.Option(False, "--background", "-b", help="åå°è¿è¡ŒæœåŠ¡"),
):
    """å¯åŠ¨æ—§ç‰ˆè¿›ç¨‹æ¨¡å¼ vLLM æœåŠ¡ã€‚"""

    typer.echo(
        "âš ï¸ è¯¥å‘½ä»¤é‡‡ç”¨æ—§çš„è¿›ç¨‹æ–¹å¼å¯åŠ¨ vLLMã€‚æ¨èä½¿ç”¨ 'sage llm run' è·å–é˜»å¡å¼å†…ç½®æœåŠ¡ä½“éªŒã€‚"
    )

    if service.lower() != "vllm":
        typer.echo(f"âŒ æš‚ä¸æ”¯æŒçš„æœåŠ¡ç±»å‹: {service}")
        typer.echo("ğŸ’¡ å½“å‰æ”¯æŒçš„æœåŠ¡ç±»å‹: vllm")
        raise typer.Exit(1)

    if _is_service_running(port):
        typer.echo(f"âš ï¸ ç«¯å£ {port} å·²è¢«å ç”¨ï¼ŒæœåŠ¡å¯èƒ½å·²åœ¨è¿è¡Œ")
        if not typer.confirm("æ˜¯å¦ç»§ç»­å¯åŠ¨ï¼Ÿ"):
            raise typer.Exit(0)

    # Try to resolve model from SAGE registry first
    try:
        model_path = vllm_registry.get_model_path(model)
        typer.echo(f"ğŸ“¦ ä½¿ç”¨ SAGE ç¼“å­˜çš„æ¨¡å‹: {model_path}")
        model_to_use = str(model_path)
    except Exception:
        # If not found in registry, use the model ID as-is
        typer.echo(f"âš ï¸ æ¨¡å‹æœªåœ¨ SAGE ç¼“å­˜ä¸­æ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨æ¨¡å‹ID: {model}")
        model_to_use = model

    cmd = [
        "vllm",
        "serve",
        model_to_use,
        "--dtype",
        "auto",
        "--api-key",
        auth_token,
        "--port",
        str(port),
        "--gpu-memory-utilization",
        str(gpu_memory_utilization),
        "--max-model-len",
        str(max_model_len),
        "--max-num-batched-tokens",
        "1024",
        "--max-num-seqs",
        "16",
        "--enforce-eager",
        "--disable-log-stats",
    ]

    if offline:
        env = os.environ.copy()
        env.update(
            {
                "HF_HUB_OFFLINE": "1",
                "TRANSFORMERS_OFFLINE": "1",
                "HF_DATASETS_OFFLINE": "1",
            }
        )
    else:
        env = None

    typer.echo("ğŸš€ å¯åŠ¨ vLLM æœåŠ¡ (è¿›ç¨‹æ¨¡å¼)...")

    try:
        if background:
            process = subprocess.Popen(
                cmd,
                env=env,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                start_new_session=True,
            )
            typer.echo(f"âœ… vLLM æœåŠ¡å·²åœ¨åå°å¯åŠ¨ (PID: {process.pid})")
            typer.echo(f"ğŸŒ æœåŠ¡åœ°å€: http://localhost:{port}")
            typer.echo("ğŸ“‹ ä½¿ç”¨ 'sage llm status' æŸ¥çœ‹æœåŠ¡çŠ¶æ€")
        else:
            typer.echo("ğŸ“ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
            subprocess.run(cmd, env=env, check=True)

    except subprocess.CalledProcessError as exc:
        typer.echo(f"âŒ å¯åŠ¨å¤±è´¥: {exc}")
        raise typer.Exit(1)
    except KeyboardInterrupt:
        typer.echo("\nğŸ›‘ æœåŠ¡å·²åœæ­¢")
        raise typer.Exit(0)


@app.command("stop")
def stop_llm_service(
    port: int = typer.Option(8000, "--port", "-p", help="è¦åœæ­¢çš„æœåŠ¡ç«¯å£"),
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶åœæ­¢æœåŠ¡"),
):
    """åœæ­¢æ—§ç‰ˆè¿›ç¨‹æ¨¡å¼çš„ vLLM æœåŠ¡ã€‚"""

    processes = _find_llm_processes(port)
    if not processes:
        typer.echo(f"âŒ æœªæ‰¾åˆ°è¿è¡Œåœ¨ç«¯å£ {port} çš„ vLLM è¿›ç¨‹")
        raise typer.Exit(1)

    typer.echo(f"ğŸ” æ‰¾åˆ° {len(processes)} ä¸ªç›¸å…³è¿›ç¨‹:")
    for proc in processes:
        try:
            typer.echo(f"  PID {proc.pid}: {' '.join(proc.cmdline())}")
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue

    if not force and not typer.confirm("ç¡®è®¤åœæ­¢è¿™äº›è¿›ç¨‹ï¼Ÿ"):
        raise typer.Exit(0)

    stopped_count = 0
    for proc in processes:
        try:
            proc.terminate()
            proc.wait(timeout=5)
            stopped_count += 1
            typer.echo(f"âœ… å·²åœæ­¢è¿›ç¨‹ {proc.pid}")
        except psutil.TimeoutExpired:
            proc.kill()
            stopped_count += 1
            typer.echo(f"ğŸ”¥ å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ {proc.pid}")
        except (psutil.NoSuchProcess, psutil.AccessDenied) as exc:
            typer.echo(f"âš ï¸ æ— æ³•åœæ­¢è¿›ç¨‹ {proc.pid}: {exc}")

    if stopped_count > 0:
        typer.echo(f"âœ… æˆåŠŸåœæ­¢ {stopped_count} ä¸ªè¿›ç¨‹")
    else:
        typer.echo("âŒ æœªèƒ½åœæ­¢ä»»ä½•è¿›ç¨‹")


@app.command("status")
def llm_service_status(
    port: int = typer.Option(8000, "--port", "-p", help="è¦æ£€æŸ¥çš„æœåŠ¡ç«¯å£"),
):
    """æŸ¥çœ‹æ—§ç‰ˆè¿›ç¨‹æ¨¡å¼ vLLM æœåŠ¡çŠ¶æ€ã€‚"""

    if not _is_service_running(port):
        typer.echo(f"âŒ ç«¯å£ {port} æœªè¢«å ç”¨")
        typer.echo("â„¹ï¸ å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ 'sage llm run'ï¼Œè¯·åœ¨å‘½ä»¤çª—å£ä¸­æŸ¥çœ‹å®æ—¶è¾“å‡ºã€‚")
        return

    processes = _find_llm_processes(port)

    typer.echo(f"ğŸ” LLM æœåŠ¡çŠ¶æ€ (ç«¯å£ {port}):")
    typer.echo("ğŸ“¡ ç«¯å£çŠ¶æ€: âœ… æ´»è·ƒ")

    if processes:
        typer.echo(f"ğŸ”§ ç›¸å…³è¿›ç¨‹ ({len(processes)} ä¸ª):")
        for proc in processes:
            try:
                with proc.oneshot():
                    memory_info = proc.memory_info()
                    cpu_percent = proc.cpu_percent()
                    create_time = time.strftime(
                        "%Y-%m-%d %H:%M:%S", time.localtime(proc.create_time())
                    )

                    typer.echo(f"  PID {proc.pid}:")
                    typer.echo(f"    å‘½ä»¤: {' '.join(proc.cmdline()[:3])}...")
                    typer.echo(f"    å†…å­˜: {memory_info.rss / 1024 / 1024:.1f} MB")
                    typer.echo(f"    CPU: {cpu_percent:.1f}%")
                    typer.echo(f"    å¯åŠ¨æ—¶é—´: {create_time}")
                    typer.echo()
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                typer.echo(f"  PID {proc.pid}: æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯")

    _test_api_endpoint(port)


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------
def _is_service_running(port: int) -> bool:
    import socket

    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        result = sock.connect_ex(("localhost", port))
        return result == 0


def _find_llm_processes(port: int) -> list[psutil.Process]:
    processes: list[psutil.Process] = []

    for proc in psutil.process_iter(["pid", "name", "cmdline"]):
        try:
            cmdline = proc.info.get("cmdline")
            if not cmdline:
                continue

            cmdline_str = " ".join(cmdline).lower()
            if any(keyword in cmdline_str for keyword in ["vllm", "ollama"]):
                processes.append(proc)
            elif str(port) in cmdline_str and any(
                keyword in cmdline_str for keyword in ["serve", "server", "api"]
            ):
                processes.append(proc)
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue

    return processes


def _test_api_endpoint(port: int) -> None:
    import urllib.error
    import urllib.request

    try:
        for token in [None, "token-abc123"]:
            try:
                req = urllib.request.Request(f"http://localhost:{port}/v1/models")
                if token:
                    req.add_header("Authorization", f"Bearer {token}")

                with urllib.request.urlopen(req, timeout=3) as response:
                    data = json.loads(response.read().decode())
                    models = [item.get("id") for item in data.get("data", [])]

                    typer.echo("ğŸŒ APIçŠ¶æ€: âœ… å¯ç”¨")
                    if models:
                        typer.echo(f"ğŸ“š å¯ç”¨æ¨¡å‹: {', '.join(models)}")
                    if token:
                        typer.echo(f"ğŸ” è®¤è¯token: {token}")
                    return

            except urllib.error.HTTPError as exc:
                if exc.code == 401:
                    continue
                typer.echo(f"ğŸŒ APIçŠ¶æ€: âŒ HTTP {exc.code}")
                return
            except Exception:
                continue

        typer.echo("ğŸŒ APIçŠ¶æ€: âŒ æ— æ³•è¿æ¥æˆ–éœ€è¦è®¤è¯")

    except Exception as exc:  # pragma: no cover - network errors
        typer.echo(f"ğŸŒ APIçŠ¶æ€: âŒ æµ‹è¯•å¤±è´¥ ({exc})")
