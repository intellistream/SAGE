#!/usr/bin/env python3
"""LLM service management commands for SAGE.

All LLM services should be managed through sageLLM (LLMAPIServer),
NOT by directly calling vLLM entrypoints.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

import typer
from rich.console import Console
from rich.table import Table

from sage.common.config import ensure_hf_mirror_configured
from sage.common.config.ports import SagePorts
from sage.common.model_registry import fetch_recommended_models, vllm_registry

try:  # Optional dependency: middleware is not required for every CLI install
    from sage.common.components.sage_llm import VLLMService
except Exception:  # pragma: no cover - handled gracefully at runtime
    VLLMService = None  # type: ignore

try:
    from sage.common.components.sage_llm import LLMAPIServer, LLMServerConfig
except Exception:  # pragma: no cover
    LLMAPIServer = None  # type: ignore
    LLMServerConfig = None  # type: ignore

# Import config subcommands
from sage.cli.commands.platform.llm_config import app as config_app

console = Console()
app = typer.Typer(help="ğŸ¤– LLM æœåŠ¡ç®¡ç†")
model_app = typer.Typer(help="ğŸ“¦ æ¨¡å‹ç®¡ç†")

# PID file for tracking background service
SAGE_DIR = Path.home() / ".sage"
LLM_PID_FILE = SAGE_DIR / "llm_service.pid"
LLM_CONFIG_FILE = SAGE_DIR / "llm_service_config.json"
LOG_DIR = SAGE_DIR / "logs"


def _ensure_dirs():
    """Ensure required directories exist."""
    SAGE_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)


def _save_service_info(pid: int, config: dict):
    """Save service PID and config for later management."""
    _ensure_dirs()
    LLM_PID_FILE.write_text(str(pid))
    LLM_CONFIG_FILE.write_text(json.dumps(config, indent=2))


def _load_service_info() -> tuple[int | None, dict | None]:
    """Load saved service info."""
    pid = None
    config = None
    if LLM_PID_FILE.exists():
        try:
            pid = int(LLM_PID_FILE.read_text().strip())
        except (ValueError, OSError):
            pass
    if LLM_CONFIG_FILE.exists():
        try:
            config = json.loads(LLM_CONFIG_FILE.read_text())
        except (json.JSONDecodeError, OSError):
            pass
    return pid, config


def _clear_service_info():
    """Clear saved service info."""
    LLM_PID_FILE.unlink(missing_ok=True)
    LLM_CONFIG_FILE.unlink(missing_ok=True)


# Add subcommands
app.add_typer(config_app, name="config")
app.add_typer(model_app, name="model")


# ---------------------------------------------------------------------------
# Model management commands
# ---------------------------------------------------------------------------
@model_app.command("show")
def show_models(json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON æ ¼å¼è¾“å‡º")):
    """åˆ—å‡ºæœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    infos = vllm_registry.list_models()
    if json_output:
        payload = [
            {
                "model_id": info.model_id,
                "revision": info.revision,
                "path": str(info.path),
                "size_bytes": info.size_bytes,
                "size_mb": round(info.size_mb, 2),
                "last_used": info.last_used_iso,
                "tags": info.tags,
            }
            for info in infos
        ]
        typer.echo(json.dumps(payload, ensure_ascii=False, indent=2))
        return

    if not infos:
        typer.echo(
            "ğŸ“­ æœ¬åœ°å°šæœªç¼“å­˜ä»»ä½• vLLM æ¨¡å‹ã€‚ä½¿ç”¨ 'sage llm model download --model <name>' å¼€å§‹ä¸‹è½½ã€‚"
        )
        return

    header = f"{'æ¨¡å‹ID':48} {'Revision':12} {'Size(MB)':>10} {'Last Used':>20}"
    typer.echo(header)
    typer.echo("-" * len(header))
    for info in infos:
        typer.echo(
            f"{info.model_id[:48]:48} {str(info.revision or '-'):12} {info.size_mb:>10.2f} {info.last_used_iso or '-':>20}"
        )


@model_app.command("list-remote")
def list_remote_models(
    json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON æ ¼å¼è¾“å‡º"),
    timeout: float = typer.Option(5.0, "--timeout", help="è¿œç¨‹è¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """å±•ç¤ºå®˜æ–¹æ¨èçš„å¸¸ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆè‡ªåŠ¨ä» GitHub æ‹‰å–ï¼‰ã€‚"""

    models = fetch_recommended_models(timeout=timeout)
    if not models:
        typer.echo("âš ï¸ æœªèƒ½è·å–æ¨èæ¨¡å‹åˆ—è¡¨ã€‚è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œã€‚")
        return

    if json_output:
        typer.echo(json.dumps(models, ensure_ascii=False, indent=2))
        return

    table = Table(show_header=True, header_style="bold")
    table.add_column("æ¨¡å‹ID", overflow="fold")
    table.add_column("æ˜¾å­˜éœ€æ±‚", justify="center")
    table.add_column("æ ‡ç­¾", justify="center")
    table.add_column("ç®€ä»‹", overflow="fold")

    for item in models:
        tags = ", ".join(item.get("tags", [])) or "-"
        memory = item.get("min_gpu_memory_gb")
        memory_str = f"{memory} GB" if memory else "-"
        table.add_row(
            item.get("model_id", "-"),
            memory_str,
            tags,
            item.get("description", ""),
        )

    console.print(table)
    typer.echo(
        "ğŸ’¡ å¦‚éœ€æ·»åŠ æ–°çš„æ¨èæ¨¡å‹ï¼Œè¯·æ›´æ–° packages/sage-common/src/sage/common/model_registry/recommended_llm_models.jsonï¼Œ"
        "æˆ–è®¾ç½® SAGE_LLM_MODEL_INDEX_URL æŒ‡å‘è‡ªå®šä¹‰ JSONã€‚"
    )


@model_app.command("download")
def download_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦ä¸‹è½½çš„æ¨¡å‹åç§°"),
    revision: str | None = typer.Option(None, "--revision", help="æ¨¡å‹ revision"),
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½"),
    no_progress: bool = typer.Option(False, "--no-progress", help="éšè—ä¸‹è½½è¿›åº¦"),
):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ã€‚"""

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    try:
        info = vllm_registry.download_model(
            model,
            revision=revision,
            force=force,
            progress=not no_progress,
        )
    except Exception as exc:  # pragma: no cover - huggingface errors
        typer.echo(f"âŒ ä¸‹è½½å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo("âœ… ä¸‹è½½å®Œæˆ")
    typer.echo(f"ğŸ“ è·¯å¾„: {info.path}")
    typer.echo(f"ğŸ“¦ å¤§å°: {info.size_mb:.2f} MB")


@model_app.command("delete")
def delete_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦åˆ é™¤çš„æ¨¡å‹åç§°"),
    assume_yes: bool = typer.Option(False, "--yes", "-y", help="æ— éœ€ç¡®è®¤ç›´æ¥åˆ é™¤"),
):
    """åˆ é™¤æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    if not assume_yes and not typer.confirm(f"ç¡®è®¤åˆ é™¤æœ¬åœ°æ¨¡å‹ '{model}'?"):
        raise typer.Exit(0)

    try:
        vllm_registry.delete_model(model)
    except Exception as exc:  # pragma: no cover - filesystem errors
        typer.echo(f"âš ï¸ åˆ é™¤å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo(f"ğŸ—‘ï¸ å·²åˆ é™¤æ¨¡å‹ {model}")


# ---------------------------------------------------------------------------
# Blocking service runner & fine-tune stub
# ---------------------------------------------------------------------------
@app.command("run")
def run_vllm_service(
    model: str = typer.Option("Qwen/Qwen2.5-1.5B-Instruct", "--model", "-m", help="ç”Ÿæˆæ¨¡å‹"),
    embedding_model: str | None = typer.Option(
        None, "--embedding-model", help="åµŒå…¥æ¨¡å‹ï¼ˆé»˜è®¤åŒç”Ÿæˆæ¨¡å‹ï¼‰"
    ),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="ç¼ºå¤±æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"
    ),
    temperature: float = typer.Option(0.7, "--temperature", help="é‡‡æ ·æ¸©åº¦"),
    top_p: float = typer.Option(0.95, "--top-p", help="Top-p é‡‡æ ·"),
    max_tokens: int = typer.Option(512, "--max-tokens", help="æœ€å¤§ç”Ÿæˆ token æ•°"),
):
    """ä»¥é˜»å¡æ¨¡å¼è¿è¡Œ vLLM æœåŠ¡ï¼Œå¹¶æä¾›äº¤äº’å¼ä½“éªŒã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-common[vllm]ï¼Œæ— æ³•åŠ è½½å†…ç½®æœåŠ¡ã€‚")
        typer.echo("   è¯·è¿è¡Œ `pip install isage-common[vllm]` åé‡è¯•ã€‚")
        raise typer.Exit(1)

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    config_dict: dict[str, Any] = {
        "model_id": model,
        "embedding_model_id": embedding_model,
        "auto_download": auto_download,
        "sampling": {
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens,
        },
    }

    service = VLLMService(config_dict)

    try:
        service.setup()
        typer.echo("âœ… vLLM æœåŠ¡å·²åŠ è½½å®Œæˆã€‚è¾“å…¥ç©ºè¡Œé€€å‡ºï¼Œæˆ– Ctrl+C ç»“æŸã€‚")
        while True:
            prompt = typer.prompt("ğŸ’¬ Prompt", default="")
            if not prompt.strip():
                break
            outputs = service.generate(prompt)
            if not outputs:
                typer.echo("âš ï¸ æœªè·å¾—ç”Ÿæˆç»“æœã€‚")
                continue
            choice = outputs[0]["generations"][0]
            typer.echo(f"ğŸ§  {choice['text'].strip()}")
    except KeyboardInterrupt:
        typer.echo("\nğŸ›‘ å·²ä¸­æ–­ã€‚")
    except Exception as exc:
        typer.echo(f"âŒ è¿è¡Œå¤±è´¥: {exc}")
        raise typer.Exit(1)
    finally:
        try:
            service.cleanup()
        except Exception:  # pragma: no cover - cleanup best-effort
            pass


@app.command("fine-tune")
def fine_tune_stub(
    base_model: str = typer.Option(..., "--base-model", help="åŸºç¡€æ¨¡å‹åç§°"),
    dataset_path: str = typer.Option(..., "--dataset", help="è®­ç»ƒæ•°æ®è·¯å¾„"),
    output_dir: str = typer.Option(..., "--output", help="è¾“å‡ºç›®å½•"),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="è‡ªåŠ¨ç¡®ä¿åŸºç¡€æ¨¡å‹å°±ç»ª"
    ),
):
    """æäº¤ fine-tune è¯·æ±‚ï¼ˆå½“å‰ä¸ºå ä½å®ç°ï¼‰ã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-common[vllm]ï¼Œæ— æ³•è°ƒç”¨ fine-tune æ¥å£ã€‚")
        raise typer.Exit(1)

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    service = VLLMService({"model_id": base_model, "auto_download": auto_download})
    try:
        try:
            service.fine_tune(
                {
                    "base_model": base_model,
                    "dataset_path": dataset_path,
                    "output_dir": output_dir,
                }
            )
        except NotImplementedError as exc:
            typer.echo(f"â„¹ï¸ {exc}")
        else:
            typer.echo("âœ… fine-tune è¯·æ±‚å·²æäº¤")
    finally:
        service.cleanup()


# ---------------------------------------------------------------------------
# Service lifecycle commands (via sageLLM LLMAPIServer)
# ---------------------------------------------------------------------------
@app.command("serve")
def serve_llm(
    model: str = typer.Option(
        "Qwen/Qwen2.5-0.5B-Instruct",
        "--model",
        "-m",
        help="LLM æ¨¡å‹åç§°",
    ),
    port: int = typer.Option(
        SagePorts.BENCHMARK_LLM,
        "--port",
        "-p",
        help=f"æœåŠ¡ç«¯å£ (é»˜è®¤: {SagePorts.BENCHMARK_LLM})",
    ),
    host: str = typer.Option(
        "0.0.0.0",
        "--host",
        help="æœåŠ¡ä¸»æœºåœ°å€",
    ),
    gpu_memory: float = typer.Option(
        0.9,
        "--gpu-memory",
        help="GPU å†…å­˜ä½¿ç”¨ç‡ (0.1-1.0)",
    ),
    max_model_len: int = typer.Option(
        4096,
        "--max-model-len",
        help="æœ€å¤§æ¨¡å‹åºåˆ—é•¿åº¦",
    ),
    tensor_parallel: int = typer.Option(
        1,
        "--tensor-parallel",
        "-tp",
        help="Tensor å¹¶è¡Œ GPU æ•°é‡",
    ),
    background: bool = typer.Option(
        True,
        "--background/--foreground",
        help="åå°è¿è¡Œï¼ˆé»˜è®¤ï¼‰æˆ–å‰å°è¿è¡Œ",
    ),
    with_embedding: bool = typer.Option(
        False,
        "--with-embedding",
        help="åŒæ—¶å¯åŠ¨ Embedding æœåŠ¡",
    ),
    embedding_model: str = typer.Option(
        "BAAI/bge-small-zh-v1.5",
        "--embedding-model",
        "-e",
        help="Embedding æ¨¡å‹åç§°",
    ),
    embedding_port: int = typer.Option(
        SagePorts.EMBEDDING_DEFAULT,
        "--embedding-port",
        help=f"Embedding æœåŠ¡ç«¯å£ (é»˜è®¤: {SagePorts.EMBEDDING_DEFAULT})",
    ),
):
    """å¯åŠ¨ LLM æ¨ç†æœåŠ¡ï¼ˆé€šè¿‡ sageLLMï¼‰ã€‚

    ä½¿ç”¨ sageLLM çš„ LLMAPIServer å¯åŠ¨ OpenAI å…¼å®¹çš„ LLM æœåŠ¡ã€‚
    é»˜è®¤åå°è¿è¡Œï¼Œå¯é€šè¿‡ 'sage llm stop' åœæ­¢ã€‚

    ç¤ºä¾‹:
        sage llm serve                           # ä½¿ç”¨é»˜è®¤å°æ¨¡å‹å¯åŠ¨
        sage llm serve -m Qwen/Qwen2.5-7B-Instruct  # æŒ‡å®šæ¨¡å‹
        sage llm serve --with-embedding          # åŒæ—¶å¯åŠ¨ Embedding æœåŠ¡
        sage llm serve --foreground              # å‰å°è¿è¡Œï¼ˆé˜»å¡ï¼‰

    å¯åŠ¨åå¯é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨:

        from sage.common.components.sage_llm import UnifiedInferenceClient

        client = UnifiedInferenceClient.create_auto()
        response = client.chat([{"role": "user", "content": "Hello"}])
    """
    if LLMAPIServer is None:
        console.print("[red]âŒ LLMAPIServer ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿å·²å®‰è£… sage-common[/red]")
        raise typer.Exit(1)

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()
    _ensure_dirs()

    # Check if service already running
    import psutil

    pid, _ = _load_service_info()
    if pid and psutil.pid_exists(pid):
        console.print(f"[yellow]âš ï¸  LLM æœåŠ¡å·²åœ¨è¿è¡Œä¸­ (PID: {pid})[/yellow]")
        console.print("ä½¿ç”¨ 'sage llm stop' åœæ­¢ç°æœ‰æœåŠ¡ï¼Œæˆ– 'sage llm status' æŸ¥çœ‹çŠ¶æ€")
        raise typer.Exit(1)

    # Create server config
    config = LLMServerConfig(
        model=model,
        backend="vllm",
        host=host,
        port=port,
        gpu_memory_utilization=gpu_memory,
        max_model_len=max_model_len,
        tensor_parallel_size=tensor_parallel,
    )

    console.print("[blue]ğŸš€ å¯åŠ¨ LLM æœåŠ¡ (sageLLM)[/blue]")
    console.print(f"   æ¨¡å‹: {model}")
    console.print(f"   ç«¯å£: {port}")
    console.print(f"   æ¨¡å¼: {'åå°' if background else 'å‰å°'}")

    server = LLMAPIServer(config)

    if background:
        log_file = LOG_DIR / f"llm_api_server_{port}.log"
        success = server.start(background=True, log_file=log_file)

        if success:
            # Save service info for management
            _save_service_info(
                server.pid,
                {
                    "model": model,
                    "port": port,
                    "host": host,
                    "log_file": str(log_file),
                },
            )

            console.print("\n[green]âœ… LLM æœåŠ¡å·²å¯åŠ¨[/green]")
            console.print(f"   PID: {server.pid}")
            console.print(f"   API: http://localhost:{port}/v1")
            console.print(f"   æ—¥å¿—: {log_file}")
            console.print("\n[dim]ä½¿ç”¨ 'sage llm status' æŸ¥çœ‹çŠ¶æ€[/dim]")
            console.print("[dim]ä½¿ç”¨ 'sage llm stop' åœæ­¢æœåŠ¡[/dim]")
        else:
            console.print("[red]âŒ LLM æœåŠ¡å¯åŠ¨å¤±è´¥[/red]")
            console.print(f"[dim]è¯·æ£€æŸ¥æ—¥å¿—: {LOG_DIR / f'llm_api_server_{port}.log'}[/dim]")
            raise typer.Exit(1)
    else:
        # Foreground mode - blocking
        console.print("\n[yellow]å‰å°æ¨¡å¼è¿è¡Œä¸­ï¼ŒæŒ‰ Ctrl+C åœæ­¢...[/yellow]")
        try:
            server.start(background=False)
        except KeyboardInterrupt:
            console.print("\n[yellow]æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...[/yellow]")
            server.stop()

    # Optionally start Embedding service
    if with_embedding:
        console.print("\n[blue]ğŸ¯ å¯åŠ¨ Embedding æœåŠ¡[/blue]")
        console.print(f"   æ¨¡å‹: {embedding_model}")
        console.print(f"   ç«¯å£: {embedding_port}")

        import subprocess
        import sys

        embedding_log = LOG_DIR / "embedding.log"
        embedding_cmd = [
            sys.executable,
            "-m",
            "sage.common.components.sage_embedding.embedding_server",
            "--model",
            embedding_model,
            "--port",
            str(embedding_port),
        ]

        with open(embedding_log, "w") as log_file:
            proc = subprocess.Popen(
                embedding_cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                start_new_session=True,
            )

        console.print(f"   [green]âœ“[/green] Embedding æœåŠ¡å·²å¯åŠ¨ (PID: {proc.pid})")
        console.print(f"   æ—¥å¿—: {embedding_log}")


@app.command("stop")
def stop_llm(
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶åœæ­¢"),
):
    """åœæ­¢ LLM æ¨ç†æœåŠ¡ã€‚"""
    import psutil

    pid, config = _load_service_info()

    if not pid:
        console.print("[dim]æ²¡æœ‰è¿è¡Œä¸­çš„ LLM æœåŠ¡[/dim]")
        return

    if not psutil.pid_exists(pid):
        console.print(f"[dim]æœåŠ¡è¿›ç¨‹ (PID: {pid}) å·²ä¸å­˜åœ¨ï¼Œæ¸…ç†è®°å½•...[/dim]")
        _clear_service_info()
        return

    console.print(f"[blue]ğŸ›‘ åœæ­¢ LLM æœåŠ¡ (PID: {pid})...[/blue]")

    try:
        proc = psutil.Process(pid)
        # Terminate children first
        children = proc.children(recursive=True)
        for child in children:
            try:
                child.terminate()
            except psutil.NoSuchProcess:
                pass

        proc.terminate()
        try:
            proc.wait(timeout=10)
            console.print("[green]âœ… LLM æœåŠ¡å·²åœæ­¢[/green]")
        except psutil.TimeoutExpired:
            if force:
                proc.kill()
                console.print("[yellow]âš ï¸  LLM æœåŠ¡å·²å¼ºåˆ¶åœæ­¢[/yellow]")
            else:
                console.print("[yellow]âš ï¸  æœåŠ¡åœæ­¢è¶…æ—¶ï¼Œä½¿ç”¨ --force å¼ºåˆ¶åœæ­¢[/yellow]")
                return

        _clear_service_info()
    except psutil.NoSuchProcess:
        console.print("[dim]æœåŠ¡è¿›ç¨‹å·²ä¸å­˜åœ¨[/dim]")
        _clear_service_info()
    except Exception as exc:
        console.print(f"[red]âŒ åœæ­¢æœåŠ¡å¤±è´¥: {exc}[/red]")
        raise typer.Exit(1)


@app.command("status")
def status_llm():
    """æŸ¥çœ‹ LLM æœåŠ¡çŠ¶æ€ã€‚"""
    import socket

    import psutil

    pid, config = _load_service_info()

    table = Table(title="LLM æœåŠ¡çŠ¶æ€", show_header=True, header_style="bold")
    table.add_column("å±æ€§")
    table.add_column("å€¼")

    # Check process status
    process_running = False
    if pid and psutil.pid_exists(pid):
        try:
            proc = psutil.Process(pid)
            process_running = proc.is_running()
        except psutil.NoSuchProcess:
            pass

    # Check port status
    port = config.get("port", SagePorts.BENCHMARK_LLM) if config else SagePorts.BENCHMARK_LLM
    port_in_use = False
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        port_in_use = sock.connect_ex(("localhost", port)) == 0

    # Determine overall status
    if process_running and port_in_use:
        status = "[green]è¿è¡Œä¸­[/green]"
    elif port_in_use:
        status = "[yellow]ç«¯å£è¢«å ç”¨ (å¤–éƒ¨è¿›ç¨‹)[/yellow]"
    else:
        status = "[red]å·²åœæ­¢[/red]"

    table.add_row("çŠ¶æ€", status)
    table.add_row("PID", str(pid) if pid else "-")
    table.add_row("ç«¯å£", str(port))

    if config:
        table.add_row("æ¨¡å‹", config.get("model", "-"))
        table.add_row("æ—¥å¿—", config.get("log_file", "-"))
        table.add_row("API ç«¯ç‚¹", f"http://localhost:{port}/v1")

    console.print(table)

    # Health check if running
    if port_in_use:
        try:
            import httpx

            resp = httpx.get(f"http://localhost:{port}/v1/models", timeout=5)
            if resp.status_code == 200:
                models = resp.json().get("data", [])
                if models:
                    console.print("\n[green]âœ“[/green] å¥åº·æ£€æŸ¥é€šè¿‡")
                    console.print(f"  åŠ è½½çš„æ¨¡å‹: {models[0].get('id', 'unknown')}")
        except Exception as e:
            console.print(f"\n[yellow]âš ï¸  å¥åº·æ£€æŸ¥å¤±è´¥: {e}[/yellow]")


@app.command("logs")
def view_logs(
    follow: bool = typer.Option(False, "--follow", "-f", help="å®æ—¶è·Ÿè¸ªæ—¥å¿—"),
    lines: int = typer.Option(50, "--lines", "-n", help="æ˜¾ç¤ºæœ€å N è¡Œ"),
):
    """æŸ¥çœ‹ LLM æœåŠ¡æ—¥å¿—ã€‚"""
    import os

    _, config = _load_service_info()

    if config and config.get("log_file"):
        log_file = Path(config["log_file"])
    else:
        # Try default log file
        log_file = LOG_DIR / f"llm_api_server_{SagePorts.BENCHMARK_LLM}.log"

    if not log_file.exists():
        console.print(f"[yellow]æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}[/yellow]")
        return

    console.print(f"[blue]ğŸ“„ æ—¥å¿—æ–‡ä»¶: {log_file}[/blue]\n")

    if follow:
        import shlex

        os.system(f"tail -f {shlex.quote(str(log_file))}")
    else:
        try:
            content = log_file.read_text()
            log_lines = content.strip().split("\n")
            for line in log_lines[-lines:]:
                console.print(line)
        except Exception as e:
            console.print(f"[red]æ— æ³•è¯»å–æ—¥å¿—: {e}[/red]")
