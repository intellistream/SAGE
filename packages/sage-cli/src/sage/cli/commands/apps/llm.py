#!/usr/bin/env python3
"""LLM service management commands for SAGE."""

from __future__ import annotations

import json
from typing import Any

import typer

from sage.common.config import ensure_hf_mirror_configured
from sage.common.model_registry import vllm_registry

try:  # Optional dependency: middleware is not required for every CLI install
    from sage.common.components.sage_llm import VLLMService
except Exception:  # pragma: no cover - handled gracefully at runtime
    VLLMService = None  # type: ignore

# Import config subcommands
from sage.cli.commands.platform.llm_config import app as config_app

app = typer.Typer(help="ğŸ¤– LLM æœåŠ¡ç®¡ç†")
model_app = typer.Typer(help="ğŸ“¦ æ¨¡å‹ç®¡ç†")

# Add subcommands
app.add_typer(config_app, name="config")
app.add_typer(model_app, name="model")


# ---------------------------------------------------------------------------
# Model management commands
# ---------------------------------------------------------------------------
@model_app.command("show")
def show_models(json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON æ ¼å¼è¾“å‡º")):
    """åˆ—å‡ºæœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    infos = vllm_registry.list_models()
    if json_output:
        payload = [
            {
                "model_id": info.model_id,
                "revision": info.revision,
                "path": str(info.path),
                "size_bytes": info.size_bytes,
                "size_mb": round(info.size_mb, 2),
                "last_used": info.last_used_iso,
                "tags": info.tags,
            }
            for info in infos
        ]
        typer.echo(json.dumps(payload, ensure_ascii=False, indent=2))
        return

    if not infos:
        typer.echo(
            "ğŸ“­ æœ¬åœ°å°šæœªç¼“å­˜ä»»ä½• vLLM æ¨¡å‹ã€‚ä½¿ç”¨ 'sage llm model download --model <name>' å¼€å§‹ä¸‹è½½ã€‚"
        )
        return

    header = f"{'æ¨¡å‹ID':48} {'Revision':12} {'Size(MB)':>10} {'Last Used':>20}"
    typer.echo(header)
    typer.echo("-" * len(header))
    for info in infos:
        typer.echo(
            f"{info.model_id[:48]:48} {str(info.revision or '-'):12} {info.size_mb:>10.2f} {info.last_used_iso or '-':>20}"
        )


@model_app.command("download")
def download_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦ä¸‹è½½çš„æ¨¡å‹åç§°"),
    revision: str | None = typer.Option(None, "--revision", help="æ¨¡å‹ revision"),
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½"),
    no_progress: bool = typer.Option(False, "--no-progress", help="éšè—ä¸‹è½½è¿›åº¦"),
):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ã€‚"""

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    try:
        info = vllm_registry.download_model(
            model,
            revision=revision,
            force=force,
            progress=not no_progress,
        )
    except Exception as exc:  # pragma: no cover - huggingface errors
        typer.echo(f"âŒ ä¸‹è½½å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo("âœ… ä¸‹è½½å®Œæˆ")
    typer.echo(f"ğŸ“ è·¯å¾„: {info.path}")
    typer.echo(f"ğŸ“¦ å¤§å°: {info.size_mb:.2f} MB")


@model_app.command("delete")
def delete_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦åˆ é™¤çš„æ¨¡å‹åç§°"),
    assume_yes: bool = typer.Option(False, "--yes", "-y", help="æ— éœ€ç¡®è®¤ç›´æ¥åˆ é™¤"),
):
    """åˆ é™¤æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    if not assume_yes and not typer.confirm(f"ç¡®è®¤åˆ é™¤æœ¬åœ°æ¨¡å‹ '{model}'?"):
        raise typer.Exit(0)

    try:
        vllm_registry.delete_model(model)
    except Exception as exc:  # pragma: no cover - filesystem errors
        typer.echo(f"âš ï¸ åˆ é™¤å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo(f"ğŸ—‘ï¸ å·²åˆ é™¤æ¨¡å‹ {model}")


# ---------------------------------------------------------------------------
# Blocking service runner & fine-tune stub
# ---------------------------------------------------------------------------
@app.command("run")
def run_vllm_service(
    model: str = typer.Option("meta-llama/Llama-3.1-8B-Instruct", "--model", "-m", help="ç”Ÿæˆæ¨¡å‹"),
    embedding_model: str | None = typer.Option(
        None, "--embedding-model", help="åµŒå…¥æ¨¡å‹ï¼ˆé»˜è®¤åŒç”Ÿæˆæ¨¡å‹ï¼‰"
    ),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="ç¼ºå¤±æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"
    ),
    temperature: float = typer.Option(0.7, "--temperature", help="é‡‡æ ·æ¸©åº¦"),
    top_p: float = typer.Option(0.95, "--top-p", help="Top-p é‡‡æ ·"),
    max_tokens: int = typer.Option(512, "--max-tokens", help="æœ€å¤§ç”Ÿæˆ token æ•°"),
):
    """ä»¥é˜»å¡æ¨¡å¼è¿è¡Œ vLLM æœåŠ¡ï¼Œå¹¶æä¾›äº¤äº’å¼ä½“éªŒã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-common[vllm]ï¼Œæ— æ³•åŠ è½½å†…ç½®æœåŠ¡ã€‚")
        typer.echo("   è¯·è¿è¡Œ `pip install isage-common[vllm]` åé‡è¯•ã€‚")
        raise typer.Exit(1)

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    config_dict: dict[str, Any] = {
        "model_id": model,
        "embedding_model_id": embedding_model,
        "auto_download": auto_download,
        "sampling": {
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens,
        },
    }

    service = VLLMService(config_dict)

    try:
        service.setup()
        typer.echo("âœ… vLLM æœåŠ¡å·²åŠ è½½å®Œæˆã€‚è¾“å…¥ç©ºè¡Œé€€å‡ºï¼Œæˆ– Ctrl+C ç»“æŸã€‚")
        while True:
            prompt = typer.prompt("ğŸ’¬ Prompt", default="")
            if not prompt.strip():
                break
            outputs = service.generate(prompt)
            if not outputs:
                typer.echo("âš ï¸ æœªè·å¾—ç”Ÿæˆç»“æœã€‚")
                continue
            choice = outputs[0]["generations"][0]
            typer.echo(f"ğŸ§  {choice['text'].strip()}")
    except KeyboardInterrupt:
        typer.echo("\nğŸ›‘ å·²ä¸­æ–­ã€‚")
    except Exception as exc:
        typer.echo(f"âŒ è¿è¡Œå¤±è´¥: {exc}")
        raise typer.Exit(1)
    finally:
        try:
            service.cleanup()
        except Exception:  # pragma: no cover - cleanup best-effort
            pass


@app.command("fine-tune")
def fine_tune_stub(
    base_model: str = typer.Option(..., "--base-model", help="åŸºç¡€æ¨¡å‹åç§°"),
    dataset_path: str = typer.Option(..., "--dataset", help="è®­ç»ƒæ•°æ®è·¯å¾„"),
    output_dir: str = typer.Option(..., "--output", help="è¾“å‡ºç›®å½•"),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="è‡ªåŠ¨ç¡®ä¿åŸºç¡€æ¨¡å‹å°±ç»ª"
    ),
):
    """æäº¤ fine-tune è¯·æ±‚ï¼ˆå½“å‰ä¸ºå ä½å®ç°ï¼‰ã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-common[vllm]ï¼Œæ— æ³•è°ƒç”¨ fine-tune æ¥å£ã€‚")
        raise typer.Exit(1)

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    service = VLLMService({"model_id": base_model, "auto_download": auto_download})
    try:
        try:
            service.fine_tune(
                {
                    "base_model": base_model,
                    "dataset_path": dataset_path,
                    "output_dir": output_dir,
                }
            )
        except NotImplementedError as exc:
            typer.echo(f"â„¹ï¸ {exc}")
        else:
            typer.echo("âœ… fine-tune è¯·æ±‚å·²æäº¤")
    finally:
        service.cleanup()
