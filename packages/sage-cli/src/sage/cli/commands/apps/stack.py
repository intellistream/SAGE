#!/usr/bin/env python3
"""Service stack management commands for SAGE.

Provides one-command startup for the complete sageLLM inference stack:
- LLM service (vLLM)
- Embedding service

Commands:
    - start: Start the complete inference stack (LLM + Embedding)
    - stop: Stop all stack services
    - status: Check status of all services

Example:
    sage apps stack start  # Start LLM + Embedding services
    sage apps stack status # Check running services
    sage apps stack stop   # Stop all services
"""

from __future__ import annotations

import os
import signal
import socket
import subprocess
import sys
import time
from pathlib import Path

import psutil
import typer
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from sage.common.config.ports import SagePorts

console = Console()
app = typer.Typer(help="ğŸš€ æœåŠ¡æ ˆç®¡ç† - ä¸€é”®å¯åŠ¨ LLM + Embedding æœåŠ¡")

# PID files location
SAGE_DIR = Path.home() / ".sage"
LLM_PID_FILE = SAGE_DIR / "llm_service.pid"
EMBEDDING_PID_FILE = SAGE_DIR / "embedding_service.pid"
LOG_DIR = SAGE_DIR / "logs"


def _ensure_dirs():
    """Ensure required directories exist."""
    SAGE_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)


def _is_port_in_use(port: int) -> bool:
    """Check if a port is in use."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        result = sock.connect_ex(("localhost", port))
        return result == 0


def _wait_for_port(port: int, timeout: int = 60, service_name: str = "service") -> bool:
    """Wait for a port to become available."""
    start_time = time.time()
    while time.time() - start_time < timeout:
        if _is_port_in_use(port):
            return True
        console.print(f"  [dim]ç­‰å¾… {service_name} å¯åŠ¨ä¸­...[/dim]", end="\r")
        time.sleep(2)
    return False


def _get_pid_from_file(pid_file: Path) -> int | None:
    """Get PID from file if process is still running."""
    if not pid_file.exists():
        return None
    try:
        pid = int(pid_file.read_text().strip())
        if psutil.pid_exists(pid):
            return pid
    except (ValueError, OSError):
        pass
    return None


def _save_pid(pid_file: Path, pid: int):
    """Save PID to file."""
    pid_file.write_text(str(pid))


def _stop_service(pid_file: Path, service_name: str) -> bool:
    """Stop a service by PID file."""
    pid = _get_pid_from_file(pid_file)
    if not pid:
        return False

    try:
        proc = psutil.Process(pid)
        # Terminate child processes first
        children = proc.children(recursive=True)
        for child in children:
            try:
                child.terminate()
            except psutil.NoSuchProcess:
                pass

        proc.terminate()
        proc.wait(timeout=10)
        pid_file.unlink(missing_ok=True)
        console.print(f"  [green]âœ“[/green] {service_name} å·²åœæ­¢ (PID: {pid})")
        return True
    except psutil.NoSuchProcess:
        pid_file.unlink(missing_ok=True)
        return False
    except psutil.TimeoutExpired:
        # Force kill
        try:
            os.kill(pid, signal.SIGKILL)
            pid_file.unlink(missing_ok=True)
            console.print(f"  [yellow]âš [/yellow] {service_name} å·²å¼ºåˆ¶åœæ­¢ (PID: {pid})")
            return True
        except OSError:
            return False


@app.command("start")
def start_stack(
    llm_model: str = typer.Option(
        "Qwen/Qwen2.5-0.5B-Instruct",
        "--llm-model",
        "-l",
        help="LLM æ¨¡å‹åç§°",
    ),
    embedding_model: str = typer.Option(
        "BAAI/bge-small-zh-v1.5",
        "--embedding-model",
        "-e",
        help="Embedding æ¨¡å‹åç§°",
    ),
    llm_port: int = typer.Option(
        SagePorts.BENCHMARK_LLM,
        "--llm-port",
        help=f"LLM æœåŠ¡ç«¯å£ (é»˜è®¤: {SagePorts.BENCHMARK_LLM})",
    ),
    embedding_port: int = typer.Option(
        SagePorts.EMBEDDING_DEFAULT,
        "--embedding-port",
        help=f"Embedding æœåŠ¡ç«¯å£ (é»˜è®¤: {SagePorts.EMBEDDING_DEFAULT})",
    ),
    gpu_memory: float = typer.Option(
        0.5,
        "--gpu-memory",
        help="vLLM GPU å†…å­˜ä½¿ç”¨ç‡ (0.1-1.0)",
    ),
    skip_llm: bool = typer.Option(
        False,
        "--skip-llm",
        help="è·³è¿‡ LLM æœåŠ¡å¯åŠ¨ï¼ˆå¦‚æœå·²æ‰‹åŠ¨å¯åŠ¨ï¼‰",
    ),
    skip_embedding: bool = typer.Option(
        False,
        "--skip-embedding",
        help="è·³è¿‡ Embedding æœåŠ¡å¯åŠ¨",
    ),
    wait: bool = typer.Option(
        True,
        "--wait/--no-wait",
        help="ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨",
    ),
):
    """ä¸€é”®å¯åŠ¨å®Œæ•´æ¨ç†æœåŠ¡æ ˆï¼ˆLLM + Embeddingï¼‰ã€‚

    è¯¥å‘½ä»¤ä¼šå¯åŠ¨ï¼š
    1. vLLM æœåŠ¡ - ç”¨äº chat/generate è¯·æ±‚
    2. Embedding æœåŠ¡ - ç”¨äºæ–‡æœ¬å‘é‡åŒ–

    å¯åŠ¨åå¯é€šè¿‡ UnifiedInferenceClient ç»Ÿä¸€è®¿é—®ï¼š

    \b
    from sage.common.components.sage_llm import UnifiedInferenceClient

    client = UnifiedInferenceClient.create_with_control_plane(
        llm_base_url="http://localhost:8901/v1",
        embedding_base_url="http://localhost:8090/v1",
    )

    # LLM
    response = client.chat([{"role": "user", "content": "Hello"}])

    # Embedding
    vectors = client.embed(["text1", "text2"])

    ç¤ºä¾‹ï¼š
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹å¯åŠ¨
        sage apps stack start

        # æŒ‡å®šæ¨¡å‹
        sage apps stack start -l Qwen/Qwen2.5-7B-Instruct -e BAAI/bge-m3

        # åªå¯åŠ¨ Embeddingï¼ˆLLM å·²æ‰‹åŠ¨å¯åŠ¨ï¼‰
        sage apps stack start --skip-llm

    """
    _ensure_dirs()

    console.print(
        Panel.fit(
            "[bold blue]ğŸš€ SAGE æ¨ç†æœåŠ¡æ ˆ[/bold blue]\n"
            f"LLM: {llm_model} @ :{llm_port}\n"
            f"Embedding: {embedding_model} @ :{embedding_port}",
            title="å¯åŠ¨é…ç½®",
        )
    )

    services_started = []

    # 1. Start LLM service (vLLM)
    if not skip_llm:
        if _is_port_in_use(llm_port):
            console.print(f"[yellow]âš [/yellow] LLM ç«¯å£ {llm_port} å·²è¢«å ç”¨ï¼Œè·³è¿‡å¯åŠ¨")
        else:
            console.print("\n[blue]1/2[/blue] å¯åŠ¨ LLM æœåŠ¡ (vLLM)...")

            llm_log = LOG_DIR / "vllm.log"
            llm_cmd = [
                sys.executable,
                "-m",
                "vllm.entrypoints.openai.api_server",
                "--model",
                llm_model,
                "--port",
                str(llm_port),
                "--gpu-memory-utilization",
                str(gpu_memory),
                "--max-model-len",
                "4096",
                "--enforce-eager",
                "--disable-log-stats",
                "--trust-remote-code",
            ]

            with open(llm_log, "w") as log_file:
                proc = subprocess.Popen(
                    llm_cmd,
                    stdout=log_file,
                    stderr=subprocess.STDOUT,
                    start_new_session=True,
                    env={**os.environ, "TOKENIZERS_PARALLELISM": "false"},
                )

            _save_pid(LLM_PID_FILE, proc.pid)
            console.print(f"  [green]âœ“[/green] vLLM è¿›ç¨‹å·²å¯åŠ¨ (PID: {proc.pid})")
            console.print(f"  [dim]æ—¥å¿—: {llm_log}[/dim]")

            if wait:
                console.print(f"  ç­‰å¾… vLLM æœåŠ¡å°±ç»ª (ç«¯å£ {llm_port})...")
                if _wait_for_port(llm_port, timeout=120, service_name="vLLM"):
                    console.print("  [green]âœ“[/green] vLLM æœåŠ¡å·²å°±ç»ª")
                    services_started.append(("LLM (vLLM)", llm_port))
                else:
                    console.print(f"  [yellow]âš [/yellow] vLLM å¯åŠ¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æ—¥å¿—: {llm_log}")
            else:
                services_started.append(("LLM (vLLM)", llm_port))
    else:
        console.print("[dim]è·³è¿‡ LLM æœåŠ¡å¯åŠ¨[/dim]")

    # 2. Start Embedding service
    if not skip_embedding:
        if _is_port_in_use(embedding_port):
            console.print(f"[yellow]âš [/yellow] Embedding ç«¯å£ {embedding_port} å·²è¢«å ç”¨ï¼Œè·³è¿‡å¯åŠ¨")
        else:
            console.print("\n[blue]2/2[/blue] å¯åŠ¨ Embedding æœåŠ¡...")

            embedding_log = LOG_DIR / "embedding.log"
            embedding_cmd = [
                sys.executable,
                "-m",
                "sage.common.components.sage_embedding.embedding_server",
                "--model",
                embedding_model,
                "--port",
                str(embedding_port),
            ]

            with open(embedding_log, "w") as log_file:
                proc = subprocess.Popen(
                    embedding_cmd,
                    stdout=log_file,
                    stderr=subprocess.STDOUT,
                    start_new_session=True,
                    env={**os.environ, "TOKENIZERS_PARALLELISM": "false"},
                )

            _save_pid(EMBEDDING_PID_FILE, proc.pid)
            console.print(f"  [green]âœ“[/green] Embedding è¿›ç¨‹å·²å¯åŠ¨ (PID: {proc.pid})")
            console.print(f"  [dim]æ—¥å¿—: {embedding_log}[/dim]")

            if wait:
                console.print(f"  ç­‰å¾… Embedding æœåŠ¡å°±ç»ª (ç«¯å£ {embedding_port})...")
                if _wait_for_port(embedding_port, timeout=60, service_name="Embedding"):
                    console.print("  [green]âœ“[/green] Embedding æœåŠ¡å·²å°±ç»ª")
                    services_started.append(("Embedding", embedding_port))
                else:
                    console.print(
                        f"  [yellow]âš [/yellow] Embedding å¯åŠ¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æ—¥å¿—: {embedding_log}"
                    )
            else:
                services_started.append(("Embedding", embedding_port))
    else:
        console.print("[dim]è·³è¿‡ Embedding æœåŠ¡å¯åŠ¨[/dim]")

    # Summary
    if services_started:
        console.print("\n" + "=" * 50)
        console.print("[bold green]âœ… æœåŠ¡æ ˆå¯åŠ¨å®Œæˆ[/bold green]\n")

        table = Table(show_header=True, header_style="bold")
        table.add_column("æœåŠ¡")
        table.add_column("ç«¯å£")
        table.add_column("API ç«¯ç‚¹")

        for svc_name, svc_port in services_started:
            table.add_row(svc_name, str(svc_port), f"http://localhost:{svc_port}/v1")

        console.print(table)

        console.print("\n[bold]ä½¿ç”¨ç¤ºä¾‹:[/bold]")
        example_code = f"""
[dim]from sage.common.components.sage_llm import UnifiedInferenceClient

client = UnifiedInferenceClient.create_with_control_plane(
    llm_base_url="http://localhost:{llm_port}/v1",
    embedding_base_url="http://localhost:{embedding_port}/v1",
)

# Chat
response = client.chat([{{"role": "user", "content": "Hello"}}])
print(response)

# Embedding
vectors = client.embed(["Hello world"])
print(f"Embedding dim: {{len(vectors[0])}}")
[/dim]"""
        console.print(example_code)

        console.print("\n[dim]ä½¿ç”¨ 'sage apps stack status' æŸ¥çœ‹çŠ¶æ€[/dim]")
        console.print("[dim]ä½¿ç”¨ 'sage apps stack stop' åœæ­¢æœåŠ¡[/dim]")
    else:
        console.print("\n[yellow]âš  æ²¡æœ‰æ–°æœåŠ¡è¢«å¯åŠ¨[/yellow]")


@app.command("stop")
def stop_stack(
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶åœæ­¢æ‰€æœ‰è¿›ç¨‹"),
):
    """åœæ­¢æ‰€æœ‰æœåŠ¡æ ˆä¸­çš„æœåŠ¡ã€‚"""
    console.print("[blue]ğŸ›‘ åœæ­¢æœåŠ¡æ ˆ...[/blue]\n")

    stopped_any = False

    # Stop LLM service
    console.print("[bold]LLM æœåŠ¡:[/bold]")
    if _stop_service(LLM_PID_FILE, "vLLM"):
        stopped_any = True
    else:
        # Try to find and kill vllm processes
        if force:
            for proc in psutil.process_iter(["pid", "name", "cmdline"]):
                try:
                    cmdline = proc.info.get("cmdline") or []
                    if any("vllm" in str(c) for c in cmdline):
                        proc.terminate()
                        console.print(f"  [yellow]âš [/yellow] ç»ˆæ­¢ vLLM è¿›ç¨‹ (PID: {proc.pid})")
                        stopped_any = True
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
        else:
            console.print("  [dim]æ— è¿è¡Œä¸­çš„ LLM æœåŠ¡[/dim]")

    # Stop Embedding service
    console.print("\n[bold]Embedding æœåŠ¡:[/bold]")
    if _stop_service(EMBEDDING_PID_FILE, "Embedding"):
        stopped_any = True
    else:
        # Try to find and kill embedding processes
        if force:
            for proc in psutil.process_iter(["pid", "name", "cmdline"]):
                try:
                    cmdline = proc.info.get("cmdline") or []
                    if any("embedding_server" in str(c) for c in cmdline):
                        proc.terminate()
                        console.print(f"  [yellow]âš [/yellow] ç»ˆæ­¢ Embedding è¿›ç¨‹ (PID: {proc.pid})")
                        stopped_any = True
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
        else:
            console.print("  [dim]æ— è¿è¡Œä¸­çš„ Embedding æœåŠ¡[/dim]")

    if stopped_any:
        console.print("\n[green]âœ… æœåŠ¡å·²åœæ­¢[/green]")
    else:
        console.print("\n[dim]æ²¡æœ‰éœ€è¦åœæ­¢çš„æœåŠ¡[/dim]")


@app.command("status")
def status_stack():
    """æŸ¥çœ‹æœåŠ¡æ ˆçŠ¶æ€ã€‚"""
    console.print("[blue]ğŸ“Š æœåŠ¡æ ˆçŠ¶æ€[/blue]\n")

    table = Table(show_header=True, header_style="bold")
    table.add_column("æœåŠ¡")
    table.add_column("çŠ¶æ€")
    table.add_column("PID")
    table.add_column("ç«¯å£")
    table.add_column("API ç«¯ç‚¹")

    # Check LLM service
    llm_pid = _get_pid_from_file(LLM_PID_FILE)
    llm_port = SagePorts.BENCHMARK_LLM
    llm_running = llm_pid is not None or _is_port_in_use(llm_port)

    if llm_running:
        table.add_row(
            "LLM (vLLM)",
            "[green]è¿è¡Œä¸­[/green]",
            str(llm_pid or "å¤–éƒ¨"),
            str(llm_port),
            f"http://localhost:{llm_port}/v1",
        )
    else:
        table.add_row("LLM (vLLM)", "[red]å·²åœæ­¢[/red]", "-", str(llm_port), "-")

    # Check Embedding service
    emb_pid = _get_pid_from_file(EMBEDDING_PID_FILE)
    emb_port = SagePorts.EMBEDDING_DEFAULT
    emb_running = emb_pid is not None or _is_port_in_use(emb_port)

    if emb_running:
        table.add_row(
            "Embedding",
            "[green]è¿è¡Œä¸­[/green]",
            str(emb_pid or "å¤–éƒ¨"),
            str(emb_port),
            f"http://localhost:{emb_port}/v1",
        )
    else:
        table.add_row("Embedding", "[red]å·²åœæ­¢[/red]", "-", str(emb_port), "-")

    console.print(table)

    # Quick test if services are responding
    if llm_running or emb_running:
        console.print("\n[bold]å¿«é€Ÿæµ‹è¯•:[/bold]")

        if llm_running:
            try:
                import httpx

                resp = httpx.get(f"http://localhost:{llm_port}/v1/models", timeout=5)
                if resp.status_code == 200:
                    models = resp.json().get("data", [])
                    if models:
                        console.print(f"  [green]âœ“[/green] LLM: {models[0].get('id', 'unknown')}")
                else:
                    console.print(f"  [yellow]âš [/yellow] LLM: å“åº”å¼‚å¸¸ ({resp.status_code})")
            except Exception as e:
                console.print(f"  [red]âœ—[/red] LLM: è¿æ¥å¤±è´¥ ({e})")

        if emb_running:
            try:
                import httpx

                resp = httpx.get(f"http://localhost:{emb_port}/v1/models", timeout=5)
                if resp.status_code == 200:
                    models = resp.json().get("data", [])
                    if models:
                        console.print(
                            f"  [green]âœ“[/green] Embedding: {models[0].get('id', 'unknown')}"
                        )
                else:
                    console.print(f"  [yellow]âš [/yellow] Embedding: å“åº”å¼‚å¸¸ ({resp.status_code})")
            except Exception as e:
                console.print(f"  [red]âœ—[/red] Embedding: è¿æ¥å¤±è´¥ ({e})")


@app.command("logs")
def view_logs(
    service: str = typer.Argument("all", help="æœåŠ¡åç§° (llm/embedding/all)"),
    follow: bool = typer.Option(False, "--follow", "-f", help="å®æ—¶è·Ÿè¸ªæ—¥å¿—"),
    lines: int = typer.Option(50, "--lines", "-n", help="æ˜¾ç¤ºæœ€å N è¡Œ"),
):
    """æŸ¥çœ‹æœåŠ¡æ—¥å¿—ã€‚"""
    log_files = []

    if service in ("all", "llm"):
        llm_log = LOG_DIR / "vllm.log"
        if llm_log.exists():
            log_files.append(("LLM", llm_log))

    if service in ("all", "embedding"):
        emb_log = LOG_DIR / "embedding.log"
        if emb_log.exists():
            log_files.append(("Embedding", emb_log))

    if not log_files:
        console.print("[yellow]æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶[/yellow]")
        raise typer.Exit(0)

    if follow:
        # Use tail -f for live following
        import shlex

        files = " ".join(shlex.quote(str(f[1])) for f in log_files)
        os.system(f"tail -f {files}")
    else:
        for name, log_file in log_files:
            console.print(f"\n[bold blue]{'=' * 20} {name} {'=' * 20}[/bold blue]")
            try:
                content = log_file.read_text()
                log_lines = content.strip().split("\n")
                for line in log_lines[-lines:]:
                    console.print(line)
            except Exception as e:
                console.print(f"[red]æ— æ³•è¯»å–æ—¥å¿—: {e}[/red]")


if __name__ == "__main__":
    app()
