"""
é«˜çº§æœåŠ¡ç³»ç»Ÿæµ‹è¯•

å±•ç¤ºæ›´å¤æ‚çš„æœåŠ¡ä½¿ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬ï¼š
1. æ•°æ®å¤„ç†ç®¡é“æœåŠ¡
2. ç¼“å­˜æœåŠ¡ä¸æ•°æ®åº“æœåŠ¡çš„åä½œ
3. ç›‘æ§å’Œç»Ÿè®¡æœåŠ¡
4. å¤šæœåŠ¡ååŒå·¥ä½œ
"""

import time
import random
import json
from typing import List, Dict, Any
from sage_core.api.local_environment import LocalEnvironment


class DataProcessorService:
    """
    æ•°æ®å¤„ç†æœåŠ¡
    æ¨¡æ‹Ÿæ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯åŠŸèƒ½
    """
    
    def __init__(self, batch_size: int = 100, workers: int = 4):
        self.batch_size = batch_size
        self.workers = workers
        self.is_running = False
        self.processed_count = 0
        self.error_count = 0
        self.ctx = None
        
        # æ¨¡æ‹Ÿå¤„ç†è§„åˆ™
        self.processing_rules = {
            'normalize_text': lambda x: x.strip().lower() if isinstance(x, str) else x,
            'validate_email': lambda x: '@' in x if isinstance(x, str) else False,
            'extract_numbers': lambda x: [int(i) for i in x.split() if i.isdigit()] if isinstance(x, str) else []
        }
    
    def start_running(self):
        self.is_running = True
        print(f"Data processor started with {self.workers} workers, batch size {self.batch_size}")
    
    def terminate(self):
        self.is_running = False
        print(f"Data processor terminated. Processed: {self.processed_count}, Errors: {self.error_count}")
    
    def process_batch(self, data_batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†ä¸€æ‰¹æ•°æ®"""
        if not self.is_running:
            raise RuntimeError("Data processor is not running")
        
        processed_batch = []
        for item in data_batch:
            try:
                processed_item = self._process_single_item(item)
                processed_batch.append(processed_item)
                self.processed_count += 1
            except Exception as e:
                self.error_count += 1
                print(f"Error processing item {item}: {e}")
        
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.1)
        return processed_batch
    
    def _process_single_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
        processed = item.copy()
        
        # åº”ç”¨å¤„ç†è§„åˆ™
        for field, value in item.items():
            if field == 'text' and value:
                processed[field] = self.processing_rules['normalize_text'](value)
            elif field == 'email' and value:
                processed['email_valid'] = self.processing_rules['validate_email'](value)
            elif field == 'numbers' and value:
                processed['extracted_numbers'] = self.processing_rules['extract_numbers'](value)
        
        processed['processed_timestamp'] = time.time()
        return processed
    
    def get_statistics(self) -> Dict[str, Any]:
        return {
            'processed_count': self.processed_count,
            'error_count': self.error_count,
            'success_rate': self.processed_count / (self.processed_count + self.error_count) if (self.processed_count + self.error_count) > 0 else 0
        }


class SmartCacheService:
    """
    æ™ºèƒ½ç¼“å­˜æœåŠ¡
    æ”¯æŒTTLã€LRUç­–ç•¥å’Œå‘½ä¸­ç‡ç»Ÿè®¡
    """
    
    def __init__(self, max_size: int = 1000, default_ttl: int = 300):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.is_running = False
        self.cache = {}
        self.access_times = {}
        self.ttl_times = {}
        self.hit_count = 0
        self.miss_count = 0
        self.ctx = None
    
    def start_running(self):
        self.is_running = True
        print(f"Smart cache started: max_size={self.max_size}, default_ttl={self.default_ttl}s")
    
    def terminate(self):
        self.is_running = False
        print(f"Smart cache terminated. Hit rate: {self.get_hit_rate():.2%}")
    
    def get(self, key: str) -> Any:
        """è·å–ç¼“å­˜å€¼"""
        if not self.is_running:
            return None
        
        current_time = time.time()
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ
        if key in self.cache and current_time < self.ttl_times.get(key, 0):
            self.access_times[key] = current_time
            self.hit_count += 1
            return self.cache[key]
        else:
            # æ¸…ç†è¿‡æœŸé¡¹
            if key in self.cache:
                self._remove_key(key)
            self.miss_count += 1
            return None
    
    def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        if not self.is_running:
            return False
        
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œä½¿ç”¨LRUç­–ç•¥ç§»é™¤æœ€ä¹…æœªè®¿é—®çš„é¡¹
        if len(self.cache) >= self.max_size and key not in self.cache:
            self._evict_lru()
        
        current_time = time.time()
        ttl = ttl or self.default_ttl
        
        self.cache[key] = value
        self.access_times[key] = current_time
        self.ttl_times[key] = current_time + ttl
        
        return True
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜é¡¹"""
        if key in self.cache:
            self._remove_key(key)
            return True
        return False
    
    def _remove_key(self, key: str):
        """ç§»é™¤æŒ‡å®škey"""
        self.cache.pop(key, None)
        self.access_times.pop(key, None)
        self.ttl_times.pop(key, None)
    
    def _evict_lru(self):
        """ç§»é™¤æœ€ä¹…æœªè®¿é—®çš„é¡¹"""
        if not self.access_times:
            return
        
        lru_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        self._remove_key(lru_key)
        print(f"Evicted LRU key: {lru_key}")
    
    def get_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.hit_count + self.miss_count
        return self.hit_count / total if total > 0 else 0
    
    def get_statistics(self) -> Dict[str, Any]:
        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': self.get_hit_rate(),
            'utilization': len(self.cache) / self.max_size
        }


class MonitoringService:
    """
    ç›‘æ§æœåŠ¡
    æ”¶é›†å’ŒæŠ¥å‘Šç³»ç»ŸæŒ‡æ ‡
    """
    
    def __init__(self, collection_interval: int = 10, max_metrics: int = 10000):
        self.collection_interval = collection_interval
        self.max_metrics = max_metrics
        self.is_running = False
        self.metrics_data = []
        self.alert_rules = {}
        self.ctx = None
    
    def start_running(self):
        self.is_running = True
        print(f"Monitoring service started: interval={self.collection_interval}s")
    
    def terminate(self):
        self.is_running = False
        print(f"Monitoring service terminated. Collected {len(self.metrics_data)} metrics")
    
    def collect_metric(self, name: str, value: float, tags: Dict[str, str] = None):
        """æ”¶é›†æŒ‡æ ‡"""
        if not self.is_running:
            return
        
        metric = {
            'name': name,
            'value': value,
            'timestamp': time.time(),
            'tags': tags or {}
        }
        
        self.metrics_data.append(metric)
        
        # ä¿æŒæœ€å¤§æŒ‡æ ‡æ•°é‡
        if len(self.metrics_data) > self.max_metrics:
            self.metrics_data = self.metrics_data[-self.max_metrics:]
        
        # æ£€æŸ¥å‘Šè­¦è§„åˆ™
        self._check_alerts(metric)
    
    def add_alert_rule(self, name: str, metric_name: str, threshold: float, operator: str = '>'):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        self.alert_rules[name] = {
            'metric_name': metric_name,
            'threshold': threshold,
            'operator': operator
        }
        print(f"Added alert rule: {name} ({metric_name} {operator} {threshold})")
    
    def _check_alerts(self, metric: Dict[str, Any]):
        """æ£€æŸ¥å‘Šè­¦"""
        for alert_name, rule in self.alert_rules.items():
            if metric['name'] == rule['metric_name']:
                value = metric['value']
                threshold = rule['threshold']
                operator = rule['operator']
                
                triggered = False
                if operator == '>' and value > threshold:
                    triggered = True
                elif operator == '<' and value < threshold:
                    triggered = True
                elif operator == '==' and value == threshold:
                    triggered = True
                
                if triggered:
                    print(f"ğŸš¨ ALERT: {alert_name} - {metric['name']}={value} {operator} {threshold}")
    
    def get_metric_summary(self, metric_name: str, window_minutes: int = 10) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        current_time = time.time()
        window_start = current_time - (window_minutes * 60)
        
        relevant_metrics = [
            m for m in self.metrics_data 
            if m['name'] == metric_name and m['timestamp'] >= window_start
        ]
        
        if not relevant_metrics:
            return {'count': 0}
        
        values = [m['value'] for m in relevant_metrics]
        
        return {
            'count': len(values),
            'min': min(values),
            'max': max(values),
            'avg': sum(values) / len(values),
            'latest': values[-1] if values else None
        }


def test_data_processing_pipeline():
    """æµ‹è¯•æ•°æ®å¤„ç†ç®¡é“"""
    print("=== æ•°æ®å¤„ç†ç®¡é“æµ‹è¯• ===")
    
    # åˆ›å»ºç¯å¢ƒ
    env = LocalEnvironment("data_pipeline_env")
    
    # æ³¨å†ŒæœåŠ¡
    env.register_service("data_processor", DataProcessorService, batch_size=50, workers=2)
    env.register_service("cache", SmartCacheService, max_size=500, default_ttl=120)
    env.register_service("monitor", MonitoringService, collection_interval=5)
    
    # åˆ›å»ºæœåŠ¡ä»»åŠ¡
    services = {}
    for service_name in env.service_task_factories:
        task_factory = env.service_task_factories[service_name]
        service_task = task_factory.create_service_task()
        services[service_name] = service_task
    
    try:
        # å¯åŠ¨æ‰€æœ‰æœåŠ¡
        print("\nå¯åŠ¨æœåŠ¡...")
        for service_name, service_task in services.items():
            service_task.start_running()
        
        # è·å–æœåŠ¡å®ä¾‹
        processor = services["data_processor"].service
        cache = services["cache"].service
        monitor = services["monitor"].service
        
        # è®¾ç½®ç›‘æ§å‘Šè­¦
        monitor.add_alert_rule("high_error_rate", "error_rate", 0.1, '>')
        monitor.add_alert_rule("low_cache_hit", "cache_hit_rate", 0.5, '<')
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        print("\nç”Ÿæˆæµ‹è¯•æ•°æ®...")
        test_data = []
        for i in range(200):
            item = {
                'id': i,
                'text': f"  Hello World {i}  ".upper() if i % 3 == 0 else f"test data {i}",
                'email': f"user{i}@test.com" if i % 5 != 0 else "invalid-email",
                'numbers': f"{random.randint(1, 100)} {random.randint(200, 300)} text {random.randint(400, 500)}"
            }
            test_data.append(item)
        
        # åˆ†æ‰¹å¤„ç†æ•°æ®
        print("\nå¼€å§‹æ•°æ®å¤„ç†...")
        batch_size = 25
        processed_total = 0
        
        for i in range(0, len(test_data), batch_size):
            batch = test_data[i:i + batch_size]
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"batch_{i // batch_size}"
            cached_result = cache.get(cache_key)
            
            if cached_result:
                print(f"ä½¿ç”¨ç¼“å­˜ç»“æœ: batch {i // batch_size}")
                processed_batch = cached_result
            else:
                # å¤„ç†æ•°æ®
                processed_batch = processor.process_batch(batch)
                # å­˜å…¥ç¼“å­˜
                cache.set(cache_key, processed_batch, ttl=60)
                print(f"å¤„ç†å®Œæˆ: batch {i // batch_size}, å¤„ç†äº† {len(processed_batch)} é¡¹")
            
            processed_total += len(processed_batch)
            
            # æ”¶é›†ç›‘æ§æŒ‡æ ‡
            processor_stats = processor.get_statistics()
            cache_stats = cache.get_statistics()
            
            monitor.collect_metric("processed_items", processed_total)
            monitor.collect_metric("error_rate", processor_stats['error_count'] / max(processor_stats['processed_count'], 1))
            monitor.collect_metric("cache_hit_rate", cache_stats['hit_rate'])
            monitor.collect_metric("cache_utilization", cache_stats['utilization'])
            
            # æ¨¡æ‹Ÿä¸€äº›éšæœºå»¶è¿Ÿ
            time.sleep(0.1)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\n=== å¤„ç†å®Œæˆ ===")
        print(f"æ€»å¤„ç†é¡¹ç›®: {processed_total}")
        
        print(f"\næ•°æ®å¤„ç†å™¨ç»Ÿè®¡:")
        processor_stats = processor.get_statistics()
        for key, value in processor_stats.items():
            print(f"  {key}: {value}")
        
        print(f"\nç¼“å­˜ç»Ÿè®¡:")
        cache_stats = cache.get_statistics()
        for key, value in cache_stats.items():
            print(f"  {key}: {value}")
        
        print(f"\nç›‘æ§æ‘˜è¦:")
        for metric_name in ["processed_items", "error_rate", "cache_hit_rate"]:
            summary = monitor.get_metric_summary(metric_name, window_minutes=5)
            print(f"  {metric_name}: {summary}")
    
    finally:
        # æ¸…ç†æœåŠ¡
        print(f"\næ¸…ç†æœåŠ¡...")
        for service_name, service_task in services.items():
            service_task.terminate()


def test_collaborative_services():
    """æµ‹è¯•æœåŠ¡åä½œåœºæ™¯"""
    print("\n=== æœåŠ¡åä½œæµ‹è¯• ===")
    
    # åˆ›å»ºç¯å¢ƒ
    env = LocalEnvironment("collaborative_env")
    
    # æ³¨å†ŒæœåŠ¡
    env.register_service("cache", SmartCacheService, max_size=100, default_ttl=30)
    env.register_service("monitor", MonitoringService, collection_interval=1)
    
    # åˆ›å»ºæœåŠ¡
    services = {}
    for service_name in env.service_task_factories:
        task_factory = env.service_task_factories[service_name]
        service_task = task_factory.create_service_task()
        services[service_name] = service_task
    
    try:
        # å¯åŠ¨æœåŠ¡
        for service_task in services.values():
            service_task.start_running()
        
        cache = services["cache"].service
        monitor = services["monitor"].service
        
        # æ¨¡æ‹Ÿåº”ç”¨å·¥ä½œè´Ÿè½½
        print("æ¨¡æ‹Ÿåº”ç”¨å·¥ä½œè´Ÿè½½...")
        
        # è®¾ç½®ç¼“å­˜æ•°æ®
        for i in range(50):
            key = f"key_{i}"
            value = {"data": f"value_{i}", "created": time.time()}
            cache.set(key, value, ttl=random.randint(10, 60))
        
        # æ¨¡æ‹Ÿè®¿é—®æ¨¡å¼
        for round_num in range(5):
            print(f"\nè®¿é—®è½®æ¬¡ {round_num + 1}")
            
            hit_count = 0
            miss_count = 0
            
            # éšæœºè®¿é—®ç¼“å­˜
            for _ in range(30):
                key = f"key_{random.randint(0, 70)}"  # æœ‰äº›keyä¸å­˜åœ¨
                result = cache.get(key)
                
                if result:
                    hit_count += 1
                else:
                    miss_count += 1
                    # æ¨¡æ‹Ÿä»æ•°æ®åº“åŠ è½½å¹¶ç¼“å­˜
                    if random.random() < 0.7:  # 70%æ¦‚ç‡èƒ½ä»"æ•°æ®åº“"æ‰¾åˆ°
                        new_value = {"data": f"loaded_{key}", "loaded_at": time.time()}
                        cache.set(key, new_value)
            
            # æ”¶é›†æŒ‡æ ‡
            cache_stats = cache.get_statistics()
            monitor.collect_metric("round_hits", hit_count, {"round": str(round_num)})
            monitor.collect_metric("round_misses", miss_count, {"round": str(round_num)})
            monitor.collect_metric("cache_size", cache_stats['size'])
            monitor.collect_metric("overall_hit_rate", cache_stats['hit_rate'])
            
            print(f"  æœ¬è½®å‘½ä¸­: {hit_count}, æœªå‘½ä¸­: {miss_count}")
            print(f"  ç¼“å­˜å¤§å°: {cache_stats['size']}, æ•´ä½“å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}")
            
            time.sleep(1)
        
        # æ˜¾ç¤ºç›‘æ§æ‘˜è¦
        print(f"\n=== ç›‘æ§æ‘˜è¦ ===")
        for metric in ["round_hits", "round_misses", "overall_hit_rate"]:
            summary = monitor.get_metric_summary(metric, window_minutes=1)
            print(f"{metric}: {summary}")
    
    finally:
        # æ¸…ç†
        for service_task in services.values():
            service_task.terminate()


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_data_processing_pipeline()
    test_collaborative_services()
    print("\nğŸ‰ æ‰€æœ‰é«˜çº§æœåŠ¡æµ‹è¯•å®Œæˆï¼")
