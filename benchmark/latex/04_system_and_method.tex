% System / Method section for the SAGE systems paper (content, not prompt)

\section{System Design}

\subsection{Overview and Design Goals}

SAGE is a Python 3.10+ framework that treats LLM/AI applications as declarative dataflow pipelines executed across heterogeneous CPU and GPU clusters. The system is designed to address several recurring challenges observed in practice: scaling multi-stage retrieval-and-generation workflows, sharing resources between LLM and embedding workloads with different latency requirements, and making complex deployments reproducible and observable. Rather than introducing yet another standalone serving engine, SAGE provides a full-stack architecture that integrates platform services, execution runtimes, performance-critical operators, user interfaces, and a unified control plane on top of existing engines such as vLLM.
The design of SAGE is guided by four goals. First, it should provide a clear separation of concerns so that common utilities, platform services, execution logic, and user interfaces can evolve independently; this motivates a strict five-layer architecture with no upward dependencies. Second, it should expose a high-level declarative dataflow abstraction that allows users to express retrieval, tool calls, LLM invocations, and post-processing without hard-coding deployment details. Third, it should offer a unified control plane for LLM and embedding services that can co-schedule workloads, enforce per-tenant SLOs, and adapt to heterogeneous hardware. Finally, it should be easy to deploy and evaluate: quickstart scripts, XDG-compliant configuration, and system-level benchmarks are treated as first-class components rather than afterthoughts.

\subsection{Layered Architecture}

At the core of SAGE is a strict five-layer architecture (L1--L5) that prevents upward dependencies and keeps each layer responsible for a focused set of concerns. The L1 layer, \texttt{sage-common}, contains foundational utilities, configuration management, XDG-based user paths, port management via \texttt{SagePorts}, and shared components such as the unified inference client and control-plane core modules. L2, implemented in \texttt{sage-platform}, provides platform services including storage integration, queuing, service supervision, and cluster configuration via project-level files like \texttt{config/cluster.yaml}. These two layers form the stable base that higher layers depend on but never modify directly.
The L3 layer comprises \texttt{sage-kernel} and \texttt{sage-libs}, which implement the core execution engine for dataflow pipelines. The kernel includes a job manager and node selector that map logical pipeline stages to concrete execution nodes, taking into account CPU-only and GPU-capable resources, load, and placement constraints. Algorithms and scheduling logic in this layer are written with awareness of LLM and embedding workloads, but do not hard-code any particular engine implementation. L4, \texttt{sage-middleware}, contains C++ operators and other performance-critical components built via CMake; these can be reused across applications and are invoked from the kernel as part of the dataflow execution. Finally, L5 provides user-facing entry points through \texttt{sage-cli} and \texttt{sage-tools}. Applications (sage-examples), benchmarks (sage-benchmark), the visual studio (sage-studio), and the LLM gateway (sageLLM) are maintained in independent repositories.
This layering has two practical benefits. First, it enables modular development: teams can evolve the control plane or middleware operators without entangling them with platform configuration or user interfaces, as long as they respect the downward-only dependency rule. Second, it simplifies reasoning about deployments and upgrades: operators can treat L1--L2 as part of the environment, roll out new kernels or middleware versions in L3--L4, and update CLI/tools in L5 with well-defined compatibility boundaries.

\subsection{Declarative Dataflow and Execution Model}

SAGE exposes to users a declarative dataflow API for constructing LLM/AI pipelines. A pipeline is modeled as a directed acyclic graph whose nodes correspond to operators such as embedding, retrieval, LLM generation, tool calls, and post-processing, and whose edges represent typed data streams. Users describe these pipelines in Python or configuration files without specifying where each operator runs, which engine instance it uses, or how requests are batched. This abstraction closely matches the mental model of LLM application developers while hiding low-level resource management details.
When a pipeline specification is submitted, SAGE compiles it into an executable plan using components in \texttt{sage-kernel} and \texttt{sage-libs}. The planner maps logical operators to physical implementations, including C++ middleware operators where beneficial, and chooses execution sites based on node capabilities and current load. The job manager then instantiates jobs for each stage, tracks their dependencies, and dispatches them to nodes selected by the node selector. During execution, the runtime performs dynamic batching where possible and propagates backpressure signals when downstream operators or engines become saturated. This compiled-execution model allows SAGE to adapt to heterogeneous environments while maintaining a stable, declarative interface for users.

\subsection{LLM and Embedding Control Plane}

To support mixed LLM and embedding workloads, SAGE introduces a unified control plane (``sageLLM'') that sits behind the \texttt{sage-gateway} OpenAI-compatible API. Requests arriving at the gateway are classified by the control plane into chat/generation and embedding categories using a request classifier component. The control plane maintains queues per workload type and tenant, and applies policies such as \texttt{HybridSchedulingPolicy} to decide which requests to admit and how to batch them. These policies are aware of the differing characteristics of LLM decoding and embedding computation: LLM requests have long-lived decode phases with idle gaps between GPU kernels, while embeddings are shorter, more uniform operations that can opportunistically use otherwise idle resources.
The control plane orchestrator, implemented by a \texttt{ControlPlaneManager} module under \texttt{sageLLM/control\_plane/}, manages a pool of backend engines consisting of vLLM instances for LLMs and embedding servers for vector encoders. It performs load balancing across engines, taking into account engine capacity, current queue lengths, and any placement constraints imposed by heterogeneous hardware. Batching and routing decisions are made at the control-plane level but respect the internal scheduling mechanisms of each engine: for example, vLLM continues to use its own continuous batching policy, while SAGE determines which requests to forward and when. This division of responsibilities allows SAGE to provide cross-engine and cross-workload scheduling without reimplementing per-engine optimizations.
From the perspective of SAGE's layered architecture, the control plane spans L1 through L3: core modules and configuration live in \texttt{sage-common}, platform-level deployment and port management are handled in \texttt{sage-platform}, and runtime policies are implemented in \texttt{sage-kernel} and associated control-plane packages. Higher layers interact with the control plane through the unified inference client and gateway, treating it as a managed service. This design makes it possible to swap in different scheduling policies, add new engine types, or adjust placement strategies without impacting application code.

\subsection{Implementation Details and Deployment}

SAGE is implemented as a multi-package Python project with selected components in C++ for performance-critical paths. Middleware operators in \texttt{sage-middleware} are built with CMake and linked into Python via extension modules, allowing the system to offload tight loops such as tokenization, feature extraction, or data transformation when necessary. The repository provides a unified development and quality toolchain through \texttt{sage-dev}, which wraps testing, linting, and formatting tools (including Ruff and Mypy) to ensure consistent code quality across packages.
Deployment is streamlined through quickstart scripts that install SAGE and its dependencies, set up environment variables, and configure user paths according to the XDG base directory specification. Runtime state and logs are stored in project-level \texttt{.sage/} directories and user-level locations derived from \texttt{get\_user\_paths()}, separating ephemeral artifacts from configuration. Cluster topology and service endpoints are described in \texttt{config/config.yaml} and \texttt{config/cluster.yaml}, which specify available CPU-only and GPU nodes, ports derived from \texttt{SagePorts}, and gateway settings. On top of this foundation, operators can use \texttt{sage-cli} and \texttt{sage-studio} to launch pipelines, inspect their status, and collect traces for debugging.
The same tooling underpins SAGE's benchmark suite in \texttt{sage-benchmark}, which defines reproducible experiments for end-to-end pipelines, control-plane policies, isolation and fairness, scalability, and heterogeneous deployments. By reusing the production control plane and dataflow runtime for benchmarks, SAGE ensures that experimental results reflect realistic system behavior rather than synthetic microbenchmarks.

\subsection{Relationship to vLLM and Other Engines}

A natural question for systems reviewers is how SAGE relates to optimized LLM serving engines such as vLLM. SAGE is explicitly designed to be complementary: it uses vLLM and similar systems as backend engines and focuses on the pipeline-level concerns that they do not address. vLLM optimizes single-model inference through mechanisms such as paged attention and continuous batching, whereas SAGE coordinates multiple LLM and embedding services, manages multi-tenant queues, and decides how to allocate heterogeneous CPU/GPU resources across competing pipelines.
In practical deployments, a SAGE cluster will consist of one or more vLLM instances registered with the control plane alongside embedding servers and other operators. For example, in a retrieval-augmented generation pipeline, SAGE can route embedding requests to CPU nodes running embedding models while reserving GPU capacity for LLM decoding, all behind a single \texttt{sage-gateway} endpoint. Without SAGE, operators must manually configure and balance separate LLM and embedding services; with SAGE, these decisions are encoded in declarative pipeline specifications and enforced by the control plane and kernel. The experiments in this paper therefore evaluate SAGE as a full system composed on top of engines like vLLM, rather than as a competitor to them.

% ---------------------------------------------------------------------------
% Suggested figures and diagrams (author notes, not part of camera-ready)
% ---------------------------------------------------------------------------
% Figure 1: SAGE layered architecture (L1--L5), showing packages and
%           dependency directions.
% Figure 2: Declarative dataflow and execution model, with a RAG pipeline
%           compiled into jobs and placed on CPU/GPU nodes.
% Figure 3: Control-plane architecture, including request classifier,
%           scheduling queues, HybridSchedulingPolicy, and backend engines.
% Figure 4: Example deployment diagram illustrating CPU-only embedding nodes
%           and GPU-backed LLM nodes, plus gateway and user interfaces.
% If page limits are tight, detailed control-plane internals or deployment
% diagrams can be moved to an appendix while retaining the high-level
% architecture and dataflow figures in the main paper.

% ---------------------------------------------------------------------------
% High-level System / Method outline (from 07_system_outline_example)
% ---------------------------------------------------------------------------
% This informal outline mirrors the structure implemented above and can be
% used as a checklist when revising the System Design section.
%
% 1) System Overview and Design Goals
%    - Problems: complex multi-step LLM pipelines, mixed LLM+embedding
%      workloads, CPU-only environments, end-to-end evaluation and
%      reproducibility.
%    - Goals: scalability, heterogeneity support, programmability via
%      declarative dataflow, debuggability/observability, reproducibility,
%      ease of evolution.
%    - Positioning: SAGE as a unified platform that combines serving,
%      workflow, control plane, and benchmarking.
%
% 2) Layered Architecture
%    - L1 (sage-common): configuration, XDG user paths, SagePorts, unified
%      inference client, control-plane core modules.
%    - L2 (sage-platform): platform services for storage, queuing, service
%      management, cluster configuration (config/config.yaml,
%      config/cluster.yaml).
%    - L3 (sage-kernel, sage-libs): execution kernels, job management,
%      node selection with CPU/GPU awareness.
%    - L4 (sage-middleware): C++ operators and performance-critical
%      components.
%    - L5 (sage-cli, sage-tools): CLI and development tools.
%    - Independent repos: sage-benchmark, sage-examples, sage-studio, sageLLM.
%
% 3) Declarative Dataflow and Execution Model
%    - User-facing APIs for composing ingestion, embedding, retrieval, LLM
%      generation, tool calls, and post-processing (examples/apps and
%      examples/tutorials).
%    - Compilation of declarative graphs into jobs placed on nodes, with
%      batching, parallelism, and backpressure handled by kernel/platform.
%    - Role of C++ middleware operators in performance-critical stages.
%
% 4) LLM & Embedding Control Plane (sageLLM, independent repo)
%    - Goals: share resources across LLM/embedding workloads, improve
%      throughput and tail latency, respect SLOs.
%    - Components: ControlPlaneManager, RequestClassifier,
%      HybridSchedulingPolicy, EmbeddingExecutor, engine pool.
%    - Interaction with sage-gateway FastAPI app and backend engines (vLLM,
%      embedding servers), port management via SagePorts (GATEWAY_DEFAULT,
%      LLM_DEFAULT, EMBEDDING_DEFAULT, WSL2-aware fallbacks).
%    - Distinction from single-instance vLLM or simple load balancers.
%
% 5) Implementation Details and Deployment
%    - Implementation choices: Python 3.10+ with selected C++ components in
%      sage-middleware, built via CMake, artifacts under .sage/.
%    - Tooling: quickstart.sh, manage.sh, CI install wrappers, sage-dev for
%      tests/quality/examples (pytest, Ruff, Mypy; tools/pytest.ini,
%      tools/ruff.toml).
%    - Deployment: CPU-only and GPU clusters via configuration files and
%      node selection; XDG-based user paths for logs, models, cache; support
%      for reproducible experiments and CI.
%
% This outline originates from docs/07_system_outline_example.md and is kept
% here as an author aid; it should not appear verbatim in the camera-ready
% paper.
