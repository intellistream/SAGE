from __future__ import annotations

import time
from typing import Type, Union, Any, List
from enum import Enum
import sage_memory.api
from sage_core.api.base_function import BaseFunction
from sage_core.api.datastream import DataStream
from sage_core.api.transformation import TransformationType, Transformation
from sage_utils.custom_logger import CustomLogger
from sage_core.api.enum import PlatformType
from sage_utils.name_server import get_name
class BaseEnvironment:

    def __init__(self, name: str, config: dict | None, *, platform: PlatformType = PlatformType.LOCAL):
        self.name = get_name(name)
        self.logger = CustomLogger(
            filename=f"Environment_{name}",
            env_name = name,
            console_output="WARNING",
            file_output=True,
            global_output = "DEBUG",
        )
        

        self.config: dict = dict(config or {})
        self.platform:PlatformType = platform
        # ç”¨äºæ”¶é›†æ‰€æœ‰ Transformationï¼Œä¾› Compiler æ„å»º DAG
        self._pipeline: List[Transformation] = []
        self.runtime_context = dict  # éœ€è¦åœ¨compileré‡Œé¢å®ä¾‹åŒ–ã€‚
        self.memory_collection = None  # ç”¨äºå­˜å‚¨å†…å­˜é›†åˆ
        self.is_running = False

    def from_source(
        self, 
        function: Union[BaseFunction, Type[BaseFunction]], 
        *args, 
        platform:PlatformType = PlatformType.LOCAL,
        **kwargs: Any) -> DataStream:
        
        """ç”¨æˆ· APIï¼šå£°æ˜ä¸€ä¸ªæ•°æ®æºå¹¶è¿”å› DataStream èµ·ç‚¹ã€‚"""
        transformation = Transformation(
            self, 
            TransformationType.SOURCE, 
            function, 
            *args,
            platform = platform,  
            **kwargs
            )
        
        self._pipeline.append(transformation)
        return DataStream(self, transformation)

    # TODO: add a new type of source with handler returned.
    def create_source(self):
        pass

    def submit(self, name="example_pipeline"):
        self.debug_print_pipeline()
        from sage_core.core.engine import Engine
        engine = Engine.get_instance()
        engine.submit_env(self)
        # time.sleep(10) # ç­‰å¾…ç®¡é“å¯åŠ¨
        while (self.initialized() is False):
            time.sleep(1)

    def run_once(self, node:str = None):
        """
        è¿è¡Œä¸€æ¬¡ç®¡é“ï¼Œé€‚ç”¨äºæµ‹è¯•æˆ–è°ƒè¯•ã€‚
        """
        if(self.is_running):
            self.logger.warning("Pipeline is already running. ")
            return
        from sage_core.core.engine import Engine
        engine = Engine.get_instance()
        engine.run_once(self)
        # time.sleep(10) # ç­‰å¾…ç®¡é“å¯åŠ¨

    def run_streaming(self, node: str = None):
        """
        è¿è¡Œç®¡é“ï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚
        """
        from sage_core.core.engine import Engine
        engine = Engine.get_instance()
        engine.run_streaming(self)
        # time.sleep(10) # ç­‰å¾…ç®¡é“å¯åŠ¨

    def stop(self):
        """
        åœæ­¢ç®¡é“è¿è¡Œã€‚
        """
        self.logger.info("Stopping pipeline...")
        from sage_core.core.engine import Engine
        engine = Engine.get_instance()
        engine.stop_pipeline(self)
        self.close()

    def close(self):
        """
        å…³é—­ç®¡é“è¿è¡Œã€‚
        """
        from sage_core.core.engine import Engine
        engine = Engine.get_instance()
        # 1) åœæ­¢æœ¬ç¯å¢ƒå¯¹åº”çš„ DAG æ‰§è¡Œ
        engine.stop_pipeline(self)
        # 2) å…³é—­è¯¥ç¯å¢ƒåœ¨ Engine ä¸­çš„å¼•ç”¨ï¼Œå¹¶åœ¨æ— å‰©ä½™ç¯å¢ƒæ—¶å½»åº• shutdown Engine
        engine.close_pipeline(self)
        # 3) æ¸…ç†è‡ªèº«å¼•ç”¨ï¼Œä»¥æ‰“ç ´å¾ªç¯é“¾
        self._pipeline.clear()

    @property
    def pipeline(self) -> List[Transformation]:  # noqa: D401
        """è¿”å› Transformation åˆ—è¡¨ï¼ˆCompiler ä¼šä½¿ç”¨ï¼‰ã€‚"""
        return self._pipeline

    def set_memory(self, config):
        self.memory_collection = sage_memory.api.get_memory(self, config, remote = (self.platform == PlatformType.REMOTE))

    def set_memory_collection(self, collection):

        self.memory_collection = collection 
        
    # TODO: å†™ä¸€ä¸ªåˆ¤æ–­Env æ˜¯å¦å·²ç»å®Œå…¨åˆå§‹åŒ–å¹¶å¼€å§‹æ‰§è¡Œçš„å‡½æ•°
    def initialized(self):
        pass

    def debug_print_pipeline(self):
        """
        è°ƒè¯•æ–¹æ³•ï¼šæ‰“å°ç¯å¢ƒåŠå…¶åŒ…å«çš„æ‰€æœ‰Transformationçš„è¯¦ç»†ä¿¡æ¯
        """
        lines = []
        lines.append("\n")
        lines.append("=" * 80)
        lines.append(f"Environment Debug Information: '{self.name}'")
        lines.append("=" * 80)
        
        # æ˜¾ç¤ºç¯å¢ƒåŸºæœ¬ä¿¡æ¯
        lines.append(f"\nğŸ—ï¸  Environment Details:")
        lines.append(f"   Name: {self.name}")
        lines.append(f"   Platform: {self.platform.value}")
        lines.append(f"   Running: {self.is_running}")
        lines.append(f"   Pipeline Size: {len(self._pipeline)} transformations")
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        if self.config:
            lines.append(f"   Configuration:")
            for key, value in self.config.items():
                lines.append(f"     {key}: {value}")
        else:
            lines.append(f"   Configuration: None")
            
        # æ˜¾ç¤ºå†…å­˜é…ç½®
        if self.memory_collection:
            lines.append(f"   Memory Collection: Configured")
        else:
            lines.append(f"   Memory Collection: None")
        
        if not self._pipeline:
            lines.append("\nğŸ“ Pipeline: Empty")
            self.logger.debug("\n".join(lines))
            return
        
        # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º Transformations
        transformation_groups = {}
        for i, transformation in enumerate(self._pipeline):
            trans_type = transformation.type.value
            if trans_type not in transformation_groups:
                transformation_groups[trans_type] = []
            transformation_groups[trans_type].append((i, transformation))
        
        lines.append(f"\nğŸ“ Pipeline Transformations:")
        lines.append(f"   Transformation Types: {len(transformation_groups)}")
        
        for trans_type, trans_list in transformation_groups.items():
            lines.append(f"\n   ğŸ“Š Type: {trans_type.upper()}")
            lines.append(f"      Count: {len(trans_list)}")
            
            for pipeline_index, transformation in trans_list:
                lines.append(f"\n      ğŸ”— [{pipeline_index}] Name: {transformation.basename}")
                lines.append(f"         Function: {transformation.function_class.__name__}")
                lines.append(f"         Parallelism: {transformation.parallelism}")
                lines.append(f"         Platform: {transformation.platform.value if hasattr(transformation, 'platform') else 'N/A'}")
                
                # æ˜¾ç¤ºæ˜¯å¦å·²å®ä¾‹åŒ–
                if transformation.is_instance:
                    lines.append(f"         Instance: âœ“ (pre-instantiated)")
                else:
                    lines.append(f"         Instance: âœ— (will instantiate at runtime)")
                
                # æ˜¾ç¤ºå‡½æ•°å‚æ•°
                if transformation.function_args:
                    lines.append(f"         Args: {transformation.function_args}")
                if transformation.kwargs:
                    lines.append(f"         Kwargs: {transformation.kwargs}")
                    
                # æ˜¾ç¤ºè¾“å…¥è¿æ¥
                if transformation.upstreams:
                    lines.append(f"         ğŸ“¥ Inputs ({len(transformation.upstreams)}):")
                    for input_tag, (upstream_trans, upstream_tag) in transformation.upstreams.items():
                        upstream_name = upstream_trans.basename
                        lines.append(f"           '{input_tag}' â† {upstream_name}['{upstream_tag}']")
                else:
                    lines.append(f"         ğŸ“¥ Inputs: None (source transformation)")
                
                # æ˜¾ç¤ºè¾“å‡ºè¿æ¥
                if transformation.downstreams:
                    total_downstream_connections = sum(len(downstream_set) for downstream_set in transformation.downstreams.values())
                    lines.append(f"         ğŸ“¤ Outputs ({total_downstream_connections} connections):")
                    for output_tag, downstream_set in transformation.downstreams.items():
                        if downstream_set:
                            downstream_names = [f"{down_trans.basename}['{down_tag}']" 
                                             for down_trans, down_tag in downstream_set]
                            lines.append(f"           '{output_tag}' â†’ {downstream_names}")
                        else:
                            lines.append(f"           '{output_tag}' â†’ None")
                else:
                    lines.append(f"         ğŸ“¤ Outputs: None (sink transformation)")
                
                # æ˜¾ç¤ºå£°æ˜çš„è¾“å…¥è¾“å‡ºæ ‡ç­¾
                declared_inputs = transformation.function_class.declare_inputs()
                declared_outputs = transformation.function_class.declare_outputs()
                lines.append(f"         ğŸ·ï¸  Declared Tags:")
                lines.append(f"           Inputs: {[tag for tag, _ in declared_inputs]}")
                lines.append(f"           Outputs: {[tag for tag, _ in declared_outputs]}")
        
        # æ˜¾ç¤ºæ‹“æ‰‘è¿æ¥ä¿¡æ¯
        lines.append(f"\nğŸ”„ Pipeline Topology:")
        for i, transformation in enumerate(self._pipeline):
            downstream_names = []
            for output_tag, downstream_set in transformation.downstreams.items():
                for down_trans, down_tag in downstream_set:
                    downstream_names.append(f"{down_trans.basename}[{down_tag}]")
            
            if downstream_names:
                lines.append(f"   [{i}] {transformation.basename} â†’ {downstream_names}")
            else:
                lines.append(f"   [{i}] {transformation.basename} â†’ [SINK]")
        
        # æ˜¾ç¤ºæ•°æ®æµéªŒè¯ä¿¡æ¯
        lines.append(f"\nâœ… Pipeline Validation:")
        source_count = sum(1 for trans in self._pipeline if trans.type == TransformationType.SOURCE)
        sink_count = sum(1 for trans in self._pipeline if trans.type == TransformationType.SINK)
        
        lines.append(f"   Sources: {source_count}")
        lines.append(f"   Sinks: {sink_count}")
        
        # æ£€æŸ¥è¿æ¥å®Œæ•´æ€§
        unconnected_inputs = []
        unconnected_outputs = []
        
        for transformation in self._pipeline:
            # æ£€æŸ¥æœªè¿æ¥çš„è¾“å…¥
            declared_inputs = transformation.function_class.declare_inputs()
            for input_tag, _ in declared_inputs:
                if input_tag not in transformation.upstreams:
                    unconnected_inputs.append(f"{transformation.basename}[{input_tag}]")
            
            # æ£€æŸ¥æœªè¿æ¥çš„è¾“å‡ºï¼ˆé™¤äº†sinkç±»å‹ï¼‰
            if transformation.type != TransformationType.SINK:
                declared_outputs = transformation.function_class.declare_outputs()
                for output_tag, _ in declared_outputs:
                    if output_tag not in transformation.downstreams or not transformation.downstreams[output_tag]:
                        unconnected_outputs.append(f"{transformation.basename}[{output_tag}]")
        
        if unconnected_inputs:
            lines.append(f"   âš ï¸  Unconnected Inputs: {unconnected_inputs}")
        else:
            lines.append(f"   âœ“ All inputs connected")
            
        if unconnected_outputs:
            lines.append(f"   âš ï¸  Unconnected Outputs: {unconnected_outputs}")
        else:
            lines.append(f"   âœ“ All outputs connected or are sinks")
        
        lines.append("=" * 80)
        
        # ä¸€æ¬¡æ€§è¾“å‡ºæ‰€æœ‰è°ƒè¯•ä¿¡æ¯
        self.logger.debug("\n".join(lines))

class LocalEnvironment(BaseEnvironment):
    """
    æœ¬åœ°æ‰§è¡Œç¯å¢ƒï¼ˆä¸ä½¿ç”¨ Rayï¼‰ï¼Œç”¨äºå¼€å‘è°ƒè¯•æˆ–å°è§„æ¨¡æµ‹è¯•ã€‚
    """

    def __init__(self, name: str = "local_environment", config: dict | None = None):
        super().__init__(name, config, platform=PlatformType.LOCAL)


class RemoteEnvironment(BaseEnvironment):
    """
    åˆ†å¸ƒå¼æ‰§è¡Œç¯å¢ƒï¼ˆRayï¼‰ï¼Œç”¨äºç”Ÿäº§æˆ–å¤§è§„æ¨¡éƒ¨ç½²ã€‚
    """

    def __init__(self, name: str = "remote_environment", config: dict | None = None):
        super().__init__(name, config, platform=PlatformType.REMOTE)


class DevEnvironment(BaseEnvironment):
    """
    æ··åˆæ‰§è¡Œç¯å¢ƒï¼Œå¯æ ¹æ®é…ç½®åŠ¨æ€é€‰æ‹©æœ¬åœ°æˆ– Rayã€‚
    config ä¸­å¯åŒ…å« 'use_ray': bool æ¥åˆ‡æ¢è¿è¡Œæ—¶ã€‚
    """

    def __init__(self, name: str = "dev_environment", config: dict | None = None):
        cfg = dict(config or {})
        super().__init__(name, cfg, platform=PlatformType.HYBRID)
