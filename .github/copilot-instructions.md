# SAGE Copilot Instructions

## Overview

**SAGE** is a Python 3.10+ framework for building AI/LLM data processing pipelines with declarative
dataflow. 11 functional packages + 1 meta-package, ~400MB dev install, uses C++ extensions (CMake).

## ğŸš¨ CRITICAL Architectural Constraints

### âŒ NEVER BYPASS CONTROL PLANE - ABSOLUTE RULE

**ALL LLM engine operations MUST go through Control Plane. Direct engine startup is FORBIDDEN.**

This is a **non-negotiable architectural constraint**. Violating this breaks resource management, scheduling, and monitoring.

#### âŒ FORBIDDEN Operations:

**1. Direct LLM service startup:**
```bash
sage llm serve -m Qwen/Qwen2.5-7B-Instruct        # âŒ FORBIDDEN
python -m vllm.entrypoints.openai.api_server      # âŒ FORBIDDEN
python -m sage.common.components.sage_embedding   # âŒ FORBIDDEN
```

**2. Direct vLLM/HF imports in user code:**
```python
from vllm import LLM  # âŒ FORBIDDEN - use UnifiedInferenceClient
engine = LLM(model="...")  # âŒ FORBIDDEN
```

**3. Bypassing Gateway:**
```python
# âŒ FORBIDDEN - direct endpoint access
requests.post("http://localhost:8001/v1/chat/completions", ...)  # allow-control-plane-bypass: example only
```

#### âœ… CORRECT Operations (Control Plane):

**1. Engine management through Control Plane:**
```bash
# âœ… CORRECT - managed by Control Plane
sage llm engine start Qwen/Qwen2.5-7B-Instruct --engine-kind llm
sage llm engine start BAAI/bge-m3 --engine-kind embedding --use-gpu
sage llm engine list
sage llm engine stop <engine-id>
```

**2. Unified client (auto-routes through Control Plane):**
```python
# âœ… CORRECT - uses Control Plane routing
from sage.llm import UnifiedInferenceClient
client = UnifiedInferenceClient.create()
response = client.chat([{"role": "user", "content": "Hello"}])
```

**3. Gateway API (Control Plane frontend):**
```python
# âœ… CORRECT - goes through Gateway â†’ Control Plane
import requests
resp = requests.post("http://localhost:8888/v1/chat/completions", ...)  # allow-control-plane-bypass: Gateway port
```

#### Why This Matters:

- **Resource Management**: Control Plane tracks GPU memory, prevents OOM
- **Load Balancing**: Distributes requests across multiple engines
- **Fault Tolerance**: Automatic failover and retry
- **Monitoring**: Centralized metrics and logging
- **Scheduling**: SLO-aware request routing

#### Enforcement:

- **Pre-commit hooks**: Block commits with `sage llm serve`, `sage llm run`, or direct vLLM calls
- **CI/CD**: All workflows use Control Plane APIs
- **Code review**: Reject PRs with direct engine startup
- **Commands removed**: `sage llm serve/run/stop/restart/status/logs` have been completely deleted

## CRITICAL Coding Principles

### âŒ NO FALLBACK LOGIC - PROJECT-WIDE RULE
**NEVER use try-except fallback patterns anywhere in the codebase.**

This is a **project-wide principle**, not just for version management. Fallbacks hide problems and make debugging harder.

#### âŒ BAD Examples (Do NOT do this):

**1. Version imports:**
```python
try:
    from ._version import __version__
except ImportError:
    __version__ = "unknown"  # âŒ NO
```

**2. Configuration loading:**
```python
try:
    config = load_config("config.yaml")
except FileNotFoundError:
    config = {}  # âŒ NO - hides missing config file
```

**3. Module imports:**
```python
try:
    import optional_module
except ImportError:
    optional_module = None  # âŒ NO - use explicit dependency checking instead
```

**4. Environment variables:**
```python
api_key = os.getenv("API_KEY") or "default_key"  # âŒ NO - hides missing env var
```

**5. File operations:**
```python
try:
    with open("file.txt") as f:
        data = f.read()
except FileNotFoundError:
    data = ""  # âŒ NO - hides missing file
```

#### âœ… GOOD Examples (Do this instead):

**1. Version imports (namespace packages):**
```python
# Direct file reading - will raise FileNotFoundError if missing
import os
_current_dir = os.path.dirname(os.path.abspath(__file__))
_version_file = os.path.join(_current_dir, "_version.py")
_version_globals = {}
with open(_version_file) as f:
    exec(f.read(), _version_globals)
__version__ = _version_globals["__version__"]  # Will raise KeyError if missing
```

**2. Configuration loading:**
```python
# Fail fast if config missing
config = load_config("config.yaml")  # Let FileNotFoundError propagate
# Or provide clear error message
if not os.path.exists("config.yaml"):
    raise FileNotFoundError(
        "config.yaml not found. Please create it from config.yaml.template"
    )
```

**3. Optional dependencies:**
```python
# Explicitly check and document the requirement
try:
    import vllm
except ImportError as e:
    raise ImportError(
        "vLLM is required for this feature. Install with: pip install vllm"
    ) from e
```

**4. Environment variables:**
```python
# Fail fast if required env var missing
api_key = os.environ["API_KEY"]  # Will raise KeyError if missing
# Or provide clear error message
if "API_KEY" not in os.environ:
    raise EnvironmentError(
        "API_KEY environment variable is required. Set it in .env file"
    )
```

**5. File operations:**
```python
# Let exceptions propagate - makes problems visible
with open("required_file.txt") as f:
    data = f.read()  # Will raise FileNotFoundError if missing
```

#### When Fallbacks ARE Acceptable (Rare Cases):

Only use fallbacks when:
1. **Feature detection** (check if optional feature is available):
   ```python
   HAS_CUDA = torch.cuda.is_available()  # âœ“ OK - explicit feature check
   ```

2. **Explicit optional behavior** (documented and intentional):
   ```python
   # âœ“ OK - clearly documented as optional
   use_gpu = config.get("use_gpu", False)  # Default to CPU if not specified
   ```

3. **Graceful degradation** (with clear logging):
   ```python
   # âœ“ OK - logs the issue and provides alternative
   try:
       fast_impl = import_fast_implementation()
   except ImportError:
       logger.warning("Fast implementation not available, using slow fallback")
       fast_impl = slow_implementation
   ```

#### Rationale:

- **Fail fast, fail loud**: Problems should be visible immediately during development
- **No hidden bugs**: Silent fallbacks hide configuration errors, missing files, broken imports
- **Better error messages**: Explicit checks can provide helpful error messages
- **Easier debugging**: Stack traces point to the real problem
- **Production safety**: "unknown" versions, empty configs, or missing dependencies are unacceptable

#### What to do instead:

1. **Let exceptions propagate** - don't catch them unless you have a specific reason
2. **Validate early** - check requirements at startup, not during runtime
3. **Provide clear error messages** - tell users exactly what's wrong and how to fix it
4. **Document dependencies** - make optional dependencies explicit in docs
5. **Use assertions** - for internal invariants that should never fail

### Version Management

Each package manages its own version independently via `_version.py`:
```python
"""Version information for <package-name>."""
__version__ = "0.2.0"
__author__ = "IntelliStream Team"
__email__ = "shuhao_zhang@hust.edu.cn"
```

**Architecture (L1-L6)** - CRITICAL: No upward dependencies

```
L6: sage-cli, sage-studio, sage-tools, sage-llm-gateway, sage-edge  # Interfaces & gateways
L5: sage-apps, sage-benchmark          # Apps & Benchmarks  
L4: sage-middleware                    # Operators (C++ extensions)
L3: sage-kernel, sage-libs             # Core & Algorithms
L2: sage-platform                      # Platform Services
L1: sage-common, sage-llm-core         # Foundation & LLM control plane/client
```

Notes:
- `sage-llm-gateway` is published to PyPI as `isage-llm-gateway` (OpenAI/Anthropic-compatible API Gateway).
- `sage-llm-core` is published to PyPI as `isage-llm-core` (Unified client + control plane).
- `sage-edge` is an opt-in aggregator shell that can mount the LLM gateway; behavior is unchanged unless it is explicitly started.
- Legacy `sage-gateway` has been superseded; do not add new code under that namespace.

All in `/packages/<name>/`. L6 imports L1-L5, L5 imports L1-L4, etc.

## How Copilot Should Learn SAGE (Readme-First)

When answering questions or making code changes in this repo, the assistant **must first rely on the project docs/READMEs instead of guessing**.

**Before doing any non-trivial work, Copilot should at least skim:**

- Root overview: `README.md` (features, quick start)  
- Dev workflow: `DEVELOPER.md`, `CONTRIBUTING.md`  
- Architecture: `docs-public/docs_src/dev-notes/package-architecture.md`  
- Cross-layer index: `docs-public/docs_src/dev-notes/cross-layer/README.md`

**When working on a specific layer/package, Copilot should additionally read:**

- The corresponding dev-notes README, e.g.  
  - `docs-public/docs_src/dev-notes/l1-common/README.md`  
  - `docs-public/docs_src/dev-notes/l2-platform/README.md`  
  - `docs-public/docs_src/dev-notes/l3-kernel/README.md` / `l3-libs/README.md`  
  - `docs-public/docs_src/dev-notes/l4-middleware/README.md`  
  - `docs-public/docs_src/dev-notes/l5-apps/README.md`, `l5-benchmark/README.md`  
  - `docs-public/docs_src/dev-notes/l6-cli/README.md`, `l6-studio/README.md`, `l6-gateway/README.md`

Only after consulting these READMEs should the assistant propose designs, refactors, or architectural explanations. If documentation and code appear inconsistent, Copilot should **call it out explicitly** in the answer and, when in doubt, ask the user which source of truth to follow.

## Inference Components Map (Reality-First)

SAGE is an inference pipeline system, not just an LLM server. When writing docs, abstracts, design notes, or code changes, prefer describing/using these existing modules (and their correct layer placement) instead of inventing new components.

Canonical namespaces (post-refactor):
- Gateway: `sage.llm.gateway.*` (package: `sage-llm-gateway`)
- Control plane + unified client: `sage.llm.*` (package: `sage-llm-core`)
- Optional edge aggregator: `sage.edge.*` (package: `sage-edge`)
- Avoid legacy `sage.gateway.*` imports; they have been superseded.

**Gateway (L6, OpenAI/Anthropic-compatible + control plane + sessions)**

- Entry point: `packages/sage-llm-gateway/src/sage/llm/gateway/server.py`
- Control plane management API: `packages/sage-llm-gateway/src/sage/llm/gateway/routes/engine_control_plane.py`
- Studio backend routes (merged into gateway): `packages/sage-llm-gateway/src/sage/llm/gateway/routes/studio.py`
- OpenAI adapter (runs persistent RAG pipeline, can trigger agentic operators):
  `packages/sage-llm-gateway/src/sage/llm/gateway/adapters/openai.py`
- Pipeline-as-a-service for RAG: `packages/sage-llm-gateway/src/sage/llm/gateway/rag_pipeline.py`
- Session + memory backends (short-term + NeuroMem VDB/KV/Graph):
  `packages/sage-llm-gateway/src/sage/llm/gateway/session/manager.py`
- Edge aggregator (optional shell mounting the gateway, keeps /v1/* intact by default):
  `packages/sage-edge/src/sage/edge/app.py`

**Control Plane + Unified Client (L1, sageLLM integration)**

- Unified LLM+Embedding client (must use factory):
  `packages/sage-llm-core/src/sage/llm/unified_client.py`
- Control plane implementation lives under:
  `packages/sage-llm-core/src/sage/llm/control_plane/`
  (tests: `.../control_plane/tests/`)

**Middleware inference building blocks (L4, including C++ extensions)**

- Vector DB core (C++20, self-developed, pluggable ANNS, multimodal fusion):
  `packages/sage-middleware/src/sage/middleware/components/sage_db/sageDB/README.md`
  - **SageDB VDB Backend**: Self-developed high-performance C++ vector database
  - **NOT FAISS-based**: Fully custom implementation
  - **ANNS Algorithms**: Migrated to `sage-libs/anns/` (faiss_HNSW, vsag_hnsw, diskann, etc.)
  - Python API: `sage.middleware.components.sage_db.python.sage_db.SageDB`
  - Supports: similarity search, metadata filtering, hybrid search, batch operations
  - NeuroMem integration: `sage_mem/neuromem/search_engine/vdb_index/sagedb_index.py`
  - Available backends in NeuroMem: FAISS (Python wrapper), SageDB (C++ self-developed)
  - Configuration: Set `backend_type="SageDB"` in VDB index config to use C++ backend
- Vector-native stream processing engine for incremental semantic state snapshots (C++):
  `packages/sage-middleware/src/sage/middleware/components/sage_flow/sageFlow/README.md`
- Memory system (NeuromMem: store/recall; VDB/KV/Graph; services wrapper):
  `packages/sage-middleware/src/sage/middleware/components/sage_mem/neuromem/README.md`
- Context compression for RAG (LongRefiner/REFORM/Provence adapters):
  `packages/sage-middleware/src/sage/middleware/components/sage_refiner/sageRefiner/README.md`
- Time-series DB + window ops/join + out-of-order handling (C++ + pybind11):
  `packages/sage-middleware/src/sage/middleware/components/sage_tsdb/sageTSDB/README.md`

**Benchmarks (L5)**

- Control plane scheduling benchmark (throughput/TTFT/TBT/p99/SLO):
  `packages/sage-benchmark/src/sage/benchmark/benchmark_control_plane/README.md`
- Agent benchmarks (tool selection / planning / timing):
  `packages/sage-benchmark/src/sage/benchmark/benchmark_agent/README.md`

**Kernel + Libs (L3)**

- Dataflow runtime, distributed execution, fault tolerance: `packages/sage-kernel/`
- Algorithms, RAG tools, agent framework/integrations: `packages/sage-libs/`
  - **ANN Interface**: `sage.libs.ann` - Unified ANN algorithm interface
    - Base classes: `AnnIndex`, `AnnIndexMeta` (in `sage.libs.ann.base`)
    - Factory: `create()`, `register()`, `registered()` (in `sage.libs.ann.factory`)
    - Implementations: `sage.libs.anns/` (faiss_HNSW, vsag_hnsw, diskann, candy_*, cufe, gti, puck, etc.)
    - Reusable by: benchmark_anns, SageDB, SageFlow

**Rule of thumb**: if you mention a capability (retrieval, memory, refinement, vector DB, streaming semantic state, scheduling), ensure it maps to a real module/path above.

## Installation

**Prerequisites**: Python 3.10+, Git, build-essential, cmake, pkg-config, libopenblas-dev,
liblapack-dev

**Commands** (10-25 min install):

```bash
./quickstart.sh --dev --yes        # Development (REQUIRED for dev)
./quickstart.sh --core --yes       # Minimal production
./quickstart.sh --standard --yes   # Standard with CLI
./quickstart.sh --full --yes       # Full with examples
```

Options: `--pip` (current env), `--conda` (create env), `--sync-submodules` / `--no-sync-submodules`

**Submodules** - CRITICAL: NEVER use `git submodule update --init`

```bash
./manage.sh                        # Bootstrap submodules + hooks
./tools/maintenance/sage-maintenance.sh submodule init    # Initialize
./tools/maintenance/sage-maintenance.sh submodule switch  # Fix detached HEAD
```

C++ extension submodules in `packages/sage-middleware/src/sage/middleware/components/`

**Environment**: Copy `.env.template` to `.env`, set `OPENAI_API_KEY`, `HF_TOKEN`

## Build, Test, Lint

**Build**: Happens during install. C++ extensions in `.sage/build/`, auto-built with `--dev`.

**Test** (ALWAYS from repo root):

```bash
sage-dev project test --coverage              # All tests
sage-dev project test --quick                 # Quick tests only
sage-dev examples test                        # Examples
pytest packages/sage-kernel/tests/unit/ -v   # Specific package
```

Config: `tools/pytest.ini`, cache: `.sage/cache/pytest/`, env: `SAGE_TEST_MODE=true`

**Lint & Format**:

```bash
sage-dev quality                              # Auto-fix
sage-dev quality --check-only                 # Check only
pre-commit run --all-files --config tools/pre-commit-config.yaml
./tools/install/check_tool_versions.sh        # Check version consistency
./tools/install/check_tool_versions.sh --fix  # Auto-fix version mismatch
```

Tools: Ruff (format+lint, line 100), Mypy (types, warning mode), Shellcheck Config:
`tools/pre-commit-config.yaml`, `tools/ruff.toml`

**Tool Version Consistency** - CRITICAL:
- `ruff` version is pinned in both `tools/pre-commit-config.yaml` (rev) and
  `packages/sage-tools/pyproject.toml` (==x.y.z) to ensure local and CI consistency.
- Run `./tools/install/check_tool_versions.sh` to verify versions match.

**Make shortcuts**: `make help`, `make test`, `make format`, `make clean`, `make docs`

## CI/CD (.github/workflows/)

**Main workflows**: build-test.yml (45m), examples-test.yml (30m), code-quality.yml (10m),
installation-test.yml, publish-pypi.yml, paper1-experiments.yml (GPU, manual)

**CI Installation Standards** - CRITICAL for new workflows:

| åœºæ™¯ | æ¨èå®‰è£…æ–¹å¼ | è¯´æ˜ |
|------|-------------|------|
| GitHub Actions (ubuntu-latest) | `./tools/install/ci_install_wrapper.sh --dev --yes` | æ ‡å‡† CIï¼Œå®‰è£…åˆ° `~/.local` |
| GitHub Actions + Conda | `unset CI GITHUB_ACTIONS && ./quickstart.sh --dev --yes --pip` | éœ€å–æ¶ˆ CI å˜é‡ï¼Œå®‰è£…åˆ° conda env |
| Self-hosted GPU runner (ä¸­å›½) | `unset CI GITHUB_ACTIONS && SAGE_FORCE_CHINA_MIRROR=true ./quickstart.sh --dev --yes --pip` | å¼ºåˆ¶ä½¿ç”¨ä¸­å›½é•œåƒ |

**ä¸ºä»€ä¹ˆéœ€è¦ `unset CI GITHUB_ACTIONS`**ï¼š
- `quickstart.sh` åœ¨æ£€æµ‹åˆ° CI ç¯å¢ƒæ—¶ä¼šæ·»åŠ  `--user` å‚æ•°ï¼Œå®‰è£…åˆ° `~/.local`
- å¦‚æœä½¿ç”¨ conda ç¯å¢ƒï¼Œéœ€è¦å–æ¶ˆè¿™äº›å˜é‡è®©åŒ…å®‰è£…åˆ°å½“å‰æ¿€æ´»çš„ç¯å¢ƒ

**`SAGE_FORCE_CHINA_MIRROR=true`**ï¼š
- å¼ºåˆ¶ä½¿ç”¨ä¸­å›½é•œåƒï¼ˆæ¸…å PyPI + hf-mirror.comï¼‰
- é€‚ç”¨äºä½äºä¸­å›½çš„ self-hosted runner
- ä¼šè¦†ç›– CI ç¯å¢ƒçš„é»˜è®¤å®˜æ–¹æºè®¾ç½®

**CI uses**: Ubuntu latest, Python 3.11, GitHub Secrets (OPENAI_API_KEY, HF_TOKEN), pip cache

**Replicate CI locally**:

```bash
./quickstart.sh --dev --yes
sage-dev project test --coverage --jobs 4 --timeout 300
pre-commit run --all-files --config tools/pre-commit-config.yaml
```

**CI debug**: Check job logs â†’ Look for submodule/C++ build issues â†’ Verify API keys â†’ Run locally

## Key Locations

```
.github/workflows/      # CI/CD
docs-public/docs_src/dev-notes/ # Dev docs (by layer: l1-l6, cross-layer)
examples/               # apps/, tutorials/ (by layer)
packages/               # 11 packages + meta
  sage-*/src/sage/      # Source
  sage-*/tests/         # Tests (unit/, integration/)
tools/
  dev.sh                # Helper (â†’ sage-dev)
  maintenance/          # Submodule mgmt
  pytest.ini            # Test config
  pre-commit-config.yaml # Hooks
  ruff.toml             # Linter
.env.template           # API keys template
.pre-commit-config.yaml # â†’ tools/pre-commit-config.yaml
.sage/                  # Build artifacts, cache, logs (gitignored, project-level)
manage.sh               # Submodule wrapper
quickstart.sh           # Installer
Makefile                # Shortcuts
```

## User Paths - XDG Standard

**CRITICAL**: User configuration and data follow [XDG Base Directory Specification](https://specifications.freedesktop.org/basedir-spec/).

```python
from sage.common.config.user_paths import get_user_paths

paths = get_user_paths()
log_file = paths.logs_dir / "app.log"  # ~/.local/state/sage/logs/app.log
model_dir = paths.models_dir           # ~/.local/share/sage/models/
```

**Project Configuration**: Edit `config/config.yaml` and `config/cluster.yaml` directly in project root.

**Directory Structure**:
| Path | Purpose |
|------|---------|
| `config/config.yaml` | Main configuration (LLM, gateway, studio) |
| `config/cluster.yaml` | Cluster configuration (nodes, SSH, Ray) |
| `~/.local/share/sage/` | Persistent data (models, sessions, vector_db) |
| `~/.local/state/sage/` | Runtime state (logs) |
| `~/.cache/sage/` | Cached data (can be deleted) |

**Project-level** `.sage/` (gitignored): Build artifacts, pytest cache, temp files.

**DO NOT** use `~/.sage/` for new code. Use `get_user_paths()` for user data.

## Common Issues

**Install hangs**: Check network, try `--resume` for checkpoint recovery (10-25min normal)
**C++ build fails**: Install deps: `build-essential cmake pkg-config libopenblas-dev liblapack-dev`
**Detached HEAD**: Use `./tools/maintenance/sage-maintenance.sh submodule switch`
**Tests fail CI not local**: Run `sage-dev project test --coverage` from repo root
**Import errors**: Must use `--dev` install, run from repo root
**Pre-commit fails**: Run `sage-dev quality` to auto-fix
**Old artifacts**: `make clean` or `rm -rf .sage/build/ build/ dist/ *.egg-info/`
**Bash exclamation mark**: NEVER use `!` in terminal commands (causes `bash: !': event not found`).
  Use period `.` instead: `print("Done.")` not `print("Done!")`

## Port Configuration - CRITICAL

**ç»Ÿä¸€ç«¯å£é…ç½®**: æ‰€æœ‰ç«¯å£å·å¿…é¡»ä½¿ç”¨ `sage.common.config.ports.SagePorts`ï¼Œç¦æ­¢ç¡¬ç¼–ç ã€‚

```python
from sage.common.config.ports import SagePorts

# âœ… æ­£ç¡®ç”¨æ³•
port = SagePorts.LLM_DEFAULT           # 8001
gateway_port = SagePorts.GATEWAY_DEFAULT  # 8889

# âœ… WSL2 ç¯å¢ƒæ¨èç”¨æ³•
port = SagePorts.get_recommended_llm_port()  # è‡ªåŠ¨æ£€æµ‹ WSL2 å¹¶é€‰æ‹©åˆé€‚ç«¯å£

# âŒ é”™è¯¯ç”¨æ³• - ç¦æ­¢ç¡¬ç¼–ç 
port = 8001  # ä¸è¦è¿™æ ·å†™
```

**ç«¯å£åˆ†é…è¡¨**:
| å¸¸é‡ | ç«¯å£ | ç”¨é€” |
|------|------|------|
| `GATEWAY_DEFAULT` | 8889 | sage-llm-gateway (OpenAI å…¼å®¹ API Gateway) |
| `EDGE_DEFAULT` | 8899 | sage-edge èšåˆå™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤æŒ‚è½½ gateway ä¿æŒ /v1/*ï¼‰ |
| `LLM_DEFAULT` | 8001 | vLLM æ¨ç†æœåŠ¡ |
| `LLM_WSL_FALLBACK` | 8901 | WSL2 å¤‡ç”¨ LLM ç«¯å£ |
| `STUDIO_BACKEND` | 8889| sage-studio åç«¯ API |
| `STUDIO_FRONTEND` | 5173 | sage-studio å‰ç«¯ (Vite) |
| `EMBEDDING_DEFAULT` | 8090 | Embedding æœåŠ¡ |
| `BENCHMARK_LLM` | 8901 | Benchmark ä¸“ç”¨ LLM ç«¯å£ |

**æ¶æ„**: `User â†’ Edge (8899, å¯é€‰) â†’ Gateway (8889) â†’ LLM (8001)`ï¼ˆEdge é»˜è®¤ç›´é€š Gatewayï¼Œç›´æ¥è®¿é—® Gateway ä¹Ÿæœ‰æ•ˆï¼‰

**WSL2 å·²çŸ¥é—®é¢˜**:
- ç«¯å£ 8001 åœ¨ WSL2 ä¸Šå¯èƒ½å‡ºç°"ç«¯å£ç›‘å¬ä½†è¿æ¥è¢«æ‹’ç»"çš„é—®é¢˜
- ä½¿ç”¨ `SagePorts.get_recommended_llm_port()` è‡ªåŠ¨é€‰æ‹©åˆé€‚ç«¯å£
- æˆ–ç›´æ¥ä½¿ç”¨ `SagePorts.BENCHMARK_LLM` (8901) ä½œä¸ºå¤‡ç”¨

**é…ç½®æ–‡ä»¶ä½ç½®**: `packages/sage-common/src/sage/common/config/ports.py`

## API Client Usage - CRITICAL

**UnifiedInferenceClient must be created via the factory** (direct instantiation is intentionally blocked).

```python
from sage.llm import UnifiedInferenceClient

client = UnifiedInferenceClient.create()
```

If you see code attempting `UnifiedInferenceClient(...)`, treat it as a bug and refactor to `create()`.

## Features

**CPU Node Support**: SAGE fully supports CPU-only compute nodes via JobManager + NodeSelector.
Tasks can specify `cpu_required`, `memory_required`, `gpu_required=0` for CPU-only execution. See
`examples/tutorials/L3-kernel/cpu_node_demo.py` and `docs/dev-notes/l3-kernel/cpu-node-setup.md`.

## Development Workflow

**Setup**: `./quickstart.sh --dev --yes` â†’ `./manage.sh` (if C++ needed)
**During**: Run `sage-dev project test`, `sage-dev quality` frequently
**Before commit**: `sage-dev quality --check-only`, `sage-dev project test --coverage`
**Commits**: `<type>(<scope>): <summary>` (types: feat, fix, refactor, docs, test, ci, etc.)
**PR**: Local CI checks first, update CHANGELOG.md, reference issues

**Critical files** (review before modifying): quickstart.sh, manage.sh, .github/workflows/,
tools/pytest.ini, tools/pre-commit-config.yaml

## Resources

- Architecture: `docs-public/docs_src/dev-notes/package-architecture.md`
- Guides: `CONTRIBUTING.md` (CN), `DEVELOPER.md` (EN)
- Dev notes: `docs/dev-notes/` (l1-l6, cross-layer/ci-cd/)

## LLM & Embedding Services - sageLLM æ¶æ„

**è®¾è®¡åŸåˆ™**: ç»Ÿä¸€è°ƒåº¦ï¼Œèµ„æºå…±äº«ã€‚æ‰€æœ‰ LLM å’Œ Embedding è¯·æ±‚é€šè¿‡ **sageLLM Control Plane** ç»Ÿä¸€ç®¡ç†ã€‚

### æ¶æ„æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           åº”ç”¨å±‚ (Application Layer)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      UnifiedInferenceClient                              â”‚
â”‚                 chat() | generate() | embed()                            â”‚
â”‚                    Control Plane Mode (å”¯ä¸€æ¨¡å¼)                         â”‚
â”‚              ï¼ˆæ‰€æœ‰è¯·æ±‚é€šè¿‡è°ƒåº¦å™¨ç»Ÿä¸€è·¯ç”±ï¼‰                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 sage.llm.gateway (L6 Gateway)                              â”‚
â”‚              (OpenAI-Compatible REST API + Control Plane)               â”‚
â”‚   /v1/chat/completions | /v1/embeddings | /v1/management/* | /sessions â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    sageLLM Control Plane (æ ¸å¿ƒ)                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  RequestClassifier (LLM_CHAT / LLM_GENERATE / EMBEDDING)        â”‚   â”‚
â”‚   â”‚  HybridSchedulingPolicy (è¯·æ±‚åˆ†ç»„ã€ä¼˜å…ˆçº§ã€æ‰¹å¤„ç†èšåˆ)            â”‚   â”‚
â”‚   â”‚  ExecutionCoordinator (LLM) | EmbeddingExecutor (Embedding)     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        ç»Ÿä¸€èµ„æºæ±  (GPU Pool)                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚  vLLM Instance  â”‚  â”‚  vLLM Instance  â”‚  â”‚  Embedding Srv  â”‚        â”‚
â”‚   â”‚  (LLM Only)     â”‚  â”‚  (LLM+Embed)    â”‚  â”‚  (Embed Only)   â”‚        â”‚
â”‚   â”‚  Type: GENERAL  â”‚  â”‚  Type: MIXED    â”‚  â”‚  Type: EMBEDDINGâ”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*å¯é€‰ Edge å±‚*: `sage-edge` (8899) å¯ä»¥å°† `sage.llm.gateway` æŒ‚è½½åœ¨ `/`ï¼ˆä¿æŒ `/v1/*` å…¼å®¹ï¼‰ï¼Œæˆ–åœ¨è‡ªå®šä¹‰å‰ç¼€ä¸‹æä¾›å¤šåŸŸèšåˆã€‚æœªå¯åŠ¨ edge æ—¶ï¼Œç›´æ¥è®¿é—® Gateway å³å¯ã€‚

### æ¨èç”¨æ³•ï¼šControl Plane æ¨¡å¼

```python
from sage.llm import UnifiedInferenceClient

# é»˜è®¤ï¼ˆæ¨èï¼‰: è‡ªåŠ¨æ£€æµ‹æœ¬åœ°/è¿œç«¯ç«¯ç‚¹ï¼Œä¼˜å…ˆæœ¬åœ°
client = UnifiedInferenceClient.create()

# å¤–éƒ¨ Control Plane: æŒ‡å‘å·²è¿è¡Œçš„ Control Plane/Gateway
client = UnifiedInferenceClient.create(
  control_plane_url="http://127.0.0.1:8888/v1",
  default_llm_model="Qwen/Qwen2.5-7B-Instruct",
  default_embedding_model="BAAI/bge-m3",
)

# å†…åµŒ Control Plane: åœ¨å½“å‰è¿›ç¨‹å¯åŠ¨è°ƒåº¦å™¨ï¼ˆå®éªŒæ€§ï¼Œé€‚åˆç¦»çº¿æ‰¹å¤„ç†ï¼‰
# embedded mode deprecated; use control_plane_url instead

### å¯åŠ¨æœåŠ¡æ ˆ

```bash
# æ¨èï¼šå¯åŠ¨ Gatewayï¼ˆåŒ…å« Control Planeï¼‰
sage gateway start                                 # å¯åŠ¨ Gatewayï¼ˆç«¯å£ 8888ï¼‰
sage gateway status                                # æŸ¥çœ‹ Gateway çŠ¶æ€
sage gateway stop                                  # åœæ­¢ Gateway
sage gateway logs --follow                         # æŸ¥çœ‹æ—¥å¿—

# å¼•æ“ç®¡ç†ï¼ˆé€šè¿‡ Gateway Control Planeï¼‰
sage llm engine start Qwen/Qwen2.5-7B-Instruct --engine-kind llm    # å¯åŠ¨ LLM å¼•æ“
sage llm engine start BAAI/bge-m3 --engine-kind embedding           # é»˜è®¤ CPU
sage llm engine start BAAI/bge-m3 --engine-kind embedding --use-gpu # ä½¿ç”¨ GPU
sage llm engine list                                                 # æŸ¥çœ‹å¼•æ“åˆ—è¡¨
sage llm engine stop <engine-id>                                     # åœæ­¢å¼•æ“

# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
ps aux | grep -E "vllm|embedding_server"
```

### Embedding å¼•æ“ GPU æ”¯æŒ

é»˜è®¤æƒ…å†µä¸‹ï¼ŒEmbedding å¼•æ“è¿è¡Œåœ¨ CPU ä¸Šã€‚å¯¹äºå¤§å‹ Embedding æ¨¡å‹ï¼ˆå¦‚ BGE-M3ï¼‰ï¼Œå¯ä»¥æ˜¾å¼å¯ç”¨ GPUï¼š

```python
# CLI æ–¹å¼
# sage llm engine start BAAI/bge-m3 --engine-kind embedding --use-gpu

# é¢„è®¾æ–‡ä»¶ (preset.yaml)
engines:
  - name: embed-gpu
    kind: embedding
    model: BAAI/bge-m3
    use_gpu: true  # æ˜¾å¼ä½¿ç”¨ GPU
```

**`use_gpu` å‚æ•°è¡Œä¸º**ï¼š
- `use_gpu=None` (é»˜è®¤): LLM ä½¿ç”¨ GPUï¼ŒEmbedding ä¸ä½¿ç”¨
- `use_gpu=True`: å¼ºåˆ¶ä½¿ç”¨ GPU
- `use_gpu=False`: å¼ºåˆ¶ä¸ä½¿ç”¨ GPUï¼ˆå³ä½¿æ˜¯ LLMï¼‰

### Control Plane æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | ä½ç½® | åŠŸèƒ½ |
|------|------|------|
| `ControlPlaneManager` | `sage.llm.control_plane.manager` | æ ¸å¿ƒè°ƒåº¦ç®¡ç†å™¨ |
| `RequestClassifier` | `sage.llm.control_plane.request_classifier` | è¯·æ±‚ç±»å‹åˆ†ç±» |
| `HybridSchedulingPolicy` | `sage.llm.control_plane.strategies.hybrid_policy` | æ··åˆè°ƒåº¦ç­–ç•¥ |
| `EmbeddingExecutor` | `sage.llm.control_plane.executors.embedding_executor` | Embedding æ‰¹å¤„ç† |
| `ControlPlaneService` | `sage.llm.control_plane_service` | Control Plane SAGE å°è£… |

### å…³é”®æ–‡ä»¶ä½ç½®

```
packages/sage-llm-core/src/sage/llm/
  unified_client.py         # UnifiedInferenceClient (factory-only construction)
  control_plane_service.py  # Control Plane facade
  control_plane/            # Control Plane core implementation
    manager.py              # è°ƒåº¦ç®¡ç†å™¨
    request_classifier.py   # è¯·æ±‚åˆ†ç±»å™¨
    strategies/hybrid_policy.py  # LLM + Embedding æ··åˆè°ƒåº¦
    executors/embedding_executor.py # Embedding æ‰¹å¤„ç†æ‰§è¡Œ

packages/sage-llm-gateway/src/sage/llm/gateway/
  server.py                 # FastAPI åº”ç”¨å…¥å£ (OpenAI/Anthropic-compatible)
  routes/
    engine_control_plane.py # Control Plane ç®¡ç† API
    llm.py                  # LLM ä»£ç†
    embedding.py            # Embedding ä»£ç†
    studio.py               # Studio backend routes (merged)
    sessions.py             # ä¼šè¯ç®¡ç†
  adapters/openai.py        # OpenAI adapter
  rag_pipeline.py           # Pipeline-as-a-service
  session/manager.py        # Session + memory backends

packages/sage-edge/src/sage/edge/
  app.py                    # FastAPI aggregator shell (mounts gateway, keeps /v1/* by default)
  server.py                 # uvicorn entrypoint / CLI target

packages/sage-common/src/sage/common/components/
  sage_embedding/
    embedding_server.py     # OpenAI å…¼å®¹ Embedding æœåŠ¡å™¨
    factory.py              # EmbeddingFactory (æœ¬åœ°æ¨¡å‹)
```

### å®¢æˆ·ç«¯æ¨¡å¼å¯¹æ¯”ï¼ˆç»Ÿä¸€ Control Planeï¼‰

> Simple æ¨¡å¼å·²ç§»é™¤ï¼›æ‰€æœ‰è¯·æ±‚éƒ½ç»ç”± Control Planeã€‚

| æ¨¡å¼ | åˆ›å»ºæ–¹å¼ | è°ƒåº¦ | é€‚ç”¨åœºæ™¯ |
|------|----------|------|---------|
| è‡ªåŠ¨æ£€æµ‹ | `UnifiedInferenceClient.create()` | è‡ªåŠ¨æ¢æµ‹æœ¬åœ°/è¿œç«¯ç«¯ç‚¹ï¼Œç»Ÿä¸€è°ƒåº¦ | é»˜è®¤æ¨èï¼ˆæœ¬åœ°å¼€å‘ã€å•æœºå®éªŒï¼‰ |
| å¤–éƒ¨ Control Plane | `UnifiedInferenceClient.create(control_plane_url=...)` | é€šè¿‡å·²è¿è¡Œçš„ Control Plane/Gateway è·¯ç”± | ç”Ÿäº§éƒ¨ç½²ã€ç½‘å…³ç»Ÿä¸€å…¥å£ |
| å†…åµŒ Control Plane (deprecated) | ä½¿ç”¨ control_plane_url æˆ–æœ¬åœ° Gateway | åœ¨è¿›ç¨‹å†…å¯åŠ¨è°ƒåº¦å™¨ | ç¦»çº¿æ‰¹å¤„ç†/æ— å¤–éƒ¨æœåŠ¡æ—¶ |

### å†…åµŒæ¨¡å¼ (VLLMService) - æ‰¹å¤„ç†ä¸“ç”¨

```python
from sage.llm import VLLMService

# è¿›ç¨‹å†…åŠ è½½æ¨¡å‹ï¼Œé€‚åˆæ‰¹å¤„ç†ä»»åŠ¡
service = VLLMService({
    "model_id": "Qwen/Qwen2.5-0.5B-Instruct",
    "auto_download": True,
})
service.setup()  # åŠ è½½æ¨¡å‹åˆ° GPU
results = service.generate("Hello, world!")
service.teardown()
```

### ç¯å¢ƒå˜é‡ (.env)

```bash
# === æœ¬åœ°æœåŠ¡ï¼ˆæ¨èï¼Œé»˜è®¤ï¼‰===
# æ— éœ€é…ç½®ï¼Œä½¿ç”¨ SagePorts é»˜è®¤ç«¯å£
# UnifiedInferenceClient ä¼šè‡ªåŠ¨æ¢æµ‹ localhost:8001, localhost:8901

# === æ˜¾å¼è¿œç«¯è¦†ç›–ï¼ˆä»…å½“éœ€è¦å¼ºåˆ¶ä½¿ç”¨äº‘ç«¯APIæ—¶è®¾ç½®ï¼‰===
# è­¦å‘Šï¼šä»…ç”¨äºæ˜¾å¼è¿œç«¯è¦†ç›–ï¼Œä¸æ˜¯é»˜è®¤è¡Œä¸º
# æœ¬åœ°å¼€å‘åº”å§‹ç»ˆä½¿ç”¨æœ¬åœ°ç«¯ç‚¹ï¼Œä¸è¦ä¾èµ–äº‘ç«¯ fallback
SAGE_CHAT_API_KEY=sk-xxx              # äº‘ç«¯ API Key (DashScope/OpenAI compatible)
SAGE_CHAT_MODEL=qwen-turbo-2025-02-11
SAGE_CHAT_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# === HuggingFace ===
HF_TOKEN=hf_xxx
# HF_ENDPOINT æ— éœ€æ‰‹åŠ¨è®¾ç½®ï¼ŒSAGE ä¼šè‡ªåŠ¨æ£€æµ‹ç½‘ç»œå¹¶é…ç½®é•œåƒ
```

> **CRITICAL**: DashScope/äº‘ç«¯å˜é‡**ä»…ç”¨äºæ˜¾å¼è¿œç«¯è¦†ç›–**ï¼Œä¸æ˜¯é»˜è®¤è¡Œä¸ºã€‚
> - **æœ¬åœ°ä¼˜å…ˆ**ï¼šé»˜è®¤æ¢æµ‹ `localhost:8001` å’Œ `localhost:8901`
> - **æ— éšå¼ fallback**ï¼šå¦‚æœæœ¬åœ°ç«¯ç‚¹ä¸å¯è¾¾ï¼Œä¼š**å¿«é€Ÿå¤±è´¥**ï¼Œä¸ä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°äº‘ç«¯
> - **æ˜¾å¼è¦†ç›–**ï¼šä»…å½“è®¾ç½®äº† `SAGE_CHAT_BASE_URL` æ—¶æ‰ä½¿ç”¨è¿œç«¯
> - **CI ç¯å¢ƒ**ï¼šGitHub Actions åœ¨æ— æœ¬åœ°æœåŠ¡æ—¶ä½¿ç”¨ DashScope fallbackï¼ˆCI onlyï¼‰

### ç½‘ç»œæ£€æµ‹å’Œ HuggingFace é•œåƒè‡ªåŠ¨é…ç½®

SAGE ä¼šåœ¨è¿è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹ç½‘ç»œåŒºåŸŸï¼Œå¦‚æœæ£€æµ‹åˆ°ä¸­å›½å¤§é™†ç½‘ç»œï¼Œä¼šè‡ªåŠ¨è®¾ç½® `HF_ENDPOINT=https://hf-mirror.com`ã€‚

```python
from sage.common.config import (
    detect_china_mainland,      # æ£€æµ‹æ˜¯å¦åœ¨ä¸­å›½å¤§é™†
    get_hf_endpoint,            # è·å–æ¨èçš„ HF endpoint
    ensure_hf_mirror_configured,  # è‡ªåŠ¨é…ç½® HF é•œåƒï¼ˆæ¨èåœ¨ CLI å‘½ä»¤å…¥å£è°ƒç”¨ï¼‰
)

# æ£€æµ‹ç½‘ç»œåŒºåŸŸ
is_china = detect_china_mainland()  # True/False

# è‡ªåŠ¨é…ç½®ï¼ˆå¦‚æœåœ¨ä¸­å›½å¤§é™†ï¼Œè®¾ç½® HF_ENDPOINT ç¯å¢ƒå˜é‡ï¼‰
ensure_hf_mirror_configured()  # åªä¼šåœ¨é¦–æ¬¡è°ƒç”¨æ—¶æ£€æµ‹ï¼Œç»“æœä¼šç¼“å­˜
```

**è‡ªåŠ¨é…ç½®çš„å‘½ä»¤**ï¼š
- `sage llm engine start` - å¯åŠ¨ LLM/Embedding å¼•æ“
- `sage llm model download` - ä¸‹è½½æ¨¡å‹
- `sage llm fine-tune` - å¾®è°ƒæ¨¡å‹
- Embedding ç›¸å…³æœåŠ¡

### EmbeddingFactory (æœ¬åœ°æ¨¡å‹ï¼Œæ— éœ€æœåŠ¡)

ç”¨äºä¸æƒ³å¯åŠ¨ Embedding æœåŠ¡çš„åœºæ™¯ï¼š

```python
from sage.common.components.sage_embedding import (
    EmbeddingFactory, EmbeddingClientAdapter
)

# æœ¬åœ°åŠ è½½ HuggingFace æ¨¡å‹
raw_embedder = EmbeddingFactory.create("hf", model="BAAI/bge-small-zh-v1.5")
client = EmbeddingClientAdapter(raw_embedder)  # é€‚é…ä¸ºæ‰¹é‡æ¥å£
vectors = client.embed(["æ–‡æœ¬1", "æ–‡æœ¬2"])
```

**æ¥å£å¯¹æ¯”**:
| æ¥å£ | ç­¾å | æ¥æº |
|------|------|------|
| å•æ–‡æœ¬ (BaseEmbedding) | `embed(text: str) -> list[float]` | `EmbeddingFactory.create()` |
| æ‰¹é‡ (EmbeddingProtocol) | `embed(texts: list[str], model=None) -> list[list[float]]` | `EmbeddingClientAdapter` |

**é”™è¯¯ç¤ºä¾‹** (ä¼šå¯¼è‡´è¿è¡Œæ—¶é”™è¯¯):
```python
# é”™è¯¯: EmbeddingFactory è¿”å›çš„æ˜¯å•æ–‡æœ¬æ¥å£
embedder = EmbeddingFactory.create("hf", model="...")
embedder.embed(texts=["a", "b"])  # TypeError: embed() got unexpected keyword argument 'texts'
```

## sage-benchmark ç»„ä»¶

Agent èƒ½åŠ›å’Œ Control Plane è¯„æµ‹æ¡†æ¶ï¼Œä½äº `packages/sage-benchmark/`ï¼š

### benchmark_agent (Agent èƒ½åŠ›è¯„æµ‹)

è¯„ä¼° Agent ä¸‰ä¸ªæ ¸å¿ƒèƒ½åŠ›ï¼šå·¥å…·é€‰æ‹©ã€ä»»åŠ¡è§„åˆ’ã€æ—¶æœºåˆ¤æ–­ã€‚

**æ ¸å¿ƒæ¨¡å—**:
```
src/sage/benchmark/benchmark_agent/
  adapter_registry.py      # ç­–ç•¥æ³¨å†Œè¡¨ (selector.*, planner.*, timing.*)
  experiments/
    base_experiment.py     # å®éªŒåŸºç±» + æ•°æ®æ¨¡å‹
    tool_selection_exp.py  # å·¥å…·é€‰æ‹©è¯„æµ‹
    planning_exp.py        # ä»»åŠ¡è§„åˆ’è¯„æµ‹
    timing_detection_exp.py # æ—¶æœºå†³ç­–è¯„æµ‹
  evaluation/
    metrics.py             # è¯„æµ‹æŒ‡æ ‡ (accuracy, precision, recall, etc.)
  scripts/                 # è¯„æµ‹è„šæœ¬
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from sage.benchmark.benchmark_agent import get_adapter_registry

registry = get_adapter_registry()

# å·¥å…·é€‰æ‹©ç­–ç•¥
selector = registry.get("selector.keyword")  # keyword, embedding, hybrid, gorilla, dfsdt

# ä»»åŠ¡è§„åˆ’ç­–ç•¥
planner = registry.get("planner.react")  # simple, hierarchical, llm_based, react, tot

# æ—¶æœºå†³ç­–ç­–ç•¥
decider = registry.get("timing.rule_based")  # rule_based, llm_based, hybrid
```

### benchmark_control_plane (è°ƒåº¦ç­–ç•¥è¯„æµ‹)

è¯„ä¼° sageLLM Control Plane çš„è°ƒåº¦ç­–ç•¥æ€§èƒ½ï¼ˆååé‡ã€å»¶è¿Ÿã€SLO åˆè§„ç‡ï¼‰ã€‚

**CLI**:
```bash
# LLM è°ƒåº¦è¯„æµ‹
sage-cp-bench run --mode llm --policy fifo --requests 100

# Hybrid (LLM + Embedding) è¯„æµ‹
sage-cp-bench run --mode hybrid --policy hybrid_slo --llm-ratio 0.7

# ç­–ç•¥å¯¹æ¯”
sage-cp-bench compare --mode llm --policies fifo,priority,slo_aware
```

**è¯¦ç»†æ–‡æ¡£**: `packages/sage-benchmark/src/sage/benchmark/benchmark_control_plane/README.md`

## SageDB Vector Database Backend

### Overview

SageDB is a **self-developed high-performance C++ vector database**, fully custom implementation (NOT based on FAISS), integrated into SAGE's NeuroMem VDB system.

**Features**:
- âœ… Self-developed C++ core (independent implementation)
- âœ… High-performance similarity search (C++ optimized)
- âœ… Metadata filtering (`filtered_search`, `search_by_metadata`)
- âœ… Hybrid search (vector + text)
- âœ… Batch operations with numpy optimization
- âœ… Persistent storage (save/load)
- âœ… Multiple index types (AUTO, FLAT, IVF, HNSW)
- âœ… Distance metrics (L2, INNER_PRODUCT, COSINE)
- âœ… **ANNS Algorithms**: Available in `sage-libs/anns/` (faiss_HNSW, vsag_hnsw, diskann, candy_*, cufe, gti, puck, etc.)

### Location

**Core Implementation**:
- C++ Backend: `packages/sage-middleware/src/sage/middleware/components/sage_db/sageDB/`
- Python API: `packages/sage-middleware/src/sage/middleware/components/sage_db/python/sage_db.py`
- NeuroMem Adapter: `packages/sage-middleware/src/sage/middleware/components/sage_mem/neuromem/search_engine/vdb_index/sagedb_index.py`

### Usage in NeuroMem VDB Collections

**Creating a VDB collection with SageDB backend**:

```python
from sage.middleware.components.sage_mem.neuromem.memory_manager import MemoryManager

manager = MemoryManager()

# Create collection
collection = manager.create_collection({
    "name": "my_collection",
    "backend_type": "VDB"
})

# Create SageDB index
collection.create_index({
    "name": "my_index",
    "dim": 1024,
    "backend_type": "SageDB",  # Use SageDB instead of FAISS
    "description": "High-performance SageDB index"
})

# Insert vectors
collection.insert("my_index", text="example text", vector=embedding_vector)

# Search
results = collection.search("my_index", query_vector, top_k=10)
```

**Gateway Session Storage Configuration**:

```python
# In packages/sage-llm-gateway/src/sage/llm/gateway/session/manager.py

# Default: FAISS backend
index_config = {
    "backend_type": "FAISS",  # Python FAISS
    ...
}

# Optimized: SageDB backend (C++ performance)
index_config = {
    "backend_type": "SageDB",  # C++ optimized
    ...
}
```

**Current Status** (2025-12-28):
- âœ… SageDB backend registered in VDB index factory
- âœ… SageDBIndex adapter implements all BaseVDBIndex methods
- âœ… Tests pass: insert, batch_insert, search, delete, update
- âš ï¸ Gateway default remains FAISS (change to "SageDB" to use C++ backend)

**Performance Characteristics** (5000 vectors, dim=128):
- âœ… **Insert**: SageDB 10x faster (single), 1.14x faster (batch) - C++ optimized write path
- âš ï¸ **Search**: FAISS 2.8-3x faster across all k values (Python wrapper overhead in current implementation)
- â¡ï¸ **Memory**: Nearly identical (~945 MB)
- âœ… **ANNS Algorithms**: Now available in `sage-libs/anns/` for modularity

**When to use SageDB**:
- Write-heavy workloads (frequent insertions/updates)
- Session storage with many new messages
- Real-time chat applications
- When insert latency is critical
- Custom C++ extensions and integrations

**When to use FAISS**:
- Read-heavy workloads (frequent similarity searches)
- Large-scale retrieval systems
- When search latency is critical
- Production RAG pipelines with high QPS

### Direct SageDB API (without NeuroMem)

**Important**: SageDB is a self-developed C++ vector database, not based on FAISS.

```python
from sage.middleware.components.sage_db.python.sage_db import SageDB, IndexType, DistanceMetric

# Create database (C++ core)
db = SageDB(dimension=128, index_type=IndexType.AUTO, metric=DistanceMetric.L2)

# Add vectors with metadata
db.add([0.1, 0.2, ...], metadata={"id": "doc_1", "category": "tech"})
db.add_batch(vectors, metadata=[{"id": f"doc_{i}"} for i in range(len(vectors))])

# Build index
db.build_index()

# Search
results = db.search(query_vector, k=10)
for result in results:
    print(f"ID: {result.metadata['id']}, Score: {result.score}")

# Filtered search
results = db.filtered_search(
    query_vector,
    params=SearchParams(k=10),
    filter_fn=lambda meta: meta.get("category") == "tech"
)

# Save/Load
db.save("/path/to/index")
db.load("/path/to/index")
```

### API Reference

**SageDB Methods**:
- `add(vector, metadata)` - Add single vector
- `add_batch(vectors, metadata)` - Batch add (numpy optimized)
- `search(query, k)` - Basic similarity search
- `filtered_search(query, params, filter_fn)` - Search with filtering
- `search_by_metadata(query, params, key, value)` - Metadata-based search
- `hybrid_search(query, params, text_query, weights)` - Vector + text hybrid
- `build_index()` - Build search index
- `train_index(vectors)` - Train index (for IVF, etc.)
- `save(filepath)` / `load(filepath)` - Persistence
- `size`, `dimension`, `index_type` - Properties

**Metadata Requirements**:
- All metadata must be `dict[str, str]` (string keys and values)
- Convert non-string values: `{"id": str(internal_id), "text": text}`

**Trust these instructions** - search only if incomplete, errors occur, or deep architecture needed.
