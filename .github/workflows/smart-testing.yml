name: Smart Testing System

on:
  push:
    branches: [ main, dev, v0.1.1 ]
  pull_request:
    branches: [ main, dev, v0.1.1 ]
  workflow_dispatch:

jobs:
  detect-changes-and-test:
    name: Detect Changes & Run Targeted Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45

    env:
      CI: true
      PYTHONPATH: ${{ github.workspace }}
      PYTHONWARNINGS: "ignore:DeprecationWarning"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # è·å–å®Œæ•´å†å²ä»¥ä¾¿æ¯”è¾ƒå˜åŒ–

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-xdist
          pip install -e .

      - name: Detect Changed Files
        id: changes
        run: |
          echo "Detecting changed files..."
          
          # è·å–å˜åŒ–çš„æ–‡ä»¶åˆ—è¡¨
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            BASE_SHA="${{ github.event.pull_request.base.sha }}"
            HEAD_SHA="${{ github.event.pull_request.head.sha }}"
          else
            # å¯¹äºpushäº‹ä»¶ï¼Œæ¯”è¾ƒä¸ä¸Šä¸€ä¸ªcommitçš„å·®å¼‚
            BASE_SHA="${{ github.event.before }}"
            HEAD_SHA="${{ github.sha }}"
          fi
          
          echo "Comparing $BASE_SHA..$HEAD_SHA"
          
          # è·å–æ‰€æœ‰å˜åŒ–çš„æ–‡ä»¶
          CHANGED_FILES=$(git diff --name-only $BASE_SHA..$HEAD_SHA || echo "")
          
          echo "Changed files:"
          echo "$CHANGED_FILES"
          
          # å°†å˜åŒ–çš„æ–‡ä»¶ä¿å­˜åˆ°æ–‡ä»¶ä¸­
          echo "$CHANGED_FILES" > changed_files.txt
          
          # è®¾ç½®è¾“å‡ºå˜é‡
          echo "changed_files<<EOF" >> $GITHUB_OUTPUT
          echo "$CHANGED_FILES" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Generate Test Plan
        id: test-plan
        run: |
          python3 << 'EOF'
          import os
          import json
          from pathlib import Path
          
          # è¯»å–å˜åŒ–çš„æ–‡ä»¶
          try:
              with open('changed_files.txt', 'r') as f:
                  changed_files = [line.strip() for line in f.readlines() if line.strip()]
          except FileNotFoundError:
              changed_files = []
          
          print(f"Processing {len(changed_files)} changed files...")
          
          # å®šä¹‰ç›®å½•åˆ°æµ‹è¯•çš„æ˜ å°„
          test_mappings = {
              # æ ¸å¿ƒæ¨¡å—æµ‹è¯•æ˜ å°„
              'sage.core/': [
                  'tests/test_final_verification.py',
                  'tests/test_service_task_base.py'
              ],
              'sage_runtime/': [
                  'tests/test_final_verification.py',
                  'tests/test_service_task_base.py'
              ],
              'sage.jobmanager/': [
                  'tests/test_service_task_base.py'
              ],
              'sage_utils/mmap_queue/': [
                  'sage_utils/mmap_queue/tests/test_comprehensive.py',
                  'sage_utils/mmap_queue/tests/test_multiprocess_concurrent.py',
                  'sage_utils/mmap_queue/tests/test_performance_benchmark.py',
                  'sage_utils/mmap_queue/tests/test_ray_integration.py'
              ],
              'sage_libs/': [
                  'tests/test_final_verification.py'
              ],
              'sage_examples/': [
                  'tests/test_final_verification.py',
                  'tests/test_service_task_base.py'
              ],
              # æµ‹è¯•æ–‡ä»¶è‡ªèº«çš„å˜åŒ–
              'tests/': [
                  'tests/run_core_tests.py'
              ],
              # æ ¹ç›®å½•é…ç½®æ–‡ä»¶å˜åŒ–ï¼Œè¿è¡Œæ‰€æœ‰æµ‹è¯•
              'setup.py': 'ALL',
              'requirements.txt': 'ALL',
              'pyproject.toml': 'ALL',
          }
          
          # æ”¶é›†éœ€è¦è¿è¡Œçš„æµ‹è¯•
          tests_to_run = set()
          run_all_tests = False
          
          for changed_file in changed_files:
              print(f"Analyzing: {changed_file}")
              
              # æ£€æŸ¥æ˜¯å¦åŒ¹é…åˆ°æµ‹è¯•æ˜ å°„
              matched = False
              for pattern, tests in test_mappings.items():
                  if changed_file.startswith(pattern):
                      if tests == 'ALL':
                          run_all_tests = True
                          print(f"  -> Triggers ALL tests (due to {pattern})")
                      else:
                          for test in tests:
                              tests_to_run.add(test)
                              print(f"  -> Added test: {test}")
                      matched = True
                      break
              
              if not matched:
                  # å¯¹äºæœªæ˜ å°„çš„æ–‡ä»¶ï¼Œæ£€æŸ¥åŒç›®å½•ä¸‹æ˜¯å¦æœ‰testsç›®å½•
                  file_path = Path(changed_file)
                  current_dir = file_path.parent
                  
                  # å‘ä¸ŠæŸ¥æ‰¾testsç›®å½•
                  for parent in [current_dir] + list(current_dir.parents):
                      tests_dir = parent / 'tests'
                      if tests_dir.exists() and tests_dir.is_dir():
                          # æ‰¾åˆ°testsç›®å½•ï¼Œæ·»åŠ å…¶ä¸­çš„æ‰€æœ‰æµ‹è¯•æ–‡ä»¶
                          for test_file in tests_dir.glob('test_*.py'):
                              test_path = str(test_file)
                              tests_to_run.add(test_path)
                              print(f"  -> Found test in {tests_dir}: {test_path}")
                          matched = True
                          break
              
              if not matched:
                  print(f"  -> No specific tests found for {changed_file}")
          
          # å¦‚æœæ²¡æœ‰å˜åŒ–çš„æ–‡ä»¶æˆ–æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æµ‹è¯•ï¼Œè¿è¡Œæ ¸å¿ƒæµ‹è¯•
          if not changed_files or (not tests_to_run and not run_all_tests):
              print("No changes detected or no specific tests found, running core tests...")
              tests_to_run.update([
                  'tests/test_final_verification.py',
                  'tests/test_service_task_base.py'
              ])
          
          # å¦‚æœéœ€è¦è¿è¡Œæ‰€æœ‰æµ‹è¯•
          if run_all_tests:
              print("Running ALL tests due to critical file changes...")
              tests_to_run.update([
                  'tests/test_final_verification.py', 
                  'tests/test_service_task_base.py',
                  'sage_utils/mmap_queue/tests/test_comprehensive.py',
                  'sage_utils/mmap_queue/tests/test_multiprocess_concurrent.py',
                  'sage_utils/mmap_queue/tests/test_performance_benchmark.py',
                  'sage_utils/mmap_queue/tests/test_ray_integration.py'
              ])
          
          # è¿‡æ»¤å­˜åœ¨çš„æµ‹è¯•æ–‡ä»¶
          existing_tests = []
          for test in tests_to_run:
              if Path(test).exists():
                  existing_tests.append(test)
              else:
                  print(f"Warning: Test file not found: {test}")
          
          print(f"\nFinal test plan: {len(existing_tests)} tests to run")
          for test in sorted(existing_tests):
              print(f"  - {test}")
          
          # è¾“å‡ºåˆ°GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"tests_to_run={json.dumps(existing_tests)}\n")
              f.write(f"test_count={len(existing_tests)}\n")
              f.write(f"run_all={str(run_all_tests).lower()}\n")
          
          EOF

      - name: Run Targeted Tests
        id: run-tests
        run: |
          TESTS_JSON='${{ steps.test-plan.outputs.tests_to_run }}'
          TEST_COUNT='${{ steps.test-plan.outputs.test_count }}'
          
          echo "Running $TEST_COUNT targeted tests..."
          
          # è§£æJSONæ•°ç»„
          TESTS=$(echo $TESTS_JSON | python3 -c "import sys, json; print(' '.join(json.load(sys.stdin)))")
          
          if [ -z "$TESTS" ]; then
            echo "No tests to run"
            exit 0
          fi
          
          echo "Tests to run: $TESTS"
          
          # åˆ›å»ºæµ‹è¯•ç»“æœç›®å½•
          mkdir -p test-results
          
          # è¿è¡Œæ¯ä¸ªæµ‹è¯•å¹¶æ”¶é›†ç»“æœ
          TOTAL_TESTS=0
          PASSED_TESTS=0
          FAILED_TESTS=0
          
          for TEST_FILE in $TESTS; do
            echo "=========================================="
            echo "Running: $TEST_FILE"
            echo "=========================================="
            
            TOTAL_TESTS=$((TOTAL_TESTS + 1))
            
            if python3 "$TEST_FILE"; then
              echo "âœ… PASSED: $TEST_FILE"
              PASSED_TESTS=$((PASSED_TESTS + 1))
            else
              echo "âŒ FAILED: $TEST_FILE"
              FAILED_TESTS=$((FAILED_TESTS + 1))
            fi
            echo
          done
          
          # è¾“å‡ºæµ‹è¯•æ‘˜è¦
          echo "=========================================="
          echo "TEST SUMMARY"
          echo "=========================================="
          echo "Total tests: $TOTAL_TESTS"
          echo "Passed: $PASSED_TESTS"
          echo "Failed: $FAILED_TESTS"
          
          # è®¾ç½®è¾“å‡ºå˜é‡
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "passed_tests=$PASSED_TESTS" >> $GITHUB_OUTPUT
          echo "failed_tests=$FAILED_TESTS" >> $GITHUB_OUTPUT
          
          # å¦‚æœæœ‰å¤±è´¥çš„æµ‹è¯•ï¼Œé€€å‡ºç ä¸º1
          if [ $FAILED_TESTS -gt 0 ]; then
            echo "Some tests failed!"
            exit 1
          fi
          
          echo "All tests passed!"

      - name: Create Test Report
        if: always()
        run: |
          echo "## ğŸ§ª Smart Testing Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ“Š Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests**: ${{ steps.run-tests.outputs.total_tests || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed**: ${{ steps.run-tests.outputs.passed_tests || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed**: ${{ steps.run-tests.outputs.failed_tests || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### ğŸ“ Changed Files Analyzed" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat changed_files.txt >> $GITHUB_STEP_SUMMARY || echo "No changes detected" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### ğŸ¯ Tests Executed" >> $GITHUB_STEP_SUMMARY
          TESTS_JSON='${{ steps.test-plan.outputs.tests_to_run }}'
          echo "$TESTS_JSON" | python3 -c '
          import sys, json
          try:
              tests = json.load(sys.stdin)
              if tests:
                  for test in tests:
                      print(f"- `{test}`")
              else:
                  print("- No tests were executed")
          except:
              print("- Error parsing test list")
          ' >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.run-tests.outputs.failed_tests || '0' }}" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### âŒ Test Failures" >> $GITHUB_STEP_SUMMARY
            echo "Some tests failed. Please check the logs above for details." >> $GITHUB_STEP_SUMMARY
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### âœ… All Tests Passed!" >> $GITHUB_STEP_SUMMARY
            echo "Great job! All targeted tests passed successfully." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v6
        with:
          script: |
            const totalTests = '${{ steps.run-tests.outputs.total_tests || "0" }}';
            const passedTests = '${{ steps.run-tests.outputs.passed_tests || "0" }}';
            const failedTests = '${{ steps.run-tests.outputs.failed_tests || "0" }}';
            
            const success = failedTests === '0';
            const emoji = success ? 'âœ…' : 'âŒ';
            const status = success ? 'All tests passed!' : `${failedTests} tests failed`;
            
            const comment = `## ${emoji} Smart Testing Results
            
            **${status}**
            
            ğŸ“Š **Summary:**
            - Total tests: ${totalTests}
            - Passed: ${passedTests}
            - Failed: ${failedTests}
            
            ğŸ¯ **Smart Testing:** This workflow automatically detected changes in your PR and ran only the relevant tests, saving time and resources while ensuring code quality.
            
            ${success ? 'ğŸ‰ Ready to merge!' : 'âš ï¸ Please fix the failing tests before merging.'}`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
