name: Smart Testing System

on:
  push:
    branches: [ main, dev, v0.1.1 ]
  pull_request:
    branches: [ main, dev, v0.1.1 ]
  workflow_dispatch:

jobs:
  detect-changes-and-test:
    name: Detect Changes & Run Targeted Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45

    env:
      CI: true
      PYTHONPATH: ${{ github.workspace }}
      PYTHONWARNINGS: "ignore:DeprecationWarning"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # 获取完整历史以便比较变化

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-xdist
          pip install -e .

      - name: Detect Changed Files
        id: changes
        run: |
          echo "Detecting changed files..."
          
          # 获取变化的文件列表
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            BASE_SHA="${{ github.event.pull_request.base.sha }}"
            HEAD_SHA="${{ github.event.pull_request.head.sha }}"
          else
            # 对于push事件，比较与上一个commit的差异
            BASE_SHA="${{ github.event.before }}"
            HEAD_SHA="${{ github.sha }}"
          fi
          
          echo "Comparing $BASE_SHA..$HEAD_SHA"
          
          # 获取所有变化的文件
          CHANGED_FILES=$(git diff --name-only $BASE_SHA..$HEAD_SHA || echo "")
          
          echo "Changed files:"
          echo "$CHANGED_FILES"
          
          # 将变化的文件保存到文件中
          echo "$CHANGED_FILES" > changed_files.txt
          
          # 设置输出变量
          echo "changed_files<<EOF" >> $GITHUB_OUTPUT
          echo "$CHANGED_FILES" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Generate Test Plan
        id: test-plan
        run: |
          python3 << 'EOF'
          import os
          import json
          from pathlib import Path
          
          # 读取变化的文件
          try:
              with open('changed_files.txt', 'r') as f:
                  changed_files = [line.strip() for line in f.readlines() if line.strip()]
          except FileNotFoundError:
              changed_files = []
          
          print(f"Processing {len(changed_files)} changed files...")
          
          # 定义目录到测试的映射
          test_mappings = {
              # 核心模块测试映射
              'sage/core/': [
                  'tests/test_final_verification.py',
                  'tests/test_service_task_base.py'
              ],
              'sage/runtime/': [
                  'tests/test_final_verification.py',
                  'tests/test_service_task_base.py'
              ],
              'sage/jobmanager/': [
                  'tests/test_service_task_base.py'
              ],
              'sage/utils/mmap_queue/': [
                  'sage/utils/mmap_queue/tests/test_comprehensive.py',
                  'sage/utils/mmap_queue/tests/test_multiprocess_concurrent.py',
                  'sage/utils/mmap_queue/tests/test_performance_benchmark.py',
                  'sage/utils/mmap_queue/tests/test_ray_integration.py'
              ],
              'sage/libs/': [
                  'tests/test_final_verification.py'
              ],
              'sage/examples/': [
                  'tests/test_final_verification.py',
                  'tests/test_service_task_base.py'
              ],
              # 测试文件自身的变化
              'tests/': [
                  'tests/run_core_tests.py'
              ],
              # 根目录配置文件变化，运行所有测试
              'setup.py': 'ALL',
              'requirements.txt': 'ALL',
              'pyproject.toml': 'ALL',
          }
          
          # 收集需要运行的测试
          tests_to_run = set()
          run_all_tests = False
          
          for changed_file in changed_files:
              print(f"Analyzing: {changed_file}")
              
              # 检查是否匹配到测试映射
              matched = False
              for pattern, tests in test_mappings.items():
                  if changed_file.startswith(pattern):
                      if tests == 'ALL':
                          run_all_tests = True
                          print(f"  -> Triggers ALL tests (due to {pattern})")
                      else:
                          for test in tests:
                              tests_to_run.add(test)
                              print(f"  -> Added test: {test}")
                      matched = True
                      break
              
              if not matched:
                  # 对于未映射的文件，检查同目录下是否有tests目录
                  file_path = Path(changed_file)
                  current_dir = file_path.parent
                  
                  # 向上查找tests目录
                  for parent in [current_dir] + list(current_dir.parents):
                      tests_dir = parent / 'tests'
                      if tests_dir.exists() and tests_dir.is_dir():
                          # 找到tests目录，添加其中的所有测试文件
                          for test_file in tests_dir.glob('test_*.py'):
                              test_path = str(test_file)
                              tests_to_run.add(test_path)
                              print(f"  -> Found test in {tests_dir}: {test_path}")
                          matched = True
                          break
              
              if not matched:
                  print(f"  -> No specific tests found for {changed_file}")
          
          # 如果没有变化的文件或没有找到相关测试，运行核心测试
          if not changed_files or (not tests_to_run and not run_all_tests):
              print("No changes detected or no specific tests found, running core tests...")
              tests_to_run.update([
                  'tests/test_final_verification.py',
                  'tests/test_service_task_base.py'
              ])
          
          # 如果需要运行所有测试
          if run_all_tests:
              print("Running ALL tests due to critical file changes...")
              tests_to_run.update([
                  'tests/test_final_verification.py', 
                  'tests/test_service_task_base.py',
                  'sage/utils/mmap_queue/tests/test_comprehensive.py',
                  'sage/utils/mmap_queue/tests/test_multiprocess_concurrent.py',
                  'sage/utils/mmap_queue/tests/test_performance_benchmark.py',
                  'sage/utils/mmap_queue/tests/test_ray_integration.py'
              ])
          
          # 过滤存在的测试文件
          existing_tests = []
          for test in tests_to_run:
              if Path(test).exists():
                  existing_tests.append(test)
              else:
                  print(f"Warning: Test file not found: {test}")
          
          print(f"\nFinal test plan: {len(existing_tests)} tests to run")
          for test in sorted(existing_tests):
              print(f"  - {test}")
          
          # 输出到GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"tests_to_run={json.dumps(existing_tests)}\n")
              f.write(f"test_count={len(existing_tests)}\n")
              f.write(f"run_all={str(run_all_tests).lower()}\n")
          
          EOF

      - name: Run Targeted Tests
        id: run-tests
        run: |
          TESTS_JSON='${{ steps.test-plan.outputs.tests_to_run }}'
          TEST_COUNT='${{ steps.test-plan.outputs.test_count }}'
          
          echo "Running $TEST_COUNT targeted tests..."
          
          # 解析JSON数组
          TESTS=$(echo $TESTS_JSON | python3 -c "import sys, json; print(' '.join(json.load(sys.stdin)))")
          
          if [ -z "$TESTS" ]; then
            echo "No tests to run"
            exit 0
          fi
          
          echo "Tests to run: $TESTS"
          
          # 创建测试结果目录
          mkdir -p test-results
          
          # 运行每个测试并收集结果
          TOTAL_TESTS=0
          PASSED_TESTS=0
          FAILED_TESTS=0
          
          for TEST_FILE in $TESTS; do
            echo "=========================================="
            echo "Running: $TEST_FILE"
            echo "=========================================="
            
            TOTAL_TESTS=$((TOTAL_TESTS + 1))
            
            if python3 "$TEST_FILE"; then
              echo "✅ PASSED: $TEST_FILE"
              PASSED_TESTS=$((PASSED_TESTS + 1))
            else
              echo "❌ FAILED: $TEST_FILE"
              FAILED_TESTS=$((FAILED_TESTS + 1))
            fi
            echo
          done
          
          # 输出测试摘要
          echo "=========================================="
          echo "TEST SUMMARY"
          echo "=========================================="
          echo "Total tests: $TOTAL_TESTS"
          echo "Passed: $PASSED_TESTS"
          echo "Failed: $FAILED_TESTS"
          
          # 设置输出变量
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "passed_tests=$PASSED_TESTS" >> $GITHUB_OUTPUT
          echo "failed_tests=$FAILED_TESTS" >> $GITHUB_OUTPUT
          
          # 如果有失败的测试，退出码为1
          if [ $FAILED_TESTS -gt 0 ]; then
            echo "Some tests failed!"
            exit 1
          fi
          
          echo "All tests passed!"

      - name: Create Test Report
        if: always()
        run: |
          echo "## 🧪 Smart Testing Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📊 Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests**: ${{ steps.run-tests.outputs.total_tests || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed**: ${{ steps.run-tests.outputs.passed_tests || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed**: ${{ steps.run-tests.outputs.failed_tests || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### 📁 Changed Files Analyzed" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat changed_files.txt >> $GITHUB_STEP_SUMMARY || echo "No changes detected" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### 🎯 Tests Executed" >> $GITHUB_STEP_SUMMARY
          TESTS_JSON='${{ steps.test-plan.outputs.tests_to_run }}'
          echo "$TESTS_JSON" | python3 -c '
          import sys, json
          try:
              tests = json.load(sys.stdin)
              if tests:
                  for test in tests:
                      print(f"- `{test}`")
              else:
                  print("- No tests were executed")
          except:
              print("- Error parsing test list")
          ' >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.run-tests.outputs.failed_tests || '0' }}" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ❌ Test Failures" >> $GITHUB_STEP_SUMMARY
            echo "Some tests failed. Please check the logs above for details." >> $GITHUB_STEP_SUMMARY
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ✅ All Tests Passed!" >> $GITHUB_STEP_SUMMARY
            echo "Great job! All targeted tests passed successfully." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v6
        with:
          script: |
            const totalTests = '${{ steps.run-tests.outputs.total_tests || "0" }}';
            const passedTests = '${{ steps.run-tests.outputs.passed_tests || "0" }}';
            const failedTests = '${{ steps.run-tests.outputs.failed_tests || "0" }}';
            
            const success = failedTests === '0';
            const emoji = success ? '✅' : '❌';
            const status = success ? 'All tests passed!' : `${failedTests} tests failed`;
            
            const comment = `## ${emoji} Smart Testing Results
            
            **${status}**
            
            📊 **Summary:**
            - Total tests: ${totalTests}
            - Passed: ${passedTests}
            - Failed: ${failedTests}
            
            🎯 **Smart Testing:** This workflow automatically detected changes in your PR and ran only the relevant tests, saving time and resources while ensuring code quality.
            
            ${success ? '🎉 Ready to merge!' : '⚠️ Please fix the failing tests before merging.'}`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
