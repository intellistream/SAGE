name: Paper 1 - SAGE Agent Bench Full Experiments

# Manual trigger only - self-hosted GPU server
on:
  workflow_dispatch:
    inputs:
      section:
        description: 'Section to run (all/5.2/5.3/5.4/5.5)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - '5.2'
          - '5.3'
          - '5.4'
          - '5.5'
      experiment:
        description: 'Specific experiment (optional)'
        required: false
        default: ''
        type: choice
        options:
          - ''
          - timing
          - planning
          - selection
          - error
          - scaling
          - robustness
          - ablation
          - cross-dataset
          - training
      quick_mode:
        description: 'Quick mode (reduced samples for testing)'
        required: false
        default: false
        type: boolean
      skip_llm:
        description: 'Skip LLM-based methods (faster)'
        required: false
        default: false
        type: boolean
      skip_llm_scaling:
        description: 'Skip LLM Scaling test (saves time, uses estimates)'
        required: false
        default: false
        type: boolean
      generate_paper:
        description: 'Generate paper figures and tables'
        required: false
        default: true
        type: boolean

env:
  CI: true
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_ENDPOINT: https://hf-mirror.com
  SAGE_TEST_MODE: false
  SAGE_LOG_LEVEL: INFO
  # LLM Service Configuration
  SAGE_CHAT_API_KEY: ${{ secrets.ALIBABA_API_KEY }}
  SAGE_CHAT_BASE_URL: https://dashscope.aliyuncs.com/compatible-mode/v1
  SAGE_CHAT_MODEL: qwen-turbo-2025-02-11


jobs:
  run-experiments:
    name: Run SAGE Agent Bench Experiments
    runs-on: [self-hosted, A100]
    timeout-minutes: 480  # 8 hours for full experiments

    # Use dedicated conda environment to avoid conflicts with any existing "sage" env
    env:
      CONDA_ENV_NAME: sage-paper1-ci

    # Use login shell to ensure conda is initialized
    defaults:
      run:
        shell: bash -l {0}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          clean: true
          submodules: 'recursive'

      # Setup dedicated conda environment for this workflow
      - name: Setup Conda Environment
        run: |
          echo "ðŸ”§ Setting up dedicated conda environment: $CONDA_ENV_NAME"

          # Function to initialize conda
          init_conda() {
            if command -v conda &> /dev/null; then
              return 0
            fi
            # Common conda locations on self-hosted runners
            for conda_path in "$HOME/miniconda3" "$HOME/anaconda3" "/opt/conda" "/usr/local/miniconda3" "/home/action-runner/miniconda3"; do
              if [ -f "$conda_path/etc/profile.d/conda.sh" ]; then
                echo "Found conda at: $conda_path"
                source "$conda_path/etc/profile.d/conda.sh"
                return 0
              fi
            done
            return 1
          }

          # Try to initialize conda
          if ! init_conda; then
            echo "âŒ Conda not found. Installing Miniconda..."
            wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh
            bash /tmp/miniconda.sh -b -p $HOME/miniconda3
            source "$HOME/miniconda3/etc/profile.d/conda.sh"
            rm /tmp/miniconda.sh
            # Add to bashrc for future steps
            echo 'source "$HOME/miniconda3/etc/profile.d/conda.sh"' >> ~/.bashrc
          fi

          conda --version
          echo "âœ… Conda is available"

          # Remove existing environment if it exists (clean slate)
          conda env remove -n $CONDA_ENV_NAME -y 2>/dev/null || true

          # Create fresh environment with Python 3.11
          conda create -n $CONDA_ENV_NAME python=3.11 -y

          # Verify environment
          conda run -n $CONDA_ENV_NAME python --version
          echo "âœ… Conda environment $CONDA_ENV_NAME created"

          # Save conda path for other steps
          echo "CONDA_PATH=$(dirname $(dirname $(which conda)))" >> $GITHUB_ENV

      - name: Install SAGE in Conda Environment
        run: |
          echo "ðŸš€ Installing SAGE in $CONDA_ENV_NAME environment..."

          # Initialize conda from saved path or search
          if [ -n "$CONDA_PATH" ]; then
            source "$CONDA_PATH/etc/profile.d/conda.sh"
          else
            for p in "$HOME/miniconda3" "$HOME/anaconda3" "/opt/conda"; do
              [ -f "$p/etc/profile.d/conda.sh" ] && source "$p/etc/profile.d/conda.sh" && break
            done
          fi

          # Activate conda environment
          conda activate $CONDA_ENV_NAME

          # Configure pip to use Chinese mirrors (for self-hosted runner in China)
          echo "ðŸ“¦ Configuring pip mirrors for faster downloads..."
          pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
          pip config set global.trusted-host pypi.tuna.tsinghua.edu.cn

          # Configure conda to use Chinese mirrors
          conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
          conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
          conda config --set show_channel_urls yes

          # Install system dependencies if needed
          for cmd in cmake pkg-config; do
            if ! command -v $cmd &> /dev/null; then
              sudo apt-get update -qq
              sudo apt-get install -y --no-install-recommends build-essential cmake pkg-config libopenblas-dev liblapack-dev git
              break
            fi
          done

          # Temporarily unset CI variables so quickstart.sh installs to conda env, not ~/.local
          # This ensures packages are installed in the active conda environment
          unset CI GITHUB_ACTIONS

          # Install SAGE using quickstart.sh (same as build-test.yml)
          chmod +x ./quickstart.sh
          ./quickstart.sh --dev --yes --pip

          # Verify installation
          python -c "import sage; print(f'SAGE {sage.__version__} installed successfully')"
          python -c "from sage.benchmark.benchmark_agent import get_adapter_registry; print('sage-benchmark OK')"
          echo "âœ… SAGE installation completed"
        timeout-minutes: 45  # Increased from 25 to handle 130+ dependencies

      - name: Create Experiment Environment File
        run: |
          # Create .env with all required settings
          cat > .env << EOF
          # SAGE Environment Configuration (auto-generated for Paper 1 experiments)
          HF_TOKEN=${{ secrets.HF_TOKEN }}
          HF_ENDPOINT=https://hf-mirror.com

          # LLM Service (DashScope/Qwen)
          SAGE_CHAT_API_KEY=${{ secrets.ALIBABA_API_KEY }}
          SAGE_CHAT_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
          SAGE_CHAT_MODEL=qwen-turbo-2025-02-11

          # Embedding Service
          SAGE_EMBEDDING_MODEL=BAAI/bge-small-zh-v1.5

          # Experiment Settings
          SAGE_TEST_MODE=false
          EOF

          echo "Done: Experiment .env settings added"

      - name: Check GPU Availability
        run: |
          echo "Checking GPU status..."
          nvidia-smi || echo "No NVIDIA GPU found"

          # Initialize conda
          source "${CONDA_PATH:-$HOME/miniconda3}/etc/profile.d/conda.sh" 2>/dev/null || true

          conda run -n $CONDA_ENV_NAME python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}'); print(f'GPUs: {torch.cuda.device_count()}')" || true

      - name: Start Local LLM Service
        if: ${{ !inputs.skip_llm }}
        run: |
          echo "ðŸš€ Starting local LLM service..."

          # Initialize conda
          source "${CONDA_PATH:-$HOME/miniconda3}/etc/profile.d/conda.sh" 2>/dev/null || true

          # Check if local vLLM is already running
          if curl -s http://localhost:8901/v1/models > /dev/null 2>&1; then
            echo "âœ… Local LLM service already running on port 8901"
          else
            echo "Starting vLLM server..."

            # Use the experiment script's LLM service manager
            cd packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

            # Start LLM service (uses Qwen2.5-0.5B by default for speed)
            # Note: llm_service.py uses subcommand style: start/stop/status
            conda run -n $CONDA_ENV_NAME python -m experiments.llm_service start \
              --model "Qwen/Qwen2.5-0.5B-Instruct" \
              --port 8901 \
              --gpu-memory 0.5 || echo "âš ï¸ Failed to start LLM service"

            # Check if service is now running
            if curl -s http://localhost:8901/v1/models > /dev/null 2>&1; then
              echo "âœ… Local LLM service is ready"
            else
              echo "âš ï¸ Local LLM service not available, will use cloud API (DashScope)"
            fi
          fi

      - name: Run Paper 1 Experiments
        run: |
          echo "Running SAGE Agent Bench experiments..."

          # Initialize conda
          source "${CONDA_PATH:-$HOME/miniconda3}/etc/profile.d/conda.sh" 2>/dev/null || true

          # Navigate to scripts directory (NOT experiments/ - need parent for proper imports)
          cd packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

          # Build command - run from scripts/ so 'experiments' is a valid package
          CMD="conda run -n $CONDA_ENV_NAME python -m experiments.run_paper1_experiments"

          # Add section selection
          if [ "${{ inputs.section }}" != "all" ]; then
            CMD="$CMD --section ${{ inputs.section }}"
          fi

          # Add specific experiment if provided
          if [ -n "${{ inputs.experiment }}" ]; then
            CMD="$CMD --exp ${{ inputs.experiment }}"
          fi

          # Quick mode
          if [ "${{ inputs.quick_mode }}" == "true" ]; then
            CMD="$CMD --quick"
          fi

          # Skip LLM if requested
          if [ "${{ inputs.skip_llm }}" == "true" ]; then
            CMD="$CMD --skip-llm"
          fi

          # Skip LLM Scaling if requested (saves ~1-2 hours)
          if [ "${{ inputs.skip_llm_scaling }}" == "true" ]; then
            CMD="$CMD --skip-llm-scaling"
          fi

          # Verbose output
          CMD="$CMD --verbose"

          echo "Running: $CMD"
          $CMD

      - name: Generate Paper Materials
        if: ${{ inputs.generate_paper && success() }}
        run: |
          echo "Generating paper figures and tables..."

          # Initialize conda
          source "${CONDA_PATH:-$HOME/miniconda3}/etc/profile.d/conda.sh" 2>/dev/null || true

          cd packages/sage-benchmark/src/sage/benchmark/benchmark_agent/scripts

          # Generate figures
          conda run -n $CONDA_ENV_NAME python -c "from experiments.figure_generator import generate_all_figures; generate_all_figures(); print('Figures generated.')"

          # Generate tables
          conda run -n $CONDA_ENV_NAME python -c "from experiments.table_generator import generate_all_tables; generate_all_tables(); print('Tables generated.')"

      - name: Upload Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: paper1-results-${{ github.run_number }}
          path: |
            .sage/benchmark/paper1/
          retention-days: 90

      - name: Upload Paper Materials
        uses: actions/upload-artifact@v4
        if: ${{ inputs.generate_paper && success() }}
        with:
          name: paper1-figures-tables-${{ github.run_number }}
          path: |
            .sage/benchmark/paper1/figures/
            .sage/benchmark/paper1/tables/
          retention-days: 90

      - name: Print Summary
        if: always()
        run: |
          echo ""
          echo "=============================================="
          echo "       SAGE Agent Bench - Experiment Summary"
          echo "=============================================="
          echo ""
          echo "Configuration:"
          echo "  Section: ${{ inputs.section }}"
          echo "  Experiment: ${{ inputs.experiment || 'all' }}"
          echo "  Quick Mode: ${{ inputs.quick_mode }}"
          echo "  Skip LLM: ${{ inputs.skip_llm }}"
          echo "  Generate Paper: ${{ inputs.generate_paper }}"
          echo ""
          echo "=============================================="
          echo "  Control Constants:"
          echo "  - RANDOM_SEED: 42"
          echo "  - EMBEDDING_MODEL: BAAI/bge-small-zh-v1.5"
          echo "  - LLM_TEMPERATURE: 0.1"
          echo "=============================================="
          echo ""

          # Print results summary if available
          if [ -d ".sage/benchmark/paper1" ]; then
            echo "Results directory structure:"
            find .sage/benchmark/paper1 -type f -name "*.json" | head -20
            echo ""

            # Print main results if available
            for section in section_5_2_main section_5_3_analysis section_5_4_generalization section_5_5_training; do
              dir=".sage/benchmark/paper1/$section"
              if [ -d "$dir" ]; then
                echo "[$section]"
                ls -la "$dir"/*.json 2>/dev/null || echo "  (no results)"
                echo ""
              fi
            done
          else
            echo "No results directory found."
          fi

          echo ""
          echo "========================================"
          echo "  HOW TO DOWNLOAD EXPERIMENT RESULTS"
          echo "========================================"
          echo ""
          echo "Your experiment results have been uploaded as GitHub Artifacts."
          echo ""
          echo "To download:"
          echo "  1. Go to this workflow run page:"
          echo "     https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          echo ""
          echo "  2. Scroll down to the 'Artifacts' section"
          echo ""
          echo "  3. Download:"
          echo "     - paper1-results-${{ github.run_number }}: Full results"
          echo "     - paper1-figures-tables-${{ github.run_number }}: Figures and LaTeX tables"
          echo ""
          echo "Artifacts retained for 90 days."
          echo ""
          echo "GitHub CLI:"
          echo "  gh run download ${{ github.run_id }} --repo ${{ github.repository }}"
          echo "========================================"
