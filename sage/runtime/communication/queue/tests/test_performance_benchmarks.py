#!/usr/bin/env python3
"""
é˜Ÿåˆ—æè¿°ç¬¦æ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•ä¸åŒé˜Ÿåˆ—ç±»å‹åœ¨å„ç§åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼š
1. å•çº¿ç¨‹ååé‡æµ‹è¯•
2. å¤šçº¿ç¨‹å¹¶å‘æ€§èƒ½æµ‹è¯•
3. å¤šè¿›ç¨‹å¹¶å‘æ€§èƒ½æµ‹è¯•ï¼ˆæ”¯æŒçš„é˜Ÿåˆ—ç±»å‹ï¼‰
4. å†…å­˜ä½¿ç”¨æƒ…å†µæµ‹è¯•
5. å»¶è¿Ÿæµ‹è¯•
"""

import sys
import os
import time
import threading
import multiprocessing
import psutil
import statistics
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Tuple
import gc

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/api-rework')

try:
    from sage.runtime.communication.queue import (
        PythonQueueDescriptor,
        RayQueueDescriptor,  
        SageQueueDescriptor
    )
    print("âœ“ æˆåŠŸå¯¼å…¥é˜Ÿåˆ—æè¿°ç¬¦")
except ImportError as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.results = {}
        self.process = psutil.Process()
    
    def measure_memory_usage(self) -> Dict[str, float]:
        """æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_info = self.process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,  # ç‰©ç†å†…å­˜
            'vms_mb': memory_info.vms / 1024 / 1024,  # è™šæ‹Ÿå†…å­˜
            'percent': self.process.memory_percent()   # å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
        }
    
    def single_thread_throughput_test(self, queue_desc, num_items: int = 10000) -> Dict[str, Any]:
        """å•çº¿ç¨‹ååé‡æµ‹è¯•"""
        print(f"\n--- å•çº¿ç¨‹ååé‡æµ‹è¯• ({queue_desc.queue_type}) ---")
        
        # å†…å­˜åŸºçº¿
        gc.collect()
        memory_before = self.measure_memory_usage()
        
        # å†™å…¥æµ‹è¯•
        start_time = time.time()
        for i in range(num_items):
            queue_desc.put(f"item_{i}")
        write_end_time = time.time()
        
        write_duration = write_end_time - start_time
        write_throughput = num_items / write_duration
        
        # è¯»å–æµ‹è¯•
        read_start_time = time.time()
        items = []
        for i in range(num_items):
            items.append(queue_desc.get())
        read_end_time = time.time()
        
        read_duration = read_end_time - read_start_time
        read_throughput = num_items / read_duration
        
        # å†…å­˜æµ‹é‡
        memory_after = self.measure_memory_usage()
        memory_delta = memory_after['rss_mb'] - memory_before['rss_mb']
        
        result = {
            'queue_type': queue_desc.queue_type,
            'num_items': num_items,
            'write_duration': write_duration,
            'read_duration': read_duration,
            'write_throughput': write_throughput,
            'read_throughput': read_throughput,
            'total_duration': write_duration + read_duration,
            'memory_delta_mb': memory_delta,
            'memory_before': memory_before,
            'memory_after': memory_after
        }
        
        print(f"å†™å…¥: {write_throughput:.0f} items/sec, è¯»å–: {read_throughput:.0f} items/sec")
        print(f"å†…å­˜å˜åŒ–: {memory_delta:.2f} MB")
        
        return result
    
    def multi_thread_throughput_test(self, queue_desc, num_threads: int = 4, 
                                   items_per_thread: int = 2500) -> Dict[str, Any]:
        """å¤šçº¿ç¨‹ååé‡æµ‹è¯•"""
        print(f"\n--- å¤šçº¿ç¨‹ååé‡æµ‹è¯• ({queue_desc.queue_type}) ---")
        print(f"é…ç½®: {num_threads}ä¸ªçº¿ç¨‹, æ¯ä¸ªå¤„ç†{items_per_thread}ä¸ªé¡¹ç›®")
        
        gc.collect()
        memory_before = self.measure_memory_usage()
        
        # ç”Ÿäº§è€…çº¿ç¨‹å‡½æ•°
        def producer_worker(thread_id: int, items: int):
            start_time = time.time()
            for i in range(items):
                queue_desc.put(f"thread_{thread_id}_item_{i}")
            end_time = time.time()
            return {
                'thread_id': thread_id,
                'duration': end_time - start_time,
                'throughput': items / (end_time - start_time)
            }
        
        # æ¶ˆè´¹è€…çº¿ç¨‹å‡½æ•°
        def consumer_worker(thread_id: int, items: int):
            start_time = time.time()
            consumed = []
            for i in range(items):
                try:
                    item = queue_desc.get(timeout=5.0)
                    consumed.append(item)
                except:
                    break
            end_time = time.time()
            return {
                'thread_id': thread_id,
                'duration': end_time - start_time,
                'consumed': len(consumed),
                'throughput': len(consumed) / (end_time - start_time) if end_time > start_time else 0
            }
        
        # è¿è¡Œç”Ÿäº§è€…çº¿ç¨‹
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            producer_futures = []
            for i in range(num_threads):
                future = executor.submit(producer_worker, i, items_per_thread)
                producer_futures.append(future)
            
            producer_results = []
            for future in as_completed(producer_futures):
                result = future.result()
                producer_results.append(result)
        
        producer_end_time = time.time()
        
        # è¿è¡Œæ¶ˆè´¹è€…çº¿ç¨‹
        consumer_start_time = time.time()
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            consumer_futures = []
            for i in range(num_threads):
                future = executor.submit(consumer_worker, i, items_per_thread)
                consumer_futures.append(future)
            
            consumer_results = []
            for future in as_completed(consumer_futures):
                result = future.result()
                consumer_results.append(result)
        
        consumer_end_time = time.time()
        
        memory_after = self.measure_memory_usage()
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_items = num_threads * items_per_thread
        producer_throughputs = [r['throughput'] for r in producer_results]
        consumer_throughputs = [r['throughput'] for r in consumer_results]
        total_consumed = sum(r['consumed'] for r in consumer_results)
        
        result = {
            'queue_type': queue_desc.queue_type,
            'num_threads': num_threads,
            'items_per_thread': items_per_thread,
            'total_items': total_items,
            'producer_duration': producer_end_time - start_time,
            'consumer_duration': consumer_end_time - consumer_start_time,
            'total_consumed': total_consumed,
            'avg_producer_throughput': statistics.mean(producer_throughputs),
            'max_producer_throughput': max(producer_throughputs),
            'avg_consumer_throughput': statistics.mean(consumer_throughputs),
            'max_consumer_throughput': max(consumer_throughputs),
            'memory_delta_mb': memory_after['rss_mb'] - memory_before['rss_mb'],
            'producer_results': producer_results,
            'consumer_results': consumer_results
        }
        
        print(f"ç”Ÿäº§è€…å¹³å‡ååé‡: {result['avg_producer_throughput']:.0f} items/sec")
        print(f"æ¶ˆè´¹è€…å¹³å‡ååé‡: {result['avg_consumer_throughput']:.0f} items/sec")
        print(f"æ€»æ¶ˆè´¹é¡¹ç›®: {total_consumed}/{total_items}")
        
        return result
    
    def latency_test(self, queue_desc, num_samples: int = 1000) -> Dict[str, Any]:
        """å»¶è¿Ÿæµ‹è¯•"""
        print(f"\n--- å»¶è¿Ÿæµ‹è¯• ({queue_desc.queue_type}) ---")
        
        latencies = []
        
        for i in range(num_samples):
            # æµ‹é‡å•æ¬¡put-getå¾ªç¯çš„å»¶è¿Ÿ
            start_time = time.perf_counter()
            queue_desc.put(f"latency_test_{i}")
            item = queue_desc.get()
            end_time = time.perf_counter()
            
            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)
        
        result = {
            'queue_type': queue_desc.queue_type,
            'num_samples': num_samples,
            'avg_latency_ms': statistics.mean(latencies),
            'median_latency_ms': statistics.median(latencies),
            'min_latency_ms': min(latencies),
            'max_latency_ms': max(latencies),
            'p95_latency_ms': statistics.quantiles(latencies, n=20)[18],  # 95th percentile
            'p99_latency_ms': statistics.quantiles(latencies, n=100)[98],  # 99th percentile
            'stddev_latency_ms': statistics.stdev(latencies)
        }
        
        print(f"å¹³å‡å»¶è¿Ÿ: {result['avg_latency_ms']:.3f}ms")
        print(f"ä¸­ä½å»¶è¿Ÿ: {result['median_latency_ms']:.3f}ms")
        print(f"P95å»¶è¿Ÿ: {result['p95_latency_ms']:.3f}ms")
        print(f"P99å»¶è¿Ÿ: {result['p99_latency_ms']:.3f}ms")
        
        return result
    
    def queue_size_performance_test(self, queue_desc, max_size: int = 100000, 
                                   step_size: int = 10000) -> Dict[str, Any]:
        """é˜Ÿåˆ—å¤§å°å¯¹æ€§èƒ½çš„å½±å“æµ‹è¯•"""
        print(f"\n--- é˜Ÿåˆ—å¤§å°æ€§èƒ½æµ‹è¯• ({queue_desc.queue_type}) ---")
        
        size_results = []
        
        for current_size in range(step_size, max_size + 1, step_size):
            print(f"æµ‹è¯•é˜Ÿåˆ—å¤§å°: {current_size}")
            
            # å¡«å……é˜Ÿåˆ—åˆ°æŒ‡å®šå¤§å°
            start_fill_time = time.time()
            for i in range(current_size):
                queue_desc.put(f"size_test_{i}")
            fill_duration = time.time() - start_fill_time
            
            # æµ‹é‡è¯»å–æ€§èƒ½
            start_read_time = time.time()
            for i in range(min(1000, current_size)):  # è¯»å–æœ€å¤š1000ä¸ªé¡¹ç›®
                queue_desc.get()
            read_duration = time.time() - start_read_time
            
            # æ¸…ç©ºå‰©ä½™é¡¹ç›®
            while not queue_desc.empty():
                try:
                    queue_desc.get_nowait()
                except:
                    break
            
            memory_usage = self.measure_memory_usage()
            
            size_results.append({
                'queue_size': current_size,
                'fill_duration': fill_duration,
                'fill_throughput': current_size / fill_duration,
                'read_duration': read_duration,
                'read_throughput': min(1000, current_size) / read_duration,
                'memory_mb': memory_usage['rss_mb']
            })
        
        result = {
            'queue_type': queue_desc.queue_type,
            'max_size': max_size,
            'step_size': step_size,
            'size_results': size_results
        }
        
        return result
    
    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è¿è¡Œé˜Ÿåˆ—æ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        # æµ‹è¯•é˜Ÿåˆ—ç±»å‹é…ç½®
        queue_configs = [
            {
                'name': 'Pythonçº¿ç¨‹é˜Ÿåˆ—',
                'creator': lambda: PythonQueueDescriptor("perf_python_thread", maxsize=50000, use_multiprocessing=False)
            },
            {
                'name': 'Pythonå¤šè¿›ç¨‹é˜Ÿåˆ—',
                'creator': lambda: PythonQueueDescriptor("perf_python_mp", maxsize=50000, use_multiprocessing=True)
            }
        ]
        
        # æ·»åŠ å¯é€‰é˜Ÿåˆ—ç±»å‹
        try:
            import ray
            queue_configs.append({
                'name': 'Rayé˜Ÿåˆ—',
                'creator': lambda: RayQueueDescriptor(queue_id="perf_ray", maxsize=50000)
            })
        except ImportError:
            print("âš ï¸ Rayä¸å¯ç”¨ï¼Œè·³è¿‡Rayé˜Ÿåˆ—æµ‹è¯•")
        
        try:
            queue_configs.append({
                'name': 'SAGEé˜Ÿåˆ—',
                'creator': lambda: SageQueueDescriptor(queue_id="perf_sage", maxsize=50*1024*1024)
            })
        except Exception:
            print("âš ï¸ SAGEé˜Ÿåˆ—ä¸å¯ç”¨ï¼Œè·³è¿‡SAGEé˜Ÿåˆ—æµ‹è¯•")
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        for config in queue_configs:
            queue_name = config['name']
            print(f"\n{'='*60}")
            print(f"æµ‹è¯•é˜Ÿåˆ—ç±»å‹: {queue_name}")
            print(f"{'='*60}")
            
            try:
                queue_desc = config['creator']()
                
                # è¿è¡Œå„ç§åŸºå‡†æµ‹è¯•
                self.results[queue_name] = {
                    'single_thread': self.single_thread_throughput_test(queue_desc, 5000),
                    'multi_thread': self.multi_thread_throughput_test(queue_desc, 4, 1250),
                    'latency': self.latency_test(queue_desc, 500),
                    'queue_size': self.queue_size_performance_test(queue_desc, 50000, 10000)
                }
                
                print(f"âœ… {queue_name} åŸºå‡†æµ‹è¯•å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ {queue_name} åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    
    def generate_benchmark_report(self):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print("åŸºå‡†æµ‹è¯•ç»“æœæŠ¥å‘Š")
        print(f"{'='*60}")
        
        if not self.results:
            print("âŒ æ²¡æœ‰æµ‹è¯•ç»“æœ")
            return
        
        # ç”Ÿæˆæ±‡æ€»è¡¨
        print("\nğŸ“Š æ€§èƒ½æ±‡æ€»è¡¨:")
        print(f"{'é˜Ÿåˆ—ç±»å‹':<15} {'å•çº¿ç¨‹å†™å…¥':<12} {'å•çº¿ç¨‹è¯»å–':<12} {'å¤šçº¿ç¨‹å†™å…¥':<12} {'å¤šçº¿ç¨‹è¯»å–':<12} {'å¹³å‡å»¶è¿Ÿ':<10}")
        print("-" * 80)
        
        for queue_name, results in self.results.items():
            single_thread = results.get('single_thread', {})
            multi_thread = results.get('multi_thread', {})
            latency = results.get('latency', {})
            
            write_throughput = f"{single_thread.get('write_throughput', 0):.0f}"
            read_throughput = f"{single_thread.get('read_throughput', 0):.0f}"
            mt_write_throughput = f"{multi_thread.get('avg_producer_throughput', 0):.0f}"
            mt_read_throughput = f"{multi_thread.get('avg_consumer_throughput', 0):.0f}"
            avg_latency = f"{latency.get('avg_latency_ms', 0):.2f}ms"
            
            print(f"{queue_name:<15} {write_throughput:<12} {read_throughput:<12} {mt_write_throughput:<12} {mt_read_throughput:<12} {avg_latency:<10}")
        
        # è¯¦ç»†æŠ¥å‘Š
        print(f"\nğŸ“ è¯¦ç»†æ€§èƒ½æŠ¥å‘Š:")
        for queue_name, results in self.results.items():
            print(f"\n--- {queue_name} ---")
            
            if 'single_thread' in results:
                st = results['single_thread']
                print(f"å•çº¿ç¨‹: å†™å…¥ {st['write_throughput']:.0f} items/sec, è¯»å– {st['read_throughput']:.0f} items/sec")
                print(f"         å†…å­˜ä½¿ç”¨ {st['memory_delta_mb']:.2f} MB")
            
            if 'multi_thread' in results:
                mt = results['multi_thread']
                print(f"å¤šçº¿ç¨‹: ç”Ÿäº§è€…å¹³å‡ {mt['avg_producer_throughput']:.0f} items/sec")
                print(f"        æ¶ˆè´¹è€…å¹³å‡ {mt['avg_consumer_throughput']:.0f} items/sec")
                print(f"        æ¶ˆè´¹æˆåŠŸç‡ {mt['total_consumed']/mt['total_items']*100:.1f}%")
            
            if 'latency' in results:
                lat = results['latency']
                print(f"å»¶è¿Ÿ: å¹³å‡ {lat['avg_latency_ms']:.3f}ms, P95 {lat['p95_latency_ms']:.3f}ms, P99 {lat['p99_latency_ms']:.3f}ms")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
        self._save_detailed_report()
    
    def _save_detailed_report(self):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        import json
        from pathlib import Path
        
        report_file = Path("benchmark_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file.absolute()}")


def run_performance_benchmarks():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    benchmark = PerformanceBenchmark()
    
    try:
        benchmark.run_all_benchmarks()
        benchmark.generate_benchmark_report()
        return True
    except Exception as e:
        print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_performance_benchmarks()
    sys.exit(0 if success else 1)
