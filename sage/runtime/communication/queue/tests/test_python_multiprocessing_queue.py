#!/usr/bin/env python3
"""
Pythoné˜Ÿåˆ—å¤šè¿›ç¨‹æ”¯æŒæµ‹è¯•

æµ‹è¯• PythonQueueDescriptor å¯¹ multiprocessing.Queue çš„æ”¯æŒ
éªŒè¯ï¼š
1. multiprocessing.Queue çš„åˆ›å»ºå’Œä½¿ç”¨
2. è¿›ç¨‹é—´é˜Ÿåˆ—å…±äº«
3. å¤šè¿›ç¨‹å¹¶å‘è¯»å†™
4. é˜Ÿåˆ—æè¿°ç¬¦çš„è·¨è¿›ç¨‹ä¼ é€’
"""

import sys
import os
import time
import multiprocessing
import queue
from multiprocessing import Process, Manager
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/api-rework')

try:
    from sage.runtime.communication.queue import (
        PythonQueueDescriptor,
    )
    print("âœ“ æˆåŠŸå¯¼å…¥Pythoné˜Ÿåˆ—æè¿°ç¬¦")
except ImportError as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


# ============ å¤šè¿›ç¨‹å·¥ä½œå‡½æ•° ============

def mp_producer_worker(queue_desc_data: Dict[str, Any], worker_id: int, num_items: int, shared_stats: Dict):
    """å¤šè¿›ç¨‹ç”Ÿäº§è€…å·¥ä½œå‡½æ•°"""
    try:
        # é‡å»ºé˜Ÿåˆ—æè¿°ç¬¦ - ä½¿ç”¨ resolve_descriptor æ›¿ä»£ from_dict
        from sage.runtime.communication.queue import resolve_descriptor
        queue_desc = resolve_descriptor(queue_desc_data)
        
        produced_count = 0
        start_time = time.time()
        
        for i in range(num_items):
            item = f"mp_producer_{worker_id}_item_{i}_{time.time()}"
            queue_desc.put(item)
            produced_count += 1
            
            # æ¯10ä¸ªé¡¹ç›®è®°å½•ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 10 == 0:
                print(f"ç”Ÿäº§è€… {worker_id}: å·²ç”Ÿäº§ {i + 1}/{num_items}")
        
        end_time = time.time()
        duration = end_time - start_time
        
        # æ›´æ–°å…±äº«ç»Ÿè®¡
        shared_stats[f'producer_{worker_id}'] = {
            'produced': produced_count,
            'duration': duration,
            'rate': produced_count / duration if duration > 0 else 0
        }
        
        return f"producer_{worker_id}_completed_{produced_count}"
        
    except Exception as e:
        shared_stats[f'producer_{worker_id}'] = {'error': str(e)}
        return f"producer_{worker_id}_error_{e}"


def mp_consumer_worker(queue_desc_data: Dict[str, Any], worker_id: int, target_items: int, 
                      timeout_per_item: float, shared_stats: Dict):
    """å¤šè¿›ç¨‹æ¶ˆè´¹è€…å·¥ä½œå‡½æ•°"""
    try:
        # é‡å»ºé˜Ÿåˆ—æè¿°ç¬¦
        queue_desc = PythonQueueDescriptor.from_dict(queue_desc_data)
        
        consumed_items = []
        start_time = time.time()
        
        while len(consumed_items) < target_items:
            try:
                item = queue_desc.get(timeout=timeout_per_item)
                consumed_items.append(item)
                
                # æ¯10ä¸ªé¡¹ç›®è®°å½•ä¸€æ¬¡è¿›åº¦
                if len(consumed_items) % 10 == 0:
                    print(f"æ¶ˆè´¹è€… {worker_id}: å·²æ¶ˆè´¹ {len(consumed_items)}/{target_items}")
                    
            except queue.Empty:
                print(f"æ¶ˆè´¹è€… {worker_id}: é˜Ÿåˆ—ç©ºï¼Œç­‰å¾…...")
                continue
            except Exception as e:
                print(f"æ¶ˆè´¹è€… {worker_id}: è·å–é¡¹ç›®æ—¶å‡ºé”™: {e}")
                break
        
        end_time = time.time()
        duration = end_time - start_time
        
        # æ›´æ–°å…±äº«ç»Ÿè®¡
        shared_stats[f'consumer_{worker_id}'] = {
            'consumed': len(consumed_items),
            'duration': duration,
            'rate': len(consumed_items) / duration if duration > 0 else 0,
            'items_sample': consumed_items[:5]  # ä¿å­˜å‰5ä¸ªé¡¹ç›®ä½œä¸ºæ ·æœ¬
        }
        
        return consumed_items
        
    except Exception as e:
        shared_stats[f'consumer_{worker_id}'] = {'error': str(e)}
        return []


def mp_mixed_worker(queue_desc_data: Dict[str, Any], worker_id: int, num_operations: int, shared_stats: Dict):
    """å¤šè¿›ç¨‹æ··åˆè¯»å†™å·¥ä½œå‡½æ•°"""
    try:
        # é‡å»ºé˜Ÿåˆ—æè¿°ç¬¦
        queue_desc = PythonQueueDescriptor.from_dict(queue_desc_data)
        
        put_count = 0
        get_count = 0
        start_time = time.time()
        
        for i in range(num_operations):
            if i % 3 == 0:  # å†™æ“ä½œ (1/3 çš„æ“ä½œ)
                try:
                    item = f"mixed_{worker_id}_{i}_{time.time()}"
                    queue_desc.put(item)
                    put_count += 1
                except Exception as e:
                    print(f"æ··åˆå·¥ä½œè€… {worker_id}: å†™å…¥å‡ºé”™: {e}")
            else:  # è¯»æ“ä½œ (2/3 çš„æ“ä½œ)
                try:
                    item = queue_desc.get(timeout=0.1)
                    get_count += 1
                except queue.Empty:
                    # é˜Ÿåˆ—ç©ºæ—¶è·³è¿‡
                    pass
                except Exception as e:
                    print(f"æ··åˆå·¥ä½œè€… {worker_id}: è¯»å–å‡ºé”™: {e}")
        
        end_time = time.time()
        duration = end_time - start_time
        total_ops = put_count + get_count
        
        # æ›´æ–°å…±äº«ç»Ÿè®¡
        shared_stats[f'mixed_{worker_id}'] = {
            'put_count': put_count,
            'get_count': get_count,
            'total_ops': total_ops,
            'duration': duration,
            'rate': total_ops / duration if duration > 0 else 0
        }
        
        return {'put': put_count, 'get': get_count, 'total': total_ops}
        
    except Exception as e:
        shared_stats[f'mixed_{worker_id}'] = {'error': str(e)}
        return {'error': str(e)}


def mp_queue_monitor(queue_desc_data: Dict[str, Any], monitor_duration: int, shared_stats: Dict):
    """å¤šè¿›ç¨‹é˜Ÿåˆ—ç›‘æ§å‡½æ•°"""
    try:
        # é‡å»ºé˜Ÿåˆ—æè¿°ç¬¦
        queue_desc = PythonQueueDescriptor.from_dict(queue_desc_data)
        
        monitor_data = []
        start_time = time.time()
        
        while time.time() - start_time < monitor_duration:
            try:
                current_time = time.time()
                size = queue_desc.qsize()
                empty = queue_desc.empty()
                
                monitor_data.append({
                    'timestamp': current_time,
                    'size': size,
                    'empty': empty
                })
                
                print(f"é˜Ÿåˆ—ç›‘æ§: å¤§å°={size}, ç©º={empty}")
                time.sleep(1)  # æ¯ç§’ç›‘æ§ä¸€æ¬¡
                
            except Exception as e:
                print(f"ç›‘æ§å‡ºé”™: {e}")
                break
        
        # æ›´æ–°å…±äº«ç»Ÿè®¡
        shared_stats['monitor'] = {
            'data_points': len(monitor_data),
            'duration': time.time() - start_time,
            'max_size': max(point['size'] for point in monitor_data) if monitor_data else 0,
            'avg_size': sum(point['size'] for point in monitor_data) / len(monitor_data) if monitor_data else 0
        }
        
        return monitor_data
        
    except Exception as e:
        shared_stats['monitor'] = {'error': str(e)}
        return []


# ============ æµ‹è¯•ç±» ============

class TestPythonQueueMultiprocessing:
    """Pythoné˜Ÿåˆ—å¤šè¿›ç¨‹æµ‹è¯•"""
    
    def test_multiprocessing_queue_creation(self):
        """æµ‹è¯•multiprocessingé˜Ÿåˆ—åˆ›å»º"""
        print("\n=== æµ‹è¯•multiprocessingé˜Ÿåˆ—åˆ›å»º ===")
        
        # åˆ›å»ºä½¿ç”¨multiprocessingçš„é˜Ÿåˆ—æè¿°ç¬¦
        mp_queue_desc = PythonQueueDescriptor(
            queue_id="test_mp_queue",
            maxsize=100,
            use_multiprocessing=True
        )
        
        print(f"é˜Ÿåˆ—ID: {mp_queue_desc.queue_id}")
        print(f"é˜Ÿåˆ—ç±»å‹: {mp_queue_desc.queue_type}")
        print(f"å¯åºåˆ—åŒ–: {mp_queue_desc.can_serialize}")
        print(f"ä½¿ç”¨å¤šè¿›ç¨‹: {mp_queue_desc.metadata.get('use_multiprocessing')}")
        
        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        mp_queue_desc.put("test_item")
        assert mp_queue_desc.qsize() == 1
        assert not mp_queue_desc.empty()
        
        item = mp_queue_desc.get()
        assert item == "test_item"
        assert mp_queue_desc.empty()
        
        print("âœ“ multiprocessingé˜Ÿåˆ—åˆ›å»ºæµ‹è¯•é€šè¿‡")
    
    def test_simple_producer_consumer(self):
        """æµ‹è¯•ç®€å•çš„ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼"""
        print("\n=== æµ‹è¯•ç®€å•ç”Ÿäº§è€…-æ¶ˆè´¹è€… ===")
        
        # åˆ›å»ºå…±äº«é˜Ÿåˆ—
        with Manager() as manager:
            shared_stats = manager.dict()
            
            # åˆ›å»ºmultiprocessingé˜Ÿåˆ—æè¿°ç¬¦
            mp_queue_desc = PythonQueueDescriptor(
                queue_id="test_simple_pc",
                maxsize=50,
                use_multiprocessing=True
            )
            
            queue_data = mp_queue_desc.to_dict()
            
            # å¯åŠ¨ä¸€ä¸ªç”Ÿäº§è€…å’Œä¸€ä¸ªæ¶ˆè´¹è€…
            producer_process = Process(
                target=mp_producer_worker,
                args=(queue_data, 1, 10, shared_stats)
            )
            
            consumer_process = Process(
                target=mp_consumer_worker,
                args=(queue_data, 1, 10, 5.0, shared_stats)
            )
            
            print("å¯åŠ¨ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…è¿›ç¨‹...")
            producer_process.start()
            time.sleep(1)  # è®©ç”Ÿäº§è€…å…ˆå¼€å§‹
            consumer_process.start()
            
            # ç­‰å¾…å®Œæˆ
            producer_process.join(timeout=10)
            consumer_process.join(timeout=10)
            
            # æ£€æŸ¥ç»“æœ
            print(f"å…±äº«ç»Ÿè®¡: {dict(shared_stats)}")
            
            assert 'producer_1' in shared_stats
            assert 'consumer_1' in shared_stats
            
            producer_stats = shared_stats['producer_1']
            consumer_stats = shared_stats['consumer_1']
            
            print(f"ç”Ÿäº§è€…ç»Ÿè®¡: {producer_stats}")
            print(f"æ¶ˆè´¹è€…ç»Ÿè®¡: {consumer_stats}")
            
            assert producer_stats.get('produced', 0) > 0
            assert consumer_stats.get('consumed', 0) > 0
        
        print("âœ“ ç®€å•ç”Ÿäº§è€…-æ¶ˆè´¹è€…æµ‹è¯•é€šè¿‡")
    
    def test_multiple_processes_concurrent(self):
        """æµ‹è¯•å¤šè¿›ç¨‹å¹¶å‘è®¿é—®"""
        print("\n=== æµ‹è¯•å¤šè¿›ç¨‹å¹¶å‘è®¿é—® ===")
        
        with Manager() as manager:
            shared_stats = manager.dict()
            
            # åˆ›å»ºå¤§å®¹é‡é˜Ÿåˆ—
            mp_queue_desc = PythonQueueDescriptor(
                queue_id="test_concurrent_mp",
                maxsize=200,
                use_multiprocessing=True
            )
            
            queue_data = mp_queue_desc.to_dict()
            
            # é…ç½®
            num_producers = 3
            num_consumers = 2
            items_per_producer = 15
            total_items = num_producers * items_per_producer
            items_per_consumer = total_items // num_consumers
            
            print(f"é…ç½®: {num_producers}ä¸ªç”Ÿäº§è€…è¿›ç¨‹, {num_consumers}ä¸ªæ¶ˆè´¹è€…è¿›ç¨‹")
            print(f"æ¯ä¸ªç”Ÿäº§è€…ç”Ÿäº§{items_per_producer}ä¸ªé¡¹ç›®, æ€»å…±{total_items}ä¸ª")
            
            # åˆ›å»ºè¿›ç¨‹åˆ—è¡¨
            processes = []
            
            # åˆ›å»ºç”Ÿäº§è€…è¿›ç¨‹
            for i in range(num_producers):
                process = Process(
                    target=mp_producer_worker,
                    args=(queue_data, i, items_per_producer, shared_stats)
                )
                processes.append(process)
            
            # åˆ›å»ºæ¶ˆè´¹è€…è¿›ç¨‹
            for i in range(num_consumers):
                process = Process(
                    target=mp_consumer_worker,
                    args=(queue_data, i, items_per_consumer, 3.0, shared_stats)
                )
                processes.append(process)
            
            # å¯åŠ¨æ‰€æœ‰è¿›ç¨‹
            start_time = time.time()
            for process in processes:
                process.start()
            
            # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
            for process in processes:
                process.join(timeout=30)
            
            end_time = time.time()
            total_duration = end_time - start_time
            
            # åˆ†æç»“æœ
            print(f"\nå¤šè¿›ç¨‹å¹¶å‘æµ‹è¯•ç»“æœ (è€—æ—¶: {total_duration:.2f}ç§’):")
            
            total_produced = 0
            total_consumed = 0
            
            for key, stats in shared_stats.items():
                if key.startswith('producer_'):
                    produced = stats.get('produced', 0)
                    rate = stats.get('rate', 0)
                    total_produced += produced
                    print(f"  {key}: ç”Ÿäº§ {produced} é¡¹ç›®, é€Ÿç‡ {rate:.2f} items/sec")
                elif key.startswith('consumer_'):
                    consumed = stats.get('consumed', 0)
                    rate = stats.get('rate', 0)
                    total_consumed += consumed
                    print(f"  {key}: æ¶ˆè´¹ {consumed} é¡¹ç›®, é€Ÿç‡ {rate:.2f} items/sec")
            
            print(f"\næ€»è®¡: ç”Ÿäº§ {total_produced}, æ¶ˆè´¹ {total_consumed}")
            
            # éªŒè¯
            assert total_produced == total_items, f"åº”è¯¥ç”Ÿäº§ {total_items} ä¸ªé¡¹ç›®ï¼Œå®é™…ç”Ÿäº§ {total_produced}"
            assert total_consumed > 0, "åº”è¯¥æ¶ˆè´¹äº†ä¸€äº›é¡¹ç›®"
        
        print("âœ“ å¤šè¿›ç¨‹å¹¶å‘è®¿é—®æµ‹è¯•é€šè¿‡")
    
    def test_mixed_operations_with_monitoring(self):
        """æµ‹è¯•æ··åˆæ“ä½œä¸ç›‘æ§"""
        print("\n=== æµ‹è¯•æ··åˆæ“ä½œä¸ç›‘æ§ ===")
        
        with Manager() as manager:
            shared_stats = manager.dict()
            
            # åˆ›å»ºé˜Ÿåˆ—
            mp_queue_desc = PythonQueueDescriptor(
                queue_id="test_mixed_monitor",
                maxsize=100,
                use_multiprocessing=True
            )
            
            # é¢„å¡«å……ä¸€äº›æ•°æ®
            for i in range(20):
                mp_queue_desc.put(f"prefill_item_{i}")
            
            queue_data = mp_queue_desc.to_dict()
            
            # é…ç½®
            num_mixed_workers = 4
            operations_per_worker = 30
            monitor_duration = 15
            
            print(f"é…ç½®: {num_mixed_workers}ä¸ªæ··åˆå·¥ä½œè¿›ç¨‹, ç›‘æ§{monitor_duration}ç§’")
            
            # åˆ›å»ºè¿›ç¨‹
            processes = []
            
            # åˆ›å»ºç›‘æ§è¿›ç¨‹
            monitor_process = Process(
                target=mp_queue_monitor,
                args=(queue_data, monitor_duration, shared_stats)
            )
            processes.append(monitor_process)
            
            # åˆ›å»ºæ··åˆå·¥ä½œè¿›ç¨‹
            for i in range(num_mixed_workers):
                process = Process(
                    target=mp_mixed_worker,
                    args=(queue_data, i, operations_per_worker, shared_stats)
                )
                processes.append(process)
            
            # å¯åŠ¨æ‰€æœ‰è¿›ç¨‹
            start_time = time.time()
            for process in processes:
                process.start()
            
            # ç­‰å¾…å®Œæˆ
            for process in processes:
                process.join(timeout=20)
            
            end_time = time.time()
            
            # åˆ†æç»“æœ
            print(f"\næ··åˆæ“ä½œæµ‹è¯•ç»“æœ:")
            
            total_puts = 0
            total_gets = 0
            
            for key, stats in shared_stats.items():
                if key.startswith('mixed_'):
                    put_count = stats.get('put_count', 0)
                    get_count = stats.get('get_count', 0)
                    rate = stats.get('rate', 0)
                    total_puts += put_count
                    total_gets += get_count
                    print(f"  {key}: PUT={put_count}, GET={get_count}, é€Ÿç‡={rate:.2f} ops/sec")
                elif key == 'monitor':
                    print(f"  ç›‘æ§ç»Ÿè®¡: {stats}")
            
            print(f"\næ€»è®¡: PUT={total_puts}, GET={total_gets}")
            
            # éªŒè¯
            assert total_puts > 0, "åº”è¯¥æ‰§è¡Œäº†ä¸€äº›PUTæ“ä½œ"
            assert total_gets > 0, "åº”è¯¥æ‰§è¡Œäº†ä¸€äº›GETæ“ä½œ"
            assert 'monitor' in shared_stats, "åº”è¯¥æœ‰ç›‘æ§æ•°æ®"
        
        print("âœ“ æ··åˆæ“ä½œä¸ç›‘æ§æµ‹è¯•é€šè¿‡")
    
    def test_queue_descriptor_serialization_across_processes(self):
        """æµ‹è¯•é˜Ÿåˆ—æè¿°ç¬¦è·¨è¿›ç¨‹åºåˆ—åŒ–"""
        print("\n=== æµ‹è¯•é˜Ÿåˆ—æè¿°ç¬¦è·¨è¿›ç¨‹åºåˆ—åŒ– ===")
        
        # åˆ›å»ºé˜Ÿåˆ—æè¿°ç¬¦
        original_desc = PythonQueueDescriptor(
            queue_id="test_serialization",
            maxsize=50,
            use_multiprocessing=True
        )
        
        # æ·»åŠ ä¸€äº›æ•°æ®
        test_items = ["serial_item1", "serial_item2", "serial_item3"]
        for item in test_items:
            original_desc.put(item)
        
        print(f"åŸå§‹é˜Ÿåˆ—å¤§å°: {original_desc.qsize()}")
        
        # åºåˆ—åŒ–
        queue_dict = original_desc.to_dict()
        print(f"åºåˆ—åŒ–æ•°æ®: {queue_dict}")
        
        # åœ¨å­è¿›ç¨‹ä¸­ååºåˆ—åŒ–å¹¶æ“ä½œ
        def subprocess_operation(queue_data):
            """å­è¿›ç¨‹æ“ä½œå‡½æ•°"""
            try:
                # ååºåˆ—åŒ–
                restored_desc = PythonQueueDescriptor.from_dict(queue_data)
                
                # è¯»å–æ‰€æœ‰æ•°æ®
                items = []
                while not restored_desc.empty():
                    try:
                        item = restored_desc.get_nowait()
                        items.append(item)
                    except queue.Empty:
                        break
                
                # å†™å…¥æ–°æ•°æ®
                restored_desc.put("subprocess_added_item")
                
                return {
                    'retrieved_items': items,
                    'final_size': restored_desc.qsize()
                }
            except Exception as e:
                return {'error': str(e)}
        
        # å¯åŠ¨å­è¿›ç¨‹
        with ProcessPoolExecutor(max_workers=1) as executor:
            future = executor.submit(subprocess_operation, queue_dict)
            result = future.result()
        
        print(f"å­è¿›ç¨‹æ“ä½œç»“æœ: {result}")
        
        # éªŒè¯ç»“æœ
        assert 'retrieved_items' in result, "åº”è¯¥æœ‰æ£€ç´¢åˆ°çš„é¡¹ç›®"
        assert len(result['retrieved_items']) == len(test_items), "åº”è¯¥æ£€ç´¢åˆ°æ‰€æœ‰åŸå§‹é¡¹ç›®"
        assert result['final_size'] == 1, "æœ€ç»ˆåº”è¯¥æœ‰1ä¸ªé¡¹ç›®ï¼ˆå­è¿›ç¨‹æ·»åŠ çš„ï¼‰"
        
        # åœ¨ä¸»è¿›ç¨‹ä¸­éªŒè¯
        remaining_item = original_desc.get()
        assert remaining_item == "subprocess_added_item", "åº”è¯¥èƒ½è·å–å­è¿›ç¨‹æ·»åŠ çš„é¡¹ç›®"
        
        print("âœ“ è·¨è¿›ç¨‹åºåˆ—åŒ–æµ‹è¯•é€šè¿‡")


def run_multiprocessing_tests():
    """è¿è¡Œå¤šè¿›ç¨‹æµ‹è¯•"""
    print("å¼€å§‹è¿è¡ŒPythoné˜Ÿåˆ—å¤šè¿›ç¨‹æµ‹è¯•...")
    
    # è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•
    if hasattr(multiprocessing, 'set_start_method'):
        try:
            multiprocessing.set_start_method('spawn', force=True)
        except RuntimeError:
            pass  # å·²ç»è®¾ç½®è¿‡äº†
    
    test_suite = TestPythonQueueMultiprocessing()
    
    try:
        test_suite.test_multiprocessing_queue_creation()
        test_suite.test_simple_producer_consumer()
        test_suite.test_multiple_processes_concurrent()
        test_suite.test_mixed_operations_with_monitoring()
        test_suite.test_queue_descriptor_serialization_across_processes()
        
        print("\nğŸ‰ æ‰€æœ‰Pythoné˜Ÿåˆ—å¤šè¿›ç¨‹æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ å¤šè¿›ç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_multiprocessing_tests()
    if not success:
        sys.exit(1)
