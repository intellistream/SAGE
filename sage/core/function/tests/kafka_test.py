# import pytest
# import time
# import threading
# import json
# from typing import Any, Dict, List
# from pathlib import Path
# import tempfile
# from unittest.mock import Mock, patch, MagicMock

# from sage.core.api.local_environment import LocalEnvironment
# from sage.core.function.kafka_source import KafkaSourceFunction
# from sage.core.function.base_function import BaseFunction
# from sage.core.function.sink_function import SinkFunction


# class MockKafkaMessage:
#     """æ¨¡æ‹ŸKafkaæ¶ˆæ¯"""
#     def __init__(self, value, key=None, timestamp=None, partition=0, offset=0):
#         self.value = value
#         self.key = key if key is not None else f"key_{offset}".encode('utf-8')
#         self.timestamp = timestamp if timestamp is not None else int(time.time() * 1000)
#         self.partition = partition
#         self.offset = offset


# class MockKafkaConsumer:
#     """æ¨¡æ‹ŸKafkaConsumer"""
#     def __init__(self, *topics, **kwargs):
#         self.topics = topics
#         self.config = kwargs
#         self.messages = []
#         self.poll_count = 0
#         self._closed = False
#         # ä¿å­˜ååºåˆ—åŒ–å™¨
#         self.value_deserializer = kwargs.get('value_deserializer', lambda x: x)
        
#     def poll(self, timeout_ms=1000, max_records=500, update_offsets=True):
#         """æ¨¡æ‹Ÿpollæ–¹æ³•"""
#         if self._closed or self.poll_count >= 3:  # é™åˆ¶pollæ¬¡æ•°é¿å…æ— é™å¾ªç¯
#             return {}
        
#         self.poll_count += 1
        
#         if self.messages:
#             # è¿”å›ä¸€æ‰¹æ¶ˆæ¯ï¼Œåº”ç”¨ååºåˆ—åŒ–å™¨
#             topic_partition = Mock()
#             messages = []
#             for msg in self.messages[:max_records]:
#                 # åˆ›å»ºæ–°çš„æ¶ˆæ¯å¯¹è±¡ï¼Œåº”ç”¨ååºåˆ—åŒ–å™¨
#                 deserialized_msg = Mock()
#                 # åº”ç”¨ååºåˆ—åŒ–å™¨åˆ°åŸå§‹æ¶ˆæ¯çš„value
#                 if callable(self.value_deserializer):
#                     deserialized_msg.value = self.value_deserializer(msg.value)
#                 else:
#                     deserialized_msg.value = msg.value
#                 deserialized_msg.key = msg.key
#                 deserialized_msg.timestamp = msg.timestamp
#                 deserialized_msg.partition = msg.partition
#                 deserialized_msg.offset = msg.offset
#                 messages.append(deserialized_msg)
            
#             self.messages = self.messages[max_records:]
#             return {topic_partition: messages}
        
#         return {}
    
#     def close(self):
#         """å…³é—­æ¶ˆè´¹è€…"""
#         self._closed = True
    
#     def add_messages(self, messages):
#         """æ·»åŠ æ¨¡æ‹Ÿæ¶ˆæ¯"""
#         self.messages.extend(messages)


# class KafkaDebugProcessor(BaseFunction):
#     """å¤„ç†Kafkaæ¶ˆæ¯çš„è°ƒè¯•å‡½æ•°"""
    
#     def __init__(self, ctx=None, **kwargs):
#         super().__init__(ctx=ctx, **kwargs)
#         self.processed_count = 0
    
#     def execute(self, data: Any):
#         if data is None:
#             return None
        
#         self.processed_count += 1
        
#         # æå–Kafkaæ¶ˆæ¯ä¿¡æ¯
#         value = data.get('value', {}) if isinstance(data, dict) else data
#         timestamp = data.get('timestamp', 0) if isinstance(data, dict) else 0
#         partition = data.get('partition', 0) if isinstance(data, dict) else 0
#         offset = data.get('offset', 0) if isinstance(data, dict) else 0
        
#         result = {
#             "type": "processed_kafka_message",
#             "processed_count": self.processed_count,
#             "original_value": value,
#             "kafka_metadata": {
#                 "timestamp": timestamp,
#                 "partition": partition,
#                 "offset": offset
#             }
#         }
        
#         if self.ctx:
#             self.logger.info(f"Processed Kafka message #{self.processed_count}: {value}")
        
#         return result


# class KafkaTestSink(SinkFunction):
#     """Kafkaæµ‹è¯•ç»“æœæ”¶é›†å™¨"""
    
#     _lock = threading.Lock()
#     _received_data = {}
    
#     def __init__(self, ctx=None, **kwargs):
#         super().__init__(ctx=ctx, **kwargs)
#         self.parallel_index = None
#         self.received_count = 0
    
#     def execute(self, data: Any):
#         if self.ctx:
#             self.parallel_index = self.ctx.parallel_index
        
#         if data is None:
#             return None
        
#         with self._lock:
#             if self.parallel_index not in self._received_data:
#                 self._received_data[self.parallel_index] = []
#             self._received_data[self.parallel_index].append(data)
        
#         self.received_count += 1
        
#         if self.ctx:
#             self.logger.info(f"[Instance {self.parallel_index}] Received: {data}")
        
#         print(f"ğŸ” [Instance {self.parallel_index}] Kafka Test Sink received: {data}")
        
#         return data
    
#     @classmethod
#     def read_results(cls):
#         """è¯»å–æµ‹è¯•ç»“æœ"""
#         with cls._lock:
#             results = dict(cls._received_data)
#         return results
    
#     @classmethod
#     def clear_results(cls):
#         """æ¸…ç†ç»“æœ"""
#         with cls._lock:
#             cls._received_data.clear()


# class TestKafkaSourceFunctionality:
#     """æµ‹è¯•Kafka SourceåŠŸèƒ½"""
    
#     def setup_method(self):
#         """æµ‹è¯•å‰æ¸…ç†"""
#         KafkaTestSink.clear_results()
    
#     @patch('kafka.KafkaConsumer')
#     def test_basic_kafka_source_pipeline(self, mock_kafka_consumer_class):
#         """æµ‹è¯•åŸºæœ¬çš„Kafka Sourceæµæ°´çº¿"""
#         print("\nğŸš€ Testing Basic Kafka Source Pipeline")
        
#         # åˆ›å»ºæ¨¡æ‹Ÿæ¶ˆæ¯ - å¯¹äºJSONååºåˆ—åŒ–å™¨ï¼ŒåŸå§‹æ•°æ®éœ€è¦æ˜¯JSONå­—ç¬¦ä¸²çš„bytes
#         test_messages = [
#             MockKafkaMessage(
#                 value=json.dumps({"event": "user_login", "user_id": "user1", "timestamp": 1640995200}).encode('utf-8'),
#                 offset=0
#             ),
#             MockKafkaMessage(
#                 value=json.dumps({"event": "page_view", "user_id": "user1", "page": "/home"}).encode('utf-8'),
#                 offset=1
#             ),
#             MockKafkaMessage(
#                 value=json.dumps({"event": "user_logout", "user_id": "user1"}).encode('utf-8'),
#                 offset=2
#             ),
#         ]
        
#         # è®¾ç½®mock
#         mock_consumer = MockKafkaConsumer()
#         mock_consumer.add_messages(test_messages)
        
#         # åˆ›å»ºä¸€ä¸ªå¸¦æœ‰è°ƒè¯•åŠŸèƒ½çš„MockKafkaConsumerå®ä¾‹
#         def create_mock_consumer(*topics, **kwargs):
#             consumer = MockKafkaConsumer(*topics, **kwargs)
#             consumer.add_messages(test_messages)
#             # åŒ…è£…ååºåˆ—åŒ–å™¨ç”¨äºè°ƒè¯•
#             if 'value_deserializer' in kwargs and callable(kwargs['value_deserializer']):
#                 original_deserializer = kwargs['value_deserializer']
#                 def debug_deserializer(raw_data):
#                     result = original_deserializer(raw_data)
#                     print(f"ğŸ”§ Debug: Deserializing {raw_data} -> {result}")
#                     return result
#                 consumer.value_deserializer = debug_deserializer
#             return consumer
            
#         mock_kafka_consumer_class.side_effect = create_mock_consumer
        
#         # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
#         env = LocalEnvironment("basic_kafka_test")
        
#         # æ„å»ºæµæ°´çº¿
#         kafka_stream = env.from_kafka_source(
#             bootstrap_servers="localhost:9092",
#             topic="test_topic",
#             group_id="test_group",
#             value_deserializer="json",
#             buffer_size=100
#         )
        
#         result_stream = (
#             kafka_stream
#             .map(KafkaDebugProcessor)
#             .sink(KafkaTestSink, parallelism=1)
#         )
        
#         print("ğŸ“Š Pipeline: KafkaSource -> map(KafkaDebugProcessor) -> Sink")
#         print("ğŸ¯ Expected: Process JSON messages from Kafka\n")
        
#         try:
#             env.submit()
#             time.sleep(2)  # ç­‰å¾…å¤„ç†æ¶ˆæ¯
#         finally:
#             env.close()
        
#         time.sleep(0.5)
#         self._verify_basic_kafka_results(test_messages)
    
#     @patch('kafka.KafkaConsumer')
#     def test_kafka_source_with_string_deserializer(self, mock_kafka_consumer_class):
#         """æµ‹è¯•å­—ç¬¦ä¸²ååºåˆ—åŒ–çš„Kafka Source"""
#         print("\nğŸš€ Testing Kafka Source with String Deserializer")
        
#         # åˆ›å»ºå­—ç¬¦ä¸²æ¶ˆæ¯
#         test_messages = [
#             MockKafkaMessage(value="Hello Kafka", offset=0),
#             MockKafkaMessage(value="String message 1", offset=1),
#             MockKafkaMessage(value="String message 2", offset=2),
#         ]
        
#         mock_consumer = MockKafkaConsumer()
#         mock_consumer.add_messages(test_messages)
#         mock_kafka_consumer_class.return_value = mock_consumer
        
#         env = LocalEnvironment("string_kafka_test")
        
#         kafka_stream = env.from_kafka_source(
#             bootstrap_servers="localhost:9092",
#             topic="string_topic",
#             group_id="string_group",
#             value_deserializer="string",
#             buffer_size=50
#         )
        
#         result_stream = (
#             kafka_stream
#             .map(KafkaDebugProcessor)
#             .sink(KafkaTestSink, parallelism=1)
#         )
        
#         print("ğŸ“Š Pipeline: KafkaSource(string) -> map(KafkaDebugProcessor) -> Sink")
#         print("ğŸ¯ Expected: Process string messages from Kafka\n")
        
#         try:
#             env.submit()
#             time.sleep(2)
#         finally:
#             env.close()
        
#         time.sleep(0.5)
#         self._verify_string_kafka_results(test_messages)
    
#     @patch('kafka.KafkaConsumer')
#     def test_kafka_source_custom_deserializer(self, mock_kafka_consumer_class):
#         """æµ‹è¯•è‡ªå®šä¹‰ååºåˆ—åŒ–å™¨çš„Kafka Source"""
#         print("\nğŸš€ Testing Kafka Source with Custom Deserializer")
        
#         def custom_deserializer(raw_data):
#             """è‡ªå®šä¹‰ååºåˆ—åŒ–ï¼šæ·»åŠ å‰ç¼€"""
#             print(f"ğŸ”§ Custom deserializer called with: {raw_data}")
#             if isinstance(raw_data, bytes):
#                 data = raw_data.decode('utf-8')
#             else:
#                 data = str(raw_data)
#             result = f"CUSTOM_{data}"
#             print(f"ğŸ”§ Deserializer output: {result}")
#             return result
        
#         # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯ - æ³¨æ„è¿™é‡Œvalueéœ€è¦æ˜¯bytes
#         test_messages = [
#             MockKafkaMessage(value=b"message1", offset=0),
#             MockKafkaMessage(value=b"message2", offset=1),
#         ]
        
#         # åˆ›å»ºmock consumerå¹¶é¢„è®¾ååºåˆ—åŒ–å™¨
#         mock_consumer = MockKafkaConsumer()
#         mock_consumer.add_messages(test_messages)
#         mock_consumer.value_deserializer = custom_deserializer
        
#         mock_kafka_consumer_class.return_value = mock_consumer
        
#         env = LocalEnvironment("custom_deserializer_kafka_test")
        
#         kafka_stream = env.from_kafka_source(
#             bootstrap_servers="localhost:9092",
#             topic="custom_topic",
#             group_id="custom_group",
#             value_deserializer=custom_deserializer,
#             buffer_size=50
#         )
        
#         result_stream = (
#             kafka_stream
#             .map(KafkaDebugProcessor)
#             .sink(KafkaTestSink, parallelism=1)
#         )
        
#         print("ğŸ“Š Pipeline: KafkaSource(custom) -> map(KafkaDebugProcessor) -> Sink")
#         print("ğŸ¯ Expected: Process custom deserialized messages\n")
        
#         try:
#             env.submit()
#             time.sleep(2)
#         finally:
#             env.close()
        
#         time.sleep(0.5)
#         self._verify_custom_kafka_results()
    
#     @patch('kafka.KafkaConsumer')
#     def test_kafka_source_parallel_processing(self, mock_kafka_consumer_class):
#         """æµ‹è¯•Kafka Sourceå¹¶è¡Œå¤„ç†"""
#         print("\nğŸš€ Testing Kafka Source Parallel Processing")
        
#         # åˆ›å»ºæ›´å¤šæ¶ˆæ¯ç”¨äºå¹¶è¡Œå¤„ç†
#         test_messages = []
#         for i in range(10):
#             test_messages.append(
#                 MockKafkaMessage(
#                     value={"batch": "parallel_test", "message_id": i, "data": f"data_{i}"},
#                     offset=i
#                 )
#             )
        
#         mock_consumer = MockKafkaConsumer()
#         mock_consumer.add_messages(test_messages)
#         mock_kafka_consumer_class.return_value = mock_consumer
        
#         env = LocalEnvironment("parallel_kafka_test")
        
#         kafka_stream = env.from_kafka_source(
#             bootstrap_servers="localhost:9092",
#             topic="parallel_topic",
#             group_id="parallel_group",
#             value_deserializer="json"
#         )
        
#         result_stream = (
#             kafka_stream
#             .map(KafkaDebugProcessor, parallelism=3)  # å¹¶è¡Œåº¦3
#             .sink(KafkaTestSink, parallelism=2)       # å¹¶è¡Œåº¦2
#         )
        
#         print("ğŸ“Š Pipeline: KafkaSource -> map(parallelism=3) -> Sink(parallelism=2)")
#         print("ğŸ¯ Expected: Parallel processing of Kafka messages\n")
        
#         try:
#             env.submit()
#             time.sleep(3)
#         finally:
#             env.close()
        
#         time.sleep(0.5)
#         self._verify_parallel_kafka_results(test_messages)
    
#     def _verify_basic_kafka_results(self, expected_messages):
#         """éªŒè¯åŸºæœ¬Kafkaæµ‹è¯•ç»“æœ"""
#         received_data = KafkaTestSink.read_results()
        
#         print("\nğŸ“‹ Basic Kafka Source Results:")
#         print("=" * 50)
        
#         processed_messages = []
        
#         for instance_id, data_list in received_data.items():
#             print(f"\nğŸ”¹ Parallel Instance {instance_id}:")
            
#             for data in data_list:
#                 if data.get("type") == "processed_kafka_message":
#                     processed_messages.append(data)
#                     original_value = data.get("original_value", {})
#                     kafka_meta = data.get("kafka_metadata", {})
                    
#                     print(f"   - Message: {original_value}")
#                     print(f"     Metadata: partition={kafka_meta.get('partition')}, "
#                           f"offset={kafka_meta.get('offset')}")
        
#         print(f"\nğŸ¯ Kafka Processing Summary:")
#         print(f"   - Expected messages: {len(expected_messages)}")
#         print(f"   - Processed messages: {len(processed_messages)}")
        
#         # éªŒè¯ï¼šåº”è¯¥æœ‰å¤„ç†è¿‡çš„æ¶ˆæ¯
#         assert len(processed_messages) > 0, "âŒ No processed Kafka messages received"
        
#         # éªŒè¯ï¼šæ¶ˆæ¯å†…å®¹åº”è¯¥åŒ¹é…
#         processed_values = [msg.get("original_value", {}) for msg in processed_messages]
#         expected_events = ["user_login", "page_view", "user_logout"]
        
#         for expected_event in expected_events:
#             found = any(
#                 isinstance(pv, dict) and pv.get("event") == expected_event 
#                 for pv in processed_values
#             )
#             assert found, f"âŒ Expected event not found: {expected_event}"
        
#         print("âœ… Basic Kafka Source test passed: Messages processed correctly")
    
#     def _verify_string_kafka_results(self, expected_messages):
#         """éªŒè¯å­—ç¬¦ä¸²Kafkaæµ‹è¯•ç»“æœ"""
#         received_data = KafkaTestSink.read_results()
        
#         print("\nğŸ“‹ String Kafka Source Results:")
#         print("=" * 50)
        
#         processed_messages = []
        
#         for instance_id, data_list in received_data.items():
#             for data in data_list:
#                 if data.get("type") == "processed_kafka_message":
#                     processed_messages.append(data)
#                     original_value = data.get("original_value")
#                     print(f"   - String Message: '{original_value}'")
        
#         print(f"\nğŸ¯ String Processing Summary:")
#         print(f"   - Expected messages: {len(expected_messages)}")
#         print(f"   - Processed messages: {len(processed_messages)}")
        
#         assert len(processed_messages) > 0, "âŒ No string messages received"
        
#         # éªŒè¯å­—ç¬¦ä¸²å†…å®¹
#         processed_values = [msg.get("original_value") for msg in processed_messages]
#         for expected_msg in expected_messages:
#             assert expected_msg.value in processed_values, f"âŒ String message not found: {expected_msg.value}"
        
#         print("âœ… String Kafka Source test passed: String messages processed correctly")
    
#     def _verify_custom_kafka_results(self):
#         """éªŒè¯è‡ªå®šä¹‰ååºåˆ—åŒ–æµ‹è¯•ç»“æœ"""
#         received_data = KafkaTestSink.read_results()
        
#         print("\nğŸ“‹ Custom Deserializer Kafka Results:")
#         print("=" * 50)
        
#         processed_messages = []
        
#         for instance_id, data_list in received_data.items():
#             for data in data_list:
#                 if data and data.get("type") == "processed_kafka_message":
#                     processed_messages.append(data)
#                     original_value = data.get("original_value")
#                     print(f"   - Custom Message: '{original_value}'")
        
#         print(f"\nğŸ¯ Custom Processing Summary:")
#         print(f"   - Processed messages: {len(processed_messages)}")
        
#         assert len(processed_messages) > 0, "âŒ No custom messages received"
        
#         # éªŒè¯è‡ªå®šä¹‰å‰ç¼€
#         for msg in processed_messages:
#             original_value = str(msg.get("original_value", ""))
#             assert original_value.startswith("CUSTOM_"), f"âŒ Custom prefix missing: {original_value}"
        
#         print("âœ… Custom deserializer test passed: Custom processing applied correctly")
    
#     def _verify_parallel_kafka_results(self, expected_messages):
#         """éªŒè¯å¹¶è¡Œå¤„ç†æµ‹è¯•ç»“æœ"""
#         received_data = KafkaTestSink.read_results()
        
#         print("\nğŸ“‹ Parallel Kafka Processing Results:")
#         print("=" * 50)
        
#         processed_messages = []
#         instance_counts = {}
        
#         for instance_id, data_list in received_data.items():
#             instance_counts[instance_id] = len(data_list)
#             print(f"\nğŸ”¹ Sink Instance {instance_id}: {len(data_list)} messages")
            
#             for data in data_list:
#                 if data.get("type") == "processed_kafka_message":
#                     processed_messages.append(data)
        
#         print(f"\nğŸ¯ Parallel Processing Summary:")
#         print(f"   - Expected messages: {len(expected_messages)}")
#         print(f"   - Processed messages: {len(processed_messages)}")
#         print(f"   - Sink instances: {len(instance_counts)}")
#         print(f"   - Distribution: {instance_counts}")
        
#         assert len(processed_messages) > 0, "âŒ No parallel messages received"
#         assert len(instance_counts) > 1, "âŒ Messages not distributed across instances"
        
#         print("âœ… Parallel Kafka processing test passed: Messages distributed correctly")


# class TestKafkaSourceConfiguration:
#     """æµ‹è¯•Kafka Sourceé…ç½®"""
    
#     def setup_method(self):
#         KafkaTestSink.clear_results()
    
#     @patch('kafka.KafkaConsumer')
#     def test_kafka_source_configuration_options(self, mock_kafka_consumer_class):
#         """æµ‹è¯•å„ç§Kafkaé…ç½®é€‰é¡¹"""
#         print("\nğŸš€ Testing Kafka Source Configuration Options")
        
#         mock_consumer = MockKafkaConsumer()
#         mock_consumer.add_messages([
#             MockKafkaMessage(value={"config_test": True}, offset=0)
#         ])
#         mock_kafka_consumer_class.return_value = mock_consumer
        
#         env = LocalEnvironment("config_kafka_test")
        
#         # æµ‹è¯•å®Œæ•´é…ç½®
#         kafka_stream = env.from_kafka_source(
#             bootstrap_servers="kafka-cluster:9092,kafka-cluster:9093",
#             topic="config_topic",
#             group_id="config_group",
#             auto_offset_reset="earliest",
#             value_deserializer="json",
#             buffer_size=5000,
#             max_poll_records=100,
#             session_timeout_ms=30000,
#             security_protocol="SASL_SSL"
#         )
        
#         result_stream = kafka_stream.sink(KafkaTestSink, parallelism=1)
        
#         print("ğŸ“Š Pipeline: KafkaSource(full_config) -> Sink")
#         print("ğŸ¯ Expected: Verify configuration parameters\n")
        
#         try:
#             env.submit()
#             time.sleep(1.5)
#         finally:
#             env.close()
        
#         # éªŒè¯KafkaConsumerè¢«æ­£ç¡®è°ƒç”¨
#         mock_kafka_consumer_class.assert_called_once()
#         call_args = mock_kafka_consumer_class.call_args
        
#         # éªŒè¯é…ç½®å‚æ•°
#         assert call_args[0] == ("config_topic",)
#         config = call_args[1]
#         assert config['bootstrap_servers'] == "kafka-cluster:9092,kafka-cluster:9093"
#         assert config['group_id'] == "config_group"
#         assert config['auto_offset_reset'] == "earliest"
#         assert config['max_poll_records'] == 100
#         assert config['session_timeout_ms'] == 30000
#         assert config['security_protocol'] == "SASL_SSL"
        
#         print("âœ… Kafka configuration test passed: All parameters configured correctly")


# if __name__ == "__main__":
#     # å¯ä»¥ç›´æ¥è¿è¡Œå•ä¸ªæµ‹è¯•
#     test = TestKafkaSourceFunctionality()
#     test.setup_method()
#     test.test_basic_kafka_source_pipeline()

# '''
# ç”¨æ³•ç¤ºä¾‹:

# # è¿è¡Œæ‰€æœ‰Kafkaæµ‹è¯•
# pytest sage_tests/core_tests/kafka_test.py -v -s

# # è¿è¡Œç‰¹å®šæµ‹è¯•
# pytest sage_tests/core_tests/kafka_test.py::TestKafkaSourceFunctionality::test_basic_kafka_source_pipeline -v -s
# pytest sage_tests/core_tests/kafka_test.py::TestKafkaSourceFunctionality::test_kafka_source_parallel_processing -v -s
# pytest sage_tests/core_tests/kafka_test.py::TestKafkaSourceConfiguration::test_kafka_source_configuration_options -v -s

# # ç›´æ¥è¿è¡Œæ–‡ä»¶è¿›è¡Œå¿«é€Ÿæµ‹è¯•
# python sage_tests/core_tests/kafka_test.py
# '''