import pytest
import time
import threading
from typing import List, Dict, Any
from core.api.local_environment import LocalEnvironment
from core.function.source_function import SourceFunction
from core.function.keyby_function import KeyByFunction
from core.function.comap_function import BaseCoMapFunction
from core.function.sink_function import SinkFunction


class UserDataSource(SourceFunction):
    """ç”Ÿæˆç”¨æˆ·æ•°æ®"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.counter = 0
        self.users = [
            {"id": 1, "user_id": "user1", "name": "Alice", "type": "user"},
            {"id": 2, "user_id": "user2", "name": "Bob", "type": "user"},
            {"id": 3, "user_id": "user3", "name": "Charlie", "type": "user"},
            {"id": 4, "user_id": "user1", "name": "Alice Updated", "type": "user"},
        ]
    
    def execute(self):
        if self.counter >= len(self.users):
            return None
        
        data = self.users[self.counter]
        self.counter += 1
        self.logger.info(f"UserSource generated: {data}")
        return data


class EventDataSource(SourceFunction):
    """ç”Ÿæˆäº‹ä»¶æ•°æ®"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.counter = 0
        self.events = [
            {"id": 101, "user_id": "user1", "event": "login", "session_id": "sess1", "type": "event"},
            {"id": 102, "user_id": "user2", "event": "click", "session_id": "sess2", "type": "event"},
            {"id": 103, "user_id": "user3", "event": "purchase", "session_id": "sess3", "type": "event"},
            {"id": 104, "user_id": "user1", "event": "logout", "session_id": "sess1", "type": "event"},
        ]
    
    def execute(self):
        if self.counter >= len(self.events):
            return None
        
        data = self.events[self.counter]
        self.counter += 1
        self.logger.info(f"EventSource generated: {data}")
        return data


class UserIdKeyExtractor(KeyByFunction):
    """æå–ç”¨æˆ·IDä½œä¸ºåˆ†åŒºé”®"""
    
    def execute(self, data: Any) -> str:
        user_id = data["user_id"]
        self.logger.info(f"UserIdExtractor: key '{user_id}' from {data.get('type', 'unknown')} data")
        return user_id


class SessionIdKeyExtractor(KeyByFunction):
    """æå–ä¼šè¯IDä½œä¸ºåˆ†åŒºé”®"""
    
    def execute(self, data: Any) -> str:
        # å¯¹äºç”¨æˆ·æ•°æ®ï¼Œä½¿ç”¨user_idä½œä¸ºsession keyï¼ˆæ¨¡æ‹Ÿåœºæ™¯ï¼‰
        if data.get("type") == "user":
            session_key = f"user_session_{data['user_id']}"
        else:
            session_key = data.get("session_id", "default_session")
        
        self.logger.info(f"SessionIdExtractor: key '{session_key}' from {data.get('type', 'unknown')} data")
        return session_key


class ConnectedDebugSink(SinkFunction):
    """è°ƒè¯•ç”¨çš„Sinkï¼Œè®°å½•æ¥æ”¶åˆ°çš„è¿æ¥æµæ•°æ®åˆ†å¸ƒ"""
    
    # ç±»çº§åˆ«çš„ç»Ÿè®¡
    _received_data: Dict[int, List[Dict]] = {}
    _lock = threading.Lock()
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.parallel_index = None
        self.received_count = 0

    def execute(self, data: Any):
        if self.ctx:
            self.parallel_index = self.ctx.parallel_index

        with self._lock:
            if self.parallel_index not in self._received_data:
                self._received_data[self.parallel_index] = []

            self._received_data[self.parallel_index].append(data)

        self.received_count += 1

        data_type = data.get('type', 'unknown')
        key_info = data.get('user_id', 'no_key')

        self.logger.info(
            f"[Instance {self.parallel_index}] "
            f"Received {data_type} data #{self.received_count}: {data}"
        )

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” [Instance {self.parallel_index}] Type: {data_type}, "
              f"Key: {key_info}, Data: {data}")

        return data
    
    @classmethod
    def get_received_data(cls) -> Dict[int, List[Dict]]:
        with cls._lock:
            return dict(cls._received_data)
    
    @classmethod
    def clear_data(cls):
        with cls._lock:
            cls._received_data.clear()


class JoinCoMapFunction(BaseCoMapFunction):
    """ç¤ºä¾‹CoMapå‡½æ•°ï¼Œç”¨äºè¿æ¥ç”¨æˆ·å’Œäº‹ä»¶æ•°æ®"""
    is_comap = True
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.user_cache = {}  # ç®€å•çš„ç”¨æˆ·æ•°æ®ç¼“å­˜
    
    def map0(self, user_data):
        """å¤„ç†ç”¨æˆ·æ•°æ®æµ (stream 0)"""
        user_id = user_data["user_id"]
        self.user_cache[user_id] = user_data
        
        result = {
            "type": "user_update",
            "user_id": user_id,
            "user_info": user_data,
            "source_stream": 0
        }
        
        self.logger.info(f"CoMap map0: processed user {user_id}")
        return result
    
    def map1(self, event_data):
        """å¤„ç†äº‹ä»¶æ•°æ®æµ (stream 1)"""
        user_id = event_data["user_id"]
        user_info = self.user_cache.get(user_id, {"name": "Unknown"})
        
        result = {
            "type": "enriched_event",
            "user_id": user_id,
            "event_info": event_data,
            "user_info": user_info,
            "source_stream": 1
        }
        
        self.logger.info(f"CoMap map1: enriched event for user {user_id}")
        return result


class TestConnectedStreamsKeyBy:
    """æµ‹è¯•ConnectedStreamsçš„KeyByåŠŸèƒ½"""
    
    def setup_method(self):
        ConnectedDebugSink.clear_data()
    
    def test_unified_keyby(self):
        """æµ‹è¯•ç»Ÿä¸€çš„KeyBy - ä¸¤ä¸ªæµä½¿ç”¨ç›¸åŒçš„key selector"""
        print("\nğŸš€ Testing Connected Streams Unified KeyBy")
        
        env = LocalEnvironment("connected_unified_keyby_test")
        
        # åˆ›å»ºä¸¤ä¸ªæ•°æ®æº
        user_stream = env.from_source(UserDataSource, delay=0.3)
        event_stream = env.from_source(EventDataSource, delay=0.4)
        
        # è¿æ¥æµå¹¶åº”ç”¨ç»Ÿä¸€çš„keyby
        result_stream = (
            user_stream
            .connect(event_stream)
            .keyby(UserIdKeyExtractor)  # ä¸¤ä¸ªæµéƒ½ä½¿ç”¨UserIdKeyExtractor
            .map(lambda x: x)  # é€æ˜ä¼ é€’
            .sink(ConnectedDebugSink, parallelism=2)
        )
        
        print("ğŸ“Š Pipeline: UserStream + EventStream -> ConnectedStreams.keyby(UserIdExtractor) -> Sink(parallelism=2)")
        print("ğŸ¯ Expected: Data with same user_id should go to same parallel instance\n")
        
        try:
            env.submit()
            
            time.sleep(3)
        finally:
            env.close()
        
        self._verify_unified_keyby_partitioning()
    
    def test_per_stream_keyby(self):
        """æµ‹è¯•Flinké£æ ¼çš„per-stream KeyBy - æ¯ä¸ªæµä½¿ç”¨ä¸åŒçš„key selector"""
        print("\nğŸš€ Testing Connected Streams Per-Stream KeyBy (Flink-style)")
        
        env = LocalEnvironment("connected_per_stream_keyby_test")
        
        user_stream = env.from_source(UserDataSource, delay=0.3)
        event_stream = env.from_source(EventDataSource, delay=0.4)
        
        # è¿æ¥æµå¹¶åº”ç”¨ä¸åŒçš„keybyç­–ç•¥
        result_stream = (
            user_stream
            .connect(event_stream)
            .keyby([UserIdKeyExtractor, SessionIdKeyExtractor])  # æ¯ä¸ªæµä¸åŒçš„extractor
            .map(lambda x: x)  # é€æ˜ä¼ é€’
            .sink(ConnectedDebugSink, parallelism=3)
        )
        
        print("ğŸ“Š Pipeline: UserStream + EventStream -> ConnectedStreams.keyby([UserIdExtractor, SessionIdExtractor]) -> Sink(parallelism=3)")
        print("ğŸ¯ Expected: Stream 0 by user_id, Stream 1 by session_id\n")
        
        try:
            env.submit()
            
            time.sleep(3)
        finally:
            env.close()
        
        self._verify_per_stream_keyby_partitioning()
    
    def test_keyby_with_comap(self):
        """æµ‹è¯•KeyByåæ¥CoMapæ“ä½œ"""
        print("\nğŸš€ Testing Connected Streams KeyBy + CoMap")
        
        env = LocalEnvironment("connected_keyby_comap_test")
        
        user_stream = env.from_source(UserDataSource, delay=0.3)
        event_stream = env.from_source(EventDataSource, delay=0.4)
        
        # KeyByåè¿›è¡ŒCoMap joinæ“ä½œ
        result_stream = (
            user_stream
            .connect(event_stream)
            .keyby(UserIdKeyExtractor)  # ç»Ÿä¸€ä½¿ç”¨user_idä½œä¸ºkey
            .comap(JoinCoMapFunction)   # è¿›è¡Œæ•°æ®join
            .sink(ConnectedDebugSink, parallelism=2)
        )
        
        print("ğŸ“Š Pipeline: UserStream + EventStream -> keyby(UserIdExtractor) -> comap(JoinCoMapFunction) -> Sink")
        print("ğŸ¯ Expected: Same user_id data co-located for join operation\n")
        
        try:
            env.submit()
            
            time.sleep(4)  # ç»™æ›´å¤šæ—¶é—´è®©joinæ“ä½œå®Œæˆ
        finally:
            env.close()
        
        self._verify_keyby_comap_results()
    
    def test_invalid_keyby_configurations(self):
        """æµ‹è¯•æ— æ•ˆçš„KeyByé…ç½®"""
        print("\nğŸš€ Testing Invalid KeyBy Configurations")
        
        env = LocalEnvironment("invalid_keyby_test")
        
        user_stream = env.from_source(UserDataSource, delay=0.5)
        event_stream = env.from_source(EventDataSource, delay=0.5)
        connected = user_stream.connect(event_stream)
        
        # æµ‹è¯•1ï¼škey selectoræ•°é‡ä¸åŒ¹é…
        with pytest.raises(ValueError, match="Key selector count .* must match stream count"):
            connected.keyby([UserIdKeyExtractor])  # åªæœ‰1ä¸ªselectorï¼Œä½†æœ‰2ä¸ªstream
        
        # æµ‹è¯•2ï¼šLambdaå‡½æ•°ä¸æ”¯æŒ
        with pytest.raises(NotImplementedError, match="Lambda functions are not supported"):
            connected.keyby(lambda x: x["user_id"])
        
        print("âœ… Invalid configuration tests passed")
        
        env.close()
    
    def _verify_unified_keyby_partitioning(self):
        """éªŒè¯ç»Ÿä¸€KeyByçš„åˆ†åŒºæ•ˆæœ"""
        received_data = ConnectedDebugSink.get_received_data()
        
        print("\nğŸ“‹ Unified KeyBy Partitioning Results:")
        print("=" * 60)
        
        user_distribution = {}
        
        for instance_id, data_list in received_data.items():
            print(f"\nğŸ”¹ Parallel Instance {instance_id}:")
            
            for data in data_list:
                user_id = data["user_id"]
                data_type = data["type"]
                
                if user_id not in user_distribution:
                    user_distribution[user_id] = set()
                user_distribution[user_id].add(instance_id)
                
                print(f"   - User {user_id} ({data_type}): {data.get('name', data.get('event', 'N/A'))}")
        
        print(f"\nğŸ¯ User Distribution Across Instances:")
        for user_id, instances in user_distribution.items():
            print(f"   - {user_id}: routed to instance(s) {instances}")
        
        # éªŒè¯ï¼šæ¯ä¸ªç”¨æˆ·çš„æ‰€æœ‰æ•°æ®ï¼ˆæ— è®ºæ¥è‡ªå“ªä¸ªæµï¼‰éƒ½åº”è¯¥è·¯ç”±åˆ°åŒä¸€ä¸ªå®ä¾‹
        for user_id, instances in user_distribution.items():
            assert len(instances) == 1, (
                f"âŒ User {user_id} data was routed to multiple instances: {instances}. "
                f"Unified keyby should send same key to same instance."
            )
        
        print("âœ… Unified keyby test passed: Each user's data from both streams routed to same instance")
    
    def _verify_per_stream_keyby_partitioning(self):
        """éªŒè¯per-stream KeyByçš„åˆ†åŒºæ•ˆæœ"""
        received_data = ConnectedDebugSink.get_received_data()
        
        print("\nğŸ“‹ Per-Stream KeyBy Partitioning Results:")
        print("=" * 60)
        
        stream0_key_distribution = {}  # user_id distribution
        stream1_key_distribution = {}  # session_id distribution
        
        for instance_id, data_list in received_data.items():
            print(f"\nğŸ”¹ Parallel Instance {instance_id}:")
            
            for data in data_list:
                data_type = data["type"]
                
                if data_type == "user":
                    # Stream 0: æŒ‰user_idåˆ†åŒº
                    key = data["user_id"]
                    if key not in stream0_key_distribution:
                        stream0_key_distribution[key] = set()
                    stream0_key_distribution[key].add(instance_id)
                    print(f"   - Stream0 key '{key}': {data['name']}")
                    
                elif data_type == "event":
                    # Stream 1: æŒ‰session_idåˆ†åŒº
                    key = data.get("session_id", "unknown")
                    if key not in stream1_key_distribution:
                        stream1_key_distribution[key] = set()
                    stream1_key_distribution[key].add(instance_id)
                    print(f"   - Stream1 key '{key}': {data['event']}")
        
        print(f"\nğŸ¯ Stream 0 (User) Key Distribution:")
        for key, instances in stream0_key_distribution.items():
            print(f"   - User {key}: routed to instance(s) {instances}")
        
        print(f"\nğŸ¯ Stream 1 (Event) Key Distribution:")
        for key, instances in stream1_key_distribution.items():
            print(f"   - Session {key}: routed to instance(s) {instances}")
        
        # éªŒè¯ï¼šæ¯ä¸ªæµçš„ç›¸åŒkeyåº”è¯¥è·¯ç”±åˆ°ç›¸åŒå®ä¾‹
        for key, instances in stream0_key_distribution.items():
            assert len(instances) == 1, f"âŒ Stream0 key {key} routed to multiple instances: {instances}"
        
        for key, instances in stream1_key_distribution.items():
            assert len(instances) == 1, f"âŒ Stream1 key {key} routed to multiple instances: {instances}"
        
        print("âœ… Per-stream keyby test passed: Each stream's keys correctly partitioned")
    
    def _verify_keyby_comap_results(self):
        """éªŒè¯KeyBy + CoMapçš„ç»“æœ"""
        received_data = ConnectedDebugSink.get_received_data()
        
        print("\nğŸ“‹ KeyBy + CoMap Results:")
        print("=" * 60)
        
        user_updates = []
        enriched_events = []
        
        for instance_id, data_list in received_data.items():
            print(f"\nğŸ”¹ Parallel Instance {instance_id}:")
            
            for data in data_list:
                result_type = data.get("type", "unknown")
                user_id = data.get("user_id", "unknown")
                source_stream = data.get("source_stream", -1)
                
                if result_type == "user_update":
                    user_updates.append(data)
                    print(f"   - User Update: {user_id} from stream {source_stream}")
                elif result_type == "enriched_event":
                    enriched_events.append(data)
                    event_name = data.get("event_info", {}).get("event", "unknown")
                    print(f"   - Enriched Event: {user_id} {event_name} from stream {source_stream}")
        
        print(f"\nğŸ¯ CoMap Results Summary:")
        print(f"   - User updates: {len(user_updates)}")
        print(f"   - Enriched events: {len(enriched_events)}")
        
        # éªŒè¯ï¼šåº”è¯¥æœ‰ç”¨æˆ·æ›´æ–°å’Œä¸°å¯Œçš„äº‹ä»¶
        assert len(user_updates) > 0, "âŒ No user updates received from CoMap"
        assert len(enriched_events) > 0, "âŒ No enriched events received from CoMap"
        
        print("âœ… KeyBy + CoMap test passed: Both user updates and enriched events received")


if __name__ == "__main__":
    # å¯ä»¥ç›´æ¥è¿è¡Œå•ä¸ªæµ‹è¯•
    test = TestConnectedStreamsKeyBy()
    test.setup_method()
    test.test_unified_keyby()

'''
# è¿è¡Œæ‰€æœ‰Connected KeyByæµ‹è¯•
pytest sage_tests/operator_tests/connected_keyby_test.py -v -s

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest sage_tests/operator_tests/connected_keyby_test.py::TestConnectedStreamsKeyBy::test_unified_keyby -v -s
pytest sage_tests/operator_tests/connected_keyby_test.py::TestConnectedStreamsKeyBy::test_per_stream_keyby -v -s
pytest sage_tests/operator_tests/connected_keyby_test.py::TestConnectedStreamsKeyBy::test_keyby_with_comap -v -s
'''