import pytest
import time
import threading
import json
from typing import Any, Dict, List
from pathlib import Path
import tempfile
from unittest.mock import Mock, patch, MagicMock

from sage_core.api.local_environment import LocalEnvironment
from sage_core.function.kafka_source import KafkaSourceFunction
from sage_core.function.base_function import BaseFunction
from sage_core.function.sink_function import SinkFunction


class MockKafkaMessage:
    """æ¨¡æ‹ŸKafkaæ¶ˆæ¯"""
    def __init__(self, value, key=None, timestamp=None, partition=0, offset=0):
        self.value = value
        self.key = key if key is not None else f"key_{offset}".encode('utf-8')
        self.timestamp = timestamp if timestamp is not None else int(time.time() * 1000)
        self.partition = partition
        self.offset = offset


class MockKafkaConsumer:
    """æ¨¡æ‹ŸKafkaConsumer"""
    def __init__(self, *topics, **kwargs):
        self.topics = topics
        self.config = kwargs
        self.messages = []
        self.poll_count = 0
        self._closed = False
        
    def poll(self, timeout_ms=1000, max_records=500, update_offsets=True):
        """æ¨¡æ‹Ÿpollæ–¹æ³•"""
        if self._closed or self.poll_count >= 3:  # é™åˆ¶pollæ¬¡æ•°é¿å…æ— é™å¾ªç¯
            return {}
        
        self.poll_count += 1
        
        if self.messages:
            # è¿”å›ä¸€æ‰¹æ¶ˆæ¯
            topic_partition = Mock()
            messages = self.messages[:max_records]
            self.messages = self.messages[max_records:]
            return {topic_partition: messages}
        
        return {}
    
    def close(self):
        """å…³é—­æ¶ˆè´¹è€…"""
        self._closed = True
    
    def add_messages(self, messages):
        """æ·»åŠ æ¨¡æ‹Ÿæ¶ˆæ¯"""
        self.messages.extend(messages)


class KafkaDebugProcessor(BaseFunction):
    """å¤„ç†Kafkaæ¶ˆæ¯çš„è°ƒè¯•å‡½æ•°"""
    
    def __init__(self, ctx=None, **kwargs):
        super().__init__(ctx=ctx, **kwargs)
        self.processed_count = 0
    
    def execute(self, data: Any):
        if data is None:
            return None
        
        self.processed_count += 1
        
        # æå–Kafkaæ¶ˆæ¯ä¿¡æ¯
        value = data.get('value', {}) if isinstance(data, dict) else data
        timestamp = data.get('timestamp', 0) if isinstance(data, dict) else 0
        partition = data.get('partition', 0) if isinstance(data, dict) else 0
        offset = data.get('offset', 0) if isinstance(data, dict) else 0
        
        result = {
            "type": "processed_kafka_message",
            "processed_count": self.processed_count,
            "original_value": value,
            "kafka_metadata": {
                "timestamp": timestamp,
                "partition": partition,
                "offset": offset
            }
        }
        
        if self.ctx:
            self.logger.info(f"Processed Kafka message #{self.processed_count}: {value}")
        
        return result


class KafkaTestSink(SinkFunction):
    """Kafkaæµ‹è¯•ç»“æœæ”¶é›†å™¨"""
    
    _lock = threading.Lock()
    _received_data = {}
    
    def __init__(self, ctx=None, **kwargs):
        super().__init__(ctx=ctx, **kwargs)
        self.parallel_index = None
        self.received_count = 0
    
    def execute(self, data: Any):
        if self.ctx:
            self.parallel_index = self.ctx.parallel_index
        
        if data is None:
            return None
        
        with self._lock:
            if self.parallel_index not in self._received_data:
                self._received_data[self.parallel_index] = []
            self._received_data[self.parallel_index].append(data)
        
        self.received_count += 1
        
        if self.ctx:
            self.logger.info(f"[Instance {self.parallel_index}] Received: {data}")
        
        print(f"ğŸ” [Instance {self.parallel_index}] Kafka Test Sink received: {data}")
        
        return data
    
    @classmethod
    def read_results(cls):
        """è¯»å–æµ‹è¯•ç»“æœ"""
        with cls._lock:
            results = dict(cls._received_data)
        return results
    
    @classmethod
    def clear_results(cls):
        """æ¸…ç†ç»“æœ"""
        with cls._lock:
            cls._received_data.clear()


class TestKafkaSourceFunctionality:
    """æµ‹è¯•Kafka SourceåŠŸèƒ½"""
    
    def setup_method(self):
        """æµ‹è¯•å‰æ¸…ç†"""
        KafkaTestSink.clear_results()
    
    @patch('kafka.KafkaConsumer')
    def test_basic_kafka_source_pipeline(self, mock_kafka_consumer_class):
        """æµ‹è¯•åŸºæœ¬çš„Kafka Sourceæµæ°´çº¿"""
        print("\nğŸš€ Testing Basic Kafka Source Pipeline")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ¶ˆæ¯
        test_messages = [
            MockKafkaMessage(
                value={"event": "user_login", "user_id": "user1", "timestamp": 1640995200},
                offset=0
            ),
            MockKafkaMessage(
                value={"event": "page_view", "user_id": "user1", "page": "/home"},
                offset=1
            ),
            MockKafkaMessage(
                value={"event": "user_logout", "user_id": "user1"},
                offset=2
            ),
        ]
        
        # è®¾ç½®mock
        mock_consumer = MockKafkaConsumer()
        mock_consumer.add_messages(test_messages)
        mock_kafka_consumer_class.return_value = mock_consumer
        
        # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
        env = LocalEnvironment("basic_kafka_test")
        
        # æ„å»ºæµæ°´çº¿
        kafka_stream = env.from_kafka_source(
            bootstrap_servers="localhost:9092",
            topic="test_topic",
            group_id="test_group",
            value_deserializer="json",
            buffer_size=100
        )
        
        result_stream = (
            kafka_stream
            .map(KafkaDebugProcessor)
            .sink(KafkaTestSink, parallelism=1)
        )
        
        print("ğŸ“Š Pipeline: KafkaSource -> map(KafkaDebugProcessor) -> Sink")
        print("ğŸ¯ Expected: Process JSON messages from Kafka\n")
        
        try:
            env.submit()
            time.sleep(2)  # ç­‰å¾…å¤„ç†æ¶ˆæ¯
        finally:
            env.close()
        
        time.sleep(0.5)
        self._verify_basic_kafka_results(test_messages)
    
    @patch('kafka.KafkaConsumer')
    def test_kafka_source_with_string_deserializer(self, mock_kafka_consumer_class):
        """æµ‹è¯•å­—ç¬¦ä¸²ååºåˆ—åŒ–çš„Kafka Source"""
        print("\nğŸš€ Testing Kafka Source with String Deserializer")
        
        # åˆ›å»ºå­—ç¬¦ä¸²æ¶ˆæ¯
        test_messages = [
            MockKafkaMessage(value="Hello Kafka", offset=0),
            MockKafkaMessage(value="String message 1", offset=1),
            MockKafkaMessage(value="String message 2", offset=2),
        ]
        
        mock_consumer = MockKafkaConsumer()
        mock_consumer.add_messages(test_messages)
        mock_kafka_consumer_class.return_value = mock_consumer
        
        env = LocalEnvironment("string_kafka_test")
        
        kafka_stream = env.from_kafka_source(
            bootstrap_servers="localhost:9092",
            topic="string_topic",
            group_id="string_group",
            value_deserializer="string",
            buffer_size=50
        )
        
        result_stream = (
            kafka_stream
            .map(KafkaDebugProcessor)
            .sink(KafkaTestSink, parallelism=1)
        )
        
        print("ğŸ“Š Pipeline: KafkaSource(string) -> map(KafkaDebugProcessor) -> Sink")
        print("ğŸ¯ Expected: Process string messages from Kafka\n")
        
        try:
            env.submit()
            time.sleep(2)
        finally:
            env.close()
        
        time.sleep(0.5)
        self._verify_string_kafka_results(test_messages)
    
    @patch('kafka.KafkaConsumer')
    def test_kafka_source_custom_deserializer(self, mock_kafka_consumer_class):
        """æµ‹è¯•è‡ªå®šä¹‰ååºåˆ—åŒ–å™¨çš„Kafka Source"""
        print("\nğŸš€ Testing Kafka Source with Custom Deserializer")
        
        def custom_deserializer(raw_data):
            """è‡ªå®šä¹‰ååºåˆ—åŒ–ï¼šæ·»åŠ å‰ç¼€"""
            if isinstance(raw_data, bytes):
                data = raw_data.decode('utf-8')
            else:
                data = str(raw_data)
            return f"CUSTOM_{data}"
        
        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
        test_messages = [
            MockKafkaMessage(value=b"message1", offset=0),
            MockKafkaMessage(value=b"message2", offset=1),
        ]
        
        mock_consumer = MockKafkaConsumer()
        mock_consumer.add_messages(test_messages)
        mock_kafka_consumer_class.return_value = mock_consumer
        
        env = LocalEnvironment("custom_deserializer_kafka_test")
        
        kafka_stream = env.from_kafka_source(
            bootstrap_servers="localhost:9092",
            topic="custom_topic",
            group_id="custom_group",
            value_deserializer=custom_deserializer,
            buffer_size=50
        )
        
        result_stream = (
            kafka_stream
            .map(KafkaDebugProcessor)
            .sink(KafkaTestSink, parallelism=1)
        )
        
        print("ğŸ“Š Pipeline: KafkaSource(custom) -> map(KafkaDebugProcessor) -> Sink")
        print("ğŸ¯ Expected: Process custom deserialized messages\n")
        
        try:
            env.submit()
            time.sleep(2)
        finally:
            env.close()
        
        time.sleep(0.5)
        self._verify_custom_kafka_results()
    
    @patch('kafka.KafkaConsumer')
    def test_kafka_source_parallel_processing(self, mock_kafka_consumer_class):
        """æµ‹è¯•Kafka Sourceå¹¶è¡Œå¤„ç†"""
        print("\nğŸš€ Testing Kafka Source Parallel Processing")
        
        # åˆ›å»ºæ›´å¤šæ¶ˆæ¯ç”¨äºå¹¶è¡Œå¤„ç†
        test_messages = []
        for i in range(10):
            test_messages.append(
                MockKafkaMessage(
                    value={"batch": "parallel_test", "message_id": i, "data": f"data_{i}"},
                    offset=i
                )
            )
        
        mock_consumer = MockKafkaConsumer()
        mock_consumer.add_messages(test_messages)
        mock_kafka_consumer_class.return_value = mock_consumer
        
        env = LocalEnvironment("parallel_kafka_test")
        
        kafka_stream = env.from_kafka_source(
            bootstrap_servers="localhost:9092",
            topic="parallel_topic",
            group_id="parallel_group",
            value_deserializer="json"
        )
        
        result_stream = (
            kafka_stream
            .map(KafkaDebugProcessor, parallelism=3)  # å¹¶è¡Œåº¦3
            .sink(KafkaTestSink, parallelism=2)       # å¹¶è¡Œåº¦2
        )
        
        print("ğŸ“Š Pipeline: KafkaSource -> map(parallelism=3) -> Sink(parallelism=2)")
        print("ğŸ¯ Expected: Parallel processing of Kafka messages\n")
        
        try:
            env.submit()
            time.sleep(3)
        finally:
            env.close()
        
        time.sleep(0.5)
        self._verify_parallel_kafka_results(test_messages)
    
    def _verify_basic_kafka_results(self, expected_messages):
        """éªŒè¯åŸºæœ¬Kafkaæµ‹è¯•ç»“æœ"""
        received_data = KafkaTestSink.read_results()
        
        print("\nğŸ“‹ Basic Kafka Source Results:")
        print("=" * 50)
        
        processed_messages = []
        
        for instance_id, data_list in received_data.items():
            print(f"\nğŸ”¹ Parallel Instance {instance_id}:")
            
            for data in data_list:
                if data.get("type") == "processed_kafka_message":
                    processed_messages.append(data)
                    original_value = data.get("original_value", {})
                    kafka_meta = data.get("kafka_metadata", {})
                    
                    print(f"   - Message: {original_value}")
                    print(f"     Metadata: partition={kafka_meta.get('partition')}, "
                          f"offset={kafka_meta.get('offset')}")
        
        print(f"\nğŸ¯ Kafka Processing Summary:")
        print(f"   - Expected messages: {len(expected_messages)}")
        print(f"   - Processed messages: {len(processed_messages)}")
        
        # éªŒè¯ï¼šåº”è¯¥æœ‰å¤„ç†è¿‡çš„æ¶ˆæ¯
        assert len(processed_messages) > 0, "âŒ No processed Kafka messages received"
        
        # éªŒè¯ï¼šæ¶ˆæ¯å†…å®¹åº”è¯¥åŒ¹é…
        processed_values = [msg.get("original_value", {}) for msg in processed_messages]
        for expected_msg in expected_messages:
            expected_value = expected_msg.value
            assert any(
                pv.get("event") == expected_value.get("event") for pv in processed_values
            ), f"âŒ Expected message not found: {expected_value}"
        
        print("âœ… Basic Kafka Source test passed: Messages processed correctly")
    
    def _verify_string_kafka_results(self, expected_messages):
        """éªŒè¯å­—ç¬¦ä¸²Kafkaæµ‹è¯•ç»“æœ"""
        received_data = KafkaTestSink.read_results()
        
        print("\nğŸ“‹ String Kafka Source Results:")
        print("=" * 50)
        
        processed_messages = []
        
        for instance_id, data_list in received_data.items():
            for data in data_list:
                if data.get("type") == "processed_kafka_message":
                    processed_messages.append(data)
                    original_value = data.get("original_value")
                    print(f"   - String Message: '{original_value}'")
        
        print(f"\nğŸ¯ String Processing Summary:")
        print(f"   - Expected messages: {len(expected_messages)}")
        print(f"   - Processed messages: {len(processed_messages)}")
        
        assert len(processed_messages) > 0, "âŒ No string messages received"
        
        # éªŒè¯å­—ç¬¦ä¸²å†…å®¹
        processed_values = [msg.get("original_value") for msg in processed_messages]
        for expected_msg in expected_messages:
            assert expected_msg.value in processed_values, f"âŒ String message not found: {expected_msg.value}"
        
        print("âœ… String Kafka Source test passed: String messages processed correctly")
    
    def _verify_custom_kafka_results(self):
        """éªŒè¯è‡ªå®šä¹‰ååºåˆ—åŒ–æµ‹è¯•ç»“æœ"""
        received_data = KafkaTestSink.read_results()
        
        print("\nğŸ“‹ Custom Deserializer Kafka Results:")
        print("=" * 50)
        
        processed_messages = []
        
        for instance_id, data_list in received_data.items():
            for data in data_list:
                if data.get("type") == "processed_kafka_message":
                    processed_messages.append(data)
                    original_value = data.get("original_value")
                    print(f"   - Custom Message: '{original_value}'")
        
        assert len(processed_messages) > 0, "âŒ No custom messages received"
        
        # éªŒè¯è‡ªå®šä¹‰å‰ç¼€
        for msg in processed_messages:
            original_value = str(msg.get("original_value", ""))
            assert original_value.startswith("CUSTOM_"), f"âŒ Custom prefix missing: {original_value}"
        
        print("âœ… Custom deserializer test passed: Custom processing applied correctly")
    
    def _verify_parallel_kafka_results(self, expected_messages):
        """éªŒè¯å¹¶è¡Œå¤„ç†æµ‹è¯•ç»“æœ"""
        received_data = KafkaTestSink.read_results()
        
        print("\nğŸ“‹ Parallel Kafka Processing Results:")
        print("=" * 50)
        
        processed_messages = []
        instance_counts = {}
        
        for instance_id, data_list in received_data.items():
            instance_counts[instance_id] = len(data_list)
            print(f"\nğŸ”¹ Sink Instance {instance_id}: {len(data_list)} messages")
            
            for data in data_list:
                if data.get("type") == "processed_kafka_message":
                    processed_messages.append(data)
        
        print(f"\nğŸ¯ Parallel Processing Summary:")
        print(f"   - Expected messages: {len(expected_messages)}")
        print(f"   - Processed messages: {len(processed_messages)}")
        print(f"   - Sink instances: {len(instance_counts)}")
        print(f"   - Distribution: {instance_counts}")
        
        assert len(processed_messages) > 0, "âŒ No parallel messages received"
        assert len(instance_counts) > 1, "âŒ Messages not distributed across instances"
        
        print("âœ… Parallel Kafka processing test passed: Messages distributed correctly")


class TestKafkaSourceConfiguration:
    """æµ‹è¯•Kafka Sourceé…ç½®"""
    
    def setup_method(self):
        KafkaTestSink.clear_results()
    
    @patch('kafka.KafkaConsumer')
    def test_kafka_source_configuration_options(self, mock_kafka_consumer_class):
        """æµ‹è¯•å„ç§Kafkaé…ç½®é€‰é¡¹"""
        print("\nğŸš€ Testing Kafka Source Configuration Options")
        
        mock_consumer = MockKafkaConsumer()
        mock_consumer.add_messages([
            MockKafkaMessage(value={"config_test": True}, offset=0)
        ])
        mock_kafka_consumer_class.return_value = mock_consumer
        
        env = LocalEnvironment("config_kafka_test")
        
        # æµ‹è¯•å®Œæ•´é…ç½®
        kafka_stream = env.from_kafka_source(
            bootstrap_servers="kafka-cluster:9092,kafka-cluster:9093",
            topic="config_topic",
            group_id="config_group",
            auto_offset_reset="earliest",
            value_deserializer="json",
            buffer_size=5000,
            max_poll_records=100,
            session_timeout_ms=30000,
            security_protocol="SASL_SSL"
        )
        
        result_stream = kafka_stream.sink(KafkaTestSink, parallelism=1)
        
        print("ğŸ“Š Pipeline: KafkaSource(full_config) -> Sink")
        print("ğŸ¯ Expected: Verify configuration parameters\n")
        
        try:
            env.submit()
            time.sleep(1.5)
        finally:
            env.close()
        
        # éªŒè¯KafkaConsumerè¢«æ­£ç¡®è°ƒç”¨
        mock_kafka_consumer_class.assert_called_once()
        call_args = mock_kafka_consumer_class.call_args
        
        # éªŒè¯é…ç½®å‚æ•°
        assert call_args[0] == ("config_topic",)
        config = call_args[1]
        assert config['bootstrap_servers'] == "kafka-cluster:9092,kafka-cluster:9093"
        assert config['group_id'] == "config_group"
        assert config['auto_offset_reset'] == "earliest"
        assert config['max_poll_records'] == 100
        assert config['session_timeout_ms'] == 30000
        assert config['security_protocol'] == "SASL_SSL"
        
        print("âœ… Kafka configuration test passed: All parameters configured correctly")


if __name__ == "__main__":
    # å¯ä»¥ç›´æ¥è¿è¡Œå•ä¸ªæµ‹è¯•
    test = TestKafkaSourceFunctionality()
    test.setup_method()
    test.test_basic_kafka_source_pipeline()

'''
ç”¨æ³•ç¤ºä¾‹:

# è¿è¡Œæ‰€æœ‰Kafkaæµ‹è¯•
pytest sage_tests/core_tests/kafka_test.py -v -s

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest sage_tests/core_tests/kafka_test.py::TestKafkaSourceFunctionality::test_basic_kafka_source_pipeline -v -s
pytest sage_tests/core_tests/kafka_test.py::TestKafkaSourceFunctionality::test_kafka_source_parallel_processing -v -s
pytest sage_tests/core_tests/kafka_test.py::TestKafkaSourceConfiguration::test_kafka_source_configuration_options -v -s

# ç›´æ¥è¿è¡Œæ–‡ä»¶è¿›è¡Œå¿«é€Ÿæµ‹è¯•
python sage_tests/core_tests/kafka_test.py
'''