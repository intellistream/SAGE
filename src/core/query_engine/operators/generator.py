import logging
import torch
import json
from transformers import AutoTokenizer, LlamaForCausalLM
from src.core.query_engine.operators.base_operator import BaseOperator
from src.core.query_engine.operators.web_workflow.workflow import WebWorkFlow

class Generator(BaseOperator):
    """
    Operator for generating natural language responses using Hugging Face's Transformers.
    """

    def __init__(self, model_name="meta-llama/Meta-Llama-3-8B-Instruct", device=None, seed=42):
        """
        Initialize the generator with a specified model.
        :param model_name: The Hugging Face model to use for generation.
        :param device: The device to load the model on ("cpu" for GitHub workflows).
        :param seed: Seed for reproducibility.
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.info(f"Initializing Generator with model: {model_name}...")

        # Set random seed for reproducibility
        torch.manual_seed(seed)

        # Automatically detect device if not provided
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device
        self.logger.info(f"Selected device: {self.device}")

        # Load tokenizer and model
        self.logger.info(f"Loading model and tokenizer for {model_name}...")
        self.llm = LlamaForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)
        self.logger.info("Model and tokenizer loaded successfully.")

    def execute(self, input_data, **kwargs):
        """
        Generate a response using the model.
        :param input_data: Preformatted QA-template input string.
        :param kwargs: Additional parameters for generation.
        :return: Generated response.
        """
        try:
            # Log the received input
            prompt = input_data[0]
            self.logger.info(f"Generating response for input data:\n{prompt}")

            inputs = self.tokenizer(prompt, return_tensors="pt")
            outputs = self.llm.generate(
                **inputs,
                max_new_tokens=50,  # Maximum number of tokens to generate per output sequence.
                do_sample=True,
                top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.
                temperature=0.1,  # Randomness of the sampling
                num_return_sequences=1  # Number of output sequences to return for each prompt.
            )
            response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # print(f"Generated response: {response_text}")

            entities = self._extract_entities(response_text)
            entity = entities[0]
            self.logger.info(f"Generated response from KG workflow: {entity}")
 
            answer = entity["answer"]
            confidence = entity["confidence"]
            if answer == "I don't know" or confidence != "high":
                # activate Web Workflow
                self.logger.info("Activate Web Workflow:")
                memory = kwargs.get("memory_layers")
                webflow = WebWorkFlow()
                webflow.execute(memory)
            else:
                self.logger.info({"Generated answer from KG Workflow:" + "answer": answer, "confidence": confidence})
               
        except Exception as e:
            self.logger.error(f"Error during response generation: {str(e)}")
            raise RuntimeError(f"Response generation failed: {str(e)}")
        
    def _extract_entities(self, generated_output, decoder=json.JSONDecoder()):
        """
        Extract the JSON data from the generated output.
        :param generated_output: The full text generated by the model.
        :return: A list of extracted JSON objects or the full output if extraction fails.
        """
        # Extract the text after "assistant"
        if "assistant" in generated_output:
            text = generated_output.split("assistant")[1].strip()
            
        pos = 0
        results = []
        while True:
            match = text.find("{", pos)
            if match == -1:
                break
            try:
                result, index = decoder.raw_decode(text[match:])
                results.append(result)
                pos = match + index
            except ValueError:
                pos = match + 1
        return results