import logging
import torch
import json
from transformers import AutoTokenizer,LlamaForCausalLM

class SAGenerator():
    """
    Operator class for generating natural language responses, including self-assessment of confidence.
    """
    
    def __init__(self, model_name="meta-llama/Meta-Llama-3-8B-Instruct", device=None, seed=42):
        """
        Initialize the generator with a specified model.
        :param model_name: The Hugging Face model to use for generation.
        :param device: The device to load the model on ("cpu" for GitHub workflows).
        :param seed: Seed for reproducibility.
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.info(f"WebWorkflow: Initializing Generator with model: {model_name}...")

        # Set random seed for reproducibility
        torch.manual_seed(seed)

        # Automatically detect device if not provided
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device
        self.logger.info(f"WebWorkflow: Selected device: {self.device}")
        self.logger.info(f"WebWorkflow: Loading model and tokenizer for {model_name}...")
        self.llm = LlamaForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)

        self.logger.info("WebWorkflow: Model and tokenizer loaded successfully.")

    def execute(self, input_data):
        """
        Generate a response using the model.
        :param input_data: Preformatted QA-template input string.
        :return: Generated response.
        """
        try:
            # Log the received input
            # self.logger.info(f"Generating response for input data:\n{input_data}")

            for prompt in input_data:
                inputs = self.tokenizer(prompt, return_tensors="pt")
                outputs = self.llm.generate(
                    **inputs,
                    max_new_tokens=50,  # Maximum number of tokens to generate per output sequence.
                    do_sample=True,
                    top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.
                    temperature=0.1,  # Randomness of the sampling
                    num_return_sequences=1  # Number of output sequences to return for each prompt.
                )
                response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                entities = self._extract_entities(response_text)
                entity = entities[0]
                self.logger.info(f"Web workflow completed successfully. Generate responses from the KG+Web integrated workflow: {entity}")
               
        except Exception as e:
            self.logger.error(f"WebWorkflow: Error during response generation: {str(e)}")
            raise RuntimeError(f"WebWorkflow: Response generation failed: {str(e)}")

    def _extract_entities(self, generated_output, decoder=json.JSONDecoder()):
        """
        Extract the JSON data from the generated output.
        :param generated_output: The full text generated by the model.
        :return: A list of extracted JSON objects or the full output if extraction fails.
        """
        # Extract the text after "assistant"
        if "assistant" in generated_output:
            text = generated_output.split("assistant")[1].strip()
            
        pos = 0
        results = []
        while True:
            match = text.find("{", pos)
            if match == -1:
                break
            try:
                result, index = decoder.raw_decode(text[match:])
                results.append(result)
                pos = match + index
            except ValueError:
                pos = match + 1
        return results